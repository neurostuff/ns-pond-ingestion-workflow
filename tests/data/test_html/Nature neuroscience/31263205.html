<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>Coexisting representations of sensory and mnemonic information in human visual cortex | Nature Neuroscience</title>
    
        
<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/neuro.rss"/>


    
        

        <script id="save-data-connection-testing">
            function hasConnection() {
                return navigator.connection || navigator.mozConnection || navigator.webkitConnection || navigator.msConnection;
            }

            function createLink(src) {
                var preloadLink = document.createElement("link");
                preloadLink.rel = "preload";
                preloadLink.href = src;
                preloadLink.as = "font";
                preloadLink.type = "font/woff2";
                preloadLink.crossOrigin = "";
                document.head.insertBefore(preloadLink, document.head.firstChild);
            }

            var connectionDetail = {
                saveDataEnabled: false,
                slowConnection: false
            };

            var connection = hasConnection();
            if (connection) {
                connectionDetail.saveDataEnabled = connection.saveData;
                if (/\slow-2g|2g/.test(connection.effectiveType)) {
                    connectionDetail.slowConnection = true;
                }
            }

            if (!(connectionDetail.saveDataEnabled || connectionDetail.slowConnection)) {
                createLink("/static/fonts/HardingText-Regular-Web-cecd90984f.woff2");
            } else {
                document.documentElement.classList.add('save-data');
            }
        </script>
    

<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"neuroscience;perception;psychology;striate-cortex;working-memory","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Nature Neuroscience","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article"}},"article":{"doi":"10.1038/s41593-019-0428-x"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["Rosanne L. Rademaker","Chaipat Chunharas","John T. Serences"],"publishedAt":1561939200,"publishedAtString":"2019-07-01","title":"Coexisting representations of sensory and mnemonic information in human visual cortex","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"neuro","title":"nature neuroscience","volume":"22","issue":"8"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"nature-onwards-journey","active":false},{"name":"getftr-entitled","active":false},{"name":"paywall_recommendations","active":true}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"US","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://collect.nature.com',
        imprint: 'nature'
    });
</script>

<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
 



     
    
    
        
    
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}button:focus{outline:3px solid #fece3e;will-change:transform}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:8px}.c-recommendations-list__item .c-card{flex-basis:100%}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 0 0 16px;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #cedbe0;height:auto;min-height:0;position:static}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0 0 8px;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:inherit}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}.c-card__title-recommendation .MathJax_Display{display:inline!important}.c-card__link:not(.c-card__link--no-block-link):before{z-index:1}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a,.c-article-recommendations-card__link{color:inherit}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-article-recommendations-card__meta-type,.c-meta .c-meta__item:first-child{font-weight:700}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:539px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px 13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.c-pdf-download__link{padding:13px 24px} } </style>




    
        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    
    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-122346e276.css" media="print">
    



<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">


<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h.indexOf('preview-www.nature.com') > -1) return;

            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('nature.com') > -1) {
                if (h.indexOf('test-www.nature.com') > -1) {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-26e142e9c6.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    
                        {src: '/static/js/global-article-es6-bundle-782fd09f66.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-d66d49033d.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-aca08c055a.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-4fba787158.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-1fe07484e5.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;
                        
                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-8fc1a30809.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-e0c7186f28.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-2399be388c.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">





    
    <script type="application/ld+json">{"mainEntity":{"headline":"Coexisting representations of sensory and mnemonic information in human visual cortex","description":"Traversing sensory environments requires keeping relevant information in mind while simultaneously processing new inputs. Visual information is kept in working memory via feature-selective responses in early visual cortex, but recent work has suggested that new sensory inputs obligatorily wipe out this information. Here we show region-wide multiplexing abilities in classic sensory areas, with population-level response patterns in early visual cortex representing the contents of working memory alongside new sensory inputs. In a second experiment, we show that when people get distracted, this leads to both disruptions of mnemonic information in early visual cortex and decrements in behavioral recall. Representations in the intraparietal sulcus reflect actively remembered information encoded in a transformed format, but not task-irrelevant sensory inputs. Together, these results suggest that early visual areas play a key role in supporting high-resolution working memory representations that can serve as a template for comparison with incoming sensory information. People often remember visual information over brief delays while also seeing new images in their immediate visual surroundings. Areas of the brain known to process visual information are shown to juggle these memory and sensory demands.","datePublished":"2019-07-01T00:00:00Z","dateModified":"2019-07-01T00:00:00Z","pageStart":"1336","pageEnd":"1344","sameAs":"https://doi.org/10.1038/s41593-019-0428-x","keywords":["Neuroscience","Perception","Psychology","Striate cortex","Working memory","Biomedicine","general","Neurosciences","Behavioral Sciences","Biological Techniques","Neurobiology","Animal Genetics and Genomics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig4_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig5_HTML.png"],"isPartOf":{"name":"Nature Neuroscience","issn":["1546-1726","1097-6256"],"volumeNumber":"22","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Rosanne L. Rademaker","url":"http://orcid.org/0000-0002-2804-8095","affiliation":[{"name":"University of California, San Diego","address":{"name":"Department of Psychology, University of California, San Diego, La Jolla, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Radboud University","address":{"name":"Donders Institute for Brain, Cognition and Behavior, Radboud University, Nijmegen, the Netherlands","@type":"PostalAddress"},"@type":"Organization"}],"email":"rosanne.rademaker@gmail.com","@type":"Person"},{"name":"Chaipat Chunharas","url":"http://orcid.org/0000-0003-1074-0160","affiliation":[{"name":"University of California, San Diego","address":{"name":"Department of Psychology, University of California, San Diego, La Jolla, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Chulalongkorn University","address":{"name":"Department of Medicine, King Chulalongkorn Memorial Hospital, Chulalongkorn University, Bangkok, Thailand","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"John T. Serences","url":"http://orcid.org/0000-0002-8551-5147","affiliation":[{"name":"University of California, San Diego","address":{"name":"Department of Psychology, University of California, San Diego, La Jolla, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of California, San Diego","address":{"name":"Neurosciences Graduate Program, University of California, San Diego, La Jolla, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of California, San Diego","address":{"name":"Kavli Institute for Brain and Mind, University of California, San Diego, La Jolla, USA","@type":"PostalAddress"},"@type":"Organization"}],"email":"jserences@ucsd.edu","@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>




    
    
    


    
    <link rel="canonical" href="https://www.nature.com/articles/s41593-019-0428-x">
    
    
    <meta name="journal_id" content="41593"/>
    <meta name="dc.title" content="Coexisting representations of sensory and mnemonic information in human visual cortex"/>
    <meta name="dc.source" content="Nature Neuroscience 2019 22:8"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2019-07-01"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2019 The Author(s), under exclusive licence to Springer Nature America, Inc."/>
    <meta name="dc.rights" content="2019 The Author(s), under exclusive licence to Springer Nature America, Inc."/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="Traversing sensory environments requires keeping relevant information in mind while simultaneously processing new inputs. Visual information is kept in working memory via feature-selective responses in early visual cortex, but recent work has suggested that new sensory inputs obligatorily wipe out this information. Here we show region-wide multiplexing abilities in classic sensory areas, with population-level response patterns in early visual cortex representing the contents of working memory alongside new sensory inputs. In a second experiment, we show that when people get distracted, this leads to both disruptions of mnemonic information in early visual cortex and decrements in behavioral recall. Representations in the intraparietal sulcus reflect actively remembered information encoded in a transformed format, but not task-irrelevant sensory inputs. Together, these results suggest that early visual areas play a key role in supporting high-resolution working memory representations that can serve as a template for comparison with incoming sensory information. People often remember visual information over brief delays while also seeing new images in their immediate visual surroundings. Areas of the brain known to process visual information are shown to juggle these memory and sensory demands."/>
    <meta name="prism.issn" content="1546-1726"/>
    <meta name="prism.publicationName" content="Nature Neuroscience"/>
    <meta name="prism.publicationDate" content="2019-07-01"/>
    <meta name="prism.volume" content="22"/>
    <meta name="prism.number" content="8"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="1336"/>
    <meta name="prism.endingPage" content="1344"/>
    <meta name="prism.copyright" content="2019 The Author(s), under exclusive licence to Springer Nature America, Inc."/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/s41593-019-0428-x"/>
    <meta name="prism.doi" content="doi:10.1038/s41593-019-0428-x"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/s41593-019-0428-x.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/s41593-019-0428-x"/>
    <meta name="citation_journal_title" content="Nature Neuroscience"/>
    <meta name="citation_journal_abbrev" content="Nat Neurosci"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="1546-1726"/>
    <meta name="citation_title" content="Coexisting representations of sensory and mnemonic information in human visual cortex"/>
    <meta name="citation_volume" content="22"/>
    <meta name="citation_issue" content="8"/>
    <meta name="citation_publication_date" content="2019/08"/>
    <meta name="citation_online_date" content="2019/07/01"/>
    <meta name="citation_firstpage" content="1336"/>
    <meta name="citation_lastpage" content="1344"/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/s41593-019-0428-x"/>
    <meta name="DOI" content="10.1038/s41593-019-0428-x"/>
    <meta name="size" content="268294"/>
    <meta name="citation_doi" content="10.1038/s41593-019-0428-x"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/s41593-019-0428-x&amp;api_key="/>
    <meta name="description" content="Traversing sensory environments requires keeping relevant information in mind while simultaneously processing new inputs. Visual information is kept in working memory via feature-selective responses in early visual cortex, but recent work has suggested that new sensory inputs obligatorily wipe out this information. Here we show region-wide multiplexing abilities in classic sensory areas, with population-level response patterns in early visual cortex representing the contents of working memory alongside new sensory inputs. In a second experiment, we show that when people get distracted, this leads to both disruptions of mnemonic information in early visual cortex and decrements in behavioral recall. Representations in the intraparietal sulcus reflect actively remembered information encoded in a transformed format, but not task-irrelevant sensory inputs. Together, these results suggest that early visual areas play a key role in supporting high-resolution working memory representations that can serve as a template for comparison with incoming sensory information. People often remember visual information over brief delays while also seeing new images in their immediate visual surroundings. Areas of the brain known to process visual information are shown to juggle these memory and sensory demands."/>
    <meta name="dc.creator" content="Rademaker, Rosanne L."/>
    <meta name="dc.creator" content="Chunharas, Chaipat"/>
    <meta name="dc.creator" content="Serences, John T."/>
    <meta name="dc.subject" content="Neuroscience"/>
    <meta name="dc.subject" content="Perception"/>
    <meta name="dc.subject" content="Psychology"/>
    <meta name="dc.subject" content="Striate cortex"/>
    <meta name="dc.subject" content="Working memory"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Decoding reveals the contents of visual working memory in early visual areas; citation_author=SA Harrison, F Tong; citation_volume=458; citation_publication_date=2009; citation_pages=632-635; citation_doi=10.1038/nature07832; citation_id=CR1"/>
    <meta name="citation_reference" content="citation_journal_title=Psych. Sci.; citation_title=Stimulus-specific delay activity in human primary visual cortex; citation_author=JT Serences, EF Ester, EK Vogel, E Awh; citation_volume=20; citation_publication_date=2009; citation_pages=207-214; citation_doi=10.1111/j.1467-9280.2009.02276.x; citation_id=CR2"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=The relationship between working memory storage and elevated activity as measured with functional magnetic resonance imagine; citation_author=AC Riggall, BR Postle; citation_volume=32; citation_publication_date=2012; citation_pages=12990-12998; citation_doi=10.1523/JNEUROSCI.1892-12.2012; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Decoding the contents of visual short-term memory from human visual and parietal cortex; citation_author=TB Christophel, MN Hebart, JD Haynes; citation_volume=32; citation_publication_date=2012; citation_pages=12983-12989; citation_doi=10.1523/JNEUROSCI.0184-12.2012; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=J. Cog. Neurosci.; citation_title=A neural measure of precision in visual working memory; citation_author=EF Ester, DE Anderson, JT Serences, E Awh; citation_volume=25; citation_publication_date=2013; citation_pages=754-761; citation_doi=10.1162/jocn_a_00357; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Decoding the content of visual short-term memory under distraction in occipital and parietal areas; citation_author=KC Bettencourt, Y Xu; citation_volume=19; citation_publication_date=2016; citation_pages=150-157; citation_doi=10.1038/nn.4174; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Sharp emergence of feature-selective sustained activity along the dorsal visual pathway; citation_author=D Mendoza-Halliday, S Torres, JC Martinez-Trujillo; citation_volume=17; citation_publication_date=2014; citation_pages=1255-1262; citation_doi=10.1038/nn.3785; citation_id=CR7"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cog. Sci.; citation_title=&#8216;Activity-silent&#8217; working memory in prefrontal cortex: a dynamic coding framework; citation_author=MG Stokes; citation_volume=19; citation_publication_date=2015; citation_pages=394-405; citation_doi=10.1016/j.tics.2015.05.004; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=eNeuro; citation_title=How do visual and parietal cortex contribute to visual short-term memory?; citation_author=EF Ester, RL Rademaker, TS Sprague; citation_volume=3; citation_publication_date=2016; citation_pages=e0041-16; citation_doi=10.1523/ENEURO.0041-16.2016; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Rev. Neurosci.; citation_title=Parallel processing strategies of the primate visual system; citation_author=JJ Nassi, EM Callaway; citation_volume=10; citation_publication_date=2009; citation_pages=360-372; citation_doi=10.1038/nrn2619; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Comm.; citation_title=Layer-specificity in the effects of attention and working memory on activity in primary visual cortex; citation_author=T Kerkoerle, MW Self, PR Roelfsema; citation_volume=8; citation_publication_date=2017; citation_pages=13804; citation_doi=10.1038/ncomms13804; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Activity of neurons in anterior inferior temporal cortex during a short-term memory task; citation_author=EK Miller, L Li, R Desimone; citation_volume=13; citation_publication_date=1993; citation_pages=1460-1478; citation_doi=10.1523/JNEUROSCI.13-04-01460.1993; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=Vis. Res.; citation_title=Neural mechanisms of information storage in visual short-term memory; citation_author=JT Serences; citation_volume=128; citation_publication_date=2016; citation_pages=53-67; citation_doi=10.1016/j.visres.2016.09.010; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Decoding and reconstructing color from responses in human visual cortex; citation_author=GJ Brouwer, DJ Heeger; citation_volume=29; citation_publication_date=2009; citation_pages=13992-14003; citation_doi=10.1523/JNEUROSCI.3577-09.2009; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Visual attention mitigates information loss in small- and large-scale neural codes; citation_author=TC Sprague, S Saproo, JT Serences; citation_volume=19; citation_publication_date=2015; citation_pages=215-226; citation_doi=10.1016/j.tics.2015.02.005; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol. Hum. Percept. Perform.; citation_title=The impact of interference on short-term memory for visual orientation; citation_author=RL Rademaker, IM Bloem, P Weerd, AS Sack; citation_volume=41; citation_publication_date=2015; citation_pages=1650-1665; citation_doi=10.1037/xhp0000110; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol. Hum. Percept. Perform.; citation_title=Supraliminal but not subliminal distracters bias working memory recall; citation_author=T Wildegger, NE Meyers, G Humphreys, AC Nobre; citation_volume=41; citation_publication_date=2015; citation_pages=826-839; citation_doi=10.1037/xhp0000052; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Topographic maps of visual spatial attention in human parietal cortex; citation_author=MA Silver, D Ress, DJ Heeger; citation_volume=94; citation_publication_date=2005; citation_pages=1358-1371; citation_doi=10.1152/jn.01316.2004; citation_id=CR18"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Selective visual attention and perceptual coherence; citation_author=JT Serences, S Yantis; citation_volume=10; citation_publication_date=2006; citation_pages=38-45; citation_doi=10.1016/j.tics.2005.11.008; citation_id=CR19"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Characterizing the effects of feature salience and top-down attention in the early visual system; citation_author=S Poltoratski, S Ling, D McCormack, F Tong; citation_volume=118; citation_publication_date=2017; citation_pages=564-573; citation_doi=10.1152/jn.00924.2016; citation_id=CR20"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Dissociable signatures of visual salience and behavioral relevance across attentional priority maps in human cortex; citation_author=TC Sprague, S Itthipuripat, VA Vo, JT Serences; citation_volume=119; citation_publication_date=2018; citation_pages=2153-2165; citation_doi=10.1152/jn.00059.2018; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex; citation_author=JD Murray; citation_volume=114; citation_publication_date=2017; citation_pages=394-399; citation_doi=10.1073/pnas.1619449114; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=How does the brain solve visual object recognition?; citation_author=JJ DiCarlo, D Zoccolan, NC Rust; citation_volume=73; citation_publication_date=2012; citation_pages=415-434; citation_doi=10.1016/j.neuron.2012.01.010; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol. Hum. Percept. Perform.; citation_title=Evidence of gradual loss of precision for simple features and complex objects in visual working memory; citation_author=RL Rademaker, YE Park, AT Sack, F Tong; citation_volume=44; citation_publication_date=2018; citation_pages=925-940; citation_doi=10.1037/xhp0000491; citation_id=CR24"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Activity of neurons in cortical area MT during a memory for motion task; citation_author=JW Bisley, D Zaksas, JA Droll, T Pasternak; citation_volume=91; citation_publication_date=2004; citation_pages=286-300; citation_doi=10.1152/jn.00870.2003; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Direction signals in the prefrontal cortex and in area MT during a working memory for visual motion task; citation_author=D Zaksas, T Paternak; citation_volume=26; citation_publication_date=2006; citation_pages=11726-11742; citation_doi=10.1523/JNEUROSCI.3420-06.2006; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Visual working memory enhances the neural response to matching visual input; citation_author=S Gayet; citation_volume=37; citation_publication_date=2017; citation_pages=6638-6647; citation_doi=10.1523/JNEUROSCI.3418-16.2017; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Comms.; citation_title=Spatial working memory alters the efficacy of input to visual cortex; citation_author=Y Merrikhi; citation_volume=8; citation_publication_date=2017; citation_pages=15041; citation_doi=10.1038/ncomms15041; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=A neural mechanism for working and recognition memory in inferior temporal cortex; citation_author=EK Miller, L Li, R Desimone; citation_volume=254; citation_publication_date=1991; citation_pages=1377-1379; citation_doi=10.1126/science.1962197; citation_id=CR29"/>
    <meta name="citation_reference" content="citation_journal_title=Vis. Neurosci.; citation_title=Extraretinal representations in area V4 in the macaque monkey; citation_author=JHR Maunsell, G Sclar, TA Nealey, DD DePriest; citation_volume=7; citation_publication_date=1991; citation_pages=561-573; citation_doi=10.1017/S095252380001035X; citation_id=CR30"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Parallel neuronal mechanisms for short-term memory; citation_author=EK Miller, R Desimone; citation_volume=263; citation_publication_date=1994; citation_pages=520-522; citation_doi=10.1126/science.8290960; citation_id=CR31"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Neural mechanisms of visual working memory in prefrontal cortex of the macaque; citation_author=EK Miller, CA Erickson, R Desimone; citation_volume=16; citation_publication_date=1996; citation_pages=5154-5167; citation_doi=10.1523/JNEUROSCI.16-16-05154.1996; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Complementary roles for primate frontal and parietal cortex in guarding working memory from distractor stimuli; citation_author=SN Jacob, A Nieder; citation_volume=83; citation_publication_date=2014; citation_pages=226-237; citation_doi=10.1016/j.neuron.2014.05.009; citation_id=CR33"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Representation of remembered stimuli and task information in the monkey dorsolateral prefrontal and posterior parietal cortex; citation_author=X-L Qi, AC Elworthy, BC Lambert, C Constantinidis; citation_volume=113; citation_publication_date=2015; citation_pages=44-57; citation_doi=10.1152/jn.00413.2014; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Topographic maps in human frontal and parietal cortex; citation_author=MA Silver, S Kastner; citation_volume=13; citation_publication_date=2009; citation_pages=488-495; citation_doi=10.1016/j.tics.2009.08.005; citation_id=CR35"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Spatial attention improves reliability of fMRI retinotopic mapping signals in occipital and parietal cortex; citation_author=DW Bressler, MA Silver; citation_volume=53; citation_publication_date=2010; citation_pages=526-533; citation_doi=10.1016/j.neuroimage.2010.06.063; citation_id=CR36"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Tones and numbers: specificity of interference in immediate memory; citation_author=D Deutsch; citation_volume=168; citation_publication_date=1970; citation_pages=1604-1605; citation_doi=10.1126/science.168.3939.1604; citation_id=CR37"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol.; citation_title=Interference in memory between tones adjacent in the musical scale; citation_author=D Deutsch; citation_volume=100; citation_publication_date=1973; citation_pages=228-231; citation_doi=10.1037/h0035440; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=Vis. Res.; citation_title=Stimulus-specific mechanisms of visual short-term memory; citation_author=S Magnussen, MW Greenlee, R Asplund, S Dyrnes; citation_volume=31; citation_publication_date=1991; citation_pages=1213-1219; citation_doi=10.1016/0042-6989(91)90046-8; citation_id=CR39"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol. Learn. Mem. Cogn.; citation_title=Retention and disruption of motion information in visual short-term memory; citation_author=S Magnussen, MW Greenlee; citation_volume=18; citation_publication_date=1992; citation_pages=151-156; citation_doi=10.1037/0278-7393.18.1.151; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Stimulus specificity and temporal dynamics of working memory for visual motion; citation_author=T Pasternak, D Zaksas; citation_volume=90; citation_publication_date=2003; citation_pages=2757-2762; citation_doi=10.1152/jn.00422.2003; citation_id=CR41"/>
    <meta name="citation_reference" content="citation_journal_title=Psychon. Bull. Rev.; citation_title=The effects of a task-irrelevant visual event on spatial working memory; citation_author=S Stigchel, H Merten, M Meeter, J Theeuwes; citation_volume=14; citation_publication_date=2007; citation_pages=1066-1071; citation_doi=10.3758/BF03193092; citation_id=CR42"/>
    <meta name="citation_reference" content="citation_journal_title=J. Vis.; citation_title=Distortions in recall from visual memory: two classes of attractors at work; citation_author=J Huang, R Sekuler; citation_volume=10; citation_publication_date=2010; citation_pages=1-27; citation_doi=10.1167/10.2.24; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_journal_title=J. Vis.; citation_title=The retention and disruption of color information in human short-term visual memory; citation_author=VA Nemes, NR Parry, D Whitaker, DJ McKeefry; citation_volume=12; citation_publication_date=2012; citation_pages=1-14; citation_doi=10.1167/12.1.26; citation_id=CR44"/>
    <meta name="citation_reference" content="citation_journal_title=Atten. Percep. Psychophys.; citation_title=Interactions between visual working memory representations; citation_author=GY Bae, SJ Luck; citation_volume=79; citation_publication_date=2017; citation_pages=2376-2395; citation_doi=10.3758/s13414-017-1404-8; citation_id=CR45"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Flexible coding of visual working memory representations during distraction; citation_author=ES Lorenc, KK Sreenivasan, DE Nee, ARE Vandenbroucke, M D&#8217;Esposito; citation_volume=38; citation_publication_date=2018; citation_pages=5267-5276; citation_doi=10.1523/JNEUROSCI.3061-17.2018; citation_id=CR46"/>
    <meta name="citation_reference" content="Chunharas, C., Rademaker, R. L., Brady, T. F. &amp; Serences, J. T. Adaptive memory distortion in visual working memory. Preprint at PsyArXiv 
                              https://psyarxiv.com/e3m5a/
                              
                            (2019)."/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Restoring latent visual working memory representations in human cortex; citation_author=TC Sprague, EF Ester, JT Serences; citation_volume=91; citation_publication_date=2016; citation_pages=694-707; citation_doi=10.1016/j.neuron.2016.07.006; citation_id=CR48"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Cortical specialization for attended versus unattended working memory; citation_author=TG Christophel, P Iamshchinina, C Yan, C Allefeld, JD Haynes; citation_volume=21; citation_publication_date=2018; citation_pages=494-496; citation_doi=10.1038/s41593-018-0094-4; citation_id=CR49"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Reactivation of latent working memories with transcranial magnetic stimulation; citation_author=NS Rose; citation_volume=354; citation_publication_date=2016; citation_pages=1136-1139; citation_doi=10.1126/science.aah7011; citation_id=CR50"/>
    <meta name="citation_reference" content="citation_journal_title=Spat. Vis.; citation_title=The Psychophysics Toolbox; citation_author=DH Brainard; citation_volume=10; citation_publication_date=1997; citation_pages=433-436; citation_doi=10.1163/156856897X00357; citation_id=CR51"/>
    <meta name="citation_reference" content="citation_journal_title=Perception; citation_title=What&#8217;s new in psychtoolbox-3; citation_author=M Kleiner; citation_volume=36; citation_publication_date=2007; citation_pages=1-16; citation_id=CR52"/>
    <meta name="citation_reference" content="citation_journal_title=Vis. Res.; citation_title=Grating induction: a new type of aftereffect; citation_author=CW Tyler, K Nakayama; citation_volume=20; citation_publication_date=1980; citation_pages=437-441; citation_doi=10.1016/0042-6989(80)90034-6; citation_id=CR53"/>
    <meta name="citation_reference" content="citation_journal_title=Cogn. Emot.; citation_title=The Karolinska directed emotional faces: a validation study; citation_author=E Goeleven, R Raedt, L Leyman, B Verschuere; citation_volume=22; citation_publication_date=2008; citation_pages=1094-1118; citation_doi=10.1080/02699930701626582; citation_id=CR54"/>
    <meta name="citation_reference" content="citation_journal_title=Exp. Brain Res.; citation_title=An estimation and application of the human cortical magnification factor; citation_author=J Rovamo, V Virsu; citation_volume=37; citation_publication_date=1979; citation_pages=495-510; citation_doi=10.1007/BF00236819; citation_id=CR55"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging; citation_author=JLR Andersson, S Skare, J Ashburner; citation_volume=20; citation_publication_date=2003; citation_pages=870-888; citation_doi=10.1016/S1053-8119(03)00336-7; citation_id=CR56"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Advances in functional and structural MR image analysis and implementation as FSL; citation_author=SM Smith; citation_volume=23; citation_publication_date=2004; citation_pages=208-219; citation_doi=10.1016/j.neuroimage.2004.07.051; citation_id=CR57"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=FSL; citation_author=M Jenkinson, CF Beckmann, TE Behrens, MW Woolrich, SM Smith; citation_volume=62; citation_publication_date=2012; citation_pages=782-790; citation_doi=10.1016/j.neuroimage.2011.09.015; citation_id=CR58"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Cortical surface-based analysis. I. Segmentation and surface reconstruction; citation_author=AM Dale, B Fischl, MI Sereno; citation_volume=9; citation_publication_date=1999; citation_pages=179-194; citation_doi=10.1006/nimg.1998.0395; citation_id=CR59"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Accurate and robust brain image alignment using boundary-based registration; citation_author=D Greve, B Fischl; citation_volume=48; citation_publication_date=2009; citation_pages=63-72; citation_doi=10.1016/j.neuroimage.2009.06.060; citation_id=CR60"/>
    <meta name="citation_reference" content="citation_journal_title=Med. Image Anal.; citation_title=A global optimisation method for robust affine registration of brain images; citation_author=M Jenkinson, SM Smith; citation_volume=5; citation_publication_date=2001; citation_pages=143-156; citation_doi=10.1016/S1361-8415(01)00036-6; citation_id=CR61"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Improved optimization for the robust and accurate linear registration and motion correction of brain images; citation_author=M Jenkinson, P Bannister, JM Brady, SM Smith; citation_volume=17; citation_publication_date=2002; citation_pages=825-841; citation_doi=10.1006/nimg.2002.1132; citation_id=CR62"/>
    <meta name="citation_reference" content="citation_journal_title=Hum. Brain Mapp.; citation_title=Optimal experimental design for event&#8208;related fMRI; citation_author=AM Dale; citation_volume=8; citation_publication_date=1999; citation_pages=109-114; citation_doi=10.1002/(SICI)1097-0193(1999)8:2/3&lt;109::AID-HBM7&gt;3.0.CO;2-W; citation_id=CR63"/>
    <meta name="citation_reference" content="citation_journal_title=Hum. Brain Mapp.; citation_title=Fast robust automated brain extraction; citation_author=SM Smith; citation_volume=17; citation_publication_date=2002; citation_pages=143-155; citation_doi=10.1002/hbm.10062; citation_id=CR64"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Temporal autocorrelation in univariate linear modeling of FMRI data; citation_author=MW Woolrich, BD Ripley, M Brady, SM Smith; citation_volume=14; citation_publication_date=2001; citation_pages=1370-1386; citation_doi=10.1006/nimg.2001.0931; citation_id=CR65"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=fMRI of human visual cortex; citation_author=SA Engel; citation_volume=369; citation_publication_date=1994; citation_pages=525; citation_doi=10.1038/369525a0; citation_id=CR66"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Visual topography of human intraparietal sulcus; citation_author=JD Swisher, MA Halko, LB Merabet, SA McMains, DC Somers; citation_volume=27; citation_publication_date=2007; citation_pages=5326-5337; citation_doi=10.1523/JNEUROSCI.0991-07.2007; citation_id=CR67"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices; citation_author=TC Sprague, JT Serences; citation_volume=16; citation_publication_date=2013; citation_pages=1879-1887; citation_doi=10.1038/nn.3574; citation_id=CR68"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Dynamic hidden states underlying working-memory-guided behavior; citation_author=MJ Wolff, J Jochim, EG Aky&#252;rek, MG Stokes; citation_volume=20; citation_publication_date=2017; citation_pages=864-871; citation_doi=10.1038/nn.4546; citation_id=CR69"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=A primer on pattern-based approaches to fMRI: principles, pitfalls, and perspectives; citation_author=JD Haynes; citation_volume=87; citation_publication_date=2015; citation_pages=257-270; citation_doi=10.1016/j.neuron.2015.05.025; citation_id=CR70"/>
    <meta name="citation_reference" content="citation_journal_title=J. Stat. Softw.; citation_title=CircStat: a MATLAB toolbox for circular statistics; citation_author=P Berens; citation_volume=31; citation_publication_date=2009; citation_pages=1-21; citation_doi=10.18637/jss.v031.i10; citation_id=CR71"/>
    <meta name="citation_author" content="Rademaker, Rosanne L."/>
    <meta name="citation_author_institution" content="Department of Psychology, University of California, San Diego, La Jolla, USA"/>
    <meta name="citation_author_institution" content="Donders Institute for Brain, Cognition and Behavior, Radboud University, Nijmegen, the Netherlands"/>
    <meta name="citation_author" content="Chunharas, Chaipat"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of California, San Diego, La Jolla, USA"/>
    <meta name="citation_author_institution" content="Department of Medicine, King Chulalongkorn Memorial Hospital, Chulalongkorn University, Bangkok, Thailand"/>
    <meta name="citation_author" content="Serences, John T."/>
    <meta name="citation_author_institution" content="Department of Psychology, University of California, San Diego, La Jolla, USA"/>
    <meta name="citation_author_institution" content="Neurosciences Graduate Program, University of California, San Diego, La Jolla, USA"/>
    <meta name="citation_author_institution" content="Kavli Institute for Brain and Mind, University of California, San Diego, La Jolla, USA"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@natureneuro"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Coexisting representations of sensory and mnemonic information in human visual cortex"/>
    <meta name="twitter:description" content="Nature Neuroscience - People often remember visual information over brief delays while also seeing new images in their immediate visual surroundings. Areas of the brain known to process visual..."/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig1_HTML.png"/>
    

    
    
    <meta property="og:url" content="https://www.nature.com/articles/s41593-019-0428-x"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="Coexisting representations of sensory and mnemonic information in human visual cortex - Nature Neuroscience"/>
    <meta property="og:description" content="People often remember visual information over brief delays while also seeing new images in their immediate visual surroundings. Areas of the brain known to process visual information are shown to juggle these memory and sensory demands."/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig1_HTML.png"/>
    

    <script>
        window.eligibleForRa21 = 'true'; 
    </script>
</head>
<body class="article-page">

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>



<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
        
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>

    

    <div class="u-lazy-ad-wrapper u-mbs-0">
            <div class="deferred-placeholder" data-replace="true"
                 data-placeholder="/placeholder/v1/institutionalBanner?bpids=[bpids] #institutional-banner-container"></div>
            <aside class="c-ad c-ad--728x90">
                <div class="c-ad__inner" data-container-type="banner-advert">
                    <p class="c-ad__label">Advertisement</p>
                    
        
            
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=s41593-019-0428-x;doi=10.1038/s41593-019-0428-x;techmeta=36,59;subjmeta=1595,1636,1723,1875,2613,2649,378,477,631;kwrd=Neuroscience,Perception,Psychology,Striate+cortex,Working+memory">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-1622131647&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-019-0428-x%26doi%3D10.1038/s41593-019-0428-x%26techmeta%3D36,59%26subjmeta%3D1595,1636,1723,1875,2613,2649,378,477,631%26kwrd%3DNeuroscience,Perception,Psychology,Striate+cortex,Working+memory">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-1622131647&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-019-0428-x%26doi%3D10.1038/s41593-019-0428-x%26techmeta%3D36,59%26subjmeta%3D1595,1636,1723,1875,2613,2649,378,477,631%26kwrd%3DNeuroscience,Perception,Psychology,Striate+cortex,Working+memory"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

        
    
                </div>
            </aside>
        </div>
    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#00928c">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">
                    
                    
                    <div class="c-header__logo-container">
                        
                        <a href="/neuro"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-0ccc487532906d646419e51f647ce79a.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-880e5942f43b9213989c58a04ab5c8e6.svg" height="32" alt="Nature Neuroscience">
                            </picture>
                        </a>
                    
                    </div>
                    
                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">
                            
                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41593-019-0428-x'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
                            
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        
            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">
                            
                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span><span class="c-header__show-text">Explore</span> content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                                
                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>
                                
                            
                            
                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">
                            
                                <li class="c-header__item">
                                    <a class="c-header__link"
                                       href="https://idp.nature.com/auth/personal/springernature?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Fmy-account%2Falerts%2Fsubscribe-journal%3Flist-id%3D6"
                                       rel="nofollow"
                                       data-track="click"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/neuro.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        
    </header>


    
    
        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature neuroscience"><span itemprop="name">nature neuroscience</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro/articles?type&#x3D;article" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="name">articles</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>
    



    

</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
        
            <div class="c-context-bar u-hide"
                 data-test="context-bar"
                 data-context-bar
                 aria-hidden="true">
                <div class="c-context-bar__container u-container">
                    <div class="c-context-bar__title">
                        Coexisting representations of sensory and mnemonic information in human visual cortex
                    </div>
                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-019-0428-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                </div>
            </div>
        
        <article lang="en">
            
            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Article</li>
    
    

                        <li class="c-article-identifiers__item">Published: <time datetime="2019-07-01">01 July 2019</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Coexisting representations of sensory and mnemonic information in human visual cortex</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Rosanne_L_-Rademaker-Aff1-Aff2" data-author-popup="auth-Rosanne_L_-Rademaker-Aff1-Aff2" data-author-search="Rademaker, Rosanne L." data-corresp-id="c1">Rosanne L. Rademaker<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><span class="u-js-hide">
            <a class="js-orcid" href="http://orcid.org/0000-0002-2804-8095"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-2804-8095</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Chaipat-Chunharas-Aff1-Aff3" data-author-popup="auth-Chaipat-Chunharas-Aff1-Aff3" data-author-search="Chunharas, Chaipat">Chaipat Chunharas</a><span class="u-js-hide">
            <a class="js-orcid" href="http://orcid.org/0000-0003-1074-0160"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-1074-0160</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff3">3</a></sup> &amp; </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-John_T_-Serences-Aff1-Aff4-Aff5" data-author-popup="auth-John_T_-Serences-Aff1-Aff4-Aff5" data-author-search="Serences, John T." data-corresp-id="c2">John T. Serences<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><span class="u-js-hide">
            <a class="js-orcid" href="http://orcid.org/0000-0002-8551-5147"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-8551-5147</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff4">4</a>,<a href="#Aff5">5</a></sup></li></ul>

                    

                    <p class="c-article-info-details" data-container-section="info">
                        
    <a data-test="journal-link" href="/neuro" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Nature Neuroscience</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span>22</b>,<span class="u-visually-hidden">pages </span>13361344 (<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>
                    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">10k <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">109 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">64 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/articles/s41593-019-0428-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
                    
                </header>

                
    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/neuroscience" data-track="click" data-track-action="view subject" data-track-label="link">Neuroscience</a></li><li class="c-article-subject-list__subject"><a href="/subjects/perception" data-track="click" data-track-action="view subject" data-track-label="link">Perception</a></li><li class="c-article-subject-list__subject"><a href="/subjects/psychology" data-track="click" data-track-action="view subject" data-track-label="link">Psychology</a></li><li class="c-article-subject-list__subject"><a href="/subjects/striate-cortex" data-track="click" data-track-action="view subject" data-track-label="link">Striate cortex</a></li><li class="c-article-subject-list__subject"><a href="/subjects/working-memory" data-track="click" data-track-action="view subject" data-track-label="link">Working memory</a></li>
        </ul>
    </div>

                
    
    

    
    

                
            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Traversing sensory environments requires keeping relevant information in mind while simultaneously processing new inputs. Visual information is kept in working memory via feature-selective responses in early visual cortex, but recent work has suggested that new sensory inputs obligatorily wipe out this information. Here we show region-wide multiplexing abilities in classic sensory areas, with population-level response patterns in early visual cortex representing the contents of working memory alongside new sensory inputs. In a second experiment, we show that when people get distracted, this leads to both disruptions of mnemonic information in early visual cortex and decrements in behavioral recall. Representations in the intraparietal sulcus reflect actively remembered information encoded in a transformed format, but not task-irrelevant sensory inputs. Together, these results suggest that early visual areas play a key role in supporting high-resolution working memory representations that can serve as a template for comparison with incoming sensory information.</p></div></div></section>

            <noscript>
                
                    
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-019-0428-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                
            </noscript>

            
                <div class="js-context-bar-sticky-point-mobile">
                    
                        <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-entitled-mobile
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-019-0428-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                    
                </div>
            

            
                
                    
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-021-24973-1/MediaObjects/41467_2021_24973_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41467-021-24973-1?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1038/s41467-021-24973-1">Working memory representations in visual cortex mediate distraction effects
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">05 August 2021</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Grace E. Hallenbeck, Thomas C. Sprague,  Clayton E. Curtis</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-022-33161-8/MediaObjects/41467_2022_33161_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41467-022-33161-8?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1038/s41467-022-33161-8">Perception and memory have distinct spatial tuning properties in human visual cortex
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">18 October 2022</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Serra E. Favila, Brice A. Kuhl &amp; Jonathan Winawer</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-019-42429-x/MediaObjects/41598_2019_42429_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41598-019-42429-x?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1038/s41598-019-42429-x">The perceptual neural trace of memorable unseen scenes
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">15 April 2019</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Yalda Mohsenzadeh, Caitlin Mullin,  Dimitrios Pantazis</p>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'speedy-BootstrappedUCB',
                        timestamp: 1711582487,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                
                
                <div class="main-content">
                    <section data-title="Main"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Main</h2><div class="c-article-section__content" id="Sec1-content"><p>When trying to attain behavioral goals, the ability to flexibly juggle thoughts is key. Visual working memory (VWM) provides the mental workspace to keep visual information online, allowing this information to guide visual search or to be recalled at a future moment in time. Neuroimaging studies have firmly established that the contents of VWM can be decoded from occipital cortex including primary visual area V1 (refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Harrison, S. A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. Nature 458, 632635 (2009)." href="#ref-CR1" id="ref-link-section-d117493720e443">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Serences, J. T., Ester, E. F., Vogel, E. K. &amp; Awh, E. Stimulus-specific delay activity in human primary visual cortex. Psych. Sci. 20, 207214 (2009)." href="#ref-CR2" id="ref-link-section-d117493720e443_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Riggall, A. C. &amp; Postle, B. R. The relationship between working memory storage and elevated activity as measured with functional magnetic resonance imagine. J. Neurosci. 32, 1299012998 (2012)." href="#ref-CR3" id="ref-link-section-d117493720e443_2">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Christophel, T. B., Hebart, M. N. &amp; Haynes, J. D. Decoding the contents of visual short-term memory from human visual and parietal cortex. J. Neurosci. 32, 1298312989 (2012)." href="/articles/s41593-019-0428-x#ref-CR4" id="ref-link-section-d117493720e446">4</a></sup>) and that the quality of this information predicts behavioral performance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ester, E. F., Anderson, D. E., Serences, J. T. &amp; Awh, E. A neural measure of precision in visual working memory. J. Cog. Neurosci. 25, 754761 (2013)." href="/articles/s41593-019-0428-x#ref-CR5" id="ref-link-section-d117493720e450">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e453">6</a></sup>, suggesting that early sensory areas are involved in the representation of visual memories.</p><p>That said, previous studies have typically relied on a traditional delayed-match-to-sample (DMTS) task in which a sample memory stimulus is encoded and remembered across a blank delay interval before a test stimulus appears for comparison. However, in everyday perception, VWM maintenance needs to be robust to the continuous influx of new visual inputs that come with each exploratory saccade or change in the environment. Thus, a delay period devoid of other visual inputs is divorced from typical visual experience. Based on this mismatch between experimental and real-world scenarios, some have argued that recruiting early sensory areas to store relevant VWM information would be counterproductive in everyday life, as new sensory inputs would destructively interfere with concurrent mnemonic representations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="#ref-CR6" id="ref-link-section-d117493720e460">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Mendoza-Halliday, D., Torres, S. &amp; Martinez-Trujillo, J. C. Sharp emergence of feature-selective sustained activity along the dorsal visual pathway. Nat. Neurosci. 17, 12551262 (2014)." href="#ref-CR7" id="ref-link-section-d117493720e460_1">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Stokes, M. G. Activity-silent working memory in prefrontal cortex: a dynamic coding framework. Trends Cog. Sci. 19, 394405 (2015)." href="/articles/s41593-019-0428-x#ref-CR8" id="ref-link-section-d117493720e463">8</a></sup>.</p><p>Based on this logic, one recent study<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e470">6</a></sup> employed a DMTS task with task-irrelevant pictures of faces and gazebos sometimes presented during the delay period. The authors used functional magnetic resonance imaging (fMRI) and found that activation patterns in early visual cortex represented the contents of VWM during blank delays, but not when delays were filled with predictable distractors (that is, the task-irrelevant pictures). The authors concluded that representations initially encoded in early visual cortex were recoded in a more durable format in parietal cortex to insulate mnemonic representations from interference induced by new sensory input. Furthermore, the authors argued that the disengagement of primary sensory regions was strategic, as it occurred only when participants expected the task-irrelevant pictures. This study challenged the importance of early visual cortex during VWM, attributing previous findings of sensory recruitment to overly artificial tasks (see ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Ester, E. F., Rademaker, R. L. &amp; Sprague, T. S. How do visual and parietal cortex contribute to visual short-term memory? eNeuro 3, e004116 (2016). 2016 13." href="/articles/s41593-019-0428-x#ref-CR9" id="ref-link-section-d117493720e474">9</a></sup> for potential caveats of this work).</p><p>The framework proposed by this previous study<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e481">6</a></sup> implies a fundamental limitation of cortical information processing: a sensory area, such as the primary visual cortex, cannot represent both top-down biasing signals associated with internal cognitive operations such as VWM and bottom-up sensory inputs evoked by newly encountered stimuli in the environment. However, this strong stance is questionable for at least two reasons. First, from a functional point of view, success on a DMTS task relies on comparing an internally stored representation to a new sensory input. For this, a local comparison circuit, able to jointly represent remembered and perceived items in the same local circuit, could be ideal. Second, separable bottom-up and top-down inputs could theoretically support the coexistence of multiple simultaneous representations, a concept we term region-wide multiplexing (after the more common usage of multiplexing to refer to flexible coding in single neurons). Bottom-up input from the lateral geniculate nucleus primarily projects to layer 4 of primary visual cortex, whereas top-down input arrives primarily in superficial layers and layer 5 (refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Nassi, J. J. &amp; Callaway, E. M. Parallel processing strategies of the primate visual system. Nat. Rev. Neurosci. 10, 360372 (2009)." href="/articles/s41593-019-0428-x#ref-CR10" id="ref-link-section-d117493720e485">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Van Kerkoerle, T., Self, M. W. &amp; Roelfsema, P. R. Layer-specificity in the effects of attention and working memory on activity in primary visual cortex. Nat. Comm. 8, 13804 (2017)." href="/articles/s41593-019-0428-x#ref-CR11" id="ref-link-section-d117493720e488">11</a></sup>). When information from layer 4 is conveyed to the superficial layers, different populations of neurons might be recruited to keep bottom-up and top-down inputs anatomically segregated<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Miller, E. K., Li, L. &amp; Desimone, R. Activity of neurons in anterior inferior temporal cortex during a short-term memory task. J. Neurosci. 13, 14601478 (1993)." href="/articles/s41593-019-0428-x#ref-CR12" id="ref-link-section-d117493720e492">12</a></sup>. In addition, the format of the codes might differ, with bottom-up signals driving changes in spike rate and top-down signals modulating membrane potentials<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Mendoza-Halliday, D., Torres, S. &amp; Martinez-Trujillo, J. C. Sharp emergence of feature-selective sustained activity along the dorsal visual pathway. Nat. Neurosci. 17, 12551262 (2014)." href="/articles/s41593-019-0428-x#ref-CR7" id="ref-link-section-d117493720e496">7</a></sup>. Such a system could promote match detection via response gain when memory and sensory information are aligned<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Serences, J. T. Neural mechanisms of information storage in visual short-term memory. Vis. Res. 128, 5367 (2016)." href="/articles/s41593-019-0428-x#ref-CR13" id="ref-link-section-d117493720e500">13</a></sup>.</p></div></div></section><section data-title="Results"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Results</h2><div class="c-article-section__content" id="Sec2-content"><p>In Experiment 1, we evaluated the ability of early visual areas to act as a multiplexing comparison circuit during VWM. Participants performed a working memory task where they remembered randomly oriented visual gratings while looking at either a blank screen or a sequence of contrast-reversing visual distractors (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1a</a>). Each trial started with a 100% valid cue (1.4s) indicating the distractor condition during the subsequent delay. Next, a target orientation was shown for 0.5s and remembered throughout a 13-s delay. To ensure a relatively uniform sampling of orientation space, the target orientation was pseudo-randomly drawn from one of six orientation bins (each bin contained 30 orientations, in integer increments), with an equal number of draws from each bin. During the middle portion of the delay, participants viewed either a gray screen or an 11-s contrast-reversing distractor. Distractors were either a Fourier-filtered white noise stimulus (an example is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1a</a>a novel noise structure was generated on every trial) or an oriented grating with pseudo-random angular offset relative to the memory target orientation (its orientation was similarly drawn from one of six bins, counterbalanced with respect to the target orientation bin; see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig6">1a</a>). After the delay, participants had 3s to rotate a recall probe to match the remembered orientation as precisely as possible before continuing to the next trial. Although the presence or absence of distractors was fully predictable, distractors were irrelevant to the task and had no observable impact on behavior during fMRI scanning (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1b</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig7">2a</a>). As expected, distractors effectively drove a sustained and highly robust increase in the overall univariate response amplitude in V1 and other visual areas (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig8">3a</a>). Note that the distractors, when presented, function as visual masks. This was a deliberate choice, optimizing the design to be most favorable to the no-distractor condition (that is, a blank screen without any visual interference during the delay).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Experiment 1 paradigm and results."><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1: Experiment 1 paradigm and results.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="483"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p><b>a</b>, After a valid cue about the distractor condition (here, the blue fixation dot cued a noise distractor), a 0.5-s target orientation was remembered for 13s. During this delay, participants viewed a gray screen or an 11-s contrast-reversing distractor. Distractors could be a Fourier-filtered noise stimulus (depicted) or an oriented grating (its orientation was pseudo-randomly selected on every trial). After the delay, participants had 3s to rotate a recall probe to match the remembered orientation. Participants then continued to the next trial after an inter-trial interval (ITI) of 3, 5 or 8 s. <b>b</b>, There were no differences in behavioral error between the three distractor conditions, as indicated by a non-parametric one-way repeated-measures within-subject ANOVA (<i>F</i><sub>(2,10)</sub>=0.044; <i>P</i>=0.943). Gray lines indicate individual subjects. <b>c</b>, Model-based reconstructions of the remembered orientation during the three different distractor conditions (left) and of the physically present orientation on trials with a grating distractor (right). Reconstructions were based on the average activation patterns 5.613.6s after target onset. <b>d</b>, The degree to which memory and sensory stimuli were represented during the delay was quantified by projecting the channel response at each degree onto a vector centered on the true orientation (that is, zero), and taking the mean of all these projected vectors. Left: a cartoon reconstruction is defined by 18 points (note, in reality there were 180). Right: this cartoon reconstruction is wrapped onto a circle. We show for one point how the channel response (<i>h</i>) is projected onto the true orientation (remembered or sensed), resulting in vector <i>b</i>. Knowing the angle (<i>A</i>) between the true orientation and the orientation at this particular point, we solve for <i>b</i> using trigonometric ratios for right triangles (that is, <span class="mathjax-tex">\(\cos A = b/h\)</span>). The mean of all projected vectors (all <i>b</i>) indexes the amount of information at the true orientation and is our metric for reconstruction fidelity (in arbitrary units, a.u.). <b>e</b>, Reconstruction fidelity for remembered (shades of teal) and sensed distractor (gray) orientations is significantly above chance in almost all ROIs (based on one-sided randomization tests comparing fidelity in each condition and ROI to zero; see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-019-0428-x#Sec4">Methods</a>). Black asterisks next to ROI names (under the <i>x</i> axis) indicate significant differences in memory fidelity between the three distractor conditions in that ROI, as determined by non-parametric one-way repeated-measures within-subjects ANOVA analyses performed separately for each ROI (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-019-0428-x#Sec4">Methods</a>; for exact <i>P</i> values and post hoc tests, see Supplementary Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">1</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">2</a>). *<i>P</i>&lt;0.05, **<i>P</i>&lt;0.01, ***<i>P</i>&lt;0.001 (uncorrected for multiple comparisons). Dots indicate individual subject fidelities in each condition and ROI. For <b>b</b>, <b>c</b> and <b>e</b>, error bars/areas represent1 within-subject s.e.m. around the average across <i>n</i>=6 independent subjects.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Next, using a multivariate inverted encoding model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Brouwer, G. J. &amp; Heeger, D. J. Decoding and reconstructing color from responses in human visual cortex. J. Neurosci. 29, 1399214003 (2009)." href="/articles/s41593-019-0428-x#ref-CR14" id="ref-link-section-d117493720e637">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Sprague, T. C., Saproo, S. &amp; Serences, J. T. Visual attention mitigates information loss in small- and large-scale neural codes. Trends Cogn. Sci. 19, 215226 (2015)." href="/articles/s41593-019-0428-x#ref-CR15" id="ref-link-section-d117493720e640">15</a></sup> (IEM, see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig9">4</a>) trained on independent data, we generated model-based reconstructions of the remembered orientation from delay-period activity patterns in primary visual cortex (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1c</a>, left panel), and all other retinotopic areas that we mapped along the visual hierarchy (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig10">5a</a>), irrespective of whether a distractor was present during the delay. The baseline offset observed between distractor conditions (vertical shift up or down the <i>y</i> axis in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1c</a>) largely reflects the univariate effect of distractor presence during the delay interval, with higher baselines during trials with distractors (see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig8">3a</a>). As a measure of reconstruction fidelity, we projected the channel response at each degree in orientation space onto the remembered orientation and then took the mean of these projected vectors (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1d</a>). Fidelity was significantly above zero, indicating that there was information about the remembered orientation during all distractor conditions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1e</a>, teal bars) and in every retinotopic region of interest (ROI), except areas in the intraparietal sulcus (IPS). Note that the independent data used to train the IEM were collected while participants directly viewed orientation stimuli. Thus, generalization from sensory evoked response patterns to memory-related response patterns during the delay epoch implies that mnemonic information was represented in a sensory-like format.</p><p>We were also able to reconstruct the orientation of the distractor that was physically present on the screen during grating-distractor trials (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1c</a>, right panel and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1e</a>, gray bars), using the exact same delay-period data. This demonstrates that, contrary to simple feedforward models that posit V1 as a passive filter, early visual cortex can represent incoming sensory information alongside mnemonic information that is no longer tethered to external inputs. While the fidelity of remembered and sensed orientations was roughly equivalent in V1V2 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1e</a>, compare mid-teal bars and gray bars), the fidelity of the sensed distractor grating dropped against the fidelity of the remembered grating when ascending the visual hierarchy to V3V4 (interaction: <i>F</i><sub>(4,20)</sub>=5.67, <i>P</i>=0.002; note that this analysis does not include IPS and LO1, as their relative hierarchical relationships are less clear). This finding captures the top-down nature of mnemonic signals, as top-down signals are thought to have more traction than bottom-up sensory inputs in higher-level regions.</p><p>Notably, timepoint-by-timepoint reconstructions revealed that remembered and perceived representations evolved together over time in primary visual cortex (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig2">2</a>), indicating that these representations coexisted throughout most of the delay period. This was also true for other early retinotopic areas along the visual hierarchy, but not for later retinotopic IPS areas (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig11">6a</a>). Note that claims about the coexistence of information are limited by the temporal resolution of our measurement (that is, a repetition time (TR) of 800ms). The notion of a comparison circuit at the level of early sensory cortex was further supported by a boost in representational quality when target and distractor orientations were similar, compared to dissimilar (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig12">7</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig13">8b</a>). This finding is mirrored by behavioral demonstrations showing higher fidelity memory recall for similar targets and distractors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Rademaker, R. L., Bloem, I. M., De Weerd, P. &amp; Sack, A. S. The impact of interference on short-term memory for visual orientation. J. Exp. Psychol. Hum. Percept. Perform. 41, 16501665 (2015)." href="/articles/s41593-019-0428-x#ref-CR16" id="ref-link-section-d117493720e703">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Wildegger, T., Meyers, N. E., Humphreys, G. &amp; Nobre, A. C. Supraliminal but not subliminal distracters bias working memory recall. J. Exp. Psychol. Hum. Percept. Perform. 41, 826839 (2015)." href="/articles/s41593-019-0428-x#ref-CR17" id="ref-link-section-d117493720e706">17</a></sup> (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig14">9b</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Model-based reconstructions of remembered orientations and sensed distractor orientations over time in V1."><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2: Model-based reconstructions of remembered orientations and sensed distractor orientations over time in V1.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="173"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p><b>a</b>, The time axis starts at 0, which is trial onset, and each slice shows the mean reconstruction across participants at each 800ms TR (for a total of 21 TRs). Reconstructions for the remembered orientation are shown in the three left-most panels (shades of teal), and sensed distractor orientation reconstructions are shown in the right-most panel (gray). <b>b</b>, The fidelity of timepoint-by-timepoint reconstructions in V1 (quantification of <b>a</b>), with time 0 representing the target onset. The three gray background panels represent the target, distractor, and recall epochs of the working memory trial. Small, medium, and large dots at the bottom indicate significance at each time point at <i>P</i>&lt;0.05, <i>P</i>&lt;0.01 and <i>P</i>&lt;0.001, respectively (based on one-sided randomization tests comparing fidelity in each condition and at each timepoint to zero, uncorrected for multiple comparisons; see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-019-0428-x#Sec4">Methods</a>). Shaded error areas represent  1 within-subject s.e.m. around the average across <i>n</i>=6 independent subjects.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>In a second experiment we evaluated the impact of more naturalistic distractors, namely, face and gazebo pictures (after ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e751">6</a></sup>). Moreover, instead of contrast-reversing our visual distractors (as in Experiment 1), we flickered distractors on (250ms) and off (250ms) for 11s during the delay, and randomized their spatial phase on each on cycle, to maximize the unpredictability of contrast changes at every pixel. The task structure (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3a</a>) was similar to that of Experiment 1, with participants again remembering a pseudo-randomly oriented grating while ignoring other inputs during the delay. A 100% predictable cue (1.4s) was presented before the to-be-remembered memory target (0.5s), with the cue indicating one of three possible events during the 12s delay: no distractor (that is, blank screen), an 11s grating distractor (with pseudo-random angular offset relative to the target orientation, see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig6">1b</a>) or 11s of picture distractors (example shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3a</a>). Participants had 4s to report the remembered orientation as precisely as possible by rotating a dial. Distractors drove robust univariate responses in all of our ROIs (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig8">3b</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Experiment 2 paradigm and results."><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3: Experiment 2 paradigm and results.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="494"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p><b>a</b>, Irrelevant distractors were cued with 100% validity by a change in fixation color (here blue indicated that picture distractors would be shown during the delay) before a 500ms target presentation. Participants remembered the target orientation for 12s, while they viewed either a gray screen or an 11-s onoff flickering distractor (a pseudo-randomly oriented grating, or pictures of faces or gazebos). After the memory delay, participants rotated a dial to match the remembered orientation. Photo used with permission (S. Itthipuripat). <b>b</b>, Distractor presence negatively affected behavioral performance, as indicated by a non-parametric one-way repeated-measures within-subjects ANOVA (<i>F</i><sub>(2,12)</sub>=10.154; <i>P</i>&lt;0.001). Errors were smaller when no distractor was shown during the delay, compared to when distractor gratings (<i>t</i><sub>(6)</sub>=6.272; <i>P</i>&lt;0.001) or pictures (<i>t</i><sub>(6)</sub>=3.375; <i>P</i>=0.018) were shown. Performance did not differ between grating and picture distractors (<i>t</i><sub>(6)</sub>=1.184; <i>P</i>=0.184). Post hoc tests were non-parametric uncorrected paired-sample <i>t</i>-tests. Gray lines indicate individual subjects. <b>c</b>, Model-based reconstructions of the remembered orientation during the three different distractor conditions (left) and of the sensed distractor orientation on trials with a grating distractor (right). Reconstructions were generated with an IEM trained on independent localizer data and based on the average activation patterns 5.613.6s after target onset. <b>d</b>, Reconstruction fidelity for remembered orientations without distraction (dark teal) and for sensed distractor orientations (gray) is significantly above zero in all ROIs except IPS0 and IPS1 (based on one-sided randomization tests in each condition and ROI; see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-019-0428-x#Sec4">Methods</a>). However, reconstruction fidelity is less robust when a distractor was presented throughout the delay (mid-teal and yellow for grating and picture distractors, respectively). Black asterisks next to ROI names indicate significant differences in memory fidelity during the three distractor conditions in that ROI, as determined by non-parametric one-way repeated-measures within-subjects ANOVA analyses performed separately for each ROI (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-019-0428-x#Sec4">Methods</a>; for exact <i>P</i> values and post hoc tests, see Supplementary Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">3</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">4</a>). Dots indicate individual subject fidelities in each condition and ROI. <b>e</b>, The fidelity of timepoint-by-timepoint reconstructions in V1. Time 0 represents target onset, and the three gray panels represent the target, distractor, and recall epochs of the working memory trial. Small, medium or large dots at the bottom of <b>e</b> indicate significance levels of *<i>P</i>&lt;0.05, **<i>P</i>&lt;0.01 or ***<i>P</i>&lt;0.001, respectively (uncorrected). For <b>b</b><b>e</b>, error bars and shaded areas represent  1 within-subject s.e.m. around the average across <i>n</i>=7 independent subjects.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Notably, the presence of distractors in Experiment 2 negatively affected behavioral performance during fMRI scanning (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3b</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig7">2b</a>). This drop in behavioral performance was accompanied by qualitatively poorer memory reconstructions when distractors were presented during the delay (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3c</a>, left panel and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig10">5b</a>). Memory fidelity in V1V4 and LO1 was reduced when grating and picture distractors were shown during the delay, compared to fidelity without distractors (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3d</a>). Alongside these (reduced) memory representations, the directly sensed distractor orientation was represented in a robust manner (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3c</a>, right panel and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3d</a>, gray bars), as were the directly sensed picture distractors (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig15">10</a>). As in Experiment 1, the IEM was trained on independent data, collected while participants directly viewed oriented gratings. Generalization to data from the memory delay implies that a sensory-like code is used to represent mnemonic information, and that this representation is less robust when people are distracted by visual inputs during the delay. Note that in IPS0 and IPS1 there was no evidence of mnemonic information, nor did these regions represent the sensed distractor grating, indicating that these areas do not represent information in a manner that is generalizable from directly viewed sensory inputs.</p><p>A direct comparison between the remembered and sensed orientations on trials with a grating distractor (compare mid-teal bars and gray bars in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3d</a>) again revealed a relative increase in mnemonic compared to sensed information when ascending the visual hierarchy (interaction: <i>F</i><sub>(4,24)</sub>=7.418, <i>P</i>=0.001)a hallmark of top-down processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Silver, M. A., Ress, D. &amp; Heeger, D. J. Topographic maps of visual spatial attention in human parietal cortex. J. Neurophysiol. 94, 13581371 (2005)." href="#ref-CR18" id="ref-link-section-d117493720e903">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Serences, J. T. &amp; Yantis, S. Selective visual attention and perceptual coherence. Trends Cogn. Sci. 10, 3845 (2006)." href="#ref-CR19" id="ref-link-section-d117493720e903_1">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Poltoratski, S., Ling, S., McCormack, D. &amp; Tong, F. Characterizing the effects of feature salience and top-down attention in the early visual system. J. Neurophysiol. 118, 564573 (2017)." href="#ref-CR20" id="ref-link-section-d117493720e903_2">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Sprague, T. C., Itthipuripat, S., Vo, V. A. &amp; Serences, J. T. Dissociable signatures of visual salience and behavioral relevance across attentional priority maps in human cortex. J. Neurophysiol. 119, 21532165 (2018)." href="/articles/s41593-019-0428-x#ref-CR21" id="ref-link-section-d117493720e906">21</a></sup>. Timepoint-by-timepoint analyses (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3e</a>) showed sustained memory representations throughout the delay when there was no distractor present, while memory representations were less sustained in the presence of visual distraction. Note that this did not hold true for IPS, where there was no evidence for representations of either the remembered orientation or the sensed distractor orientation, sustained or otherwise (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig11">6b</a>).</p><p>In the analyses presented thus far, we trained the IEM solely on data from independent sensory localizers, demonstrating that visual areas up to IPS encode remembered features in a sensory-like format. Here, sensory-like refers to a format akin to that of a stimulus-driven sensory response. An independent sensory localizer makes no demands on memory, and gives rise to information in a stimulus-driven format. However, not all mnemonic signals are necessarily stored in this format, and might also be stored in a format that is somehow transformed. For example, pixel-by-pixel representations in early visual cortex might undergo some dimensionality reduction in upstream cortical sites<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Murray, J. D. et al. Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex. Proc. Natl Acad. Sci. USA 114, 394399 (2017)." href="/articles/s41593-019-0428-x#ref-CR22" id="ref-link-section-d117493720e920">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="DiCarlo, J. J., Zoccolan, D. &amp; Rust, N. C. How does the brain solve visual object recognition? Neuron 73, 415434 (2012)." href="/articles/s41593-019-0428-x#ref-CR23" id="ref-link-section-d117493720e923">23</a></sup>. To look for a mnemonic code that is not necessarily sensory-like, we trained the IEM on data from the memory delay via a leave-one-out procedure (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-019-0428-x#Sec4">Methods</a>). In both Experiments 1 and 2 we saw robust VWM representations, despite visual distraction, in all retinotopic ROIs, including IPS0 and IPS1 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig4">4</a> and Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig16">11</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig17">12</a>), and including more anterior and non-visually responsive IPS ROIs (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig18">13</a>). Despite the ubiquity of mnemonic information in IPS revealed by this analysis, there was still no apparent information about the directly sensed distractor grating (gray bars) in IPS (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig4">4</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig18">13</a>). Taken together, this implies that IPS does represent mnemonic information, and that it uses a code that is transformed away from the stimulus-driven driven response. Furthermore, representations in IPS were impervious to visual distraction, with equivalent memory fidelity during all distractor conditions even in Experiment 2 (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig18">13</a>), despite differences at the behavioral level (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3b</a>). Thus, representations in V1V4 and LO1 (lower fidelity during visual distraction), but not IPS (stable fidelity), mirrored how well people did on a VWM task.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Reconstruction fidelity when training and testing an IEM on data from the memory delay."><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4: Reconstruction fidelity when training and testing an IEM on data from the memory delay.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="233"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p><b>a</b>,<b>b</b>, Experiment 1 (<b>a</b>) and Experiment 2 (<b>b</b>). There are robust memory representations throughout the visual hierarchy, including retinotopic IPS. This implies that the representational format in IPS is not in a stimulus-driven format. The proposed transformed nature of the IPS code is also supported by the lack of information about the directly sensed grating distractor (gray bars). As before, differences in memory fidelity between the three distractor conditions (black asterisks next to ROI names) were virtually absent in Experiment 1 (<b>a</b>; for exact <i>P</i> values and post hoc tests see Supplementary Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">5</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">6</a>), while in Experiment 2 the presence of distractors was accompanied by a drop in memory fidelity in many ROIs (<b>b</b>; for exact <i>P</i> values and post hoc tests see Supplementary Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">7</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">8</a>). Note, however, that mnemonic representations in IPS were unaffected by visual distraction (see also Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig18">13</a>). *<i>P</i>&lt;0.05, **<i>P</i>&lt;0.01, ***<i>P</i>&lt;0.001. Dots indicate individual subject fidelities in each condition and ROI. Error bars represent 1 within-subject s.e.m. (for <i>n</i>=6 and <i>n</i>=7 independent subjects in <b>a</b> and <b>b</b>, respectively). Statistical testing was identical to that in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1e</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3d</a>. When ascending the visual hierarchy from V1 to V4, a weakening sensory representation paired with a strengthening mnemonic representation illustrates the top-down nature of VWM (compare gray and mid-teal bars). This signature interaction was present in both Experiment 1 (<i>F</i><sub>(4,20)</sub>=13.6, <i>P</i>&lt;0.001) and Experiment 2 (<i>F</i><sub>(4,24)</sub>=7.769, <i>P</i>&lt;0.001).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Note that this analysis does not necessarily speak to the representational format in early visual areas V1V4 or LO1. While generalization from independent sensory data demonstrated a sensory-like mnemonic format, generalization within the memory delay does not by definition indicate a non-stimulus-driven format. After all, training and testing on a sensory-like mnemonic code would yield robust reconstructions as long as there was information present. Thus, we can only ascertain the presence of non-stimulus-driven transformed codes in IPS, as uncovering mnemonic information in IPS is only possible after training the IEM on memory (and not sensory) data.</p><p>Finally, none of the findings reported here depend on our choice of analysis approach (IEM model), and conventional decoding analyses yield similar patterns of results (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig5">5</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Decoding analyses yield highly comparable results to the IEM analyses."><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5: Decoding analyses yield highly comparable results to the IEM analyses.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="559"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p><b>a</b>, In Experiments 1 and 2 we used random orientations (1180), while relevant previous work has used orthogonal orientations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Harrison, S. A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. Nature 458, 632635 (2009)." href="/articles/s41593-019-0428-x#ref-CR1" id="ref-link-section-d117493720e1065">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e1068">6</a></sup>. To closely mimic the two-way classification performed in previous work, we divided our random orientations into four bins and performed two two-way classifications. The first classification determined whether orientations were around vertical (between 157.5 and 22.5) or horizontal (between 67.5 and 112.5), shown schematically in the left diagram. The second classification determined whether orientations were around one or the other obliques (that is, between 22.5 and 67.5 or between 112.5 and 157.5), shown schematically in the right diagram. Decoding performance was averaged across these two-way classifications to yield an overall classification accuracy for each ROI. For all decoding analyses we ensured balanced training sets. <b>b</b>, We trained the SVM on independent data from the visual mapping tasks. Results mirrored those from the IEM analyses. In Experiment 1 (top) we found above chance decoding in V1V4 and LO1, but not IPS0 and IPS1. There were no differences between the three distractor conditions in any of the ROIs (all <i>F</i><sub>(5,10)</sub>&lt;1.024, all <i>P</i>&gt;0.429). Also, in Experiment 2 (bottom) there was little above-chance decoding in IPS regions. In V1V4 and LO1, memory decoding in Experiment 2 differed between the three distractor conditions (all <i>F</i><sub>(5,10)</sub>&gt;10.419, all <i>P</i>&lt;0.004) and was generally better when no visual distraction was presented during the delay, compared to delays with a grating or a picture distractor. In both Experiments 1 and 2, the grating-distractor condition revealed an interaction between remembered and sensed representations (compare mid-teal and gray bars), considered a signature of top-down processing (<i>F</i><sub><i>(</i>4,20)</sub>=2.469, <i>P</i>=0.046 and <i>F</i><sub>(4,24)</sub>=3.198, <i>P</i>=0.024, respectively). <b>c</b>, We also trained the SVM on data from the memory delay via a leave-one-out cross-validation procedure. This led to robust decoding of mnemonic information in IPS0 and IPS1 for both Experiments 1 (top) and 2 (bottom), indicating a non-stimulus-driven mnemonic code in these areas. Lack of information about the ignored sensory distractor orientation (gray bars) further corroborates that IPS uses non-stimulus-driven codes to represent task-relevant information. In Experiment 1 (top) the three distractor conditions differed in V1 and LO1 (<i>F</i><sub>(2,10)</sub>=3.517, <i>P</i>=0.045 and <i>F</i><sub>(2,10)</sub>=12.723, <i>P</i>=0.003, respectively) but not in any other ROIs (all <i>F</i><sub>(2,10)</sub>&lt;1.062, all <i>P</i>&gt;0.386). In Experiment 2 (bottom) the three distractor conditions differed in almost all ROIs (V1IPS0, all <i>F</i><sub>(2,12)</sub>&gt;5.399, all <i>P</i>&lt;0.022;). Again, both Experiments 1 and 2 revealed an interaction between remembered and sensed representations (compare mid-teal and gray bars) in the grating-distractor condition (<i>F</i><sub>(4,20)</sub>=11.499, <i>P</i>&lt;0.001 and <i>F</i><sub>(4,24)</sub>=3.331, <i>P</i>=0.029, respectively). For both <b>b</b> and <b>c</b>, statistical testing was identical to that in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1e</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3d</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig4">4</a> except that randomization tests were against chance (0.5) instead of zero (see also <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-019-0428-x#Sec4">Methods</a>). *<i>P</i>&lt;0.05, **<i>P</i>&lt;0.01, ***<i>P</i>&lt;0.001. Dots indicate individual subject decoding in each condition and ROI. Error bars represent 1 within-subject s.e.m. (for <i>n</i>=6 and <i>n</i>=7 independent subjects in Experiments 1 and 2, respectively).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-019-0428-x/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Discussion</h2><div class="c-article-section__content" id="Sec3-content"><p>Visual information held in mind to attain behavioral goals should withstand interference from ongoing sensory inputs. In Experiment 1, we demonstrated that recall of an orientation was unimpeded by irrelevant visual distractors, and that the fidelity of mnemonic representations in visual cortex was similar with and without distractors. By contrast, participants in Experiment 2 did get distracted, showing impairments at both the behavioral and neural level. Participants viewed noise distractors in Experiment 1, and picture distractors (faces and gazebos) in Experiment 2. Different distractor types might result in different degrees of distractibility, which could explain the discrepancy between the two experimental outcomes. However, grating distractors were shown during both experiments, and even for this shared condition there was a drop in the fidelity of behavioral and brain responses in Experiment 2. This was also true for the three participants who completed both experiments and had not been affected by distractors in Experiment 1. Instead of distractor type, a likely variable causing the differences in distractibility is the intensity of the distractors, namely, whether they were contrast-reversing (Experiment 1) or flickering on and off (Experiment 2). Integrated over a contrast-reversal cycle, the contrast at every pixel is always mean gray. In comparison, flickering between a grating with a random phase (on) and a mean gray screen (off) results in contrast fluctuations from cycle to cycle, and thus a stronger temporal gradient of change at every pixel.</p><p>A previous paper reported one experiment implying that task-irrelevant visual distraction wiped out mnemonic representations in V1 at no behavioral cost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e1203">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Ester, E. F., Rademaker, R. L. &amp; Sprague, T. S. How do visual and parietal cortex contribute to visual short-term memory? eNeuro 3, e004116 (2016). 2016 13." href="/articles/s41593-019-0428-x#ref-CR9" id="ref-link-section-d117493720e1206">9</a></sup>. Our results from Experiment 2 are reminiscent of this finding, where distractor presence caused a marked drop in mnemonic information in early retinotopic areas. However, information did not generally dissipate altogether (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3d</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig4">4b</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig5">5</a> and Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig10">5b</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig11">6b</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig16">11b</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">12b</a>). This reduction of information was mirrored by a clear decline in behavioral performance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3b</a>). Previous failures to uncover behavioral effects when only a single feature is remembered can be readily explained by the need for adequate statistical power in paradigms where memory fidelity tends to be high<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Rademaker, R. L., Park, Y. E., Sack, A. T. &amp; Tong, F. Evidence of gradual loss of precision for simple features and complex objects in visual working memory. J. Exp. Psychol. Hum. Percept. Perform. 44, 925940 (2018)." href="/articles/s41593-019-0428-x#ref-CR24" id="ref-link-section-d117493720e1235">24</a></sup>. Instead of a traditional DMTS task with only two answer alternatives, the recall procedure we used here allowed a much more sensitive detection of numerically small behavioral effect. Thus, differences in distractor intensity, as well as a behavioral paradigm that lacks sensitivity, can account for the biggest discrepancies between previous work<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e1239">6</a></sup> and our current findings.</p><p>The coexistence of bottom-up information about current sensory inputs, combined with feature-selective top-down inputs that carry information about remembered items, could provide a powerful local mechanism for comparing memory contents to the sensory environment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Miller, E. K., Li, L. &amp; Desimone, R. Activity of neurons in anterior inferior temporal cortex during a short-term memory task. J. Neurosci. 13, 14601478 (1993)." href="/articles/s41593-019-0428-x#ref-CR12" id="ref-link-section-d117493720e1246">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Bisley, J. W., Zaksas, D., Droll, J. A. &amp; Pasternak, T. Activity of neurons in cortical area MT during a memory for motion task. J. Neurophysiol. 91, 286300 (2004)." href="/articles/s41593-019-0428-x#ref-CR25" id="ref-link-section-d117493720e1249">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Zaksas, D. &amp; Paternak, T. Direction signals in the prefrontal cortex and in area MT during a working memory for visual motion task. J. Neurosci. 26, 1172611742 (2006)." href="/articles/s41593-019-0428-x#ref-CR26" id="ref-link-section-d117493720e1252">26</a></sup>. To support this functionality, sensory and memory information could be multiplexed by different populations of neurons<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Miller, E. K., Li, L. &amp; Desimone, R. Activity of neurons in anterior inferior temporal cortex during a short-term memory task. J. Neurosci. 13, 14601478 (1993)." href="/articles/s41593-019-0428-x#ref-CR12" id="ref-link-section-d117493720e1256">12</a></sup>. For example, if neurons tuned to the features of a memory target were selectively activated during the delay, the output of a local comparison circuit would be relatively high when a matching test stimulus was encountered that selectively excited similarly tuned neurons. On the other hand, a mismatch test stimulus would drive a set of differently tuned neurons, which may lead to a lower overall response due to inhibitory competition with the already active neurons tuned to the sample feature<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Serences, J. T. Neural mechanisms of information storage in visual short-term memory. Vis. Res. 128, 5367 (2016)." href="/articles/s41593-019-0428-x#ref-CR13" id="ref-link-section-d117493720e1260">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Gayet, S. et al. Visual working memory enhances the neural response to matching visual input. J. Neurosci. 37, 66386647 (2017)." href="/articles/s41593-019-0428-x#ref-CR27" id="ref-link-section-d117493720e1263">27</a></sup>. The same logic also applies if the top-down modulations supporting VWM do not lead to sustained patterns of spiking in sensory cortices, and instead only influence sub-threshold potentials<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Mendoza-Halliday, D., Torres, S. &amp; Martinez-Trujillo, J. C. Sharp emergence of feature-selective sustained activity along the dorsal visual pathway. Nat. Neurosci. 17, 12551262 (2014)." href="/articles/s41593-019-0428-x#ref-CR7" id="ref-link-section-d117493720e1267">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Merrikhi, Y. et al. Spatial working memory alters the efficacy of input to visual cortex. Nat. Comms. 8, 15041 (2017)." href="/articles/s41593-019-0428-x#ref-CR28" id="ref-link-section-d117493720e1270">28</a></sup>. Differences in the local comparison circuit output would still be expected due to interactions between the top-down feature-selective bias and the sensory response evoked by the test stimulus. Moreover, content-specific patterns of sub-threshold membrane potentials could persevere through bouts of local spiking driven by sensory inputs during a delay, and thus protect mnemonic contents from distraction<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Serences, J. T. Neural mechanisms of information storage in visual short-term memory. Vis. Res. 128, 5367 (2016)." href="/articles/s41593-019-0428-x#ref-CR13" id="ref-link-section-d117493720e1274">13</a></sup>. Because fMRI measures an aggregate of signals (that is, spikes, local field potentials and so on), here we can only speculate about the exact nature of this comparison circuit and cannot draw inferences about single neurons, the likely scale of anatomical separation, or precise temporal integration of the comparison circuit.</p><p>Here, we find distractor-resistant mnemonic representations throughout the delay in early visual cortex (Experiment 1), whereas classic single-neuron physiology has generally found that mnemonic representations in later stages of visual cortex are disrupted by visual transients<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Miller, E. K., Li, L. &amp; Desimone, R. Activity of neurons in anterior inferior temporal cortex during a short-term memory task. J. Neurosci. 13, 14601478 (1993)." href="/articles/s41593-019-0428-x#ref-CR12" id="ref-link-section-d117493720e1281">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Miller, E. K., Li, L. &amp; Desimone, R. A neural mechanism for working and recognition memory in inferior temporal cortex. Science 254, 13771379 (1991)." href="/articles/s41593-019-0428-x#ref-CR29" id="ref-link-section-d117493720e1284">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Maunsell, J. H. R., Sclar, G., Nealey, T. A. &amp; DePriest, D. D. Extraretinal representations in area V4 in the macaque monkey. Vis. Neurosci. 7, 561573 (1991)." href="/articles/s41593-019-0428-x#ref-CR30" id="ref-link-section-d117493720e1287">30</a></sup>. For example, when monkeys viewed a target image and subsequently looked for matches in a series of test images, neuronal responses in inferior temporal cortex signaled an active memory trace via enhanced firing for matches<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Miller, E. K. &amp; Desimone, R. Parallel neuronal mechanisms for short-term memory. Science 263, 520522 (1994)." href="/articles/s41593-019-0428-x#ref-CR31" id="ref-link-section-d117493720e1291">31</a></sup>. However, this memory signal did not bridge the intervening delays between test images. By contrast, delay activity in prefrontal cortex survived intervening test stimuli and was maintained during each delay<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Miller, E. K., Erickson, C. A. &amp; Desimone, R. Neural mechanisms of visual working memory in prefrontal cortex of the macaque. J. Neurosci. 16, 51545167 (1996)." href="/articles/s41593-019-0428-x#ref-CR32" id="ref-link-section-d117493720e1295">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Jacob, S. N. &amp; Nieder, A. Complementary roles for primate frontal and parietal cortex in guarding working memory from distractor stimuli. Neuron 83, 226237 (2014)." href="/articles/s41593-019-0428-x#ref-CR33" id="ref-link-section-d117493720e1298">33</a></sup>. Note how this task, unlike ours, required the animal to perform a matching operation on each intervening stimulus, and thus each distractor was actually a behaviorally relevant image that required attentive processing. In a set of studies more directly comparable to our experiments, monkeys had to mentally trace a curved line that was no longer in view, which led to sustained delay-period spiking in V1. Spiking was briefly interrupted by an irrelevant mask, but reinstated soon thereafter<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Van Kerkoerle, T., Self, M. W. &amp; Roelfsema, P. R. Layer-specificity in the effects of attention and working memory on activity in primary visual cortex. Nat. Comm. 8, 13804 (2017)." href="/articles/s41593-019-0428-x#ref-CR11" id="ref-link-section-d117493720e1302">11</a></sup>. Thus, the status of a distractor as relevant or irrelevant might play an important role in how memories are maintained. Also, different memory contents (that is, highly familiar categorical objects versus fine-grained line orientations) might require different levels of representational precision.</p><p>Prefrontal and parietal cortices play a central role in maintaining distractor-resistant memory representations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Miller, E. K., Erickson, C. A. &amp; Desimone, R. Neural mechanisms of visual working memory in prefrontal cortex of the macaque. J. Neurosci. 16, 51545167 (1996)." href="#ref-CR32" id="ref-link-section-d117493720e1310">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Jacob, S. N. &amp; Nieder, A. Complementary roles for primate frontal and parietal cortex in guarding working memory from distractor stimuli. Neuron 83, 226237 (2014)." href="#ref-CR33" id="ref-link-section-d117493720e1310_1">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Qi, X.-L., Elworthy, A. C., Lambert, B. C. &amp; Constantinidis, C. Representation of remembered stimuli and task information in the monkey dorsolateral prefrontal and posterior parietal cortex. J. Neurophysiol. 113, 4457 (2015)." href="/articles/s41593-019-0428-x#ref-CR34" id="ref-link-section-d117493720e1313">34</a></sup>, and feedback from these regions likely supports the persistent mnemonic representations in early visual cortex found here. Early retinotopic representations were sensory-like in nature, as evidenced by the generalization from independent sensory data (used to train our multivariate models) to delay epoch data. A sensory-like format would indeed be well suited for a local comparison circuit, readily able to contrast mnemonic information and ongoing sensory inputs. By contrast, sensory information did not generalize to the memory delay in IPS. Instead, only training and testing on data obtained during the memory delay itself revealed information in IPS. This indicates a code that is transformed away from a purely stimulus-driven format. The notion of such a non-stimulus-driven code in IPS is further supported by the absence of information about the directly sensed grating distractor. Maintaining multiple replicas of a remembered sensory stimulus at all cortical levels would be computationally expensive and inefficient. Instead, high-resolution pixel-by-pixel representations might be condensed into stable and low dimensional representations in higher cortical regions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Murray, J. D. et al. Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex. Proc. Natl Acad. Sci. USA 114, 394399 (2017)." href="/articles/s41593-019-0428-x#ref-CR22" id="ref-link-section-d117493720e1317">22</a></sup>. Accordingly, these stable and potentially compressed representations might not support high-fidelity mnemonic information. Indeed, behavioral performance in Experiment 2 was impaired in the same conditions where representations in early visual areas were disrupted, while mnemonic representations in IPS remained intact. Of course, our failure to find sensory-like representations in IPS does not mean they do not exist there. For one, IPS does not have orientation columns in the same way that early retinotopic regions do, which could impede our ability to pick up on macroscopic information at the voxel level. Moreover, participants did not attend the orientations of the sensory stimuli used to train our model (instead, they performed an orthogonal task). A confluence of both perception and attention might be required to get reliable sensory responses from IPS<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Silver, M. A. &amp; Kastner, S. Topographic maps in human frontal and parietal cortex. Trends Cogn. Sci. 13, 488495 (2009)." href="/articles/s41593-019-0428-x#ref-CR35" id="ref-link-section-d117493720e1321">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Bressler, D. W. &amp; Silver, M. A. Spatial attention improves reliability of fMRI retinotopic mapping signals in occipital and parietal cortex. Neuroimage 53, 526533 (2010)." href="/articles/s41593-019-0428-x#ref-CR36" id="ref-link-section-d117493720e1324">36</a></sup>. Even though we cannot exclude the possibility of a mnemonic code in IPS that reflects stimulus-driven responses, our data do demonstrate that a transformed non-stimulus-driven code exists in IPS.</p><p>Previous studies have shown that there are interactions between remembered and seen stimuli, such as interference by<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Rademaker, R. L., Bloem, I. M., De Weerd, P. &amp; Sack, A. S. The impact of interference on short-term memory for visual orientation. J. Exp. Psychol. Hum. Percept. Perform. 41, 16501665 (2015)." href="/articles/s41593-019-0428-x#ref-CR16" id="ref-link-section-d117493720e1331">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Wildegger, T., Meyers, N. E., Humphreys, G. &amp; Nobre, A. C. Supraliminal but not subliminal distracters bias working memory recall. J. Exp. Psychol. Hum. Percept. Perform. 41, 826839 (2015)." href="/articles/s41593-019-0428-x#ref-CR17" id="ref-link-section-d117493720e1334">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Deutsch, D. Tones and numbers: specificity of interference in immediate memory. Science 168, 16041605 (1970)." href="#ref-CR37" id="ref-link-section-d117493720e1337">37</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Deutsch, D. Interference in memory between tones adjacent in the musical scale. J. Exp. Psychol. 100, 228231 (1973)." href="#ref-CR38" id="ref-link-section-d117493720e1337_1">38</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Magnussen, S., Greenlee, M. W., Asplund, R. &amp; Dyrnes, S. Stimulus-specific mechanisms of visual short-term memory. Vis. Res. 31, 12131219 (1991)." href="#ref-CR39" id="ref-link-section-d117493720e1337_2">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Magnussen, S. &amp; Greenlee, M. W. Retention and disruption of motion information in visual short-term memory. J. Exp. Psychol. Learn. Mem. Cogn. 18, 151156 (1992)." href="#ref-CR40" id="ref-link-section-d117493720e1337_3">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Pasternak, T. &amp; Zaksas, D. Stimulus specificity and temporal dynamics of working memory for visual motion. J. Neurophysiol. 90, 27572762 (2003)." href="/articles/s41593-019-0428-x#ref-CR41" id="ref-link-section-d117493720e1340">41</a></sup> and attraction toward<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Rademaker, R. L., Bloem, I. M., De Weerd, P. &amp; Sack, A. S. The impact of interference on short-term memory for visual orientation. J. Exp. Psychol. Hum. Percept. Perform. 41, 16501665 (2015)." href="/articles/s41593-019-0428-x#ref-CR16" id="ref-link-section-d117493720e1344">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Wildegger, T., Meyers, N. E., Humphreys, G. &amp; Nobre, A. C. Supraliminal but not subliminal distracters bias working memory recall. J. Exp. Psychol. Hum. Percept. Perform. 41, 826839 (2015)." href="/articles/s41593-019-0428-x#ref-CR17" id="ref-link-section-d117493720e1347">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Van der Stigchel, S., Merten, H., Meeter, M. &amp; Theeuwes, J. The effects of a task-irrelevant visual event on spatial working memory. Psychon. Bull. Rev. 14, 10661071 (2007)." href="#ref-CR42" id="ref-link-section-d117493720e1350">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Huang, J. &amp; Sekuler, R. Distortions in recall from visual memory: two classes of attractors at work. J. Vis. 10, 127 (2010)." href="#ref-CR43" id="ref-link-section-d117493720e1350_1">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nemes, V. A., Parry, N. R., Whitaker, D. &amp; McKeefry, D. J. The retention and disruption of color information in human short-term visual memory. J. Vis. 12, 114 (2012)." href="#ref-CR44" id="ref-link-section-d117493720e1350_2">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bae, G. Y. &amp; Luck, S. J. Interactions between visual working memory representations. Atten. Percep. Psychophys. 79, 23762395 (2017)." href="#ref-CR45" id="ref-link-section-d117493720e1350_3">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lorenc, E. S., Sreenivasan, K. K., Nee, D. E., Vandenbroucke, A. R. E. &amp; DEsposito, M. Flexible coding of visual working memory representations during distraction. J. Neurosci. 38, 52675276 (2018)." href="#ref-CR46" id="ref-link-section-d117493720e1350_4">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Chunharas, C., Rademaker, R. L., Brady, T. F. &amp; Serences, J. T. Adaptive memory distortion in visual working memory. Preprint at PsyArXiv &#xA;                              https://psyarxiv.com/e3m5a/&#xA;                              &#xA;                            (2019)." href="/articles/s41593-019-0428-x#ref-CR47" id="ref-link-section-d117493720e1353">47</a></sup> irrelevant distractors (see also Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig14">9</a>). One recent fMRI study<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Lorenc, E. S., Sreenivasan, K. K., Nee, D. E., Vandenbroucke, A. R. E. &amp; DEsposito, M. Flexible coding of visual working memory representations during distraction. J. Neurosci. 38, 52675276 (2018)." href="/articles/s41593-019-0428-x#ref-CR46" id="ref-link-section-d117493720e1360">46</a></sup> looked at visual cortex representations of a remembered orientation in the delay period before and after a brief (0.5s) irrelevant grating. The irrelevant grating always differed 4050 from the target orientation. In the delay before the irrelevant grating, the remembered orientation could be recovered from early visual areas V1V3 combined. In the delay after the irrelevant grating, the recovered orientation was shifted in the direction of the distractor, dovetailing with known behavioral attraction biases toward irrelevant orientations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Rademaker, R. L., Bloem, I. M., De Weerd, P. &amp; Sack, A. S. The impact of interference on short-term memory for visual orientation. J. Exp. Psychol. Hum. Percept. Perform. 41, 16501665 (2015)." href="/articles/s41593-019-0428-x#ref-CR16" id="ref-link-section-d117493720e1364">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Wildegger, T., Meyers, N. E., Humphreys, G. &amp; Nobre, A. C. Supraliminal but not subliminal distracters bias working memory recall. J. Exp. Psychol. Hum. Percept. Perform. 41, 826839 (2015)." href="/articles/s41593-019-0428-x#ref-CR17" id="ref-link-section-d117493720e1367">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Chunharas, C., Rademaker, R. L., Brady, T. F. &amp; Serences, J. T. Adaptive memory distortion in visual working memory. Preprint at PsyArXiv &#xA;                              https://psyarxiv.com/e3m5a/&#xA;                              &#xA;                            (2019)." href="/articles/s41593-019-0428-x#ref-CR47" id="ref-link-section-d117493720e1370">47</a></sup> (see also Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig14">9a</a>). However, this previous study only looked at memory representations before and after distraction, so nothing can be said about the joint representation of information. Furthermore, the target and distractor orientations were yoked together, so the representations associated with the target and the distractor could not be independently assessed. In the present work, we were able to detect biases toward irrelevant gratings during distraction (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig12">7</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig13">8a</a>) while using randomized targetdistractor differences (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig6">1</a>). However, when people remember an orientation while a grating distractor is presented, both the target and distractor orientations contribute to the measured brain response, which could explain such biases. At the single-trial level, the relative contributions of mnemonic and perceived signals to an IEM reconstruction can be hard to untangle. Nevertheless, the uncorrelated nature of target and distractor orientations allowed us to assess memory representations in the presence of an orientation distractor across many trials. The finding that the average fidelity of mnemonic representations in Experiment 1 were unaltered by concurrent sensory inputs can therefore not be an artifact of the distractor orientation. This is further supported by comparably durable memory representations in the presence of grating and noise distractors alikethe latter having no discernible orientation information.</p><p>What if, instead of coexisting mnemonic and sensory representations, people were exclusively representing either the target or the distractor orientation on some fraction of trials or time points within a trial? This alternative account is unlikely for several reasons. First, switching between representations would impose a drop in the representation of the memory target. No such drop from the no-distractor condition to the grating-distractor condition was observed in Experiment 1. Second, the 11s continuous presentation of distractors necessarily activates V1. Thus, while V1 is representing ongoing sensory inputs, mnemonic information can still be recovered at every TR throughout the delay.</p><p>Neuroimaging studies on working memory routinely use a retro-cue paradigm where two stimuli are presented in quick succession, followed by a numerical cue indicating which of the two to remember<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Harrison, S. A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. Nature 458, 632635 (2009)." href="/articles/s41593-019-0428-x#ref-CR1" id="ref-link-section-d117493720e1393">1</a></sup>. Using this paradigm, information can be decoded equally well when the first stimulus was cued instead of the second<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Harrison, S. A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. Nature 458, 632635 (2009)." href="/articles/s41593-019-0428-x#ref-CR1" id="ref-link-section-d117493720e1397">1</a></sup>, demonstrating a robustness to potential interference from the second stimulus. In Experiment 1, we extend this finding by showing that mnemonic representations persisted in the presence of visual masks shown for 11s (the distractors). Mnemonic representations were just as robust during distractor and no-distractor conditionsthe latter entirely without visual interference by deliberate omission of the retro-cue paradigm. Furthermore, mnemonic information could be recovered at every time point during the 13s delay (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig2">2</a>) despite the poorer signal-to-noise of single TR data (compared to data averaged over multiple TRs). Note that this timeframe far surpasses the duration of the stimulus-evoked blood-oxygen-level-dependent (BOLD) response. A comprehensive body of work has shown that stimulus-evoked BOLD alone is generally insufficient for stimulus information to persist into the working memory delay. For example, when people make their response immediately after a retro-cue stimulus sequence, instead of after a long delay, the cued target cannot be decoded<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Harrison, S. A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. Nature 458, 632635 (2009)." href="/articles/s41593-019-0428-x#ref-CR1" id="ref-link-section-d117493720e1404">1</a></sup>. In addition, when presented with a stimulus that has two independent features, only the attended and remembered feature can be decoded during the delay period<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Serences, J. T., Ester, E. F., Vogel, E. K. &amp; Awh, E. Stimulus-specific delay activity in human primary visual cortex. Psych. Sci. 20, 207214 (2009)." href="/articles/s41593-019-0428-x#ref-CR2" id="ref-link-section-d117493720e1408">2</a></sup>. This means that, despite identical sensory inputs and task demands at encoding, stimulus-evoked BOLD responses do not carry information about a cued target in the absence of a continued memory requirement. Indeed, once active maintenance of a stimulus feature is no longer needed, information about that feature rapidly drops to chance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Harrison, S. A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. Nature 458, 632635 (2009)." href="#ref-CR1" id="ref-link-section-d117493720e1413">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Serences, J. T., Ester, E. F., Vogel, E. K. &amp; Awh, E. Stimulus-specific delay activity in human primary visual cortex. Psych. Sci. 20, 207214 (2009)." href="#ref-CR2" id="ref-link-section-d117493720e1413_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Riggall, A. C. &amp; Postle, B. R. The relationship between working memory storage and elevated activity as measured with functional magnetic resonance imagine. J. Neurosci. 32, 1299012998 (2012)." href="/articles/s41593-019-0428-x#ref-CR3" id="ref-link-section-d117493720e1416">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Sprague, T. C., Ester, E. F. &amp; Serences, J. T. Restoring latent visual working memory representations in human cortex. Neuron 91, 694707 (2016)." href="#ref-CR48" id="ref-link-section-d117493720e1419">48</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Christophel, T. G., Iamshchinina, P., Yan, C., Allefeld, C. &amp; Haynes, J. D. Cortical specialization for attended versus unattended working memory. Nat. Neurosci. 21, 494496 (2018)." href="#ref-CR49" id="ref-link-section-d117493720e1419_1">49</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Rose, N. S. et al. Reactivation of latent working memories with transcranial magnetic stimulation. Science 354, 11361139 (2016)." href="/articles/s41593-019-0428-x#ref-CR50" id="ref-link-section-d117493720e1422">50</a></sup>. Thus, active mnemonic maintenance, and not stimulus-evoked BOLD, can drive the information contained in multivariate fMRI signals during the working memory delay.</p><p>In sum, new sensory inputs do not automatically purge working memory information from early retinotopic cortex. Salient and distracting information can, not surprisingly, negatively affect neural representations and behavioral performance. Together, these data suggest that early visual areas actively participate in both sensory and mnemonic processing, possibly serving as a local comparison circuit, and that high-fidelity memories rely on sustained representations in early visual cortex.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Methods</h2><div class="c-article-section__content" id="Sec4-content"><h3 class="c-article__sub-heading" id="Sec5">Participants</h3><p>Six volunteers (five female) between the ages of 21 and 32 years (s.d.=3.67) participated in Experiment 1, and seven volunteers (five female) between the ages of 24 and 35 years (s.d.=3.994) participated in Experiment 2. Three volunteers (S03, S04 and S05) participated in both experiments. No statistical methods were used to pre-determine sample sizes, but our sample sizes are similar to those reported in previous publications<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Harrison, S. A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. Nature 458, 632635 (2009)." href="/articles/s41593-019-0428-x#ref-CR1" id="ref-link-section-d117493720e1441">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Serences, J. T., Ester, E. F., Vogel, E. K. &amp; Awh, E. Stimulus-specific delay activity in human primary visual cortex. Psych. Sci. 20, 207214 (2009)." href="/articles/s41593-019-0428-x#ref-CR2" id="ref-link-section-d117493720e1444">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e1447">6</a></sup>. Participants had varying amounts of experience with fMRI experiments, ranging from scanner-nave (S02, S07 and S09) to highly experienced (that is, &gt;10h in the scanner; S01, S04, S05 and S10). For a separate behavioral experiment (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig14">9</a>) we recruited 21 participants (14 female; mean age=20.12, s.d. = 2.007), of whom 17 were included in the analysis (three dropped out and one was excluded due to chance-level performance). The study was conducted at the University of California, San Diego, and approved by the local Institutional Review Board. All participants provided written informed consent, had normal or corrected-to-normal vision and received monetary reimbursement for their time ($10 per hour for behavior, $20 per hour for fMRI, except for S10, who is one of the authors).</p><h3 class="c-article__sub-heading" id="Sec6">Stimuli and procedure Experiment 1</h3><p>All stimuli in Experiment 1 were projected on a 12090cm screen placed at the foot-end of the scanner and viewed through a tilted mirror from ~370cm in an otherwise darkened room. Stimuli were generated on a Macbook Air running OSX using MATLAB 8.1 and the Psychophysics toolbox<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Brainard, D. H. The Psychophysics Toolbox. Spat. Vis. 10, 433436 (1997)." href="/articles/s41593-019-0428-x#ref-CR51" id="ref-link-section-d117493720e1462">51</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Kleiner, M. et al. Whats new in psychtoolbox-3. Perception 36, 116 (2007)." href="/articles/s41593-019-0428-x#ref-CR52" id="ref-link-section-d117493720e1465">52</a></sup>. The luminance output from the projector was linearized in the stimulus presentation code. All stimuli were presented against a 62.82cdm<sup>2</sup> uniform gray background. Stimuli presented during the memory task (targets and distractors, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1a</a>) were configured in a donut-shaped circular aperture with a 1.5 and 7 inner and outer radius, respectively, and smoothed edges (1 Gaussian kernel; s.d.=0.5). Memory targets were full contrast sinusoidal gratings with a spatial frequency of two cycles per degree. Distractors were either gratings or Fourier-filtered noise stimuli, both with a Michelson contrast of 50%. Noise distractors were created by filtering white noise to include only spatial frequencies between 1 and 4 cycles per degree (all stimulus code is available on OSF).</p><p>To ensure that the distribution of remembered orientations was approximately uniform across all trials in the experiment, the orientation of the memory target on each trial was chosen from one of six orientation bins. Each bin contained 30 orientations in integer increments, and orientations were drawn randomly from each of the six bins with equal probability. The orientation of the distractor, on trials that contained an oriented grating distractor, was chosen using the same procedure. Moreover, we counterbalanced the orientation bins from which target and distractor orientations were drawn. This ensured that distractor orientations were also distributed uniformly across all trials in the experiment and that the target and distractor orientations were uncorrelated across trials (see also Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig6">1a</a>).</p><p>On every trial, we randomly chose the spatial phase of the memory target grating. Depending on the distractor condition, we also selected either a random spatial phase for the distractor grating or a random seed to generate the noise distractor. Each initial stimulus was then toggled back and forth between its original and inverted contrast at 4Hz, without blank gaps in between, for as long as the stimulus was on the screen. Thus, the memory target (500ms total duration) cycled through one contrast reversal (that is, 250ms per contrast). This single counter-phase contrast reversal was specifically designed to minimize afterimages induced by the memory target<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Tyler, C. W. &amp; Nakayama, K. Grating induction: a new type of aftereffect. Vis. Res. 20, 437441 (1980)." href="/articles/s41593-019-0428-x#ref-CR53" id="ref-link-section-d117493720e1483">53</a></sup>. Similarly, distractors (11s total duration) contrast reversed for 22 cycles. The recall probe consisted of two white line segments that were 5.5 long and 0.035 wide, with each segment presented at the same distance from fixation as the donut-shaped target and distractor stimuli. A 0.4 central black dot was presented continuously on each block of trials to facilitate fixation.</p><p>Each trial of the memory task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1a</a>) started with a 1.4s change in the color of the central fixation dot, indicating with 100% validity the distractor condition during the delay (no distractor, grating distractor, noise distractor). Cues could be blue, green or red. The pairing of cue-colors with distractor conditions was randomized across participants. Following the cue, a memory target was shown for 500ms and participants remembered its orientation over a 13s delay. A contrast-reversing noise (one-third of trials) or grating (one-third of trials) distractor was presented for 11s during the middle portion of the delay, or the screen remained gray throughout the 13s delay (one-third of trials). After the delay, participants used four buttons to rotate the recall probe around fixation, matching the remembered orientation as precisely as possible. The left two buttons rotated the line counter clockwise, while the right two buttons rotated it clockwise. Using the outer- or inner-most buttons would result in faster or slower rotation of the recall probe, respectively. Participants had 3s to respond before being presented with the next memory target 3, 5, or 8s later. Each run consisted of 12 memory trials, and lasted 4min and 40.8s. Distractor type (none, grating or noise) and the orientation bin (one of six) from which the target or distractor grating orientations were drawn, were fully counterbalanced across nine consecutive runs of the memory task. Data for 27 total runs were acquired across three separate scanning sessions. Before starting the fMRI experiment, participants practiced the memory task outside the scanner until they were comfortable using the response buttons to recall the target orientation within the temporally restricted response window, and mean absolute response error was &lt;10 (this took between 6 and 12 trials for all participants).</p><p>In addition to the memory task, Experiment 1 also included an independent mapping task. During this task, participants viewed 9-s blocks of donut-shaped gratings (same dimensions as in the memory task) or circle-shaped gratings (1.5 radius) that were contrast-reversing at 5Hz. The orientation of each grating was chosen at random from one of ten orientation bins, and from each bin equally often during a run, to approximate an even sampling of orientation space. Per run, 20 blocks of donut-shaped gratings were alternated with 20 blocks of circle-shaped gratings, with four fixation blocks interspersed. Each run took 7min. Participants performed a detection task to ensure attention at the physical location of the stimuli: Grating contrast was probabilistically dimmed twice every 9s, from 100 to 80% for 200ms. Because the contrast change was probabilistic, there was no change on some stimulus blocks, while on others there were &gt;2 changes. Participants maintained fixation on a 0.4 mean gray dot with a 0.2 magenta dot on top. Note that the donut-shaped stimuli in the mapping task occupied the same physical location as the donut-shaped target and distractor stimuli in the main memory task. This allowed us to independently identify voxels in early visual areas that selectively responded to the spatial position of the memory target. During each scanning session, participants completed 46 runs of the mapping task (1517 total runs across days). Three participants (S02, S03 and S04) practiced one block of the mapping task before the experiment.</p><h3 class="c-article__sub-heading" id="Sec7">Stimuli and procedure Experiment 2</h3><p>In Experiment 2, all stimuli were projected on a 21.3cm  16 cm screen placed inside the scanner bore, viewed from ~40cm through a tilted mirror. Stimuli were generated using Ubuntu 14.04, MATLAB 9.3 and the Psychophysics toolbox<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Brainard, D. H. The Psychophysics Toolbox. Spat. Vis. 10, 433436 (1997)." href="/articles/s41593-019-0428-x#ref-CR51" id="ref-link-section-d117493720e1505">51</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Kleiner, M. et al. Whats new in psychtoolbox-3. Perception 36, 116 (2007)." href="/articles/s41593-019-0428-x#ref-CR52" id="ref-link-section-d117493720e1508">52</a></sup>. During the memory task, memory targets were full contrast circular sinusoidal gratings (radius=14.58) with smoothed edges (1.33 kernel; s.d.=0.67) and a spatial frequency of 1.5 cycles per degree. Distractor stimuli were either gratings shown at 50% Michelson, or pictures of faces<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Goeleven, E., De Raedt, R., Leyman, L. &amp; Verschuere, B. The Karolinska directed emotional faces: a validation study. Cogn. Emot. 22, 10941118 (2008)." href="/articles/s41593-019-0428-x#ref-CR54" id="ref-link-section-d117493720e1512">54</a></sup> and gazebos<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e1516">6</a></sup> (maximal extent=27.83, adapted after ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e1520">6</a></sup>). All pictures had the same mean luminance, which was equal to the gray background. The memory target contrast-reversed once, just as in Experiment 1. However, unlike Experiment 1 (where distractors were also contrast reversing), distractors in Experiment 2 were toggled on and off at 4Hz (that is, one cycle consisted of a 250ms distractor image and a 250ms blank screen). On a picture distractor trial, we either showed the full set of 22 unique face images or the full set of 22 unique gazebo images in a randomly shuffled order. On a grating-distractor trial, we showed 22 gratings, each with the same orientation but a randomly chosen phase (02<i></i>). Target and distractor grating orientations were pseudo-randomly chosen from one of six orientation bins to ensure a roughly uniform sampling of orientation space, identical to the procedure used in Experiment 1 (see also Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig6">1b</a>). The recall probe consisted of a 0.056 wide and 29.17 long black line. This line was interrupted by a 0.53 black central fixation dot presented on top of a 0.81 mean gray circle. This fixation dot was presented throughout to aid fixation.</p><p>The procedure during the memory task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3a</a>) was identical to that of Experiment 1, with the following exceptions: the noise distractor condition was replaced with a picture distractor condition. On half of these trials, pictures of faces were shown and, on the other half, pictures of gazebos were shown. In both the grating and picture distractor conditions, the distractors started flickering on and off 1s into the 12-s delay, and the recall probe appeared immediately after the last off period. Participants had 4s to rotate the dial. Participants were scanned on three separate days, completing a total of 27 total runs (9 runs per day, 12 trials per run) of the memory task. Before scanning, participants practiced for 1224 trials until their absolute performance was &lt;10. Only S09 did not reach this criterion during practice, with a performance of 11.5 after 36 practice trials.</p><p>Experiment 2 used two different mapping tasks: During the first mapping task, participants viewed a series of gratings (50% of trials), face pictures (25% of trials) or gazebo pictures (25% of trials) that were flickered on (250ms) and off (250ms) at 4Hz for a total of 5.5s (that is, 11 stimuli per trial). Each trial was followed by a 3, 5, or 8s inter-trial interval. Grating and picture stimuli were identical to the ones described above for the Experiment 2 memory task. On grating trials, the orientation was chosen at random from one of 12 orientation bins (to ensure approximately uniform sampling of orientation space as in Experiment 1). Each of the 22 unique face images was shown three times during a run. Face images were randomly shuffled across all trials in a run, with the restriction that the same image was never shown twice in a row. The same was true for gazebo images. Participants completed 24 trials per run (4min and 31.2s per run). Across the three scanning days, participants completed between 20 and 29 total runs of this first mapping task. Three participants (S04, S05 and S09) practiced one run of the task before going into the scanner.</p><p>The second mapping task of Experiment 2 comprised trials showing either a circle-shaped (1.06 radius) or donut-shaped (1.06 inner and 14.74 outer radius) grating stimulus (spatial frequency 1.43 cycles per degree; edges smoothed with 0.69 kernel and s.d.=0.36). On every trial, a 6s grating was contrast-reversing (as in Experiment 1) at 4Hz (that is, 250ms per contrast), followed by a 3, 5, or 8s inter-trial interval. Grating orientation was randomly chosen from one of nine orientation bins on each trial, and equally often from each bin within a run. Participants completed 36 trials per run (18 circle-shaped grating trials and 18 donut-shaped grating trials, randomly interleaved), and each run took 7min and 5.6s. A central black dot (0.56) aided fixation throughout. Data for this second mapping task were collected separately from the other Experiment 2 data (that is, different scanning sessions). Participants completed between 10 and 20 total runs of the second mapping task.</p><p>During both mapping tasks in Experiment 2, we occasionally (03 times per trial) superimposed small smoothed circles (of a uniform light gray color) on the mapping stimuli for 250ms. These brief blobs could be centered at any distance from fixation occupied by a stimulus (although no closer than 0.056 and no further than 13.78), and at any angle relative to fixation (1360). Blobs were scaled for cortical magnification<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Rovamo, J. &amp; Virsu, V. An estimation and application of the human cortical magnification factor. Exp. Brain Res. 37, 495510 (1979)." href="/articles/s41593-019-0428-x#ref-CR55" id="ref-link-section-d117493720e1547">55</a></sup>, such that all blobs (that is, at every distance from fixation) stimulated roughly 1mm of cortex. In terms of visual angle, this means blobs had radii spanning from 0.18 to 0.75. No blobs were presented during the first or last 500ms of a trial, or within 500ms of each other. Participants pressed a button every time they detected a blob superimposed on a stimulus image, such that they stayed alert and attending the location of the mapping stimuli.</p><h3 class="c-article__sub-heading" id="Sec8">Magnetic resonance imaging</h3><p>All scans were performed on a General Electric Discovery MR750 3.0T scanner located at the University of California, San Diego (UCSD), Keck Center for Functional Magnetic Resonance Imaging (CFMRI). High-resolution (1mm<sup>3</sup> isotropic) anatomical images were acquired during a retinotopic mapping session, using an Invivo eight-channel head coil. Functional echo-planar imaging (EPI) data for the current experiment were acquired using a Nova Medical 32-channel head coil (NMSC075-32-3GE-MR750) and the Stanford Simultaneous Multi-Slice EPI sequence (MUX EPI), using nine axial slices per band and a multiband factor of eight (total slices=72; 2mm<sup>3</sup> isotropic; 0mm gap; matrix=104104; field of view=20.8cm; repetition time/echo time (TR/TE)=800/35ms, flip angle=52; inplane acceleration=1). At sequence onset, the initial 16 TRs served as reference images critical to the transformation from <i>k</i>-space to image space. Un-aliasing and image reconstruction procedures were performed on local servers using CNI-based reconstruction code. Forward and reverse phase-encoding directions were used during the acquisition of two short (17s) topup datasets. From these images, susceptibility-induced off-resonance fields were estimated<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Andersson, J. L. R., Skare, S. &amp; Ashburner, J. How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging. Neuroimage 20, 870888 (2003)." href="/articles/s41593-019-0428-x#ref-CR56" id="ref-link-section-d117493720e1566">56</a></sup> and used to correct signal distortion inherent in EPI sequences, using FSL topup<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Smith, S. M. et al. Advances in functional and structural MR image analysis and implementation as FSL. Neuroimage 23, 208219 (2004)." href="/articles/s41593-019-0428-x#ref-CR57" id="ref-link-section-d117493720e1570">57</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Jenkinson, M., Beckmann, C. F., Behrens, T. E., Woolrich, M. W. &amp; Smith, S. M. FSL. Neuroimage 62, 782790 (2012)." href="/articles/s41593-019-0428-x#ref-CR58" id="ref-link-section-d117493720e1573">58</a></sup>.</p><h3 class="c-article__sub-heading" id="Sec9">Preprocessing</h3><p>All imaging data were preprocessed using software tools developed and distributed by FreeSurfer and FSL (free to download at <a href="https://surfer.nmr.mgh.harvard.edu">https://surfer.nmr.mgh.harvard.edu</a> and <a href="http://www.fmrib.ox.ac.uk/fsl">http://www.fmrib.ox.ac.uk/fsl</a>). Cortical surface gray-white matter volumetric segmentation of the high-resolution anatomical image was performed using the recon-all utility in the FreeSurfer analysis suite<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Dale, A. M., Fischl, B. &amp; Sereno, M. I. Cortical surface-based analysis. I. Segmentation and surface reconstruction. Neuroimage 9, 179194 (1999)." href="/articles/s41593-019-0428-x#ref-CR59" id="ref-link-section-d117493720e1594">59</a></sup>. Segmented T1 data were used to define ROIs for use in subsequent analyses. The first volume of every first functional run of a scanning session was then coregistered to this common anatomical image. Transformation matrices were generated using FreeSurfers manual and boundary-based registration tools<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Greve, D. &amp; Fischl, B. Accurate and robust brain image alignment using boundary-based registration. Neuroimage 48, 6372 (2009)." href="/articles/s41593-019-0428-x#ref-CR60" id="ref-link-section-d117493720e1598">60</a></sup>. These matrices were then used to transform each four-dimensional functional volume using FSL FLIRT<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Jenkinson, M. &amp; Smith, S. M. A global optimisation method for robust affine registration of brain images. Med. Image Anal. 5, 143156 (2001)." href="/articles/s41593-019-0428-x#ref-CR61" id="ref-link-section-d117493720e1602">61</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Jenkinson, M., Bannister, P., Brady, J. M. &amp; Smith, S. M. Improved optimization for the robust and accurate linear registration and motion correction of brain images. Neuroimage 17, 825841 (2002)." href="/articles/s41593-019-0428-x#ref-CR62" id="ref-link-section-d117493720e1605">62</a></sup>, such that all cross-session data from a single participant was in the same space. Next, motion correction was performed using the FSL tool MCFLIRT<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Jenkinson, M., Bannister, P., Brady, J. M. &amp; Smith, S. M. Improved optimization for the robust and accurate linear registration and motion correction of brain images. Neuroimage 17, 825841 (2002)." href="/articles/s41593-019-0428-x#ref-CR62" id="ref-link-section-d117493720e1610">62</a></sup> without spatial smoothing, a final sinc interpolation stage, and 12 degrees of freedom. Slow drifts in the data were removed last, using a high pass filer (1/40Hz cutoff). No additional spatial smoothing was applied to the data apart from the smoothing inherent to resampling and motion correction.</p><p>Signal amplitude time-series were normalized via <i>Z</i>-scoring on a voxel-by-voxel and run-by-run basis. <i>Z</i>-scored data were used for all further analyses. Trial events were jittered with respect to TR onsets, and trial events were rounded to the nearest TR. To recover the univariate BOLD time courses for all three memory distractor conditions in Experiments 1 and 2, we estimated the hemodynamic response function for each voxel at each time point of interest (019.5s from memory target onset). This was done using a finite impulse response function model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Dale, A. M. Optimal experimental design for eventrelated fMRI. Hum. Brain Mapp. 8, 109114 (1999)." href="/articles/s41593-019-0428-x#ref-CR63" id="ref-link-section-d117493720e1623">63</a></sup> consisting of a column marking the onset of each event (memory target onset) with a 1, and then a series of temporally shifted version of that initial regressor in subsequent columns to model the BOLD response at each subsequent time point. Estimated hemodynamic response functions were then averaged across all voxels in each ROI (see also Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig8">3</a>). The analyses performed after preprocessing was completed were all done in MATLAB 9.1 using custom functions.</p><h3 class="c-article__sub-heading" id="Sec10">Identifying ROIs</h3><p>To identify voxels that were visually responsive to the donut-stimuli, a general linear model was performed on data from the mapping task (for Experiment 2 we used data from the second mapping task) using FSL FEAT (FMRI Expert Analysis Tool, v.6.00). Individual mapping runs were analyzed using the brain extraction tool<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Smith, S. M. Fast robust automated brain extraction. Hum. Brain Mapp. 17, 143155 (2002)." href="/articles/s41593-019-0428-x#ref-CR64" id="ref-link-section-d117493720e1638">64</a></sup> and data prewhitening using FILM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Woolrich, M. W., Ripley, B. D., Brady, M. &amp; Smith, S. M. Temporal autocorrelation in univariate linear modeling of FMRI data. Neuroimage 14, 13701386 (2001)." href="/articles/s41593-019-0428-x#ref-CR65" id="ref-link-section-d117493720e1642">65</a></sup>. Predicted BOLD responses were generated for blocks of donut and circle stimuli by convolving the stimulus sequence with a canonical gamma hemodynamic response function (phase=0s, s.d.=3s, lag=6s). The temporal derivative was included as an additional regressor to accommodate slight temporal shifts in the waveform to yield better model fits and to increase explained variance. Individual runs were combined using a standard weighted fixed effects model. Voxels that were significantly more activated by the donut compared to the circle (<i>P</i>=0.05; false discovery rate corrected) were defined as visually responsive and used in subsequent analyses.</p><p>Standard retinotopic mapping procedures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Engel, S. A. et al. fMRI of human visual cortex. Nature 369, 525 (1994)." href="/articles/s41593-019-0428-x#ref-CR66" id="ref-link-section-d117493720e1652">66</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Swisher, J. D., Halko, M. A., Merabet, L. B., McMains, S. A. &amp; Somers, D. C. Visual topography of human intraparietal sulcus. J. Neurosci. 27, 53265337 (2007)." href="/articles/s41593-019-0428-x#ref-CR67" id="ref-link-section-d117493720e1655">67</a></sup> were employed to define nine a priori ROIs in early visual (V1V3, V3AB, hV4) and parietal (IPS0IPS3) cortex. Retinotopic mapping data were acquired during an independent scanning session that used both meridian mapping techniques (with checkerboard bowtie stimuli shown alternating between the horizontal and vertical meridian) and polar angle techniques (with a slowly rotating checkerboard wedge) to identify the visual field preferences of voxels (stimuli described in more detail in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Sprague, T. C. &amp; Serences, J. T. Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices. Nat. Neurosci. 16, 18791887 (2013)." href="/articles/s41593-019-0428-x#ref-CR68" id="ref-link-section-d117493720e1659">68</a></sup>). Anatomical and functional retinotopy analyses were performed using a set of custom wrappers that encapsulated existing FreeSurfer and FSL functionality. ROIs were combined across left and right hemispheres and across dorsal and ventral areas (for V1V3) by concatenating voxels.</p><p>For all analyses (except the one presented in Supplementray Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">13</a>), only visually responsive voxels, selected using the localizer procedure described above, were included in the ROI of each retinotopic area. We only included data for retinotopic areas in which the number of visually responsive voxels exceeded 20 for every single participant. Exact voxel counts for each participant in each ROI can be found in Supplementary Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">9</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">10</a> for Experiments 1 and 2, respectively.</p><h3 class="c-article__sub-heading" id="Sec11">fMRI analyses: IEM</h3><p>To generate model-based reconstructions of remembered and perceived orientations from voxel responses, an IEM was implemented<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Brouwer, G. J. &amp; Heeger, D. J. Decoding and reconstructing color from responses in human visual cortex. J. Neurosci. 29, 1399214003 (2009)." href="/articles/s41593-019-0428-x#ref-CR14" id="ref-link-section-d117493720e1683">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Sprague, T. C., Saproo, S. &amp; Serences, J. T. Visual attention mitigates information loss in small- and large-scale neural codes. Trends Cogn. Sci. 19, 215226 (2015)." href="/articles/s41593-019-0428-x#ref-CR15" id="ref-link-section-d117493720e1686">15</a></sup> with orientation as the feature dimension. The first step in this analysis is to estimate an encoding model using voxel responses in a cortical region of interest. Data used during this step are considered the training set (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig9">4a</a>, left), and are combined with nine idealized tuning functions, or channels (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig9">4a</a>, right), to parameterize an orientation sensitivity profile for each voxel. The second step in the analysis combines the estimated sensitivity profiles in each voxel with a novel pattern of all voxel responses in a ROI on a single trial (data from the test set, Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig9">4b</a>, left) to reconstruct a model-based representation of the orientation that was remembered or viewed on that trial (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig9">4b</a>, right). The encoding model for a single voxel has the general form</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$R_j = \mathop {\sum }\limits_i^9 w_i\,c_i$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>R</i><sub><i>j</i></sub> is the response <i>R</i> of voxel <i>j</i>, and <i>c</i><sub><i>i</i></sub> is the channel magnitude <i>c</i> at the <i>i</i>th of nine channels. A voxels sensitivity profile over orientation space is captured by nine weights, <i>w</i>. Channels were modeled as</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$c\left( \theta \right) = {\mathrm{\cos}} \left( {\left( {\theta - \mu } \right) \frac{\pi }{{180}}} \right)^8$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <i></i> is degrees in orientation space (defined over 180) and the channel center is <i></i>. Channel centers were spaced 20 apart.</p><p>For the first step of the IEM, equation (1) can be expressed as</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$B_1 = WC_1$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>Here, a matrix of observed BOLD responses <i>B</i><sub>1</sub> (<i>m</i> voxels<i>n</i> trials) is related to a matrix of modeled channel responses <i>C</i><sub>1</sub> (<i>k</i> channels<i>n</i> trials) by a weight matrix <i>W</i> (<i>m</i> voxels<i>k</i> channels). For each trial, <i>C</i><sub>1</sub> contains the pointwise product of a stimulus mask (that is, 1 at the true stimulus orientation, 0 at all other orientations) with the idealized tuning functions. <i>W</i> quantifies the sensitivity of each voxel at each idealized orientation channel, and can be computed with least-squares linear regression:</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\hat W = B_1C_1^{\mathrm{T}}(C_1C_1^{\mathrm{T}})^{ - 1}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>Estimating the sensitivity profiles concludes the first encoding step of the IEM. The second step of the IEM inverts the model, using the estimated sensitivity profiles of all voxels <span class="mathjax-tex">\(\hat W\)</span> (<i>m</i> voxels<i>k</i> channels) in combination with a test set of novel BOLD response data <i>B</i><sub>2</sub> (<i>m</i> voxels<i>n</i> trials) to estimate the response of each channel on each trial <span class="mathjax-tex">\(\hat C_2\)</span> (<i>k</i> channels<i>n</i> trials):</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\hat C_2 = \left( {\hat W^{\mathrm{T}}\hat W} \right)^{ - 1}\hat W^{\mathrm{T}}B_2$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>This step uses the MoorePenrose pseudoinverse of <span class="mathjax-tex">\(\hat W\)</span> and it is multivariate in nature, since it uses the sensitivity profiles across all voxels to jointly estimate channel responses <span class="mathjax-tex">\(\hat C_2\)</span> for each trial of the test set. This effectively forms a model-based reconstruction of the remembered or seen stimulus feature on a trial-by-trial basis.</p><p>Because grating orientations could take any integer value between 1 and 180, both the encoding (equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/articles/s41593-019-0428-x#Equ4">4</a>)) and inversion (equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/articles/s41593-019-0428-x#Equ5">5</a>)) steps of the IEM were repeated 20 times. On each repeat, the centers of nine idealized tuning functions were shifted by 1 (equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/articles/s41593-019-0428-x#Equ2">2</a>)), and we estimated the channel responses <span class="mathjax-tex">\(\hat C_2\)</span> at those nine centers, until the entire 180 orientation space was estimated in 1 steps. This procedure thus yielded estimated channel responses <span class="mathjax-tex">\(\hat C_2\)</span> for each degree in orientation space. After generating reconstructions for each trial, all single-trial reconstructions were re-centered on the remembered orientation (when looking at information for mnemonic orientations) or on the orientation of the directly viewed distractor grating (when looking at information for viewed orientations).</p><p>For the IEM analyses investigating mnemonic codes based on sensory-driven responses, we used independent data from the mapping tasks as the training set and data from the memory task as the test set. For Experiment 2, we combined data across the two mapping tasks to comprise the training set. For the IEM analyses investigating mnemonic codes that are not necessarily based on sensory-driven responses, we used a within-condition leave-one-out procedure. Here, all but one trial is used as the training set, and the left-out trial constitutes the test set. This procedure is repeated until all trials in a given condition have been left out once. Of note, we also did these analyses leaving one session out, yielding qualitatively similar results. To obtain single-trial activity estimates, memory data were averaged over a time window of 5.613.6s (717 TRs) after target onset. Mapping data in Experiment 1 were averaged over 4.89.6s (612 TRs) after donut onset. Mapping data from both Experiment 2 mapping tasks were averaged over 2.47.2s (39 TRs) after donut onset.</p><h3 class="c-article__sub-heading" id="Sec12">fMRI analyses: reconstruction fidelity</h3><p>Model-based reconstructions of orientation were quantified using a fidelity metric derived from trigonometry (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1d</a>)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Wolff, M. J., Jochim, J., Akyrek, E. G. &amp; Stokes, M. G. Dynamic hidden states underlying working-memory-guided behavior. Nat. Neurosci. 20, 864871 (2017)." href="/articles/s41593-019-0428-x#ref-CR69" id="ref-link-section-d117493720e2075">69</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Sprague, T. C., Ester, E. F. &amp; Serences, J. T. Restoring latent visual working memory representations in human cortex. Neuron 91, 694707 (2016)." href="/articles/s41593-019-0428-x#ref-CR48" id="ref-link-section-d117493720e2078">48</a></sup>. Unless specified otherwise, this fidelity metric was applied to the average of 108 single-trial reconstructions (that is, all trials from a given distractor condition), separately for each condition, participant, and ROI. For each reconstruction, the fidelity metric was calculated by taking the channel response at each degree in orientation space (wrapped onto a 2<i></i> circle), and projecting this vector onto the center of the stimulus space (that is, onto 0) via <span class="mathjax-tex">\(\cos A_{\mathrm{abs}(0^ \circ - d)} = \frac{b}{h}\)</span>, where <i>A</i><sub>abs</sub> is the absolute angle between the reconstruction center (at 0) and the degree in orientation space being evaluated (<i>d</i>), and <i>h</i> is the channel response at <i>d</i> (that is, the hypotenuse of a right triangle). In other words, we projected the length of vector <i>h</i> onto 0 by solving for <i>b</i> (that is, the adjacent side of a right triangle). This procedure was repeated for all 180 in orientation space, after which we calculated the mean of all 180 projected vectors. Thus, the mean projected vectorour fidelity metricreflects the amount of energy at a remembered or sensed orientation. Note that this metric, by design, gets rid of additive offsets and captures only the amount of information at the center of the reconstruction.</p><h3 class="c-article__sub-heading" id="Sec13">fMRI analyses: decoding</h3><p>In addition to using an inverted encoding model to analyze our data (that is, the IEM analysis described above), we also used a more conventional multivariate pattern analysis decoding approach. This allowed us to evaluate the extent to which our results generalized across different analysis techniques. It also allowed us to more directly compare our results to previous work by Bettencourt and Xu<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. Nat. Neurosci. 19, 150157 (2016)." href="/articles/s41593-019-0428-x#ref-CR6" id="ref-link-section-d117493720e2146">6</a></sup>, who used a decoder to perform a two-way classification between orthogonal orientations. To analyze our data in an analogous manner, despite our use of continuous orientations (1180), we performed two two-way classifications. For the first classification, we binned all orientations within a 45 window around vertical, and we binned all orientations within a 45 window around horizontal. We then performed a two-way classification to decode between the two cardinal axes (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig5">5a</a>). For the second classification, we binned all orientations within a 45 window around the two oblique axes (that is, around 45 and 135) to classify between the two oblique orientations (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig5">5a</a>). Finally, decoding performance was averaged across the two two-way classifications (that is, the cardinal classification and the oblique classification) before performing statistics and plotting (Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">5b</a>). We used the MATLAB built-in multi-class support vector machine (SVM) (fitcecoc and predict functions). As with our IEM analyses, we performed the decoding analysis in two different ways: (1) training the SVM on independent localizer data and decoding the orientation from the working memory delay epoch, and (2) training and testing on data from the memory delay, using a leave-one-trial-out procedure. Decoding results based on the independent and leave-one-out training schemes are plotted on the left and right side of Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">5b</a>, respectively. For Experiment 2, we also evaluated whether the picture distractors shown during the delay (faces and gazebos) could be decoded. Using the same classifier described above, we trained an SVM on independent data from the first Experiment 2 mapping task (that is, the one with trials showing pictures of faces and gazebos) to do a two-way classification. We then decoded the presence of either a face of gazebo distractor during the working memory delay-period epoch (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig15">10</a>).</p><h3 class="c-article__sub-heading" id="Sec14">Statistical procedures</h3><p>All statistical statements reported here were based on permutation (or randomization) testing over 1,000 iterations with scrambled data labels. Note that this constrains the resolution of our <i>P</i> values to a lower limit of <i>P</i>0.001. To test whether fidelity metrics were significantly greater than zero, we generated permuted null distributions of fidelities for each participant, ROI, and condition (and for each timepoint, in the analyses shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig2">2b</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3e</a> and Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig11">6</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig17">12</a>). On each permutation, we first reshuffled target orientation labels for all trials before performing the inversion step of the IEM (effectively randomizing single-trial reconstructions relative to the true orientation). Second, we calculated the fidelity for the trial-averaged shuffled orientation reconstructions in a manner identical to calculating fidelity for the intact reconstructions. This resulted in one null fidelity estimate per permutation. Combining the null fidelities across all participants (so six and seven fidelities for Experiments 1 and 2, respectively) resulted in one <i>t</i>-statistic per permutation <span class="mathjax-tex">\(t = \frac{{\bar x - \mu _0}}{{s/\sqrt n }}\)</span>, where <span class="mathjax-tex">\(\bar x\)</span> and <i>s</i> are the mean and standard deviation of fidelities across participants, <i></i><sub>0</sub> is the null hypothesis mean (that is, 0) and <i>n</i> is the sample size. To test across-participant fidelities against zero (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1e</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig2">2b</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3d,e</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig4">4</a> and Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig11">6</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig16">12</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig18">13</a>) we compared the <i>t</i>-statistic calculated from the intact data against the permuted null distribution of <i>t</i>-statistics for that condition, ROI, and timepoint. Reported tests against zero were one-sided and uncorrected. Note that the same procedure was used for the decoding analyses (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig5">5</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig15">10</a>), with the exception that the null hypothesis for the <i>t</i>-statistic was 0.5 (that is, chance level). Significant fidelity (and decoding) is indicated in our figures by colored and gray asterisks.</p><p>To test whether there were differences in fidelity between distractor conditions (within each ROI; Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1e</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3d, 4</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig4">5</a> and Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig13">8</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig17">13</a>), we used within-subjects repeated-measures one-way analysis of variance (ANOVA). First, we calculated the <i>F</i>-statistic from the intact data for the effect of condition. Next, we generated permuted null distributions of <i>F</i> by shuffling the condition labels per subject, and calculating the null <i>F</i>-statistic on each permutation. Each data-derived <i>F</i> was then compared to its null distribution of <i>F</i> to get the <i>P</i> value. Significant effects were followed up with post-hoc paired-sample <i>t</i>-tests: We performed pairwise comparisons between each of the conditions, comparing the data-derived <span class="mathjax-tex">\(t = \frac{{\bar X_D - \mu _0}}{{s_D/\sqrt n }}\)</span> (with <span class="mathjax-tex">\(\bar X_D\)</span> and <i>s</i><sub><i>D</i></sub> denoting the mean and s.d. of the pairwise differences), against a permuted <i>t</i> distribution generated by reshuffling condition labels on each iteration. Significant comparisons are indicated in our figures by black asterisks.</p><p>To look for a signature of top-down related processing, we used the grating-distractor condition, as it allowed us to directly compare fidelities from remembered and sensed orientations (which were derived from the same exact data). We used a within-subjects repeated-measures two-way ANOVA to track differences in fidelity across ROIs and memory/sensory condition (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig1">1e</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig3">3d</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig4">4</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig5">5</a>). We only included V1, V2, V3, V3AB, and V4 as our ROIs, as the hierarchical relationship between these areas is still fairly clear. Permutations were done as described above, but now based on <i>F</i>-statistics for the two main effects of ROI and memory/sensory condition, as well as their interaction. Specifically, the aim of this analysis was to look for significant interactions between ROI and memory/sensory condition, and to test whether memory and sensory representations became more and less pronounced, respectively, as one ascends the visual hierarchy. There are various ways in which ROIs might systematically differ from one another, such as their size (that is, number of voxels), sensitivity to neural activity, or signal-to-noise ratio. Because these factors might affect the attainable accuracy of multivariate analysis tools, a direct comparison across ROIs is generally not recommended<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Haynes, J. D. A primer on pattern-based approaches to fMRI: principles, pitfalls, and perspectives. Neuron 87, 257270 (2015)." href="/articles/s41593-019-0428-x#ref-CR70" id="ref-link-section-d117493720e2406">70</a></sup>. However, because here we are looking for an interaction specifically (and not an absolute difference between ROIs), and because we apply our multivariate techniques to the exact same data in each ROI (that is, information about either the remembered or sensed orientation) these caveats are not of concern in this particular case. One remaining concern is that also the scaling might not be comparable between ROIs (that is, a difference of <i>X</i> in one ROI may not mean the same as a similar difference of <i>X</i> in another ROI), although this concern is not reflected by the data presented here.</p><p>When circular statistics were used, these were calculated using the circular statistics toolbox<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Berens, P. CircStat: a MATLAB toolbox for circular statistics. J. Stat. Softw. 31, 121 (2009)." href="/articles/s41593-019-0428-x#ref-CR71" id="ref-link-section-d117493720e2419">71</a></sup>.</p><h3 class="c-article__sub-heading" id="Sec15">Glitches</h3><p>S01 completed four sessions of scanning, but on the first scanning day the projector settings had been changed such that we were presenting stimuli as ovals rather than circles. Data from this session were excluded from analysis. S02 was also scanned four times, but data transfer after one of the sessions failed, and the data were removed from the scanner centers servers before being backed upand thus lost. For S04, we only collected four mapping runs on the first day of scanning because the scanner computer hard drive was full by the time we approached the end of the scan, causing the computer to freeze. On the first day of scanning S05 the scanner computer started spontaneously deleting data files half way through the session. Consequently, data from the second mapping run were deleted and the fifth memory run was aborted (with imaging data collection incomplete, while behavioral data collection was complete). To ensure full counterbalancing, an exact replica of this fifth memory run was repeated as the first run on the second day of scanning. During the last scanning session of S09, the subject reported repeated but brief instances of falling asleep. Probed further, S09 indicated having also slept occasionally during the previous three sessions. Because we could no longer determine the runs during which S09 was asleep, we decided to keep all S09 data for analyses and to scan one additional subject for Experiment 2.</p><h3 class="c-article__sub-heading" id="Sec16">Miscellaneous</h3><p>During data collection, participants were not blinded to the experimental conditions (that is, they could clearly perceive the distractor condition on every trial), while experimenters were blinded (that is, they were not in the room and conditions were interleaved). Analyses were not performed blind to the conditions of the experiments.</p><h3 class="c-article__sub-heading" id="Sec17">Reporting Summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM2">Nature Research Reporting Summary</a> linked to this article.</p></div></div></section>
                </div>
            

            <div>
                <section data-title="Data availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data availability</h2><div class="c-article-section__content" id="data-availability-content">
              
              <p>We have uploaded all preprocessed fMRI and behavioral data, from each subject and ROI, to the Open Science Framework (OSF) at <a href="https://osf.io/dkx6y">https://osf.io/dkx6y</a>. An accompanying wiki is available here as well, providing an overview of all the data and code.</p>
            </div></div></section><section data-title="Code availability"><div class="c-article-section" id="code-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="code-availability">Code availability</h2><div class="c-article-section__content" id="code-availability-content">
              
              <p>The experiment code used during data collection and the analysis code used to generate the figures in the main manuscript and Supplementary Materials is available from the Open Science Framework (OSF) at <a href="https://osf.io/dkx6y">https://osf.io/dkx6y</a>.</p>
            </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Harrison, S. A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. <i>Nature</i> <b>458</b>, 632635 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature07832" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature07832" aria-label="Article reference 1" data-doi="10.1038/nature07832">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXitFKmtb4%3D" aria-label="CAS reference 1">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20reveals%20the%20contents%20of%20visual%20working%20memory%20in%20early%20visual%20areas&amp;journal=Nature&amp;doi=10.1038%2Fnature07832&amp;volume=458&amp;pages=632-635&amp;publication_year=2009&amp;author=Harrison%2CSA&amp;author=Tong%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Serences, J. T., Ester, E. F., Vogel, E. K. &amp; Awh, E. Stimulus-specific delay activity in human primary visual cortex. <i>Psych. Sci.</i> <b>20</b>, 207214 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1111/j.1467-9280.2009.02276.x" data-track-action="article reference" href="https://doi.org/10.1111%2Fj.1467-9280.2009.02276.x" aria-label="Article reference 2" data-doi="10.1111/j.1467-9280.2009.02276.x">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Stimulus-specific%20delay%20activity%20in%20human%20primary%20visual%20cortex&amp;journal=Psych.%20Sci.&amp;doi=10.1111%2Fj.1467-9280.2009.02276.x&amp;volume=20&amp;pages=207-214&amp;publication_year=2009&amp;author=Serences%2CJT&amp;author=Ester%2CEF&amp;author=Vogel%2CEK&amp;author=Awh%2CE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Riggall, A. C. &amp; Postle, B. R. The relationship between working memory storage and elevated activity as measured with functional magnetic resonance imagine. <i>J. Neurosci.</i> <b>32</b>, 1299012998 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.1892-12.2012" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.1892-12.2012" aria-label="Article reference 3" data-doi="10.1523/JNEUROSCI.1892-12.2012">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhsVGitLrK" aria-label="CAS reference 3">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20relationship%20between%20working%20memory%20storage%20and%20elevated%20activity%20as%20measured%20with%20functional%20magnetic%20resonance%20imagine&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.1892-12.2012&amp;volume=32&amp;pages=12990-12998&amp;publication_year=2012&amp;author=Riggall%2CAC&amp;author=Postle%2CBR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Christophel, T. B., Hebart, M. N. &amp; Haynes, J. D. Decoding the contents of visual short-term memory from human visual and parietal cortex. <i>J. Neurosci.</i> <b>32</b>, 1298312989 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.0184-12.2012" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.0184-12.2012" aria-label="Article reference 4" data-doi="10.1523/JNEUROSCI.0184-12.2012">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhsVGitLrI" aria-label="CAS reference 4">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20the%20contents%20of%20visual%20short-term%20memory%20from%20human%20visual%20and%20parietal%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.0184-12.2012&amp;volume=32&amp;pages=12983-12989&amp;publication_year=2012&amp;author=Christophel%2CTB&amp;author=Hebart%2CMN&amp;author=Haynes%2CJD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Ester, E. F., Anderson, D. E., Serences, J. T. &amp; Awh, E. A neural measure of precision in visual working memory. <i>J. Cog. Neurosci.</i> <b>25</b>, 754761 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1162/jocn_a_00357" data-track-action="article reference" href="https://doi.org/10.1162%2Fjocn_a_00357" aria-label="Article reference 5" data-doi="10.1162/jocn_a_00357">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20neural%20measure%20of%20precision%20in%20visual%20working%20memory&amp;journal=J.%20Cog.%20Neurosci.&amp;doi=10.1162%2Fjocn_a_00357&amp;volume=25&amp;pages=754-761&amp;publication_year=2013&amp;author=Ester%2CEF&amp;author=Anderson%2CDE&amp;author=Serences%2CJT&amp;author=Awh%2CE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Bettencourt, K. C. &amp; Xu, Y. Decoding the content of visual short-term memory under distraction in occipital and parietal areas. <i>Nat. Neurosci.</i> <b>19</b>, 150157 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.4174" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.4174" aria-label="Article reference 6" data-doi="10.1038/nn.4174">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXhvVOgtLfP" aria-label="CAS reference 6">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20the%20content%20of%20visual%20short-term%20memory%20under%20distraction%20in%20occipital%20and%20parietal%20areas&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.4174&amp;volume=19&amp;pages=150-157&amp;publication_year=2016&amp;author=Bettencourt%2CKC&amp;author=Xu%2CY">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Mendoza-Halliday, D., Torres, S. &amp; Martinez-Trujillo, J. C. Sharp emergence of feature-selective sustained activity along the dorsal visual pathway. <i>Nat. Neurosci.</i> <b>17</b>, 12551262 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.3785" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.3785" aria-label="Article reference 7" data-doi="10.1038/nn.3785">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXhtlahurvE" aria-label="CAS reference 7">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Sharp%20emergence%20of%20feature-selective%20sustained%20activity%20along%20the%20dorsal%20visual%20pathway&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.3785&amp;volume=17&amp;pages=1255-1262&amp;publication_year=2014&amp;author=Mendoza-Halliday%2CD&amp;author=Torres%2CS&amp;author=Martinez-Trujillo%2CJC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Stokes, M. G. Activity-silent working memory in prefrontal cortex: a dynamic coding framework. <i>Trends Cog. Sci.</i> <b>19</b>, 394405 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2015.05.004" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2015.05.004" aria-label="Article reference 8" data-doi="10.1016/j.tics.2015.05.004">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=%E2%80%98Activity-silent%E2%80%99%20working%20memory%20in%20prefrontal%20cortex%3A%20a%20dynamic%20coding%20framework&amp;journal=Trends%20Cog.%20Sci.&amp;doi=10.1016%2Fj.tics.2015.05.004&amp;volume=19&amp;pages=394-405&amp;publication_year=2015&amp;author=Stokes%2CMG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Ester, E. F., Rademaker, R. L. &amp; Sprague, T. S. How do visual and parietal cortex contribute to visual short-term memory? <i>eNeuro</i> <b>3</b>, e004116 (2016). 2016 13.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/ENEURO.0041-16.2016" data-track-action="article reference" href="https://doi.org/10.1523%2FENEURO.0041-16.2016" aria-label="Article reference 9" data-doi="10.1523/ENEURO.0041-16.2016">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20do%20visual%20and%20parietal%20cortex%20contribute%20to%20visual%20short-term%20memory%3F&amp;journal=eNeuro&amp;doi=10.1523%2FENEURO.0041-16.2016&amp;volume=3&amp;pages=e0041-16&amp;publication_year=2016&amp;author=Ester%2CEF&amp;author=Rademaker%2CRL&amp;author=Sprague%2CTS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Nassi, J. J. &amp; Callaway, E. M. Parallel processing strategies of the primate visual system. <i>Nat. Rev. Neurosci.</i> <b>10</b>, 360372 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nrn2619" data-track-action="article reference" href="https://doi.org/10.1038%2Fnrn2619" aria-label="Article reference 10" data-doi="10.1038/nrn2619">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXktFGrs7c%3D" aria-label="CAS reference 10">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Parallel%20processing%20strategies%20of%20the%20primate%20visual%20system&amp;journal=Nat.%20Rev.%20Neurosci.&amp;doi=10.1038%2Fnrn2619&amp;volume=10&amp;pages=360-372&amp;publication_year=2009&amp;author=Nassi%2CJJ&amp;author=Callaway%2CEM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Van Kerkoerle, T., Self, M. W. &amp; Roelfsema, P. R. Layer-specificity in the effects of attention and working memory on activity in primary visual cortex. <i>Nat. Comm.</i> <b>8</b>, 13804 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/ncomms13804" data-track-action="article reference" href="https://doi.org/10.1038%2Fncomms13804" aria-label="Article reference 11" data-doi="10.1038/ncomms13804">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Layer-specificity%20in%20the%20effects%20of%20attention%20and%20working%20memory%20on%20activity%20in%20primary%20visual%20cortex&amp;journal=Nat.%20Comm.&amp;doi=10.1038%2Fncomms13804&amp;volume=8&amp;publication_year=2017&amp;author=Kerkoerle%2CT&amp;author=Self%2CMW&amp;author=Roelfsema%2CPR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Miller, E. K., Li, L. &amp; Desimone, R. Activity of neurons in anterior inferior temporal cortex during a short-term memory task. <i>J. Neurosci.</i> <b>13</b>, 14601478 (1993).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.13-04-01460.1993" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.13-04-01460.1993" aria-label="Article reference 12" data-doi="10.1523/JNEUROSCI.13-04-01460.1993">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK3s3hslGgsg%3D%3D" aria-label="CAS reference 12">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Activity%20of%20neurons%20in%20anterior%20inferior%20temporal%20cortex%20during%20a%20short-term%20memory%20task&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.13-04-01460.1993&amp;volume=13&amp;pages=1460-1478&amp;publication_year=1993&amp;author=Miller%2CEK&amp;author=Li%2CL&amp;author=Desimone%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Serences, J. T. Neural mechanisms of information storage in visual short-term memory. <i>Vis. Res.</i> <b>128</b>, 5367 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.visres.2016.09.010" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.visres.2016.09.010" aria-label="Article reference 13" data-doi="10.1016/j.visres.2016.09.010">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20mechanisms%20of%20information%20storage%20in%20visual%20short-term%20memory&amp;journal=Vis.%20Res.&amp;doi=10.1016%2Fj.visres.2016.09.010&amp;volume=128&amp;pages=53-67&amp;publication_year=2016&amp;author=Serences%2CJT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Brouwer, G. J. &amp; Heeger, D. J. Decoding and reconstructing color from responses in human visual cortex. <i>J. Neurosci.</i> <b>29</b>, 1399214003 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.3577-09.2009" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.3577-09.2009" aria-label="Article reference 14" data-doi="10.1523/JNEUROSCI.3577-09.2009">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXhsVSmsbnK" aria-label="CAS reference 14">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20and%20reconstructing%20color%20from%20responses%20in%20human%20visual%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.3577-09.2009&amp;volume=29&amp;pages=13992-14003&amp;publication_year=2009&amp;author=Brouwer%2CGJ&amp;author=Heeger%2CDJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Sprague, T. C., Saproo, S. &amp; Serences, J. T. Visual attention mitigates information loss in small- and large-scale neural codes. <i>Trends Cogn. Sci.</i> <b>19</b>, 215226 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2015.02.005" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2015.02.005" aria-label="Article reference 15" data-doi="10.1016/j.tics.2015.02.005">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20attention%20mitigates%20information%20loss%20in%20small-%20and%20large-scale%20neural%20codes&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2015.02.005&amp;volume=19&amp;pages=215-226&amp;publication_year=2015&amp;author=Sprague%2CTC&amp;author=Saproo%2CS&amp;author=Serences%2CJT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Rademaker, R. L., Bloem, I. M., De Weerd, P. &amp; Sack, A. S. The impact of interference on short-term memory for visual orientation. <i>J. Exp. Psychol. Hum. Percept. Perform.</i> <b>41</b>, 16501665 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/xhp0000110" data-track-action="article reference" href="https://doi.org/10.1037%2Fxhp0000110" aria-label="Article reference 16" data-doi="10.1037/xhp0000110">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20impact%20of%20interference%20on%20short-term%20memory%20for%20visual%20orientation&amp;journal=J.%20Exp.%20Psychol.%20Hum.%20Percept.%20Perform.&amp;doi=10.1037%2Fxhp0000110&amp;volume=41&amp;pages=1650-1665&amp;publication_year=2015&amp;author=Rademaker%2CRL&amp;author=Bloem%2CIM&amp;author=Weerd%2CP&amp;author=Sack%2CAS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Wildegger, T., Meyers, N. E., Humphreys, G. &amp; Nobre, A. C. Supraliminal but not subliminal distracters bias working memory recall. <i>J. Exp. Psychol. Hum. Percept. Perform.</i> <b>41</b>, 826839 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/xhp0000052" data-track-action="article reference" href="https://doi.org/10.1037%2Fxhp0000052" aria-label="Article reference 17" data-doi="10.1037/xhp0000052">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Supraliminal%20but%20not%20subliminal%20distracters%20bias%20working%20memory%20recall&amp;journal=J.%20Exp.%20Psychol.%20Hum.%20Percept.%20Perform.&amp;doi=10.1037%2Fxhp0000052&amp;volume=41&amp;pages=826-839&amp;publication_year=2015&amp;author=Wildegger%2CT&amp;author=Meyers%2CNE&amp;author=Humphreys%2CG&amp;author=Nobre%2CAC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Silver, M. A., Ress, D. &amp; Heeger, D. J. Topographic maps of visual spatial attention in human parietal cortex. <i>J. Neurophysiol.</i> <b>94</b>, 13581371 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.01316.2004" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.01316.2004" aria-label="Article reference 18" data-doi="10.1152/jn.01316.2004">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Topographic%20maps%20of%20visual%20spatial%20attention%20in%20human%20parietal%20cortex&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.01316.2004&amp;volume=94&amp;pages=1358-1371&amp;publication_year=2005&amp;author=Silver%2CMA&amp;author=Ress%2CD&amp;author=Heeger%2CDJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Serences, J. T. &amp; Yantis, S. Selective visual attention and perceptual coherence. <i>Trends Cogn. Sci.</i> <b>10</b>, 3845 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2005.11.008" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2005.11.008" aria-label="Article reference 19" data-doi="10.1016/j.tics.2005.11.008">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Selective%20visual%20attention%20and%20perceptual%20coherence&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2005.11.008&amp;volume=10&amp;pages=38-45&amp;publication_year=2006&amp;author=Serences%2CJT&amp;author=Yantis%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Poltoratski, S., Ling, S., McCormack, D. &amp; Tong, F. Characterizing the effects of feature salience and top-down attention in the early visual system. <i>J. Neurophysiol.</i> <b>118</b>, 564573 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.00924.2016" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.00924.2016" aria-label="Article reference 20" data-doi="10.1152/jn.00924.2016">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Characterizing%20the%20effects%20of%20feature%20salience%20and%20top-down%20attention%20in%20the%20early%20visual%20system&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.00924.2016&amp;volume=118&amp;pages=564-573&amp;publication_year=2017&amp;author=Poltoratski%2CS&amp;author=Ling%2CS&amp;author=McCormack%2CD&amp;author=Tong%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Sprague, T. C., Itthipuripat, S., Vo, V. A. &amp; Serences, J. T. Dissociable signatures of visual salience and behavioral relevance across attentional priority maps in human cortex. <i>J. Neurophysiol.</i> <b>119</b>, 21532165 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.00059.2018" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.00059.2018" aria-label="Article reference 21" data-doi="10.1152/jn.00059.2018">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissociable%20signatures%20of%20visual%20salience%20and%20behavioral%20relevance%20across%20attentional%20priority%20maps%20in%20human%20cortex&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.00059.2018&amp;volume=119&amp;pages=2153-2165&amp;publication_year=2018&amp;author=Sprague%2CTC&amp;author=Itthipuripat%2CS&amp;author=Vo%2CVA&amp;author=Serences%2CJT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Murray, J. D. et al. Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex. <i>Proc. Natl Acad. Sci. USA</i> <b>114</b>, 394399 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1619449114" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1619449114" aria-label="Article reference 22" data-doi="10.1073/pnas.1619449114">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC28XitFGntr7E" aria-label="CAS reference 22">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Stable%20population%20coding%20for%20working%20memory%20coexists%20with%20heterogeneous%20neural%20dynamics%20in%20prefrontal%20cortex&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1619449114&amp;volume=114&amp;pages=394-399&amp;publication_year=2017&amp;author=Murray%2CJD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">DiCarlo, J. J., Zoccolan, D. &amp; Rust, N. C. How does the brain solve visual object recognition? <i>Neuron</i> <b>73</b>, 415434 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2012.01.010" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2012.01.010" aria-label="Article reference 23" data-doi="10.1016/j.neuron.2012.01.010">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XitFOjtL4%3D" aria-label="CAS reference 23">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20does%20the%20brain%20solve%20visual%20object%20recognition%3F&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2012.01.010&amp;volume=73&amp;pages=415-434&amp;publication_year=2012&amp;author=DiCarlo%2CJJ&amp;author=Zoccolan%2CD&amp;author=Rust%2CNC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Rademaker, R. L., Park, Y. E., Sack, A. T. &amp; Tong, F. Evidence of gradual loss of precision for simple features and complex objects in visual working memory. <i>J. Exp. Psychol. Hum. Percept. Perform.</i> <b>44</b>, 925940 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/xhp0000491" data-track-action="article reference" href="https://doi.org/10.1037%2Fxhp0000491" aria-label="Article reference 24" data-doi="10.1037/xhp0000491">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Evidence%20of%20gradual%20loss%20of%20precision%20for%20simple%20features%20and%20complex%20objects%20in%20visual%20working%20memory&amp;journal=J.%20Exp.%20Psychol.%20Hum.%20Percept.%20Perform.&amp;doi=10.1037%2Fxhp0000491&amp;volume=44&amp;pages=925-940&amp;publication_year=2018&amp;author=Rademaker%2CRL&amp;author=Park%2CYE&amp;author=Sack%2CAT&amp;author=Tong%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Bisley, J. W., Zaksas, D., Droll, J. A. &amp; Pasternak, T. Activity of neurons in cortical area MT during a memory for motion task. <i>J. Neurophysiol.</i> <b>91</b>, 286300 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.00870.2003" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.00870.2003" aria-label="Article reference 25" data-doi="10.1152/jn.00870.2003">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Activity%20of%20neurons%20in%20cortical%20area%20MT%20during%20a%20memory%20for%20motion%20task&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.00870.2003&amp;volume=91&amp;pages=286-300&amp;publication_year=2004&amp;author=Bisley%2CJW&amp;author=Zaksas%2CD&amp;author=Droll%2CJA&amp;author=Pasternak%2CT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Zaksas, D. &amp; Paternak, T. Direction signals in the prefrontal cortex and in area MT during a working memory for visual motion task. <i>J. Neurosci.</i> <b>26</b>, 1172611742 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.3420-06.2006" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.3420-06.2006" aria-label="Article reference 26" data-doi="10.1523/JNEUROSCI.3420-06.2006">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28Xht1Glu7jM" aria-label="CAS reference 26">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Direction%20signals%20in%20the%20prefrontal%20cortex%20and%20in%20area%20MT%20during%20a%20working%20memory%20for%20visual%20motion%20task&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.3420-06.2006&amp;volume=26&amp;pages=11726-11742&amp;publication_year=2006&amp;author=Zaksas%2CD&amp;author=Paternak%2CT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Gayet, S. et al. Visual working memory enhances the neural response to matching visual input. <i>J. Neurosci.</i> <b>37</b>, 66386647 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.3418-16.2017" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.3418-16.2017" aria-label="Article reference 27" data-doi="10.1523/JNEUROSCI.3418-16.2017">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1cXptlKqtQ%3D%3D" aria-label="CAS reference 27">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20working%20memory%20enhances%20the%20neural%20response%20to%20matching%20visual%20input&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.3418-16.2017&amp;volume=37&amp;pages=6638-6647&amp;publication_year=2017&amp;author=Gayet%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Merrikhi, Y. et al. Spatial working memory alters the efficacy of input to visual cortex. <i>Nat. Comms.</i> <b>8</b>, 15041 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/ncomms15041" data-track-action="article reference" href="https://doi.org/10.1038%2Fncomms15041" aria-label="Article reference 28" data-doi="10.1038/ncomms15041">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20working%20memory%20alters%20the%20efficacy%20of%20input%20to%20visual%20cortex&amp;journal=Nat.%20Comms.&amp;doi=10.1038%2Fncomms15041&amp;volume=8&amp;publication_year=2017&amp;author=Merrikhi%2CY">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Miller, E. K., Li, L. &amp; Desimone, R. A neural mechanism for working and recognition memory in inferior temporal cortex. <i>Science</i> <b>254</b>, 13771379 (1991).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1962197" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1962197" aria-label="Article reference 29" data-doi="10.1126/science.1962197">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK38%2Fntl2jug%3D%3D" aria-label="CAS reference 29">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20neural%20mechanism%20for%20working%20and%20recognition%20memory%20in%20inferior%20temporal%20cortex&amp;journal=Science&amp;doi=10.1126%2Fscience.1962197&amp;volume=254&amp;pages=1377-1379&amp;publication_year=1991&amp;author=Miller%2CEK&amp;author=Li%2CL&amp;author=Desimone%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Maunsell, J. H. R., Sclar, G., Nealey, T. A. &amp; DePriest, D. D. Extraretinal representations in area V4 in the macaque monkey. <i>Vis. Neurosci.</i> <b>7</b>, 561573 (1991).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1017/S095252380001035X" data-track-action="article reference" href="https://doi.org/10.1017%2FS095252380001035X" aria-label="Article reference 30" data-doi="10.1017/S095252380001035X">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK387is1Gktw%3D%3D" aria-label="CAS reference 30">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Extraretinal%20representations%20in%20area%20V4%20in%20the%20macaque%20monkey&amp;journal=Vis.%20Neurosci.&amp;doi=10.1017%2FS095252380001035X&amp;volume=7&amp;pages=561-573&amp;publication_year=1991&amp;author=Maunsell%2CJHR&amp;author=Sclar%2CG&amp;author=Nealey%2CTA&amp;author=DePriest%2CDD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Miller, E. K. &amp; Desimone, R. Parallel neuronal mechanisms for short-term memory. <i>Science</i> <b>263</b>, 520522 (1994).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.8290960" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.8290960" aria-label="Article reference 31" data-doi="10.1126/science.8290960">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK2c7itVCisw%3D%3D" aria-label="CAS reference 31">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Parallel%20neuronal%20mechanisms%20for%20short-term%20memory&amp;journal=Science&amp;doi=10.1126%2Fscience.8290960&amp;volume=263&amp;pages=520-522&amp;publication_year=1994&amp;author=Miller%2CEK&amp;author=Desimone%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Miller, E. K., Erickson, C. A. &amp; Desimone, R. Neural mechanisms of visual working memory in prefrontal cortex of the macaque. <i>J. Neurosci.</i> <b>16</b>, 51545167 (1996).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.16-16-05154.1996" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.16-16-05154.1996" aria-label="Article reference 32" data-doi="10.1523/JNEUROSCI.16-16-05154.1996">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK28XkvFaht74%3D" aria-label="CAS reference 32">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20mechanisms%20of%20visual%20working%20memory%20in%20prefrontal%20cortex%20of%20the%20macaque&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.16-16-05154.1996&amp;volume=16&amp;pages=5154-5167&amp;publication_year=1996&amp;author=Miller%2CEK&amp;author=Erickson%2CCA&amp;author=Desimone%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Jacob, S. N. &amp; Nieder, A. Complementary roles for primate frontal and parietal cortex in guarding working memory from distractor stimuli. <i>Neuron</i> <b>83</b>, 226237 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2014.05.009" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2014.05.009" aria-label="Article reference 33" data-doi="10.1016/j.neuron.2014.05.009">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXhtFWgt7bP" aria-label="CAS reference 33">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Complementary%20roles%20for%20primate%20frontal%20and%20parietal%20cortex%20in%20guarding%20working%20memory%20from%20distractor%20stimuli&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2014.05.009&amp;volume=83&amp;pages=226-237&amp;publication_year=2014&amp;author=Jacob%2CSN&amp;author=Nieder%2CA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Qi, X.-L., Elworthy, A. C., Lambert, B. C. &amp; Constantinidis, C. Representation of remembered stimuli and task information in the monkey dorsolateral prefrontal and posterior parietal cortex. <i>J. Neurophysiol.</i> <b>113</b>, 4457 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.00413.2014" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.00413.2014" aria-label="Article reference 34" data-doi="10.1152/jn.00413.2014">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Representation%20of%20remembered%20stimuli%20and%20task%20information%20in%20the%20monkey%20dorsolateral%20prefrontal%20and%20posterior%20parietal%20cortex&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.00413.2014&amp;volume=113&amp;pages=44-57&amp;publication_year=2015&amp;author=Qi%2CX-L&amp;author=Elworthy%2CAC&amp;author=Lambert%2CBC&amp;author=Constantinidis%2CC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Silver, M. A. &amp; Kastner, S. Topographic maps in human frontal and parietal cortex. <i>Trends Cogn. Sci.</i> <b>13</b>, 488495 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2009.08.005" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2009.08.005" aria-label="Article reference 35" data-doi="10.1016/j.tics.2009.08.005">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Topographic%20maps%20in%20human%20frontal%20and%20parietal%20cortex&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2009.08.005&amp;volume=13&amp;pages=488-495&amp;publication_year=2009&amp;author=Silver%2CMA&amp;author=Kastner%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Bressler, D. W. &amp; Silver, M. A. Spatial attention improves reliability of fMRI retinotopic mapping signals in occipital and parietal cortex. <i>Neuroimage</i> <b>53</b>, 526533 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2010.06.063" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2010.06.063" aria-label="Article reference 36" data-doi="10.1016/j.neuroimage.2010.06.063">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20attention%20improves%20reliability%20of%20fMRI%20retinotopic%20mapping%20signals%20in%20occipital%20and%20parietal%20cortex&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2010.06.063&amp;volume=53&amp;pages=526-533&amp;publication_year=2010&amp;author=Bressler%2CDW&amp;author=Silver%2CMA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Deutsch, D. Tones and numbers: specificity of interference in immediate memory. <i>Science</i> <b>168</b>, 16041605 (1970).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.168.3939.1604" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.168.3939.1604" aria-label="Article reference 37" data-doi="10.1126/science.168.3939.1604">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaE3c3gs1ektg%3D%3D" aria-label="CAS reference 37">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Tones%20and%20numbers%3A%20specificity%20of%20interference%20in%20immediate%20memory&amp;journal=Science&amp;doi=10.1126%2Fscience.168.3939.1604&amp;volume=168&amp;pages=1604-1605&amp;publication_year=1970&amp;author=Deutsch%2CD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Deutsch, D. Interference in memory between tones adjacent in the musical scale. <i>J. Exp. Psychol.</i> <b>100</b>, 228231 (1973).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/h0035440" data-track-action="article reference" href="https://doi.org/10.1037%2Fh0035440" aria-label="Article reference 38" data-doi="10.1037/h0035440">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaE2c%2FhtFarug%3D%3D" aria-label="CAS reference 38">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Interference%20in%20memory%20between%20tones%20adjacent%20in%20the%20musical%20scale&amp;journal=J.%20Exp.%20Psychol.&amp;doi=10.1037%2Fh0035440&amp;volume=100&amp;pages=228-231&amp;publication_year=1973&amp;author=Deutsch%2CD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Magnussen, S., Greenlee, M. W., Asplund, R. &amp; Dyrnes, S. Stimulus-specific mechanisms of visual short-term memory. <i>Vis. Res.</i> <b>31</b>, 12131219 (1991).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/0042-6989(91)90046-8" data-track-action="article reference" href="https://doi.org/10.1016%2F0042-6989%2891%2990046-8" aria-label="Article reference 39" data-doi="10.1016/0042-6989(91)90046-8">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK3MzmvVerug%3D%3D" aria-label="CAS reference 39">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Stimulus-specific%20mechanisms%20of%20visual%20short-term%20memory&amp;journal=Vis.%20Res.&amp;doi=10.1016%2F0042-6989%2891%2990046-8&amp;volume=31&amp;pages=1213-1219&amp;publication_year=1991&amp;author=Magnussen%2CS&amp;author=Greenlee%2CMW&amp;author=Asplund%2CR&amp;author=Dyrnes%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Magnussen, S. &amp; Greenlee, M. W. Retention and disruption of motion information in visual short-term memory. <i>J. Exp. Psychol. Learn. Mem. Cogn.</i> <b>18</b>, 151156 (1992).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/0278-7393.18.1.151" data-track-action="article reference" href="https://doi.org/10.1037%2F0278-7393.18.1.151" aria-label="Article reference 40" data-doi="10.1037/0278-7393.18.1.151">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK387osFCiug%3D%3D" aria-label="CAS reference 40">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Retention%20and%20disruption%20of%20motion%20information%20in%20visual%20short-term%20memory&amp;journal=J.%20Exp.%20Psychol.%20Learn.%20Mem.%20Cogn.&amp;doi=10.1037%2F0278-7393.18.1.151&amp;volume=18&amp;pages=151-156&amp;publication_year=1992&amp;author=Magnussen%2CS&amp;author=Greenlee%2CMW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Pasternak, T. &amp; Zaksas, D. Stimulus specificity and temporal dynamics of working memory for visual motion. <i>J. Neurophysiol.</i> <b>90</b>, 27572762 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.00422.2003" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.00422.2003" aria-label="Article reference 41" data-doi="10.1152/jn.00422.2003">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Stimulus%20specificity%20and%20temporal%20dynamics%20of%20working%20memory%20for%20visual%20motion&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.00422.2003&amp;volume=90&amp;pages=2757-2762&amp;publication_year=2003&amp;author=Pasternak%2CT&amp;author=Zaksas%2CD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Van der Stigchel, S., Merten, H., Meeter, M. &amp; Theeuwes, J. The effects of a task-irrelevant visual event on spatial working memory. <i>Psychon. Bull. Rev.</i> <b>14</b>, 10661071 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3758/BF03193092" data-track-action="article reference" href="https://doi.org/10.3758%2FBF03193092" aria-label="Article reference 42" data-doi="10.3758/BF03193092">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effects%20of%20a%20task-irrelevant%20visual%20event%20on%20spatial%20working%20memory&amp;journal=Psychon.%20Bull.%20Rev.&amp;doi=10.3758%2FBF03193092&amp;volume=14&amp;pages=1066-1071&amp;publication_year=2007&amp;author=Stigchel%2CS&amp;author=Merten%2CH&amp;author=Meeter%2CM&amp;author=Theeuwes%2CJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Huang, J. &amp; Sekuler, R. Distortions in recall from visual memory: two classes of attractors at work. <i>J. Vis.</i> <b>10</b>, 127 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1167/10.2.24" data-track-action="article reference" href="https://doi.org/10.1167%2F10.2.24" aria-label="Article reference 43" data-doi="10.1167/10.2.24">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3cXhs1aqu7bK" aria-label="CAS reference 43">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Distortions%20in%20recall%20from%20visual%20memory%3A%20two%20classes%20of%20attractors%20at%20work&amp;journal=J.%20Vis.&amp;doi=10.1167%2F10.2.24&amp;volume=10&amp;pages=1-27&amp;publication_year=2010&amp;author=Huang%2CJ&amp;author=Sekuler%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Nemes, V. A., Parry, N. R., Whitaker, D. &amp; McKeefry, D. J. The retention and disruption of color information in human short-term visual memory. <i>J. Vis.</i> <b>12</b>, 114 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1167/12.1.26" data-track-action="article reference" href="https://doi.org/10.1167%2F12.1.26" aria-label="Article reference 44" data-doi="10.1167/12.1.26">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20retention%20and%20disruption%20of%20color%20information%20in%20human%20short-term%20visual%20memory&amp;journal=J.%20Vis.&amp;doi=10.1167%2F12.1.26&amp;volume=12&amp;pages=1-14&amp;publication_year=2012&amp;author=Nemes%2CVA&amp;author=Parry%2CNR&amp;author=Whitaker%2CD&amp;author=McKeefry%2CDJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Bae, G. Y. &amp; Luck, S. J. Interactions between visual working memory representations. <i>Atten. Percep. Psychophys.</i> <b>79</b>, 23762395 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3758/s13414-017-1404-8" data-track-action="article reference" href="https://doi.org/10.3758%2Fs13414-017-1404-8" aria-label="Article reference 45" data-doi="10.3758/s13414-017-1404-8">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactions%20between%20visual%20working%20memory%20representations&amp;journal=Atten.%20Percep.%20Psychophys.&amp;doi=10.3758%2Fs13414-017-1404-8&amp;volume=79&amp;pages=2376-2395&amp;publication_year=2017&amp;author=Bae%2CGY&amp;author=Luck%2CSJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Lorenc, E. S., Sreenivasan, K. K., Nee, D. E., Vandenbroucke, A. R. E. &amp; DEsposito, M. Flexible coding of visual working memory representations during distraction. <i>J. Neurosci.</i> <b>38</b>, 52675276 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.3061-17.2018" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.3061-17.2018" aria-label="Article reference 46" data-doi="10.1523/JNEUROSCI.3061-17.2018">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1cXit1Gjt73I" aria-label="CAS reference 46">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Flexible%20coding%20of%20visual%20working%20memory%20representations%20during%20distraction&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.3061-17.2018&amp;volume=38&amp;pages=5267-5276&amp;publication_year=2018&amp;author=Lorenc%2CES&amp;author=Sreenivasan%2CKK&amp;author=Nee%2CDE&amp;author=Vandenbroucke%2CARE&amp;author=D%E2%80%99Esposito%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Chunharas, C., Rademaker, R. L., Brady, T. F. &amp; Serences, J. T. Adaptive memory distortion in visual working memory. Preprint at <i>PsyArXiv</i> <a href="https://psyarxiv.com/e3m5a/" data-track="click" data-track-action="external reference" data-track-label="https://psyarxiv.com/e3m5a/">https://psyarxiv.com/e3m5a/</a> (2019).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">Sprague, T. C., Ester, E. F. &amp; Serences, J. T. Restoring latent visual working memory representations in human cortex. <i>Neuron</i> <b>91</b>, 694707 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2016.07.006" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2016.07.006" aria-label="Article reference 48" data-doi="10.1016/j.neuron.2016.07.006">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC28XhtlSgt7zF" aria-label="CAS reference 48">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Restoring%20latent%20visual%20working%20memory%20representations%20in%20human%20cortex&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2016.07.006&amp;volume=91&amp;pages=694-707&amp;publication_year=2016&amp;author=Sprague%2CTC&amp;author=Ester%2CEF&amp;author=Serences%2CJT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">Christophel, T. G., Iamshchinina, P., Yan, C., Allefeld, C. &amp; Haynes, J. D. Cortical specialization for attended versus unattended working memory. <i>Nat. Neurosci.</i> <b>21</b>, 494496 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41593-018-0094-4" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41593-018-0094-4" aria-label="Article reference 49" data-doi="10.1038/s41593-018-0094-4">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1cXltVyhtrw%3D" aria-label="CAS reference 49">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Cortical%20specialization%20for%20attended%20versus%20unattended%20working%20memory&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fs41593-018-0094-4&amp;volume=21&amp;pages=494-496&amp;publication_year=2018&amp;author=Christophel%2CTG&amp;author=Iamshchinina%2CP&amp;author=Yan%2CC&amp;author=Allefeld%2CC&amp;author=Haynes%2CJD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">Rose, N. S. et al. Reactivation of latent working memories with transcranial magnetic stimulation. <i>Science</i> <b>354</b>, 11361139 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.aah7011" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.aah7011" aria-label="Article reference 50" data-doi="10.1126/science.aah7011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC28XhvFGrt7bN" aria-label="CAS reference 50">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Reactivation%20of%20latent%20working%20memories%20with%20transcranial%20magnetic%20stimulation&amp;journal=Science&amp;doi=10.1126%2Fscience.aah7011&amp;volume=354&amp;pages=1136-1139&amp;publication_year=2016&amp;author=Rose%2CNS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">Brainard, D. H. The Psychophysics Toolbox. <i>Spat. Vis.</i> <b>10</b>, 433436 (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1163/156856897X00357" data-track-action="article reference" href="https://doi.org/10.1163%2F156856897X00357" aria-label="Article reference 51" data-doi="10.1163/156856897X00357">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK2szitVSlug%3D%3D" aria-label="CAS reference 51">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Psychophysics%20Toolbox&amp;journal=Spat.%20Vis.&amp;doi=10.1163%2F156856897X00357&amp;volume=10&amp;pages=433-436&amp;publication_year=1997&amp;author=Brainard%2CDH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">Kleiner, M. et al. Whats new in psychtoolbox-3. <i>Perception</i> <b>36</b>, 116 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=What%E2%80%99s%20new%20in%20psychtoolbox-3&amp;journal=Perception&amp;volume=36&amp;pages=1-16&amp;publication_year=2007&amp;author=Kleiner%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">Tyler, C. W. &amp; Nakayama, K. Grating induction: a new type of aftereffect. <i>Vis. Res.</i> <b>20</b>, 437441 (1980).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/0042-6989(80)90034-6" data-track-action="article reference" href="https://doi.org/10.1016%2F0042-6989%2880%2990034-6" aria-label="Article reference 53" data-doi="10.1016/0042-6989(80)90034-6">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaL3M%2Fhs1Oqug%3D%3D" aria-label="CAS reference 53">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Grating%20induction%3A%20a%20new%20type%20of%20aftereffect&amp;journal=Vis.%20Res.&amp;doi=10.1016%2F0042-6989%2880%2990034-6&amp;volume=20&amp;pages=437-441&amp;publication_year=1980&amp;author=Tyler%2CCW&amp;author=Nakayama%2CK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR54">Goeleven, E., De Raedt, R., Leyman, L. &amp; Verschuere, B. The Karolinska directed emotional faces: a validation study. <i>Cogn. Emot.</i> <b>22</b>, 10941118 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1080/02699930701626582" data-track-action="article reference" href="https://doi.org/10.1080%2F02699930701626582" aria-label="Article reference 54" data-doi="10.1080/02699930701626582">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Karolinska%20directed%20emotional%20faces%3A%20a%20validation%20study&amp;journal=Cogn.%20Emot.&amp;doi=10.1080%2F02699930701626582&amp;volume=22&amp;pages=1094-1118&amp;publication_year=2008&amp;author=Goeleven%2CE&amp;author=Raedt%2CR&amp;author=Leyman%2CL&amp;author=Verschuere%2CB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55."><p class="c-article-references__text" id="ref-CR55">Rovamo, J. &amp; Virsu, V. An estimation and application of the human cortical magnification factor. <i>Exp. Brain Res.</i> <b>37</b>, 495510 (1979).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="noopener" data-track-label="10.1007/BF00236819" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/BF00236819" aria-label="Article reference 55" data-doi="10.1007/BF00236819">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaL3c%2FptFyrug%3D%3D" aria-label="CAS reference 55">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20estimation%20and%20application%20of%20the%20human%20cortical%20magnification%20factor&amp;journal=Exp.%20Brain%20Res.&amp;doi=10.1007%2FBF00236819&amp;volume=37&amp;pages=495-510&amp;publication_year=1979&amp;author=Rovamo%2CJ&amp;author=Virsu%2CV">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56."><p class="c-article-references__text" id="ref-CR56">Andersson, J. L. R., Skare, S. &amp; Ashburner, J. How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging. <i>Neuroimage</i> <b>20</b>, 870888 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S1053-8119(03)00336-7" data-track-action="article reference" href="https://doi.org/10.1016%2FS1053-8119%2803%2900336-7" aria-label="Article reference 56" data-doi="10.1016/S1053-8119(03)00336-7">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20to%20correct%20susceptibility%20distortions%20in%20spin-echo%20echo-planar%20images%3A%20application%20to%20diffusion%20tensor%20imaging&amp;journal=Neuroimage&amp;doi=10.1016%2FS1053-8119%2803%2900336-7&amp;volume=20&amp;pages=870-888&amp;publication_year=2003&amp;author=Andersson%2CJLR&amp;author=Skare%2CS&amp;author=Ashburner%2CJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57."><p class="c-article-references__text" id="ref-CR57">Smith, S. M. et al. Advances in functional and structural MR image analysis and implementation as FSL. <i>Neuroimage</i> <b>23</b>, 208219 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2004.07.051" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2004.07.051" aria-label="Article reference 57" data-doi="10.1016/j.neuroimage.2004.07.051">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Advances%20in%20functional%20and%20structural%20MR%20image%20analysis%20and%20implementation%20as%20FSL&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2004.07.051&amp;volume=23&amp;pages=208-219&amp;publication_year=2004&amp;author=Smith%2CSM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58."><p class="c-article-references__text" id="ref-CR58">Jenkinson, M., Beckmann, C. F., Behrens, T. E., Woolrich, M. W. &amp; Smith, S. M. FSL. <i>Neuroimage</i> <b>62</b>, 782790 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2011.09.015" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2011.09.015" aria-label="Article reference 58" data-doi="10.1016/j.neuroimage.2011.09.015">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=FSL&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2011.09.015&amp;volume=62&amp;pages=782-790&amp;publication_year=2012&amp;author=Jenkinson%2CM&amp;author=Beckmann%2CCF&amp;author=Behrens%2CTE&amp;author=Woolrich%2CMW&amp;author=Smith%2CSM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59."><p class="c-article-references__text" id="ref-CR59">Dale, A. M., Fischl, B. &amp; Sereno, M. I. Cortical surface-based analysis. I. Segmentation and surface reconstruction. <i>Neuroimage</i> <b>9</b>, 179194 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1006/nimg.1998.0395" data-track-action="article reference" href="https://doi.org/10.1006%2Fnimg.1998.0395" aria-label="Article reference 59" data-doi="10.1006/nimg.1998.0395">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK1M7jt1Gisg%3D%3D" aria-label="CAS reference 59">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Cortical%20surface-based%20analysis.%20I.%20Segmentation%20and%20surface%20reconstruction&amp;journal=Neuroimage&amp;doi=10.1006%2Fnimg.1998.0395&amp;volume=9&amp;pages=179-194&amp;publication_year=1999&amp;author=Dale%2CAM&amp;author=Fischl%2CB&amp;author=Sereno%2CMI">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60."><p class="c-article-references__text" id="ref-CR60">Greve, D. &amp; Fischl, B. Accurate and robust brain image alignment using boundary-based registration. <i>Neuroimage</i> <b>48</b>, 6372 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2009.06.060" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2009.06.060" aria-label="Article reference 60" data-doi="10.1016/j.neuroimage.2009.06.060">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Accurate%20and%20robust%20brain%20image%20alignment%20using%20boundary-based%20registration&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2009.06.060&amp;volume=48&amp;pages=63-72&amp;publication_year=2009&amp;author=Greve%2CD&amp;author=Fischl%2CB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61."><p class="c-article-references__text" id="ref-CR61">Jenkinson, M. &amp; Smith, S. M. A global optimisation method for robust affine registration of brain images. <i>Med. Image Anal.</i> <b>5</b>, 143156 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S1361-8415(01)00036-6" data-track-action="article reference" href="https://doi.org/10.1016%2FS1361-8415%2801%2900036-6" aria-label="Article reference 61" data-doi="10.1016/S1361-8415(01)00036-6">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD3MvntVWrtw%3D%3D" aria-label="CAS reference 61">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20global%20optimisation%20method%20for%20robust%20affine%20registration%20of%20brain%20images&amp;journal=Med.%20Image%20Anal.&amp;doi=10.1016%2FS1361-8415%2801%2900036-6&amp;volume=5&amp;pages=143-156&amp;publication_year=2001&amp;author=Jenkinson%2CM&amp;author=Smith%2CSM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62."><p class="c-article-references__text" id="ref-CR62">Jenkinson, M., Bannister, P., Brady, J. M. &amp; Smith, S. M. Improved optimization for the robust and accurate linear registration and motion correction of brain images. <i>Neuroimage</i> <b>17</b>, 825841 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1006/nimg.2002.1132" data-track-action="article reference" href="https://doi.org/10.1006%2Fnimg.2002.1132" aria-label="Article reference 62" data-doi="10.1006/nimg.2002.1132">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Improved%20optimization%20for%20the%20robust%20and%20accurate%20linear%20registration%20and%20motion%20correction%20of%20brain%20images&amp;journal=Neuroimage&amp;doi=10.1006%2Fnimg.2002.1132&amp;volume=17&amp;pages=825-841&amp;publication_year=2002&amp;author=Jenkinson%2CM&amp;author=Bannister%2CP&amp;author=Brady%2CJM&amp;author=Smith%2CSM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63."><p class="c-article-references__text" id="ref-CR63">Dale, A. M. Optimal experimental design for eventrelated fMRI. <i>Hum. Brain Mapp.</i> <b>8</b>, 109114 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1002/(SICI)1097-0193(1999)8:2/3<109::AID-HBM7&gt;3.0.CO;2-W" data-track-action="article reference" href="https://doi.org/10.1002%2F%28SICI%291097-0193%281999%298%3A2%2F3%3C109%3A%3AAID-HBM7%3E3.0.CO%3B2-W" aria-label="Article reference 63" data-doi="10.1002/(SICI)1097-0193(1999)8:2/3<109::AID-HBM7&gt;3.0.CO;2-W">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK1Mvlt1agsA%3D%3D" aria-label="CAS reference 63">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 63" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimal%20experimental%20design%20for%20event%E2%80%90related%20fMRI&amp;journal=Hum.%20Brain%20Mapp.&amp;doi=10.1002%2F%28SICI%291097-0193%281999%298%3A2%2F3%3C109%3A%3AAID-HBM7%3E3.0.CO%3B2-W&amp;volume=8&amp;pages=109-114&amp;publication_year=1999&amp;author=Dale%2CAM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64."><p class="c-article-references__text" id="ref-CR64">Smith, S. M. Fast robust automated brain extraction. <i>Hum. Brain Mapp.</i> <b>17</b>, 143155 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1002/hbm.10062" data-track-action="article reference" href="https://doi.org/10.1002%2Fhbm.10062" aria-label="Article reference 64" data-doi="10.1002/hbm.10062">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=Fast%20robust%20automated%20brain%20extraction&amp;journal=Hum.%20Brain%20Mapp.&amp;doi=10.1002%2Fhbm.10062&amp;volume=17&amp;pages=143-155&amp;publication_year=2002&amp;author=Smith%2CSM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65."><p class="c-article-references__text" id="ref-CR65">Woolrich, M. W., Ripley, B. D., Brady, M. &amp; Smith, S. M. Temporal autocorrelation in univariate linear modeling of FMRI data. <i>Neuroimage</i> <b>14</b>, 13701386 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1006/nimg.2001.0931" data-track-action="article reference" href="https://doi.org/10.1006%2Fnimg.2001.0931" aria-label="Article reference 65" data-doi="10.1006/nimg.2001.0931">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD3MnlsVyjtg%3D%3D" aria-label="CAS reference 65">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=Temporal%20autocorrelation%20in%20univariate%20linear%20modeling%20of%20FMRI%20data&amp;journal=Neuroimage&amp;doi=10.1006%2Fnimg.2001.0931&amp;volume=14&amp;pages=1370-1386&amp;publication_year=2001&amp;author=Woolrich%2CMW&amp;author=Ripley%2CBD&amp;author=Brady%2CM&amp;author=Smith%2CSM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66."><p class="c-article-references__text" id="ref-CR66">Engel, S. A. et al. fMRI of human visual cortex. <i>Nature</i> <b>369</b>, 525 (1994).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/369525a0" data-track-action="article reference" href="https://doi.org/10.1038%2F369525a0" aria-label="Article reference 66" data-doi="10.1038/369525a0">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK2c3mtFWquw%3D%3D" aria-label="CAS reference 66">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 66" href="http://scholar.google.com/scholar_lookup?&amp;title=fMRI%20of%20human%20visual%20cortex&amp;journal=Nature&amp;doi=10.1038%2F369525a0&amp;volume=369&amp;publication_year=1994&amp;author=Engel%2CSA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67."><p class="c-article-references__text" id="ref-CR67">Swisher, J. D., Halko, M. A., Merabet, L. B., McMains, S. A. &amp; Somers, D. C. Visual topography of human intraparietal sulcus. <i>J. Neurosci.</i> <b>27</b>, 53265337 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.0991-07.2007" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.0991-07.2007" aria-label="Article reference 67" data-doi="10.1523/JNEUROSCI.0991-07.2007">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2sXlvFGhsr8%3D" aria-label="CAS reference 67">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 67" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20topography%20of%20human%20intraparietal%20sulcus&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.0991-07.2007&amp;volume=27&amp;pages=5326-5337&amp;publication_year=2007&amp;author=Swisher%2CJD&amp;author=Halko%2CMA&amp;author=Merabet%2CLB&amp;author=McMains%2CSA&amp;author=Somers%2CDC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68."><p class="c-article-references__text" id="ref-CR68">Sprague, T. C. &amp; Serences, J. T. Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices. <i>Nat. Neurosci.</i> <b>16</b>, 18791887 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.3574" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.3574" aria-label="Article reference 68" data-doi="10.1038/nn.3574">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhslCnt7vN" aria-label="CAS reference 68">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 68" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20modulates%20spatial%20priority%20maps%20in%20the%20human%20occipital%2C%20parietal%20and%20frontal%20cortices&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.3574&amp;volume=16&amp;pages=1879-1887&amp;publication_year=2013&amp;author=Sprague%2CTC&amp;author=Serences%2CJT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69."><p class="c-article-references__text" id="ref-CR69">Wolff, M. J., Jochim, J., Akyrek, E. G. &amp; Stokes, M. G. Dynamic hidden states underlying working-memory-guided behavior. <i>Nat. Neurosci.</i> <b>20</b>, 864871 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.4546" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.4546" aria-label="Article reference 69" data-doi="10.1038/nn.4546">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2sXmtVCqtb8%3D" aria-label="CAS reference 69">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 69" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20hidden%20states%20underlying%20working-memory-guided%20behavior&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.4546&amp;volume=20&amp;pages=864-871&amp;publication_year=2017&amp;author=Wolff%2CMJ&amp;author=Jochim%2CJ&amp;author=Aky%C3%BCrek%2CEG&amp;author=Stokes%2CMG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70."><p class="c-article-references__text" id="ref-CR70">Haynes, J. D. A primer on pattern-based approaches to fMRI: principles, pitfalls, and perspectives. <i>Neuron</i> <b>87</b>, 257270 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2015.05.025" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2015.05.025" aria-label="Article reference 70" data-doi="10.1016/j.neuron.2015.05.025">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXht1GrtLzK" aria-label="CAS reference 70">CAS</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20primer%20on%20pattern-based%20approaches%20to%20fMRI%3A%20principles%2C%20pitfalls%2C%20and%20perspectives&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2015.05.025&amp;volume=87&amp;pages=257-270&amp;publication_year=2015&amp;author=Haynes%2CJD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="71."><p class="c-article-references__text" id="ref-CR71">Berens, P. CircStat: a MATLAB toolbox for circular statistics. <i>J. Stat. Softw.</i> <b>31</b>, 121 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.18637/jss.v031.i10" data-track-action="article reference" href="https://doi.org/10.18637%2Fjss.v031.i10" aria-label="Article reference 71" data-doi="10.18637/jss.v031.i10">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 71" href="http://scholar.google.com/scholar_lookup?&amp;title=CircStat%3A%20a%20MATLAB%20toolbox%20for%20circular%20statistics&amp;journal=J.%20Stat.%20Softw.&amp;doi=10.18637%2Fjss.v031.i10&amp;volume=31&amp;pages=1-21&amp;publication_year=2009&amp;author=Berens%2CP">
                    Google Scholar</a>
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-019-0428-x?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported by grant no. NEI R01-EY025872 to J.T.S., and by the European Unions Horizon 2020 research and innovation program under the Marie Sklodowska-Curie Grant Agreement No 743941 to R.L.R. We thank A. Jacobson at the UCSD CFMRI for assistance with multiband imaging protocols. We also thank R. van Bergen for assistance setting up an FSL/FreeSurfer retinotopy pipeline, A. Chakraborty for collecting the behavioral data shown in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">9</a> and V. Vo for discussions on statistical analyses.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Psychology, University of California, San Diego, La Jolla, CA, USA</p><p class="c-article-author-affiliation__authors-list">Rosanne L. Rademaker,Chaipat Chunharas&amp;John T. Serences</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Donders Institute for Brain, Cognition and Behavior, Radboud University, Nijmegen, the Netherlands</p><p class="c-article-author-affiliation__authors-list">Rosanne L. Rademaker</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Medicine, King Chulalongkorn Memorial Hospital, Chulalongkorn University, Bangkok, Thailand</p><p class="c-article-author-affiliation__authors-list">Chaipat Chunharas</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Neurosciences Graduate Program, University of California, San Diego, La Jolla, CA, USA</p><p class="c-article-author-affiliation__authors-list">John T. Serences</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">Kavli Institute for Brain and Mind, University of California, San Diego, La Jolla, CA, USA</p><p class="c-article-author-affiliation__authors-list">John T. Serences</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Rosanne_L_-Rademaker-Aff1-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Rosanne L. Rademaker</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Rosanne%20L.%20Rademaker" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Rosanne%20L.%20Rademaker" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Rosanne%20L.%20Rademaker%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Chaipat-Chunharas-Aff1-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Chaipat Chunharas</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Chaipat%20Chunharas" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chaipat%20Chunharas" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chaipat%20Chunharas%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-John_T_-Serences-Aff1-Aff4-Aff5"><span class="c-article-authors-search__title u-h3 js-search-name">John T. Serences</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=John%20T.%20Serences" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=John%20T.%20Serences" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22John%20T.%20Serences%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>This study was designed by R.L.R., C.C. and J.T.S. Data were collected by R.L.R., and C.C. and R.L.R. preprocessed the data. R.L.R. and J.T.S. did the main analyses and wrote the manuscript.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:rosanne.rademaker@gmail.com">Rosanne L. Rademaker</a> or <a id="corresp-c2" href="mailto:jserences@ucsd.edu">John T. Serences</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading" id="FPar1">Competing interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p><b>Peer review information:</b> <i>Nature Neuroscience</i> thanks Thomas Christophel and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p><p><b>Publishers note:</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Integrated supplementary information"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Integrated supplementary information</h2><div class="c-article-section__content" id="Sec19-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig6"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 1 no systematic relationship " href="/articles/s41593-019-0428-x/figures/6" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig6_ESM.jpg">Supplementary Figure 1 No systematic relationship between target and distractor orientations.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Both Experiments 1 (<b>a</b>) and 2 (<b>b</b>) included a condition where participants were shown a grating distractor during the memory delay. Here we plot the distractor orientation (y-axis) against the target orientation (x-axis) on each single trial (dots) for all experimental subjects (subplots). To ensure relatively uniform sampling of target and distractor orientations across orientation space, both orientations were drawn pseudo-randomly from one of six orientation bins (each bin spanning 30). The boundaries between these bins are indicated with dashed lines. Importantly, orientations were drawn from each bin equally often. Thus, of the 108 total trials in the grating distractor condition, the target orientation was randomly drawn from the first orientation bin (130) 18 times, from the second bin (3160) 18 times, and so on. Thus, for each subject there are 18 points (that is trials) in each of the six columns (defined by the vertical dashed lines) of the scatter plots. Similarly, distractor orientations were randomly drawn 18 times from each bin. Thus, each row of the scatter plot (defined by the horizontal dashed lines) also contains 18 points (that is trials). Moreover, we counterbalanced the orientation bins from which target and distractor orientations were drawn. Thus, each bin combination (that is each square defined by the dashed lines) contains a total of 3 points (that is trials). We quantified the relationship between target and distractor orientations via circular correlation (rho) for Experiment 1 (<b>a</b>): 0.019, 0.021, 0.03, 0.052, 0.102, and 0.001 for all subjects in subplots from left-to-right (with p-values of 0.84, 0.833, 0.754, 0.588, 0.301, and 0.992 respectively). The same test was used for Experiment 2 yielding the following correlations (<b>b</b>): 0.016, 0.039, 0.007, 0.026, 0.036, 0.004, and 0.024 for all subjects in subplots from left-to-right (with p-values of 0.866, 0.689, 0.946, 0.791, 0.704, 0.966, and 0.808 respectively).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig7"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 2 behavioral performance on t" href="/articles/s41593-019-0428-x/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig7_ESM.jpg">Supplementary Figure 2 Behavioral performance on the working memory task in the scanner for Experiments 1 and 2.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Behavioral performance in Experiment 1 (<b>a</b>) and Experiment 2 (<b>b</b>) with the top row showing distributions of recall error combined across participants (in degrees). Recall error was calculated by subtracting the target orientation from a participants response on every trial (that is response  target). Each subplot shows a different distractor condition, and each insert is a cartoon version of the image on the screen during the delay in each condition (photo used with permission). The bottom row shows the within-subject mean signed errors (on the y-axis) across orientation space (on the x-axis, where 0 represents vertical, and larger numbers are degrees clockwise relative to 0) for each distractor condition (in subplots). A characteristic bias away from cardinal (Wei, X.X. &amp; Stocker, A.A. A Bayesian observer model constrained by efficient coding can explain Anti-Bayesian percepts. <i>Nat. Neurosci</i>. <b>18</b>, 15091517, 2015) can be observed irrespective of distractor condition. Shaded error areas represent bootstrapped 95% confidence intervals on the mean signed errors (for n=6 and n=7 independent subjects in <b>a</b> and <b>b</b>, respectively). The mean signed error at each degree was calculated within a window of <u>+</u> 6, thus smoothing the data within that range.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig8"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 3 hemodynamic response functi" href="/articles/s41593-019-0428-x/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig8_ESM.jpg">Supplementary Figure 3 Hemodynamic response functions across all retinotopically defined .</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) Experiment 1 BOLD responses for the no distractor, grating distractor, and noise distractor conditions in dark teal, mid teal, and light teal, respectively. Distractors presented during the delay effectively drove univariate response in all ROIs (left-to-right subplots), with the Fourier filtered noise distractor (light teal) yielding especially strong activation in V1 and V2. (<b>b</b>) BOLD in Experiment 2 when there was no distractor (darkest teal), a grating distractor (mid teal), or picture distractors (yellow). Both grating and picture distractors presented during the delay effectively drove univariate response in all ROIs (left-to-right subplots), with pictures yielding the strongest activation overall, especially in extra-striate cortex. For both <b>a</b> and <b>b</b>, the three gray background panels within each subplot represent the target (00.5s), distractor (1.512.5s), and recall (13.516.5) epochs of the working memory trial. Lines are group averaged BOLD responses, with shaded error areas representing <u>+</u> 1 within-subject SEM (for n=6 and n=7 independent subjects in <b>a</b> and <b>b</b>, respectively).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig9"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 4 inverted encoding model (ie" href="/articles/s41593-019-0428-x/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig9_ESM.jpg">Supplementary Figure 4 Inverted Encoding Model (IEM).</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) Estimating the encoding model is the first step in the IEM. Each voxel differs with respect to the size of the response evoked by each orientation, and showing many orientations over many trials is used to quantify this response profile (left). Response profiles (R) for each voxel (j) are the weighted (w) sum of 9 hypothetical orientation channels (i), as shown on the right. (<b>b</b>) Inverting the encoding model is the second step in the IEM. Channel weights represent each voxels orientation selectivity, and when a new response is evoked (left), the combined selectivity of all voxels is used to generate a model-based reconstruction of the new orientation from the voxel pattern. These reconstructed channel response profiles describe the activation of modeled channels in response to either a remembered or a seen orientation, and the resulting response profile is in a stimulus-referred space.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig10"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 5 reconstructions based on in" href="/articles/s41593-019-0428-x/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig10_ESM.jpg">Supplementary Figure 5 Reconstructions based on independent training data, tested on working memory delay data.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>In (<b>a</b>) and (<b>b</b>) we show model-based reconstructions for Experiments 1 and 2, respectively. Top rows show reconstructions for the orientation held in memory (in color), and bottom rows show reconstructions for the orientation that was physically on the screen during grating distractor trials (in grey). Shifts in the baseline offset for the different distractor conditions largely reflect differences in the mean BOLD response across all voxels. Note that V1 reconstructions are also shown in <b>Fig. 1c</b> and <b>Fig. 3c</b> but are included here as well for ease of comparison. Lines are group averaged reconstructions (based on delay data averaged across 5.613.6 seconds after stimulus onset), with shaded error areas representing <u>+</u> 1 within-subject SEM (for n=6 and n=7 independent subjects in <b>a</b> and <b>b</b>, respectively).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig11"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 6 reconstruction fidelity ove" href="/articles/s41593-019-0428-x/figures/11" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig11_ESM.jpg">Supplementary Figure 6 Reconstruction fidelity over time based on independent training data, tested on each individual TR of the working memory trial.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) In Experiment 1, a significant reconstruction fidelity for the remembered orientation arises about 34 seconds into the trial irrespective of distractor condition (in shades of teal), and persists throughout the delay in all early retinotopic areas (V1V4) and LO1, but not in IPS0 and IPS1. On trials with a grating distractor, the physically-present orientation is also represented throughout in V1-V4 and LO1 (though arising a little later, around 45 seconds into the delay, consistent with its delayed onset of 1.5 seconds after the target), and dissipates roughly 2 seconds after distractor offset. (<b>b</b>) When distractors were presented during the delay in Experiment 2, reconstruction fidelity for the remembered orientation is significant at some TRs, but not consistently significant throughout the trial (mid teal and yellow) in early retinotopic areas (V1V4) and LO1. Again, there was no above chance fidelity in IPS0 and IPS1. In both (<b>a</b>) and (<b>b</b>), the three gray panels in each subplot represent the target, distractor, and recall epochs of the working memory trial. Dots in the bottom of each subplot indicate the time points at which fidelity is significantly above zero. Small, medium, and large dot sizes indicate significance levels of p <u>&lt;</u> 0.05, p <u>&lt;</u> 0.01, and p <u>&lt;</u> 0.001, respectively. Statistical tests were identical to those employed for <b>Figs. 2b</b> and <b>3e</b>. Lines are group averaged fidelities, with shaded error areas representing <u>+</u> 1 within-subject SEM (for n=6 and n=7 independent subjects in <b>a</b> and <b>b</b>, respectively). Note that fidelity over time for V1 is also shown in <b>Fig. 2b</b> and <b>Fig. 3e</b>, but is included here for ease of comparison.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig12"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 7 reconstructions during the " href="/articles/s41593-019-0428-x/figures/12" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig12_ESM.jpg">Supplementary Figure 7 Reconstructions during the grating distractor condition in Experiment 1, as a function of target and distractor orientation difference.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Data in the grating distractor condition were binned according to the target-distractor orientation similarly (in columns) for each ROI (in rows). Bins centers were chosen in 30 steps between maximal (0) and minimal (90) target-distractor similarity (columns depicting maximum and minimum similarity are highlighted by gray panels). The solid-black and dashed-red vertical lines represent the memory target orientation, and bin-center of the distractor orientation, respectively. For every participant, all model-based reconstructions within a bin were averaged together, after which we determined the circular mean of the reconstruction in that bin (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig13">8a</a>) and the fidelity at that mean (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig13">8b</a>). Shown here are the group averaged reconstructions in each bin, with black error areas indicating <u>+</u> 1 within-subject SEM for n=6 independent subjects.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig13"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 8 reconstructions shift and h" href="/articles/s41593-019-0428-x/figures/13" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig13_ESM.jpg">Supplementary Figure 8 Reconstructions shift and have lower fidelity as target and grating distractor orientations become more dissimilar in Experiment 1.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) Circular mean (<i>mu</i>) of model-based reconstructions of remembered orientation during the grating distractor condition, as a function of target-distractor similarity (see also Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig12">7</a>). Non-parametric one-way ANOVAs in each ROI (uncorrected) show that reconstruction <i>mu</i>s are significantly shifted as a function of target-distractor similarity in many early retinotopic ROIs (in subplots; from left-to-right F<sub>(5,25)</sub> = 12.709, 22.406, 13.527, 2.189, 7.498, 1.875, 1.229, and 1.364). Note that for this test (and plot) we excluded the 90 difference bin, which yielded <i>mu</i> estimates that were essentially noise due to the flatness of reconstructions in this bin. (<b>b</b>) Reconstruction fidelity similarly showed interdependencies between target and distractor in early retinotopic ROIs and LO1 (in subplots; from left-to-right F<sub>(5,25)</sub> = 10.492, 17.526, 7.119, 2.684, 4.113, 0.526, 0.486, and 2.985), as indicated by the same non-parametric test used in <b>a</b> with the exception that now the 90 difference bin was included. These interdependencies make it hard to separately estimate information from the target and the distractor, as they co-vary, and the superimposed reconstructions cancel each other out. Nevertheless, this analysis shows that fidelity is enhanced when the target and distractor are more similar, and highlights local interactions between remembered and directly sensed information in early visual cortex. Bars in each subplot represent the average <i>mu</i> (<b>a</b>) and fidelity (<b>b</b>) at each of the target-distractor differences. In both <b>a</b> and <b>b</b>, unfilled circles represent individual participants. Note that in <b>a</b>, IPS1 fidelity in the -60 bin is missing two individual subject data points, this is because their values exceeded the y-axis scale (that is fidelities of -0.236 and 0.692 respectively). One, two, or three asterisks indicate significance in each ROI of p <u>&lt;</u> 0.05, p <u>&lt;</u> 0.01, and p <u>&lt;</u> 0.001, respectively. Error bars represent <u>+</u> 1 within-subject SEM for n=6 independent subjects.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig14"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 9 better behavioral performan" href="/articles/s41593-019-0428-x/figures/14" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig14_ESM.jpg">Supplementary Figure 9 Better behavioral performance when target and distractor orientations are similar versus dissimilar.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>A separate psychophysical experiment was conducted to explore the distractor grating condition in more detail. Participants (n=17) remembered a random target orientation, and an irrelevant distractor orientation was shown during the delay on 90% of trials. Target and distractor orientations were chosen independently and pseudo-randomly, in a manner identical to the two main imaging experiments (Experiments 1 and 2). Unlike the imaging experiments, trials in this behavioral experiment were shorter: A 200ms target was followed by a 3000ms delay (and a 200ms distractor presented during the central portion of the delay). After the delay participants provided an unspeeded response, followed by an 800-1000ms inter-trial interval. Furthermore, grating stimuli in this behavioral experiment were smaller (2 radius, 2 c/, 20% contrast, phase jittered). Each participant performed a total of 1620 trials over the course of several days. The analyses and plots presented in both <b>a</b> and <b>b</b> show circular statistics at each target-distractor difference calculated within a window of <u>+</u> 5. These data were normalized by first subtracting out individual-subject means, and the resultant within-subject average is depicted by the white lines. Black error areas represent bootstrapped 95% confidence intervals on the within-subject data (across all possible target-distractor differences). The single data points presented on the far right of each subplot are from the 10% of trials where no distractor was shown during the delay. (<b>a</b>) The normalized mean response to the target as a function of the target-distractor similarity. Behavioral responses were attracted towards the irrelevant distractor orientation, and this attraction was most pronounced at target-distractor differences around ~22. At its strongest, attraction had a magnitude of ~1. (<b>b</b>) The precision of participants responses (as indexed by the circular standard deviation) fluctuated as a function of target-distractor similarity: Memory was more precise for more similar orientations, and less precise for less similar orientations. The overall magnitude of this effect was ~1.5. Both the findings in (<b>a</b>) and (<b>b</b>) replicate previously published work looking at the impact of irrelevant distractors<sup>16</sup>, with the most notable difference in paradigms that here we used entirely independent target and distractor orientations, whereas the previous work had built-in dependencies between the two. While these effects on the circular mean and standard deviation are small, they far exceed the JND for orientation, and provide evidence for an interaction between a memory target and irrelevant distractors.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig15"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 10 decoding the picture distr" href="/articles/s41593-019-0428-x/figures/15" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig15_ESM.jpg">Supplementary Figure 10 Decoding the picture distractor (face or gazebo) during the memory delay of Experiment 2.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>A two-way classifier (linear support vector machine) was trained to distinguish between face and gazebo pictures using independent localizer data. Next, this classifier proved highly successful at determining whether a face or a gazebo picture was shown during the working memory delay (decoding was based on the average activation patterns from 5.613.6 seconds after stimulus onset), with near-perfect decoding in all retinotopically defined ROIs. Note that this is not at all surprising given that the face and gazebo pictures were not controlled for low-level image statistics. In fact, these stimuli occupied different portions of visual space and therefore systematically activated different subsets of voxels. Statistics were based on one-sided randomization tests comparing decoding accuracy in each ROI to chance (that is 0.5; see Methods). Three asterisks indicate significant decoding of p <u>&lt;</u> 0.001 in all ROIs (the upper limit of resolvable p-values based on 1000 permutations). Dots indicate individual subject decoding in each ROI. Error bars represent <u>+</u> 1 within-subject SEM (for n=7 independent subjects).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig16"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 11 reconstructions based on a" href="/articles/s41593-019-0428-x/figures/16" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig16_ESM.jpg">Supplementary Figure 11 Reconstructions based on a leave-one-out procedure, where model training and testing was performed on data from the working memory delay.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>In (<b>a</b>) and (<b>b</b>) we show model-based reconstructions for Experiments 1 and 2, respectively. Top rows show reconstructions for the orientation held in memory (in color), and bottom rows show reconstructions for the orientation that was physically on the screen during grating distractor trials (in grey). Lines are group averaged reconstructions (based on delay data averaged across 5.613.6 seconds after stimulus onset), with shaded error areas representing <u>+</u> 1 within-subject SEM (for n=6 and n=7 independent subjects in <b>a</b> and <b>b</b>, respectively).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig17"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 12 reconstruction fidelity ov" href="/articles/s41593-019-0428-x/figures/17" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig17_ESM.jpg">Supplementary Figure 12 Reconstruction fidelity over time based on a leave-one-out procedure, where model training and testing was performed on data from individual TRs of the working memory trial.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>In (<b>a</b>) and (<b>b</b>) we show reconstruction fidelity over time for Experiments 1 and 2, respectively. For both <b>a</b> and <b>b</b>, the leave-one-trial out procedure meant that at each TR, we trained the IEM on data from all trials but one, and tested on the left-out trial. This was repeated until all trials had been left out once, after which the reconstructions were averaged, and a single fidelity value was calculated at that TR. This was then done for all TRs. The V1-V4 and LO1 data presented here in both <b>a</b> and <b>b</b>, look roughly the same as those in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-019-0428-x#Fig11">6</a>. However, there is one notable difference: Mnemonic representations in IPS0 and IPS1 are clearly present in this leave-one-out analysis, and remain fairly stable for all distractor conditions throughout trials of both Experiments 1 and 2. In both (<b>a</b>) and (<b>b</b>), the three gray panels in each subplot represent the target, distractor, and recall epochs of the working memory trial. Dots in the bottom of each subplot indicate the time points at which fidelity is significantly above zero. Small, medium, and large dot sizes indicate significance levels of p <u>&lt;</u> 0.05, p <u>&lt;</u> 0.01, and p <u>&lt;</u> 0.001, respectively. Statistical tests were identical to those employed for <b>Figs. 2b</b>, <b>3e</b>, and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-019-0428-x#MOESM1">6</a>. Lines are group averaged fidelities, with shaded error areas representing <u>+</u> 1 within-subject SEM (for n=6 and n=7 independent subjects in <b>a</b> and <b>b</b>, respectively).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig18"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 13 memory fidelity in all vox" href="/articles/s41593-019-0428-x/figures/18" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_Fig18_ESM.jpg">Supplementary Figure 13 Memory fidelity in all voxels from retinotopically defined IPS0IPS3 areas, analyzed without voxel selection based on visual sensitivity.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Our main analyses (<b>Fig. 1e</b>, <b>3d</b>, and <b>4</b>) might bias IPS results in favor of sensory-like stimulus-driven codes by virtue of including only voxels with a significant sensory response. To avoid this potential bias, here we analyze data from all IPS voxels irrespective of their visual sensitivity. We again see that there is little mnemonic information represented in IPS when the IEM is trained on independent sensory data (subplots on the left). By contrast, when training and testing the IEM on data from within the memory epoch itself, the remembered orientation is robustly represented (subplots on the right). There are no differences in memory fidelity between the three distractor conditions when training on independent sensory data (left subplots; Experiment 1: all F<sub>(2,10)</sub> &lt; 2.145, all p &gt; 0.194; Experiment 2: all F<sub>(2,12)</sub> &lt; 2.041, all p &lt; 0.197), which is not surprising given the overall absence of information. However, also when trained on memory delay data (right subplots) there are no differences in memory fidelity between the three distractor conditions in Experiment 1 (all F<sub>(2,10)</sub> &lt; 1.259, all p &gt; 0.333) and Experiment 2 (all F<sub>(2,12)</sub> &lt; 0.822, all p &lt; 0.473; with the exception of IPS0, F<sub>(2,12)</sub> = 4.31, p = 0.032; a difference that didnt hold up in post-hoc tests, with all t<sub>(6)</sub> &lt; 2.494, and all p &gt; 0.054). Note that the sensory distractor (in grey) is not represented in either analysis, implying that IPS is not representing visual inputs when they are task-irrelevant. Statistical testing was identical to that in <b>Figs. 1e</b>, <b>3d</b>, and <b>4</b> (see also Methods). One, two, or three asterisks indicate significance levels of p <u>&lt;</u> 0.05, p <u>&lt;</u> 0.01, or p <u>&lt;</u> 0.001, respectively. Dots indicate individual subject fidelities in each ROI and condition. Error bars represent <u>+</u> 1 within-subject SEM (for n=6 and n=7 independent subjects in Experiments 1 and 2, respectively).</p></div></div></div></div></div></section><section data-title="Supplementary information"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Supplementary information</h2><div class="c-article-section__content" id="Sec20-content"><div data-test="supplementary-info"><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figs. 113 and supplementary tables " href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Figs. 113 and Supplementary Tables 110.</a></h3></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="reporting summary" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-019-0428-x/MediaObjects/41593_2019_428_MOESM2_ESM.pdf" data-supp-info-image="">Reporting Summary</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Coexisting%20representations%20of%20sensory%20and%20mnemonic%20information%20in%20human%20visual%20cortex&amp;author=Rosanne%20L.%20Rademaker%20et%20al&amp;contentID=10.1038%2Fs41593-019-0428-x&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer%20Nature%20America%2C%20Inc.&amp;publication=1097-6256&amp;publicationDate=2019-07-01&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/s41593-019-0428-x" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41593-019-0428-x" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Rademaker, R.L., Chunharas, C. &amp; Serences, J.T. Coexisting representations of sensory and mnemonic information in human visual cortex.
                    <i>Nat Neurosci</i> <b>22</b>, 13361344 (2019). https://doi.org/10.1038/s41593-019-0428-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-019-0428-x?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-01-04">04 January 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-05-16">16 May 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-07-01">01 July 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-08">August 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/s41593-019-0428-x</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>

            
        <section>
            <div class="c-article-section js-article-section" id="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:A retinotopic code structures the interaction between perception and memory systems" href="https://doi.org/10.1038/s41593-023-01512-3">
                                        A retinotopic code structures the interaction between perception and memory systems
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Adam Steel</li><li>Edward H. Silson</li><li>Caroline E. Robertson</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Neuroscience</i> (2024)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:High-altitude exposure leads to increased modularity of brain functional network with the increased occupation of attention resources in early processing of visual working memory" href="https://doi.org/10.1007/s11571-024-10091-3">
                                        High-altitude exposure leads to increased modularity of brain functional network with the increased occupation of attention resources in early processing of visual working memory
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Jing Zhou</li><li>Nian-Nian Wang</li><li>De-Long Zhang</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Cognitive Neurodynamics</i> (2024)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Bilateral increase in MEG planar gradients prior to saccade onset" href="https://doi.org/10.1038/s41598-023-32980-z">
                                        Bilateral increase in MEG planar gradients prior to saccade onset
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Jasper H. Fabius</li><li>Alessio Fracasso</li><li>Stefan Van der Stigchel</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Scientific Reports</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Geometry of visuospatial working memory information in miniature gaze patterns" href="https://doi.org/10.1038/s41562-023-01737-z">
                                        Geometry of visuospatial working memory information in miniature gaze patterns
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Juan Linde-Domingo</li><li>Bernhard Spitzer</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Human Behaviour</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Perception and memory have distinct spatial tuning properties in human visual cortex" href="https://doi.org/10.1038/s41467-022-33161-8">
                                        Perception and memory have distinct spatial tuning properties in human visual cortex
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Serra E. Favila</li><li>Brice A. Kuhl</li><li>Jonathan Winawer</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Communications</i> (2022)</p>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </section>
    

            
        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="js-context-bar-sticky-point-desktop">
        

        
            <noscript>
                
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-019-0428-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </noscript>
            <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
                <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-right-column
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-019-0428-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </div>
        
    </div>

    
        
    

    
    

    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>
                            
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=s41593-019-0428-x;doi=10.1038/s41593-019-0428-x;techmeta=36,59;subjmeta=1595,1636,1723,1875,2613,2649,378,477,631;kwrd=Neuroscience,Perception,Psychology,Striate+cortex,Working+memory">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=2125673765&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-019-0428-x%26doi%3D10.1038/s41593-019-0428-x%26techmeta%3D36,59%26subjmeta%3D1595,1636,1723,1875,2613,2649,378,477,631%26kwrd%3DNeuroscience,Perception,Psychology,Striate+cortex,Working+memory">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=2125673765&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-019-0428-x%26doi%3D10.1038/s41593-019-0428-x%26techmeta%3D36,59%26subjmeta%3D1595,1636,1723,1875,2613,2649,378,477,631%26kwrd%3DNeuroscience,Perception,Psychology,Striate+cortex,Working+memory"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>


    
        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-and-analysis"
                                   data-track="click"
                                   data-track-action="reviews &amp; analysis"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Reviews &amp; Analysis
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/video"
                                   data-track="click"
                                   data-track-action="videos"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Videos
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/current-issue"
                                   data-track="click"
                                   data-track-action="current issue"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Current issue
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>
                        
                    
                </ul>
                <ul class="c-header__list c-header__list--js-stack">
                    
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/natureneuro"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on Twitter
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;6"
                               rel="nofollow"
                               data-track="click"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/neuro.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </nav>
    
    
        
            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/aims"
                                   data-track="click"
                                   data-track-action="aims &amp; scope"
                                   data-track-label="link">
                                    Aims &amp; Scope
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-information"
                                   data-track="click"
                                   data-track-action="journal information"
                                   data-track-label="link">
                                    Journal Information
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-impact"
                                   data-track="click"
                                   data-track-action="journal metrics"
                                   data-track-label="link">
                                    Journal Metrics
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editors"
                                   data-track="click"
                                   data-track-action="about the editors"
                                   data-track-label="link">
                                    About the Editors
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/our-publishing-models"
                                   data-track="click"
                                   data-track-action="our publishing models"
                                   data-track-label="link">
                                    Our publishing models
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-values-statement"
                                   data-track="click"
                                   data-track-action="editorial values statement"
                                   data-track-label="link">
                                    Editorial Values Statement
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-policies"
                                   data-track="click"
                                   data-track-action="editorial policies"
                                   data-track-label="link">
                                    Editorial Policies
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/content"
                                   data-track="click"
                                   data-track-action="content types"
                                   data-track-label="link">
                                    Content Types
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/web-feeds"
                                   data-track="click"
                                   data-track-action="web feeds"
                                   data-track-label="link">
                                    Web Feeds
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/posters"
                                   data-track="click"
                                   data-track-action="posters"
                                   data-track-label="link">
                                    Posters
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="research cross-journal editorial team"
                                   data-track-label="link">
                                    Research Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="reviews cross-journal editorial team"
                                   data-track-label="link">
                                    Reviews Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        

        
            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/submission-guidelines"
                                   data-track="click"
                                   data-track-action="submission guidelines"
                                   data-track-label="link">
                                    Submission Guidelines
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/for-reviewers"
                                   data-track="click"
                                   data-track-action="for reviewers"
                                   data-track-label="link">
                                    For Reviewers
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="click"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://mts-nn.nature.com/cgi-bin/main.plex?form_type&#x3D;home&amp;from_idp&#x3D;1"
                                   data-track="click"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external>Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        
    


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">
                            
                                
                                    <option value="" selected>All journals</option>
                                    <option value="neuro">This journal</option>
                                
                            
                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        

        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
            

            <p class="c-meta u-ma-0 u-flex-shrink">
                <span class="c-meta__item">
                    Nature Neuroscience (<i>Nat Neurosci</i>)
                </span>
                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="onlineIssn">1546-1726</span> (online)
    </span>
    


                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="printIssn">1097-6256</span> (print)
    </span>
    

            </p>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/protocolexchange/"
                                                  data-track="click" data-track-action="protocol exchange"
                                                  data-track-label="link">Protocol Exchange</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natitaly"
                                                  data-track="click" data-track-action="nature Italy"
                                                  data-track-label="link">Nature Italy</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ko-kr"
                                                  data-track="click" data-track-action="nature korea"
                                                  data-track-label="link">Nature Korea</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>
            
        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-ea63208b81.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2024 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path id="shape" fill-rule="evenodd" clip-rule="evenodd" d="M1 3.78571C1 2.75867 1.85698 2 2.8209 2H6.1791C7.14302 2 8 2.75867 8 3.78571V4H11.1668C11.885 4 12.5585 4.42017 12.8494 5.07033C12.9893 4.98169 13.1425 4.91101 13.3056 4.86206L16.5222 3.89704C17.4454 3.62005 18.4843 4.10046 18.7794 5.08419L22.9256 18.9042C23.2207 19.8878 22.618 20.8608 21.6947 21.1378L18.4781 22.1029C17.5548 22.3799 16.516 21.8993 16.2209 20.9157L13.0001 10.1804V20.2143C13.0001 21.255 12.1231 22 11.1668 22H7.83346C7.54206 22 7.25803 21.9308 7.00392 21.8052C6.75263 21.9305 6.47077 22 6.1791 22H2.8209C1.85693 22 1 21.2412 1 20.2143V3.78571ZM3 4V15H6V4H3ZM3 20V17H6V20H3ZM18.0749 20.1358L17.2129 17.2623L20.0863 16.4002L20.9484 19.2737L18.0749 20.1358ZM19.5116 14.4846L16.6381 15.3466L14.0519 6.72624L16.9254 5.86416L19.5116 14.4846ZM8.00012 20L8.00012 6H11.0001L11.0001 20H8.00012Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</footer>




    

    

<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">

    
    <div class="c-site-messages__banner-large">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing" src="/static/images/logos/nature-briefing-logo-n150-white-d81c9da3ec.svg" width="250" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing</em> newsletter  what matters in science, free to your inbox daily.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/briefing" method="post" data-location="banner" data-track="submit||nature_briefing_sign_up" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="MainBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick">
                        <input type="hidden" value="false" name="marketing" id="marketing">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick">
                        <input type="hidden" value="MainBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">
                            
                            <input type="hidden" value="true" name="N:nature_briefing_daily" id="defaultNewsletter">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>

    
    <div class="c-site-messages__banner-small">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner">Sign up for Nature Briefing
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/s41593-019-0428-x&amp;format=js&amp;last_modified=2019-07-01" async></script>
<img src="/g4jpl8v8/article/s41593-019-0428-x" width="1" height="1" alt="" class="u-visually-hidden">
</body>
</html>