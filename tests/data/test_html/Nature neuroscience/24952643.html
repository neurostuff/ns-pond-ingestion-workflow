<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>Population coding of affect across stimuli, modalities and individuals | Nature Neuroscience</title>
    
        
<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/neuro.rss"/>


    
        

        <script id="save-data-connection-testing">
            function hasConnection() {
                return navigator.connection || navigator.mozConnection || navigator.webkitConnection || navigator.msConnection;
            }

            function createLink(src) {
                var preloadLink = document.createElement("link");
                preloadLink.rel = "preload";
                preloadLink.href = src;
                preloadLink.as = "font";
                preloadLink.type = "font/woff2";
                preloadLink.crossOrigin = "";
                document.head.insertBefore(preloadLink, document.head.firstChild);
            }

            var connectionDetail = {
                saveDataEnabled: false,
                slowConnection: false
            };

            var connection = hasConnection();
            if (connection) {
                connectionDetail.saveDataEnabled = connection.saveData;
                if (/\slow-2g|2g/.test(connection.effectiveType)) {
                    connectionDetail.slowConnection = true;
                }
            }

            if (!(connectionDetail.saveDataEnabled || connectionDetail.slowConnection)) {
                createLink("/static/fonts/HardingText-Regular-Web-cecd90984f.woff2");
            } else {
                document.documentElement.classList.add('save-data');
            }
        </script>
    

<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"prefrontal-cortex","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Nature Neuroscience","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article"}},"article":{"doi":"10.1038/nn.3749"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["Junichi Chikazoe","Daniel H Lee","Nikolaus Kriegeskorte","Adam K Anderson"],"publishedAt":1403395200,"publishedAtString":"2014-06-22","title":"Population coding of affect across stimuli, modalities and individuals","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"neuro","title":"nature neuroscience","volume":"17","issue":"8"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"nature-onwards-journey","active":false},{"name":"getftr-entitled","active":false},{"name":"paywall_recommendations","active":true}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"US","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://collect.nature.com',
        imprint: 'nature'
    });
</script>

<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
 



     
    
    
        
    
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}button:focus{outline:3px solid #fece3e;will-change:transform}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:8px}.c-recommendations-list__item .c-card{flex-basis:100%}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 0 0 16px;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #cedbe0;height:auto;min-height:0;position:static}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0 0 8px;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:inherit}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}.c-card__title-recommendation .MathJax_Display{display:inline!important}.c-card__link:not(.c-card__link--no-block-link):before{z-index:1}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a,.c-article-recommendations-card__link{color:inherit}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-article-recommendations-card__meta-type,.c-meta .c-meta__item:first-child{font-weight:700}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:539px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px 13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.c-pdf-download__link{padding:13px 24px} } </style>




    
        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    
    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-122346e276.css" media="print">
    



<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">


<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h.indexOf('preview-www.nature.com') > -1) return;

            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('nature.com') > -1) {
                if (h.indexOf('test-www.nature.com') > -1) {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-26e142e9c6.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    
                        {src: '/static/js/global-article-es6-bundle-782fd09f66.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-d66d49033d.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-aca08c055a.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-4fba787158.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-1fe07484e5.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;
                        
                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-8fc1a30809.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-e0c7186f28.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-2399be388c.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">





    
    <script type="application/ld+json">{"mainEntity":{"headline":"Population coding of affect across stimuli, modalities and individuals","description":"The authors found human neuroimaging evidence that entire valence spectrum is represented as a collective pattern in regional neural activity, with sensory-specific signals in the ventral temporal and anterior insular cortices and abstract codes in the orbitofrontal cortices. In this way, the subjective quality of affect can be objectively quantified across stimuli, modalities and people. It remains unclear how the brain represents external objective sensory events alongside our internal subjective impressions of themâ€”affect. Representational mapping of population activity evoked by complex scenes and basic tastes in humans revealed a neural code supporting a continuous axis of pleasant-to-unpleasant valence. This valence code was distinct from low-level physical and high-level object properties. Although ventral temporal and anterior insular cortices supported valence codes specific to vision and taste, both the medial and lateral orbitofrontal cortices (OFC) maintained a valence code independent of sensory origin. Furthermore, only the OFC code could classify experienced affect across participants. The entire valence spectrum was represented as a collective pattern in regional neural activity as sensory-specific and abstract codes, whereby the subjective quality of affect can be objectively quantified across stimuli, modalities and people.","datePublished":"2014-06-22T00:00:00Z","dateModified":"2014-06-22T00:00:00Z","pageStart":"1114","pageEnd":"1122","sameAs":"https://doi.org/10.1038/nn.3749","keywords":["Prefrontal cortex","Biomedicine","general","Neurosciences","Behavioral Sciences","Biological Techniques","Neurobiology","Animal Genetics and Genomics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig1_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig2_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig3_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig4_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig5_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig6_HTML.jpg"],"isPartOf":{"name":"Nature Neuroscience","issn":["1546-1726","1097-6256"],"volumeNumber":"17","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Junichi Chikazoe","affiliation":[{"name":"Human Neuroscience Institute, College of Human Ecology, Cornell University","address":{"name":"Department of Human Development, Human Neuroscience Institute, College of Human Ecology, Cornell University, Ithaca, USA","@type":"PostalAddress"},"@type":"Organization"}],"email":"j.chikazoe@gmail.com","@type":"Person"},{"name":"Daniel H Lee","affiliation":[{"name":"University of Toronto","address":{"name":"Department of Psychology, University of Toronto, Toronto, Canada","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Nikolaus Kriegeskorte","affiliation":[{"name":"Medical Research Council, Cognition and Brain Sciences Unit","address":{"name":"Medical Research Council, Cognition and Brain Sciences Unit, Cambridge, UK","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Adam K Anderson","affiliation":[{"name":"Human Neuroscience Institute, College of Human Ecology, Cornell University","address":{"name":"Department of Human Development, Human Neuroscience Institute, College of Human Ecology, Cornell University, Ithaca, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Toronto","address":{"name":"Department of Psychology, University of Toronto, Toronto, Canada","@type":"PostalAddress"},"@type":"Organization"}],"email":"aka47@cornell.edu","@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>




    
    
    


    
    <link rel="canonical" href="https://www.nature.com/articles/nn.3749">
    
    
    <meta name="journal_id" content="41593"/>
    <meta name="dc.title" content="Population coding of affect across stimuli, modalities and individuals"/>
    <meta name="dc.source" content="Nature Neuroscience 2014 17:8"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2014-06-22"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2014 Springer Nature America, Inc."/>
    <meta name="dc.rights" content="2014 Springer Nature America, Inc."/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="The authors found human neuroimaging evidence that entire valence spectrum is represented as a collective pattern in regional neural activity, with sensory-specific signals in the ventral temporal and anterior insular cortices and abstract codes in the orbitofrontal cortices. In this way, the subjective quality of affect can be objectively quantified across stimuli, modalities and people. It remains unclear how the brain represents external objective sensory events alongside our internal subjective impressions of them&#8212;affect. Representational mapping of population activity evoked by complex scenes and basic tastes in humans revealed a neural code supporting a continuous axis of pleasant-to-unpleasant valence. This valence code was distinct from low-level physical and high-level object properties. Although ventral temporal and anterior insular cortices supported valence codes specific to vision and taste, both the medial and lateral orbitofrontal cortices (OFC) maintained a valence code independent of sensory origin. Furthermore, only the OFC code could classify experienced affect across participants. The entire valence spectrum was represented as a collective pattern in regional neural activity as sensory-specific and abstract codes, whereby the subjective quality of affect can be objectively quantified across stimuli, modalities and people."/>
    <meta name="prism.issn" content="1546-1726"/>
    <meta name="prism.publicationName" content="Nature Neuroscience"/>
    <meta name="prism.publicationDate" content="2014-06-22"/>
    <meta name="prism.volume" content="17"/>
    <meta name="prism.number" content="8"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="1114"/>
    <meta name="prism.endingPage" content="1122"/>
    <meta name="prism.copyright" content="2014 Springer Nature America, Inc."/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/nn.3749"/>
    <meta name="prism.doi" content="doi:10.1038/nn.3749"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/nn.3749.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/nn.3749"/>
    <meta name="citation_journal_title" content="Nature Neuroscience"/>
    <meta name="citation_journal_abbrev" content="Nat Neurosci"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="1546-1726"/>
    <meta name="citation_title" content="Population coding of affect across stimuli, modalities and individuals"/>
    <meta name="citation_volume" content="17"/>
    <meta name="citation_issue" content="8"/>
    <meta name="citation_publication_date" content="2014/08"/>
    <meta name="citation_online_date" content="2014/06/22"/>
    <meta name="citation_firstpage" content="1114"/>
    <meta name="citation_lastpage" content="1122"/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/nn.3749"/>
    <meta name="DOI" content="10.1038/nn.3749"/>
    <meta name="size" content="208862"/>
    <meta name="citation_doi" content="10.1038/nn.3749"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/nn.3749&amp;api_key="/>
    <meta name="description" content="The authors found human neuroimaging evidence that entire valence spectrum is represented as a collective pattern in regional neural activity, with sensory-specific signals in the ventral temporal and anterior insular cortices and abstract codes in the orbitofrontal cortices. In this way, the subjective quality of affect can be objectively quantified across stimuli, modalities and people. It remains unclear how the brain represents external objective sensory events alongside our internal subjective impressions of them&#8212;affect. Representational mapping of population activity evoked by complex scenes and basic tastes in humans revealed a neural code supporting a continuous axis of pleasant-to-unpleasant valence. This valence code was distinct from low-level physical and high-level object properties. Although ventral temporal and anterior insular cortices supported valence codes specific to vision and taste, both the medial and lateral orbitofrontal cortices (OFC) maintained a valence code independent of sensory origin. Furthermore, only the OFC code could classify experienced affect across participants. The entire valence spectrum was represented as a collective pattern in regional neural activity as sensory-specific and abstract codes, whereby the subjective quality of affect can be objectively quantified across stimuli, modalities and people."/>
    <meta name="dc.creator" content="Chikazoe, Junichi"/>
    <meta name="dc.creator" content="Lee, Daniel H"/>
    <meta name="dc.creator" content="Kriegeskorte, Nikolaus"/>
    <meta name="dc.creator" content="Anderson, Adam K"/>
    <meta name="dc.subject" content="Prefrontal cortex"/>
    <meta name="citation_reference" content="Wundt, W. Grundriss der Psychologie, von Wilhelm Wundt (W. Engelmann, Leipzig, 1897)."/>
    <meta name="citation_reference" content="citation_journal_title=Brain; citation_title=Somatic motor and sensory representation in the cerebral cortex of man as studies by electrical stimulation; citation_author=W Penfield, E Boldrey; citation_volume=60; citation_publication_date=1937; citation_pages=389-443; citation_doi=10.1093/brain/60.4.389; citation_id=CR2"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=A continuous semantic space describes the representation of thousands of object and action categories across the human brain; citation_author=AG Huth, S Nishimoto, AT Vu, JL Gallant; citation_volume=76; citation_publication_date=2012; citation_pages=1210-1224; citation_doi=10.1016/j.neuron.2012.10.014; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Distributed and overlapping representations of faces and objects in ventral temporal cortex; citation_author=JV Haxby; citation_volume=293; citation_publication_date=2001; citation_pages=2425-2430; citation_doi=10.1126/science.1063736; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=The fusiform face area: a module in human extrastriate cortex specialized for face perception; citation_author=N Kanwisher, J McDermott, MM Chun; citation_volume=17; citation_publication_date=1997; citation_pages=4302-4311; citation_doi=10.1523/JNEUROSCI.17-11-04302.1997; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Syst. Neurosci.; citation_title=Representational similarity analysis - connecting the branches of systems neuroscience; citation_author=N Kriegeskorte, M Mur, P Bandettini; citation_volume=2; citation_publication_date=2008; citation_pages=4; citation_doi=10.3389/neuro.01.016.2008; citation_id=CR6"/>
    <meta name="citation_reference" content="Hinton, G.E., McClelland, J.L. &amp; Rumelhart, D.E. Distributed representations. in Parallel Distributed Processing: Explorations in the Microstructure of Cognition (eds. Rumelhart, D.E. &amp; McClelland, J.L.) 77&#8211;109 (The MIT Press, Cambridge, Massachusetts, 1986)."/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Neural correlates of processing valence and arousal in affective words; citation_author=PA Lewis, HD Critchley, P Rotshtein, RJ Dolan; citation_volume=17; citation_publication_date=2007; citation_pages=742-748; citation_doi=10.1093/cercor/bhk024; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Neurons in the orbitofrontal cortex encode economic value; citation_author=C Padoa-Schioppa, JA Assad; citation_volume=441; citation_publication_date=2006; citation_pages=223-226; citation_doi=10.1038/nature04676; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Representational geometry: integrating cognition, computation and the brain; citation_author=N Kriegeskorte, RA Kievit; citation_volume=17; citation_publication_date=2013; citation_pages=401-412; citation_doi=10.1016/j.tics.2013.06.007; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=Ann. NY Acad. Sci.; citation_title=Decoding and predicting intentions; citation_author=JD Haynes; citation_volume=1224; citation_publication_date=2011; citation_pages=9-21; citation_doi=10.1111/j.1749-6632.2011.05994.x; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Decoding the visual and subjective contents of the human brain; citation_author=Y Kamitani, F Tong; citation_volume=8; citation_publication_date=2005; citation_pages=679-685; citation_doi=10.1038/nn1444; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Orientation decoding depends on maps, not columns; citation_author=J Freeman, GJ Brouwer, DJ Heeger, EP Merriam; citation_volume=31; citation_publication_date=2011; citation_pages=4792-4804; citation_doi=10.1523/JNEUROSCI.5160-10.2011; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=The radial bias: a different slant on visual orientation sensitivity in human and nonhuman primates; citation_author=Y Sasaki; citation_volume=51; citation_publication_date=2006; citation_pages=661-670; citation_doi=10.1016/j.neuron.2006.07.021; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Psychol.; citation_title=fMRI orientation decoding in V1 does not require global maps or globally coherent orientation stimuli; citation_author=A Alink, A Krugliak, A Walther, N Kriegeskorte; citation_volume=4; citation_publication_date=2013; citation_pages=493; citation_doi=10.3389/fpsyg.2013.00493; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Matching categorical object representations in inferior temporal cortex of man and monkey; citation_author=N Kriegeskorte; citation_volume=60; citation_publication_date=2008; citation_pages=1126-1141; citation_doi=10.1016/j.neuron.2008.10.043; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex; citation_author=D McNamee, A Rangel, JP O&#39;Doherty; citation_volume=16; citation_publication_date=2013; citation_pages=479-485; citation_doi=10.1038/nn.3337; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Decoding and reconstructing color from responses in human visual cortex; citation_author=GJ Brouwer, DJ Heeger; citation_volume=29; citation_publication_date=2009; citation_pages=13992-14003; citation_doi=10.1523/JNEUROSCI.3577-09.2009; citation_id=CR18"/>
    <meta name="citation_reference" content="Osgood, C.E., May, W.H. &amp; Miron, M.S. Cross-Cultural Universals of Affective Meaning (University of Illinois Press, Urbana, Illinois, 1975)."/>
    <meta name="citation_reference" content="citation_journal_title=J. Pers. Soc. Psychol.; citation_title=A circumplex model of affect; citation_author=JA Russell; citation_volume=39; citation_publication_date=1980; citation_pages=1161-1178; citation_doi=10.1037/h0077714; citation_id=CR20"/>
    <meta name="citation_reference" content="citation_journal_title=Annu. Rev. Neurosci.; citation_title=The human visual cortex; citation_author=K Grill-Spector, R Malach; citation_volume=27; citation_publication_date=2004; citation_pages=649-677; citation_doi=10.1146/annurev.neuro.27.070203.144220; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Dissociated neural representations of intensity and valence in human olfaction; citation_author=AK Anderson; citation_volume=6; citation_publication_date=2003; citation_pages=196-202; citation_doi=10.1038/nn1001; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Abstract reward and punishment representations in the human orbitofrontal cortex; citation_author=J O&#39;Doherty, ML Kringelbach, ET Rolls, J Hornak, C Andrews; citation_volume=4; citation_publication_date=2001; citation_pages=95-102; citation_doi=10.1038/82959; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Dissociation of neural representation of intensity and affective valuation in human gustation; citation_author=DM Small; citation_volume=39; citation_publication_date=2003; citation_pages=701-711; citation_doi=10.1016/S0896-6273(03)00467-7; citation_id=CR24"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans; citation_author=D Ong&#252;r, JL Price; citation_volume=10; citation_publication_date=2000; citation_pages=206-219; citation_doi=10.1093/cercor/10.3.206; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=Cogn. Affect. Behav. Neurosci.; citation_title=Affective value and associative processing share a cortical substrate; citation_author=A Shenhav, LF Barrett, M Bar; citation_volume=13; citation_publication_date=2013; citation_pages=46-59; citation_doi=10.3758/s13415-012-0128-4; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Appetitive and aversive olfactory learning in humans studied using event-related functional magnetic resonance imaging; citation_author=JA Gottfried, J O&#39;Doherty, RJ Dolan; citation_volume=22; citation_publication_date=2002; citation_pages=10829-10837; citation_doi=10.1523/JNEUROSCI.22-24-10829.2002; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=Eur. J. Neurosci.; citation_title=Different representations of pleasant and unpleasant odours in the human brain; citation_author=ET Rolls, ML Kringelbach, IE de Araujo; citation_volume=18; citation_publication_date=2003; citation_pages=695-703; citation_doi=10.1046/j.1460-9568.2003.02779.x; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Neural economics and the biological substrates of valuation; citation_author=PR Montague, GS Berns; citation_volume=36; citation_publication_date=2002; citation_pages=265-284; citation_doi=10.1016/S0896-6273(02)00974-1; citation_id=CR29"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Sci.; citation_title=Neural evidence that human emotions share core affective properties; citation_author=CD Wilson-Mendenhall, LF Barrett, LW Barsalou; citation_volume=24; citation_publication_date=2013; citation_pages=947-956; citation_doi=10.1177/0956797612464242; citation_id=CR30"/>
    <meta name="citation_reference" content="citation_journal_title=Behav. Brain Sci.; citation_title=The brain basis of emotion: a meta-analytic review; citation_author=KA Lindquist, TD Wager, H Kober, E Bliss-Moreau, LF Barrett; citation_volume=35; citation_publication_date=2012; citation_pages=121-143; citation_doi=10.1017/S0140525X11000446; citation_id=CR31"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=The convergence of information about rewarding and aversive stimuli in single neurons; citation_author=SE Morrison, CD Salzman; citation_volume=29; citation_publication_date=2009; citation_pages=11471-11483; citation_doi=10.1523/JNEUROSCI.1815-09.2009; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Psychophysical and neural evidence for emotion-enhanced perceptual vividness; citation_author=RM Todd, D Talmi, TW Schmitz, J Susskind, AK Anderson; citation_volume=32; citation_publication_date=2012; citation_pages=11201-11212; citation_doi=10.1523/JNEUROSCI.0155-12.2012; citation_id=CR33"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=A common neural scale for the subjective pleasantness of different primary rewards; citation_author=F Grabenhorst, AA D&#39;Souza, BA Parris, ET Rolls, RE Passingham; citation_volume=51; citation_publication_date=2010; citation_pages=1265-1274; citation_doi=10.1016/j.neuroimage.2010.03.043; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=J. Cogn. Neurosci.; citation_title=What makes different people&#39;s representations alike: neural similarity space solves the problem of across-subject fMRI decoding; citation_author=RD Raizada, AC Connolly; citation_volume=24; citation_publication_date=2012; citation_pages=868-877; citation_doi=10.1162/jocn_a_00189; citation_id=CR35"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=A common, high-dimensional model of the representational space in human ventral temporal cortex; citation_author=JV Haxby; citation_volume=72; citation_publication_date=2011; citation_pages=404-416; citation_doi=10.1016/j.neuron.2011.08.026; citation_id=CR36"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Sci.; citation_title=How are you feeling? Revisiting the quantification of emotional qualia; citation_author=A Kron, A Goldstein, DH Lee, K Gardhouse, AK Anderson; citation_volume=24; citation_publication_date=2013; citation_pages=1503-1511; citation_doi=10.1177/0956797613475456; citation_id=CR37"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Information-based functional brain mapping; citation_author=N Kriegeskorte, R Goebel, P Bandettini; citation_volume=103; citation_publication_date=2006; citation_pages=3863-3868; citation_doi=10.1073/pnas.0600244103; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Dissociable effects of arousal and valence on prefrontal activity indexing emotional evaluation and subsequent memory: an event-related fMRI study; citation_author=F Dolcos, KS LaBar, R Cabeza; citation_volume=23; citation_publication_date=2004; citation_pages=64-74; citation_doi=10.1016/j.neuroimage.2004.05.015; citation_id=CR39"/>
    <meta name="citation_reference" content="Lazarus, R.S. &amp; Folkman, S. Stress, Appraisal and Coping (Springer Publishing Company, 1984)."/>
    <meta name="citation_reference" content="citation_journal_title=Neuroreport; citation_title=Human cortical gustatory areas: a review of functional neuroimaging data; citation_author=DM Small; citation_volume=10; citation_publication_date=1999; citation_pages=7-14; citation_doi=10.1097/00001756-199901180-00002; citation_id=CR41"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Stimulus value signals in ventromedial PFC reflect the integration of attribute value signals computed in fusiform gyrus and posterior superior temporal gyrus; citation_author=SL Lim, JP O&#39;Doherty, A Rangel; citation_volume=33; citation_publication_date=2013; citation_pages=8729-8741; citation_doi=10.1523/JNEUROSCI.4809-12.2013; citation_id=CR42"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Reward association affects neuronal responses to visual stimuli in macaque te and perirhinal cortices; citation_author=T Mogami, K Tanaka; citation_volume=26; citation_publication_date=2006; citation_pages=6761-6770; citation_doi=10.1523/JNEUROSCI.4924-05.2006; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Activation and habituation in olfaction: an fMRI study; citation_author=A Poellinger; citation_volume=13; citation_publication_date=2001; citation_pages=547-560; citation_doi=10.1006/nimg.2000.0713; citation_id=CR44"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Comparison of multivariate classifiers and response normalizations for pattern-information fMRI; citation_author=M Misaki, Y Kim, PA Bandettini, N Kriegeskorte; citation_volume=53; citation_publication_date=2010; citation_pages=103-118; citation_doi=10.1016/j.neuroimage.2010.05.051; citation_id=CR45"/>
    <meta name="citation_reference" content="citation_journal_title=Brain; citation_title=The enigmatic temporal pole: a review of findings on social and emotional processing; citation_author=IR Olson, A Plotzker, Y Ezzyat; citation_volume=130; citation_publication_date=2007; citation_pages=1718-1731; citation_doi=10.1093/brain/awm052; citation_id=CR46"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Neural activity at the human olfactory epithelium reflects olfactory perception; citation_author=H Lapid; citation_volume=14; citation_publication_date=2011; citation_pages=1455-1461; citation_doi=10.1038/nn.2926; citation_id=CR47"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Aversive learning enhances perceptual and cortical discrimination of indiscriminable odor cues; citation_author=W Li, JD Howard, TB Parrish, JA Gottfried; citation_volume=319; citation_publication_date=2008; citation_pages=1842-1845; citation_doi=10.1126/science.1152837; citation_id=CR48"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex; citation_author=MP Noonan; citation_volume=107; citation_publication_date=2010; citation_pages=20547-20552; citation_doi=10.1073/pnas.1012246107; citation_id=CR49"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=In bad taste: evidence for the oral origins of moral disgust; citation_author=HA Chapman, DA Kim, JM Susskind, AK Anderson; citation_volume=323; citation_publication_date=2009; citation_pages=1222-1226; citation_doi=10.1126/science.1165565; citation_id=CR50"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain; citation_author=N Tzourio-Mazoyer; citation_volume=15; citation_publication_date=2002; citation_pages=273-289; citation_doi=10.1006/nimg.2001.0978; citation_id=CR51"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data; citation_author=SB Eickhoff; citation_volume=25; citation_publication_date=2005; citation_pages=1325-1335; citation_doi=10.1016/j.neuroimage.2004.12.034; citation_id=CR52"/>
    <meta name="citation_reference" content="citation_journal_title=ACM Trans. Graph.; citation_title=Photographic tone reproduction for digital images; citation_author=ESM Reinhard, P Shirley, J Ferwerda; citation_volume=21; citation_publication_date=2002; citation_pages=267-276; citation_doi=10.1145/566654.566575; citation_id=CR53"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Rev. Neurosci.; citation_title=Computational modelling of visual attention; citation_author=L Itti, C Koch; citation_volume=2; citation_publication_date=2001; citation_pages=194-203; citation_doi=10.1038/35058500; citation_id=CR54"/>
    <meta name="citation_reference" content="citation_journal_title=Neural Netw.; citation_title=Modeling attention to salient proto-objects; citation_author=D Walther, C Koch; citation_volume=19; citation_publication_date=2006; citation_pages=1395-1407; citation_doi=10.1016/j.neunet.2006.10.001; citation_id=CR55"/>
    <meta name="citation_reference" content="citation_journal_title=J. Physiol. Paris; citation_title=Cortical representation of animate and inanimate objects in complex natural scenes; citation_author=T Naselaris, DE Stansbury, JL Gallant; citation_volume=106; citation_publication_date=2012; citation_pages=239-249; citation_doi=10.1016/j.jphysparis.2012.02.001; citation_id=CR56"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Inferotemporal neurons represent low-dimensional configurations of parameterized shapes; citation_author=H Op de Beeck, J Wagemans, R Vogels; citation_volume=4; citation_publication_date=2001; citation_pages=1244-1252; citation_doi=10.1038/nn767; citation_id=CR57"/>
    <meta name="citation_author" content="Chikazoe, Junichi"/>
    <meta name="citation_author_institution" content="Department of Human Development, Human Neuroscience Institute, College of Human Ecology, Cornell University, Ithaca, USA"/>
    <meta name="citation_author" content="Lee, Daniel H"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of Toronto, Toronto, Canada"/>
    <meta name="citation_author" content="Kriegeskorte, Nikolaus"/>
    <meta name="citation_author_institution" content="Medical Research Council, Cognition and Brain Sciences Unit, Cambridge, UK"/>
    <meta name="citation_author" content="Anderson, Adam K"/>
    <meta name="citation_author_institution" content="Department of Human Development, Human Neuroscience Institute, College of Human Ecology, Cornell University, Ithaca, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of Toronto, Toronto, Canada"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@natureneuro"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Population coding of affect across stimuli, modalities and individuals"/>
    <meta name="twitter:description" content="Nature Neuroscience - The authors found human neuroimaging evidence that entire valence spectrum is represented as a collective pattern in regional neural activity, with sensory-specific signals in..."/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig1_HTML.jpg"/>
    

    
    
    <meta property="og:url" content="https://www.nature.com/articles/nn.3749"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="Population coding of affect across stimuli, modalities and individuals - Nature Neuroscience"/>
    <meta property="og:description" content="The authors found human neuroimaging evidence that entire valence spectrum is represented as a collective pattern in regional neural activity, with sensory-specific signals in the ventral temporal and anterior insular cortices and abstract codes in the orbitofrontal cortices. In this way, the subjective quality of affect can be objectively quantified across stimuli, modalities and people."/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig1_HTML.jpg"/>
    

    <script>
        window.eligibleForRa21 = 'true'; 
    </script>
</head>
<body class="article-page">

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>



<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
        
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>

    

    <div class="u-lazy-ad-wrapper u-mbs-0">
            <div class="deferred-placeholder" data-replace="true"
                 data-placeholder="/placeholder/v1/institutionalBanner?bpids=[bpids] #institutional-banner-container"></div>
            <aside class="c-ad c-ad--728x90">
                <div class="c-ad__inner" data-container-type="banner-advert">
                    <p class="c-ad__label">Advertisement</p>
                    
        
            
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=nn.3749;doi=10.1038/nn.3749;techmeta=36,59;subjmeta=1457,1945,378,631;kwrd=Prefrontal+cortex">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-1411288282&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Dnn.3749%26doi%3D10.1038/nn.3749%26techmeta%3D36,59%26subjmeta%3D1457,1945,378,631%26kwrd%3DPrefrontal+cortex">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-1411288282&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Dnn.3749%26doi%3D10.1038/nn.3749%26techmeta%3D36,59%26subjmeta%3D1457,1945,378,631%26kwrd%3DPrefrontal+cortex"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

        
    
                </div>
            </aside>
        </div>
    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#00928c">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">
                    
                    
                    <div class="c-header__logo-container">
                        
                        <a href="/neuro"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-0ccc487532906d646419e51f647ce79a.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-880e5942f43b9213989c58a04ab5c8e6.svg" height="32" alt="Nature Neuroscience">
                            </picture>
                        </a>
                    
                    </div>
                    
                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">
                            
                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/nn.3749'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
                            
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        
            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">
                            
                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span><span class="c-header__show-text">Explore</span> content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                                
                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>
                                
                            
                            
                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">
                            
                                <li class="c-header__item">
                                    <a class="c-header__link"
                                       href="https://idp.nature.com/auth/personal/springernature?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Fmy-account%2Falerts%2Fsubscribe-journal%3Flist-id%3D6"
                                       rel="nofollow"
                                       data-track="click"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/neuro.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        
    </header>


    
    
        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature neuroscience"><span itemprop="name">nature neuroscience</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro/articles?type&#x3D;article" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="name">articles</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>
    



    

</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
        
            <div class="c-context-bar u-hide"
                 data-test="context-bar"
                 data-context-bar
                 aria-hidden="true">
                <div class="c-context-bar__container u-container">
                    <div class="c-context-bar__title">
                        Population coding of affect across stimuli, modalities and individuals
                    </div>
                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3749.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                </div>
            </div>
        
        <article lang="en">
            
            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Article</li>
    
    

                        <li class="c-article-identifiers__item">Published: <time datetime="2014-06-22">22 June 2014</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Population coding of affect across stimuli, modalities and individuals</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Junichi-Chikazoe-Aff1" data-author-popup="auth-Junichi-Chikazoe-Aff1" data-author-search="Chikazoe, Junichi" data-corresp-id="c1">Junichi Chikazoe<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daniel_H-Lee-Aff2" data-author-popup="auth-Daniel_H-Lee-Aff2" data-author-search="Lee, Daniel H">Daniel H Lee</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Nikolaus-Kriegeskorte-Aff3" data-author-popup="auth-Nikolaus-Kriegeskorte-Aff3" data-author-search="Kriegeskorte, Nikolaus">Nikolaus Kriegeskorte</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 4 authors for this article" title="Show all 4 authors for this article">â€¦</li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Adam_K-Anderson-Aff1-Aff2" data-author-popup="auth-Adam_K-Anderson-Aff1-Aff2" data-author-search="Anderson, Adam K" data-corresp-id="c2">Adam K Anderson<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a></sup>Â </li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>

                    

                    <p class="c-article-info-details" data-container-section="info">
                        
    <a data-test="journal-link" href="/neuro" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Nature Neuroscience</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span>Â 17</b>,Â <span class="u-visually-hidden">pages </span>1114â€“1122 (<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>
                    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">13k <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">173 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">115 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/articles/nn.3749/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
                    
                </header>

                
    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/prefrontal-cortex" data-track="click" data-track-action="view subject" data-track-label="link">Prefrontal cortex</a></li>
        </ul>
    </div>

                
    
    

    
    

                
            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs2" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs2">Abstract</h2><div class="c-article-section__content" id="Abs2-content"><p>It remains unclear how the brain represents external objective sensory events alongside our internal subjective impressions of themâ€”affect. Representational mapping of population activity evoked by complex scenes and basic tastes in humans revealed a neural code supporting a continuous axis of pleasant-to-unpleasant valence. This valence code was distinct from low-level physical and high-level object properties. Although ventral temporal and anterior insular cortices supported valence codes specific to vision and taste, both the medial and lateral orbitofrontal cortices (OFC) maintained a valence code independent of sensory origin. Furthermore, only the OFC code could classify experienced affect across participants. The entire valence spectrum was represented as a collective pattern in regional neural activity as sensory-specific and abstract codes, whereby the subjective quality of affect can be objectively quantified across stimuli, modalities and people.</p></div></div></section>

            <noscript>
                
                    
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3749.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                
            </noscript>

            
                <div class="js-context-bar-sticky-point-mobile">
                    
                        <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-entitled-mobile
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3749.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                    
                </div>
            

            
                
                    
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42003-022-04324-6/MediaObjects/42003_2022_4324_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s42003-022-04324-6?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1038/s42003-022-04324-6">The neurobiological basis of affect is consistent with psychological construction theory and shares a common neural basis across emotional categories
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">09 December 2022</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">DoÄŸa GÃ¼ndem, Jure PotoÄnik, â€¦ Jan Van den Stock</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41593-022-01082-w/MediaObjects/41593_2022_1082_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41593-022-01082-w?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1038/s41593-022-01082-w">Common and stimulus-type-specific brain representations of negative affect
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">30 May 2022</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Marta ÄŒeko, Philip A. Kragel, â€¦ Tor D. Wager</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-019-11877-4/MediaObjects/41467_2019_11877_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41467-019-11877-4?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1038/s41467-019-11877-4">A dual role of prestimulus spontaneous neural activity in visual object recognition
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">02 September 2019</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Ella Podvalny, Matthew W. Flounders, â€¦ Biyu J. He</p>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'speedy-BootstrappedUCB',
                        timestamp: 1711545118,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                
                
                <div class="main-content">
                    <section data-title="Main"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Main</h2><div class="c-article-section__content" id="Sec1-content"><p>Even when we observe exactly the same object, subjective experience of that object often varies considerably among individuals, allowing us to form unique impressions of the sensory world around us. Wilhelm Wundt appropriately referred to these aspects of perception that are inherently the most subjective as 'affect'<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Wundt, W. Grundriss der Psychologie, von Wilhelm Wundt (W. Engelmann, Leipzig, 1897)." href="/articles/nn.3749#ref-CR1" id="ref-link-section-d30882912e399">1</a></sup>: the way sensory events affect us. Beyond basic sensory processing and object recognition, Wundt argued, the most pervasive aspect of human experience is this internal affective coloring of external sensory events. Despite its prominence in human experience, little is known about how the brain represents the affective coloring of perceptual experience compared with the rich neural characterizations of our other perceptual representations, such as the somatosensory system<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Penfield, W. &amp; Boldrey, E. Somatic motor and sensory representation in the cerebral cortex of man as studies by electrical stimulation. Brain 60, 389â€“443 (1937)." href="/articles/nn.3749#ref-CR2" id="ref-link-section-d30882912e403">2</a></sup>, semantics<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Huth, A.G., Nishimoto, S., Vu, A.T. &amp; Gallant, J.L. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron 76, 1210â€“1224 (2012)." href="/articles/nn.3749#ref-CR3" id="ref-link-section-d30882912e407">3</a></sup>, and visual features and categories<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Haxby, J.V. et al. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science 293, 2425â€“2430 (2001)." href="/articles/nn.3749#ref-CR4" id="ref-link-section-d30882912e411">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Kanwisher, N., McDermott, J. &amp; Chun, M.M. The fusiform face area: a module in human extrastriate cortex specialized for face perception. J. Neurosci. 17, 4302â€“4311 (1997)." href="/articles/nn.3749#ref-CR5" id="ref-link-section-d30882912e414">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.3749#ref-CR6" id="ref-link-section-d30882912e417">6</a></sup>.</p><p>Much of what we glean from external objects is not directly available on the sensory surface. The brain may transform sensory representations into higher order object representations (for example, animate, edible, dangerous, etc.)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Hinton, G.E., McClelland, J.L. &amp; Rumelhart, D.E. Distributed representations. in Parallel Distributed Processing: Explorations in the Microstructure of Cognition (eds. Rumelhart, D.E. &amp; McClelland, J.L.) 77â€“109 (The MIT Press, Cambridge, Massachusetts, 1986)." href="/articles/nn.3749#ref-CR7" id="ref-link-section-d30882912e424">7</a></sup>. The traditional approach to understanding how the brain represents these abstractions has been to investigate the magnitude of activity of specialized neurons or brain regions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Lewis, P.A., Critchley, H.D., Rotshtein, P. &amp; Dolan, R.J. Neural correlates of processing valence and arousal in affective words. Cereb. Cortex 17, 742â€“748 (2007)." href="/articles/nn.3749#ref-CR8" id="ref-link-section-d30882912e428">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Padoa-Schioppa, C. &amp; Assad, J.A. Neurons in the orbitofrontal cortex encode economic value. Nature 441, 223â€“226 (2006)." href="/articles/nn.3749#ref-CR9" id="ref-link-section-d30882912e431">9</a></sup>. An alternative approach treats neural populations in a region of cortex as supporting dimensions of higher order object representations according to their similarity in an abstract feature space<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.3749#ref-CR6" id="ref-link-section-d30882912e435">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Kriegeskorte, N. &amp; Kievit, R.A. Representational geometry: integrating cognition, computation and the brain. Trends Cogn. Sci. 17, 401â€“412 (2013)." href="/articles/nn.3749#ref-CR10" id="ref-link-section-d30882912e438">10</a></sup>. Measuring these patterns of neuronal activity has been made possible by advances in multivoxel pattern analysis (MVPA)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Haynes, J.D. Decoding and predicting intentions. Ann. NY Acad. Sci. 1224, 9â€“21 (2011)." href="/articles/nn.3749#ref-CR11" id="ref-link-section-d30882912e442">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Kamitani, Y. &amp; Tong, F. Decoding the visual and subjective contents of the human brain. Nat. Neurosci. 8, 679â€“685 (2005)." href="/articles/nn.3749#ref-CR12" id="ref-link-section-d30882912e445">12</a></sup>. For example, MVPA of human blood oxygen levelâ€“dependent (BOLD) response to visual objects has revealed that low-level feature dimensions can be decoded on the basis of topographic structure in the early visual cortices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Kamitani, Y. &amp; Tong, F. Decoding the visual and subjective contents of the human brain. Nat. Neurosci. 8, 679â€“685 (2005)." href="/articles/nn.3749#ref-CR12" id="ref-link-section-d30882912e449">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Freeman, J., Brouwer, G.J., Heeger, D.J. &amp; Merriam, E.P. Orientation decoding depends on maps, not columns. J. Neurosci. 31, 4792â€“4804 (2011)." href="/articles/nn.3749#ref-CR13" id="ref-link-section-d30882912e452">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Sasaki, Y. et al. The radial bias: a different slant on visual orientation sensitivity in human and nonhuman primates. Neuron 51, 661â€“670 (2006)." href="/articles/nn.3749#ref-CR14" id="ref-link-section-d30882912e455">14</a></sup> (but see ref. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Alink, A., Krugliak, A., Walther, A. &amp; Kriegeskorte, N. fMRI orientation decoding in V1 does not require global maps or globally coherent orientation stimuli. Front. Psychol. 4, 493 (2013)." href="/articles/nn.3749#ref-CR15" id="ref-link-section-d30882912e459">15</a>). In addition, high-level object dimensions, such as object categories<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Haxby, J.V. et al. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science 293, 2425â€“2430 (2001)." href="/articles/nn.3749#ref-CR4" id="ref-link-section-d30882912e463">4</a></sup> or animacy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Kriegeskorte, N. et al. Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron 60, 1126â€“1141 (2008)." href="/articles/nn.3749#ref-CR16" id="ref-link-section-d30882912e467">16</a></sup>, have been revealed in the distributed population codes of the ventral temporal cortex (VTC).</p><p>Although pattern classifier decoding<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Haxby, J.V. et al. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science 293, 2425â€“2430 (2001)." href="/articles/nn.3749#ref-CR4" id="ref-link-section-d30882912e474">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="McNamee, D., Rangel, A. &amp; O'Doherty, J.P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479â€“485 (2013)." href="/articles/nn.3749#ref-CR17" id="ref-link-section-d30882912e477">17</a></sup> is sensitive to information encoded combinatorially in fine-grained patterns of activity, it typically focuses on binary distinctions to indicate whether a region contains information about stimulus type (for example, face versus chair). By contrast, representational mapping further affords an examination of the space in which information is represented in a region (for example, how specific faces are related to each other)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Kriegeskorte, N. &amp; Kievit, R.A. Representational geometry: integrating cognition, computation and the brain. Trends Cogn. Sci. 17, 401â€“412 (2013)." href="/articles/nn.3749#ref-CR10" id="ref-link-section-d30882912e481">10</a></sup>. By characterizing the representational geometry of regional activity patterns, representational mapping reveals not only where and what, but also how information is represented. Representational mapping emphasizes the relationships between stimulus or experiential properties and their distances in high-dimensional space defined by the collective patterns of voxel activity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.3749#ref-CR6" id="ref-link-section-d30882912e485">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Kriegeskorte, N. &amp; Kievit, R.A. Representational geometry: integrating cognition, computation and the brain. Trends Cogn. Sci. 17, 401â€“412 (2013)." href="/articles/nn.3749#ref-CR10" id="ref-link-section-d30882912e488">10</a></sup>. For example, although population activity in the primary visual cortex can discriminate distinct colors, the representational geometry in extrastriate region V4 captures the distances between colors as they relate to perceptual experience<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Brouwer, G.J. &amp; Heeger, D.J. Decoding and reconstructing color from responses in human visual cortex. J. Neurosci. 29, 13992â€“14003 (2009)." href="/articles/nn.3749#ref-CR18" id="ref-link-section-d30882912e492">18</a></sup>.</p><p>We asked how external events come to be represented as internal subjective affect compared with other lower level physical and higher level categorical properties. Supporting Wundt's assertion of affect as central to perceptual experience, surveys across dozens of cultures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Osgood, C.E., May, W.H. &amp; Miron, M.S. Cross-Cultural Universals of Affective Meaning (University of Illinois Press, Urbana, Illinois, 1975)." href="/articles/nn.3749#ref-CR19" id="ref-link-section-d30882912e499">19</a></sup> have shown the primary dimension capturing the characterization of the world's varied contents is the evaluation of their goodness-badness, which is often referred to as valence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Russell, J.A. A circumplex model of affect. J. Pers. Soc. Psychol. 39, 1161â€“1178 (1980)." href="/articles/nn.3749#ref-CR20" id="ref-link-section-d30882912e503">20</a></sup>. We examined whether collective patterns of activity in the human brain support a continuous dimension of positive-to-negative valence, and where in the neural hierarchy this dimension is represented. Similarity-dissimilarity in subjective valence experience would then correspond to population level activity across stimuli, with representational geometry of activity patterns indicating that extreme positive and negative valence are furthest apart.</p><p>It has been traditionally thought that affect is not only represented separately from the perceptual cortices, which represent the sensory and perceptual properties of objects<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Grill-Spector, K. &amp; Malach, R. The human visual cortex. Annu. Rev. Neurosci. 27, 649â€“677 (2004)." href="/articles/nn.3749#ref-CR21" id="ref-link-section-d30882912e511">21</a></sup>, but also in distinct affective zones for positive and negative valence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Anderson, A.K. et al. Dissociated neural representations of intensity and valence in human olfaction. Nat. Neurosci. 6, 196â€“202 (2003)." href="/articles/nn.3749#ref-CR22" id="ref-link-section-d30882912e515">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="O'Doherty, J., Kringelbach, M.L., Rolls, E.T., Hornak, J. &amp; Andrews, C. Abstract reward and punishment representations in the human orbitofrontal cortex. Nat. Neurosci. 4, 95â€“102 (2001)." href="/articles/nn.3749#ref-CR23" id="ref-link-section-d30882912e518">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Small, D.M. et al. Dissociation of neural representation of intensity and affective valuation in human gustation. Neuron 39, 701â€“711 (2003)." href="/articles/nn.3749#ref-CR24" id="ref-link-section-d30882912e521">24</a></sup>. Lesion and neuroimaging studies of affective processes suggest a central role for the OFC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="OngÃ¼r, D. &amp; Price, J.L. The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans. Cereb. Cortex 10, 206â€“219 (2000)." href="/articles/nn.3749#ref-CR25" id="ref-link-section-d30882912e525">25</a></sup>, with evidence pointing to the lateral and medial OFC regions for affective representations of visual<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Shenhav, A., Barrett, L.F. &amp; Bar, M. Affective value and associative processing share a cortical substrate. Cogn. Affect. Behav. Neurosci. 13, 46â€“59 (2013)." href="/articles/nn.3749#ref-CR26" id="ref-link-section-d30882912e529">26</a></sup>, gustatory<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Small, D.M. et al. Dissociation of neural representation of intensity and affective valuation in human gustation. Neuron 39, 701â€“711 (2003)." href="/articles/nn.3749#ref-CR24" id="ref-link-section-d30882912e533">24</a></sup> and olfactory stimuli<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Anderson, A.K. et al. Dissociated neural representations of intensity and valence in human olfaction. Nat. Neurosci. 6, 196â€“202 (2003)." href="/articles/nn.3749#ref-CR22" id="ref-link-section-d30882912e538">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Gottfried, J.A., O'Doherty, J. &amp; Dolan, R.J. Appetitive and aversive olfactory learning in humans studied using event-related functional magnetic resonance imaging. J. Neurosci. 22, 10829â€“10837 (2002)." href="/articles/nn.3749#ref-CR27" id="ref-link-section-d30882912e541">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Rolls, E.T., Kringelbach, M.L. &amp; de Araujo, I.E. Different representations of pleasant and unpleasant odours in the human brain. Eur. J. Neurosci. 18, 695â€“703 (2003)." href="/articles/nn.3749#ref-CR28" id="ref-link-section-d30882912e544">28</a></sup>. Increasing activity in the medial OFC and adjacent ventromedial prefrontal cortices have been associated with goal value, reward expectancy, outcome values and experienced positive valence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="O'Doherty, J., Kringelbach, M.L., Rolls, E.T., Hornak, J. &amp; Andrews, C. Abstract reward and punishment representations in the human orbitofrontal cortex. Nat. Neurosci. 4, 95â€“102 (2001)." href="/articles/nn.3749#ref-CR23" id="ref-link-section-d30882912e548">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Montague, P.R. &amp; Berns, G.S. Neural economics and the biological substrates of valuation. Neuron 36, 265â€“284 (2002)." href="/articles/nn.3749#ref-CR29" id="ref-link-section-d30882912e551">29</a></sup>, and may support 'core affect'<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Wilson-Mendenhall, C.D., Barrett, L.F. &amp; Barsalou, L.W. Neural evidence that human emotions share core affective properties. Psychol. Sci. 24, 947â€“956 (2013)." href="/articles/nn.3749#ref-CR30" id="ref-link-section-d30882912e555">30</a></sup>. However, meta-analyses of neuroimaging studies have uncovered substantial regional overlap between positive and negative valence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Lindquist, K.A., Wager, T.D., Kober, H., Bliss-Moreau, E. &amp; Barrett, L.F. The brain basis of emotion: a meta-analytic review. Behav. Brain Sci. 35, 121â€“143 (2012)." href="/articles/nn.3749#ref-CR31" id="ref-link-section-d30882912e559">31</a></sup>. In conjunction with evidence against distinct basic emotions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Wilson-Mendenhall, C.D., Barrett, L.F. &amp; Barsalou, L.W. Neural evidence that human emotions share core affective properties. Psychol. Sci. 24, 947â€“956 (2013)." href="/articles/nn.3749#ref-CR30" id="ref-link-section-d30882912e563">30</a></sup>, localizing distinct varieties of emotional experience has been a great challenge<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Lindquist, K.A., Wager, T.D., Kober, H., Bliss-Moreau, E. &amp; Barrett, L.F. The brain basis of emotion: a meta-analytic review. Behav. Brain Sci. 35, 121â€“143 (2012)." href="/articles/nn.3749#ref-CR31" id="ref-link-section-d30882912e567">31</a></sup>. Potentially undermining simple regional distinctions in valence, recent monkey electrophysiological studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Morrison, S.E. &amp; Salzman, C.D. The convergence of information about rewarding and aversive stimuli in single neurons. J. Neurosci. 29, 11471â€“11483 (2009)." href="/articles/nn.3749#ref-CR32" id="ref-link-section-d30882912e572">32</a></sup> have reported that valence-coding neurons with different properties (that is, neurons coding positivity, negativity, and both positivity and negativity) are anatomically interspersed in Walker's area 13, the homolog of human OFC. Thus, exploring distinct regions that code for positive and negative valence may not be fruitful, as the interspersed structure of the aversive and appetitive neurons, at the scale of cortical regions, respond equivocally and confound traditional univariate functional magnetic resonance imaging (fMRI) analysis methods. With a voxel-level neuronal bias, multivoxel patterns can reveal whether the representational geometry of valence is captured by distance in high-dimensional neural space.</p><p>The notion that affect is largely represented outside the perceptual cortices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Grill-Spector, K. &amp; Malach, R. The human visual cortex. Annu. Rev. Neurosci. 27, 649â€“677 (2004)." href="/articles/nn.3749#ref-CR21" id="ref-link-section-d30882912e579">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Anderson, A.K. et al. Dissociated neural representations of intensity and valence in human olfaction. Nat. Neurosci. 6, 196â€“202 (2003)." href="/articles/nn.3749#ref-CR22" id="ref-link-section-d30882912e582">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="O'Doherty, J., Kringelbach, M.L., Rolls, E.T., Hornak, J. &amp; Andrews, C. Abstract reward and punishment representations in the human orbitofrontal cortex. Nat. Neurosci. 4, 95â€“102 (2001)." href="/articles/nn.3749#ref-CR23" id="ref-link-section-d30882912e585">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Small, D.M. et al. Dissociation of neural representation of intensity and affective valuation in human gustation. Neuron 39, 701â€“711 (2003)." href="/articles/nn.3749#ref-CR24" id="ref-link-section-d30882912e588">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="OngÃ¼r, D. &amp; Price, J.L. The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans. Cereb. Cortex 10, 206â€“219 (2000)." href="/articles/nn.3749#ref-CR25" id="ref-link-section-d30882912e591">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Shenhav, A., Barrett, L.F. &amp; Bar, M. Affective value and associative processing share a cortical substrate. Cogn. Affect. Behav. Neurosci. 13, 46â€“59 (2013)." href="/articles/nn.3749#ref-CR26" id="ref-link-section-d30882912e594">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Gottfried, J.A., O'Doherty, J. &amp; Dolan, R.J. Appetitive and aversive olfactory learning in humans studied using event-related functional magnetic resonance imaging. J. Neurosci. 22, 10829â€“10837 (2002)." href="/articles/nn.3749#ref-CR27" id="ref-link-section-d30882912e598">27</a></sup> has also not been tested with rigor. Average regional neural activity may miss the information contained in population level response in the perceptual cortices themselves. Rather than depending on distinct representations, affect may be manifest in the same regions that support sensory and object processing. Although posterior cortical regions are often modulated by affect, it remains unclear whether valence is coded in the perceptual cortices or whether perceptual representations are merely amplified by it<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Todd, R.M., Talmi, D., Schmitz, T.W., Susskind, J. &amp; Anderson, A.K. Psychophysical and neural evidence for emotion-enhanced perceptual vividness. J. Neurosci. 32, 11201â€“11212 (2012)." href="/articles/nn.3749#ref-CR33" id="ref-link-section-d30882912e602">33</a></sup>. Examining the representational geometry of population codes can address whether affect overlaps with other modality-specific stimulus representations that support basic visual features or object category membership. If population codes reveal that valence is embodied in modality-specific neuronal activity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Todd, R.M., Talmi, D., Schmitz, T.W., Susskind, J. &amp; Anderson, A.K. Psychophysical and neural evidence for emotion-enhanced perceptual vividness. J. Neurosci. 32, 11201â€“11212 (2012)." href="/articles/nn.3749#ref-CR33" id="ref-link-section-d30882912e606">33</a></sup>, then this would provide direct support for Wundt's observations that affect is a dimension central to perceptual experience.</p><p>A modality-specific conception of affective experience may suggest that affect is not commonly coded across events originating from distinct modalities. This would allow valence from distinct stimuli and modalities to be objectively quantified and then compared. It is presently unknown whether the displeasure evoked by the sight of a rotting carcass and the taste of spoiled wine are at some level supported by a common neural code. Although fMRI studies have shown overlapping neural responses in the OFC related to distinct modalities<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Grabenhorst, F., D'Souza, A.A., Parris, B.A., Rolls, E.T. &amp; Passingham, R.E. A common neural scale for the subjective pleasantness of different primary rewards. Neuroimage 51, 1265â€“1274 (2010)." href="/articles/nn.3749#ref-CR34" id="ref-link-section-d30882912e613">34</a></sup>, overlapping average activity is not necessarily diagnostic of engagement of the same representations. Fine-grained patterns of activity with these regions may be distinct, indicating that the underlying representations are modality specific, although appearing colocalized given the spatial limits of fMRI. This leaves unanswered whether there is a common neural affect code across stimuli originating from distinct modalities, whether evoked by distal photons or proximal molecules. If valence is represented supramodally, then, at an even more abstract level, we may ask whether the representation of affect demonstrates correspondence across people, affording a common reference frame across brains. This would provide evidence that even the most subjective aspect of an individual's experience, its internal affective coloring, can be predicted on the basis of the patterns observed in other brains, similar to what has been found for external object properties<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Raizada, R.D. &amp; Connolly, A.C. What makes different people's representations alike: neural similarity space solves the problem of across-subject fMRI decoding. J. Cogn. Neurosci. 24, 868â€“877 (2012)." href="/articles/nn.3749#ref-CR35" id="ref-link-section-d30882912e617">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Haxby, J.V. et al. A common, high-dimensional model of the representational space in human ventral temporal cortex. Neuron 72, 404â€“416 (2011)." href="/articles/nn.3749#ref-CR36" id="ref-link-section-d30882912e620">36</a></sup>.</p><p>To answer these questions of how affect is represented in the human brain, we first examined whether population vectors in response to complex visual scenes support a continuous representation of experienced valence and their relation to objective low-level visual properties (for example, luminance, visual salience) and higher order object properties (animacy) in the early visual cortex (EVC), VTC and OFC. We then examined whether the representation of affect to complex visual scenes was shared with basic gustatory stimuli, supporting a valence coordinate space common to objects stimulating the eye or the tongue. Lastly, we examined whether an individual's subjective affect codes corresponds to that observed in the brains of others.</p></div></div></section><section data-title="Results"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Results</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Visual feature, object and affect representations of complex visual scenes</h3><p>To investigate how object information and affective experience of complex visual stimuli are represented in the human brain, we presented 128 unique visual scenes to 16 participants during fMRI. After each picture presentation (3 s), participants rated their subjective affect on separate positivity and negativity scales (1â€“7, least to most). We examined the similarity of activation patterns as related to three distinct properties, each increasing in degree of abstraction: low-level visual features, object animacy and subjective affect (Online Methods and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig7">Supplementary Fig. 1</a>). Consistent with their substantial independence, visual feature, animacy and valence scores were largely uncorrelated, sharing 0.2% (visual features and animacy, all <i>R</i><sup>2</sup> = 0.002), 1.0% (visual features and valence, all <i>R</i><sup>2</sup> â‰¤ 0.022) and 2.3% (animacy and valence) of variance (all <i>R</i><sup>2</sup> â‰¤ 0.057; <i>n</i> = 128 trials). This orthogonality allowed us to examine whether distinct or similar codes support visual feature, object and affect representations.</p><p>Prior to multivariate analyses, we conducted a univariate parametric modulation analysis to test the monkey electrophysiological findings of bivalent neuronal coding<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Morrison, S.E. &amp; Salzman, C.D. The convergence of information about rewarding and aversive stimuli in single neurons. J. Neurosci. 29, 11471â€“11483 (2009)." href="/articles/nn.3749#ref-CR32" id="ref-link-section-d30882912e661">32</a></sup>. When using positivity and negativity ratings as independent parameters, a large majority of the valence-sensitive regions in the medial and lateral OFC were responsive to both positivity and negativity (75.7%; positivity only, 17.1%; negativity only, 7.2%; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig1">Fig. 1a</a>). Specifically, we found that the medial OFC and more dorsal regions in the ventromedial prefrontal cortex (vmPFC), areas typically associated with value coding and positive valence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="O'Doherty, J., Kringelbach, M.L., Rolls, E.T., Hornak, J. &amp; Andrews, C. Abstract reward and punishment representations in the human orbitofrontal cortex. Nat. Neurosci. 4, 95â€“102 (2001)." href="/articles/nn.3749#ref-CR23" id="ref-link-section-d30882912e668">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Montague, P.R. &amp; Berns, G.S. Neural economics and the biological substrates of valuation. Neuron 36, 265â€“284 (2002)." href="/articles/nn.3749#ref-CR29" id="ref-link-section-d30882912e671">29</a></sup>, exhibited parallel linear increases in activation with increasing ratings of both negative and positive valence (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig1">Fig. 1b</a>). The peak voxel (<i>x</i> = âˆ’8, <i>y</i> = 42, <i>z</i> = âˆ’12, <i>t</i><sub>15</sub> = 8.7, false discovery rate (FDR) â‰¤ 0.05) activity, which was maximally sensitive to positive valence also linearly increased with negative valence, whereas the peak voxel (<i>x</i> = âˆ’8, <i>y</i> = 52, <i>z</i> = âˆ’8, <i>t</i><sub>15</sub> = 6.8, FDR â‰¤ 0.05) activity, which was maximally sensitive to negative valence, also linearly increased with negative valence. These responses may reflect a common underlying arousal coding and therefore contain little diagnostic information about experienced valence. Alternatively, this univariate activity may reflect coding of both positive and negative valence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Kron, A., Goldstein, A., Lee, D.H., Gardhouse, K. &amp; Anderson, A.K. How are you feeling? Revisiting the quantification of emotional qualia. Psychol. Sci. 24, 1503â€“1511 (2013)." href="/articles/nn.3749#ref-CR37" id="ref-link-section-d30882912e706">37</a></sup>, which is equivocal at voxel signal resolution, consistent with the interspersed structure of the aversive and appetitive neurons, as observed in the monkey OFC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Morrison, S.E. &amp; Salzman, C.D. The convergence of information about rewarding and aversive stimuli in single neurons. J. Neurosci. 29, 11471â€“11483 (2009)." href="/articles/nn.3749#ref-CR32" id="ref-link-section-d30882912e710">32</a></sup>. Representational mapping of population level responses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Kriegeskorte, N. &amp; Kievit, R.A. Representational geometry: integrating cognition, computation and the brain. Trends Cogn. Sci. 17, 401â€“412 (2013)." href="/articles/nn.3749#ref-CR10" id="ref-link-section-d30882912e714">10</a></sup> may address this ambiguity. Given that positive and negative valence were experienced as experientially distant (average <i>r</i> = âˆ’0.53), if the brain supports a valence code, then increasing dissimilarity in valence experience would be supported by increasing dissimilarity in population activity patterns, despite their similarity in univariate magnitude.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Parametric modulation analysis (univariate) for independent ratings of positive and negative valence."><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 1: Parametric modulation analysis (univariate) for independent ratings of positive and negative valence.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3749/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig1_HTML.jpg?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig1_HTML.jpg" alt="figure 1" loading="lazy" width="685" height="181"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>(<b>a</b>) Activation map of sensitivity to positive valence, negative valence and both. Yellow indicates voxels sensitive to positive valence (<i>P</i> &lt; 0.001 for positive, <i>P</i> &gt; 0.05 for negative), blue indicates voxels sensitive to negative valence (<i>P</i> &lt; 0.001 for negative, <i>P</i> &gt; 0.05 for positive) and green indicates the conjunction of positive and negative valence (<i>P</i> &lt; 0.031 for positive, <i>P</i> &lt; 0.031 for negative). (<b>b</b>) Mean activity in vmPFC and medial OFC increased along with increases in both positive and negative valence scores. Yellow lines indicate signals of the peak voxel (<i>x</i> = âˆ’8, <i>y</i> = 42, <i>z</i> = âˆ’12, <i>t</i><sub>15</sub> = 8.7, <i>P</i> = 0.0000003, FDR â‰¤ 0.05) maximally sensitive to positive valence. Blue lines indicate signals of the peak voxel (<i>x</i> = âˆ’8, <i>y</i> = 52, <i>z</i> = âˆ’8, <i>t</i><sub>15</sub> = 6.8, <i>P</i> = 0.000006, FDR â‰¤ 0.05) maximally sensitive to negative valence. Dotted lines indicate signal for opposite valence (that is, negative valence in the peak positive voxel, and positive valence in the peak negative voxel). <i>n</i> = 16 participants. Error bars represent s.e.m.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3749/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>We used representational similarity analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.3749#ref-CR6" id="ref-link-section-d30882912e807">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Kriegeskorte, N. &amp; Kievit, R.A. Representational geometry: integrating cognition, computation and the brain. Trends Cogn. Sci. 17, 401â€“412 (2013)." href="/articles/nn.3749#ref-CR10" id="ref-link-section-d30882912e810">10</a></sup>, a method for uncovering representational properties underlying multivariate data. We first modeled each trial as a separate event, and then examined multivoxel brain activation patterns in broad, anatomically defined regions of interest (ROIs) across different levels of the neural hierarchy, including EVC, VTC and OFC (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig2">Fig. 2a</a>). To assess how information was represented in each region, we constructed representational similarity matrices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.3749#ref-CR6" id="ref-link-section-d30882912e817">6</a></sup> from the correlation coefficients of the activation patterns between trials for all picture combinations (128 Ã— 127 / 2) separately in the EVC, VTC and OFC. To the degree that activity patterns corresponded to specific property, relations among pictures provide a mapping of the representational contents of each region. These similarity matrices were submitted to multidimensional scaling (MDS) for visualization. This assessment revealed response patterns organized maximally by gradations of low-level visual features in the EVC (<i>r</i> = 0.38, <i>P</i> = 0.00001), object animacy in the VTC (<i>r</i> = 0.73, <i>P</i> = 7.7 Ã— 10<sup>âˆ’23</sup>) and valence in the OFC (<i>r</i> = 0.46, <i>P</i> = 0.00000005) (<i>n</i> = 128 trials; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig2">Fig. 2b</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig8">Supplementary Fig. 2</a>), with all MDS analyses reaching fair levels of fit (Stress-I; EVC, 0.2043; VTC, 0.2230; OFC, 0.2912; stress values denote how well the MDS fits the measured distances).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Representational geometry of multi-voxel activity patterns in EVC, VTC and OFC."><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 2: Representational geometry of multi-voxel activity patterns in EVC, VTC and OFC.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3749/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig2_HTML.jpg?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig2_HTML.jpg" alt="figure 2" loading="lazy" width="685" height="488"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>(<b>a</b>) ROIs were determined on the basis of anatomical gray matter masks. (<b>b</b>) The 128 visual scene stimuli were arranged using MDS such that pairwise distances reflected neural response-pattern similarity. Color code indicates feature magnitude scores for low-level visual features in EVC (top), animacy in VTC (middle) and subjective valence in OFC (bottom) for the same stimuli. Examples i, ii, iii, iv and v traverse the primary dimension in each feature space, with pictures illustrating visual features (for example, luminance) (top), animacy (middle) and valence (bottom).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3749/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Property-region associations were further examined by converting each region's representational similarity matrix, which relate trial representations, into property representational similarity matrices, which relate property representations of visual feature, animacy or valence. For example, a valence representational similarity matrix was created by sorting the trial-based representational similarity matrix (128 Ã— 127 /2) into 13 Ã— 13 valence bins (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig3">Fig. 3</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig9">Supplementary Figs. 3</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig10">4</a>). As such, representational similarity matrices were sorted according to distinct stimulus properties, allowing us to visualize a representation map of each region according to each property. As presented in an ideal representational similarity matrix (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig3">Fig. 3a</a>), if activity patterns across pictures correspond to a property, then we would expect higher correlations along the main diagonal. Higher correlations were observed along the main diagonal for visual features in the EVC, animacy in the VTC and valence in the OFC (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig3">Fig. 3b</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig11">Supplementary Fig. 5</a>). To statistically test the validity of these representational maps, we used a general linear model (GLM) decomposition of the representational similarity matrices (Online Methods and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig9">Supplementary Fig. 3</a>) to derive a distance correspondence index (DCI): a measure of how well distance (dissimilarity) in neural activation pattern space corresponds to distance in the distinct property spaces. The DCI for visual feature, animacy and valence from each region were computed for each participant and submitted to one-sample <i>t</i> tests. This revealed representation maps of distinct kinds of property distance, increasing in abstraction from physical features to object categories to subjective affect along a posterior-to-anterior neural axis (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig3">Fig. 3b,c</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig11">Supplementary Fig. 5</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 1</a>). Valence distance was maximally coded in the OFC (<i>t</i><sub>15</sub> = 7.6, <i>P</i> = 0.000008), to a lesser degree in the VTC (<i>t</i><sub>15</sub> = 5.0, <i>P</i> = 0.0008) and not reliably in the EVC (<i>t</i><sub>15</sub> = 2.5, <i>P</i> = 0.11) (all Bonferroni corrected). A two-way repeated-measures ANOVA revealed a highly significant interaction (<i>F</i><sub>1.9, 28.8</sub> = 69.5, <i>P</i> = 4.5 Ã— 10<sup>âˆ’9</sup>, Greenhouse-Geisser correction applied) between regions (EVC, VTC, OFC) and property type (visual feature, animacy, valence). These results indicate that visual scenes differing in objective visual features and object animacy, but evoking similar subjective affect, resulted in similar representation in the OFC. In these activity pattern analyses, mean activity in a region was removed and the activity magnitude of each voxel was normalized by subtracting mean values across trials; thus, mean activity could not account for the observed representation mapping. However, to further test whether valence representations were driven by regions that only differed in activity magnitude, we reran these analyses in each ROI after removing voxels showing significant main effects of valence in activity under a liberal threshold (<i>P</i> &lt; 0.05, uncorrected). Even after this removal, very similar results were obtained (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Population coding of visual, object and affect properties of visual scenes."><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 3: Population coding of visual, object and affect properties of visual scenes.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3749/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig3_HTML.jpg?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig3_HTML.jpg" alt="figure 3" loading="lazy" width="685" height="495"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>(<b>a</b>) Correlations of activation patterns across trials were rank-ordered within a participant. In the ideal representation similarity matrix (RSM), trials with similar features (for example, matching valence) demonstrate higher correlations along the diagonal than those with dissimilar features on the off-diagonal. (<b>b</b>) After regressing out other properties and effects of no interest, residual correlations were sorted on the basis of visual features (13 Ã— 13), animacy (13 Ã— 13) or valence (13 Ã— 13) properties, and then separately examined in the EVC, VTC and OFC. Correlation ranks were averaged for each cell, providing visual (13 Ã— 13), animacy (13 Ã— 13) and valence RSMs (13 Ã— 13). Higher correlations were observed along the main diagonal in the visual RSM in the EVC, animacy RSM in the VTC and valence RSM in the OFC. (<b>c</b>) Correlation ranks in the EVC, VTC and OFC were subject to GLM with differences in visual (left), animacy (middle) and valence (right) features as linear predictors. GLM coefficients (DCI) represent to what extent correlations were predicted by the property types. For visual-features DCI, we used <i>t</i> test (EVC: <i>t</i><sub>15</sub> = 6.7, <i>P</i> = 0.00003; VTC: <i>t</i><sub>15</sub> = 8.5, <i>P</i> = 0.000002; OFC: <i>t</i><sub>15</sub> = 0.8, <i>P</i> = 1) and paired <i>t</i> test (EVC versus VTC: <i>t</i><sub>15</sub> = 0.8, <i>P</i> = 1; EVC versus OFC: <i>t</i><sub>15</sub> = 4.2, <i>P</i> = 0.008; VTC versus OFC: <i>t</i><sub>15</sub> = 4.4, <i>P</i> = 0.005). For animacy DCI, we used <i>t</i> test (EVC: <i>t</i><sub>15</sub> = 3.6, <i>P</i> = 0.01; VTC: <i>t</i><sub>15</sub> = 10.3, <i>P</i> = 1.5 Ã— 10<sup>âˆ’7</sup>; OFC: <i>t</i><sub>15</sub> = 3.9, <i>P</i> = 0.006) and paired <i>t</i> test (EVC versus VTC: <i>t</i><sub>15</sub> = âˆ’9.0, <i>P</i> = 1.7 Ã— 10<sup>âˆ’6</sup>; EVC versus OFC: <i>t</i><sub>15</sub> = âˆ’1.0, <i>P</i> = 1; VTC versus OFC: <i>t</i><sub>15</sub> = 11.3, <i>P</i> = 9.2 Ã— 10<sup>âˆ’8</sup>). For valence DCI, we used <i>t</i> test (EVC: <i>t</i><sub>15</sub> = 2.5, <i>P</i> = 0.11; VTC: <i>t</i><sub>15</sub> = 5.0, <i>P</i> = 0.0008; OFC: <i>t</i><sub>15</sub> = 7.6, <i>P</i> = 7.7 Ã— 10<sup>âˆ’6</sup>) and paired <i>t</i> test (EVC versus VTC: <i>t</i><sub>15</sub> = 1.8, <i>P</i> = 0.81; EVC versus OFC: <i>t</i><sub>15</sub> = âˆ’4.2, <i>P</i> = 0.007; VTC versus OFC: <i>t</i><sub>15</sub> = âˆ’4.8, <i>P</i> = 0.002). <i>t</i> tests in a region were one-tailed and paired <i>t</i> tests were two-tailed. <i>n</i> = 16 participants. Error bars represent s.e.m. ***<i>P</i> &lt; 0.001, **<i>P</i> &lt; 0.01, *<i>P</i> &lt; 0.05, Bonferroni corrected.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3749/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>These results suggest that distributed activation patterns across broad swaths of cortex can support affect coding distinct from other object properties. To investigate whether distinct or overlapping subregions in the EVC, VTC and OFC support visual feature, object and affect codes, as well as the contribution of other brain regions, we conducted a cubic searchlight analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Kriegeskorte, N., Goebel, R. &amp; Bandettini, P. Information-based functional brain mapping. Proc. Natl. Acad. Sci. USA 103, 3863â€“3868 (2006)." href="/articles/nn.3749#ref-CR38" id="ref-link-section-d30882912e1167">38</a></sup>. In a given cube, correlations across trials were calculated and were subjected to the same GLM decomposition used above to compute DCIs (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig9">Supplementary Fig. 3</a>). This revealed a similar posterior-to-anterior pattern of increasing affective subjectivity and abstraction from physical features. Visual features were maximally represented primarily in the EVC and moderately in the VTC, object animacy was maximally represented in the VTC, and affect was maximally represented in the vmPFC, including the medial OFC, and the lateral OFC, and moderately represented in ventral and anterior temporal regions including the temporal pole (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Fig. 4a,b</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig12">Supplementary Fig. 6</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Tables 2</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">3</a>). These results indicate that object and affect representations are not only represented as distributed activation patterns across large areas of cortex, but are also represented as distinct region-specific population codes (that is, in a 1-cm<sup>3</sup> cube).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Region-specific population coding of visual features, object animacy and valence in visual scenes."><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 4: Region-specific population coding of visual features, object animacy and valence in visual scenes.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3749/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig4_HTML.jpg?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig4_HTML.jpg" alt="figure 4" loading="lazy" width="685" height="643"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>(<b>a</b>) Multivariate searchlight analysis revealed distinct areas represent coding of visual features (green), animacy (yellow) and valence (red) properties. Activations were thresholded at <i>P</i> &lt; 0.001, uncorrected. (<b>b</b>) GLM coefficients (DCI) represent the extent to which correlations were predicted by the property types (visual features, animacy and valence). For visual-features DCI, we used <i>t</i> test (EVC: <i>t</i><sub>15</sub> = 8.4, <i>P</i> = 0.000003; VTC: <i>t</i><sub>15</sub> = 4.3, <i>P</i> = 0.004; temporal pole (TP): <i>t</i><sub>15</sub> = âˆ’0.1, <i>P</i> = 1; OFC: <i>t</i><sub>15</sub> = 1.4, <i>P</i> = 1) and paired <i>t</i> test (EVC versus VTC: <i>t</i><sub>15</sub> = 6.4, <i>P</i> = 0.0002; EVC versus TP: <i>t</i><sub>15</sub> = 5.8, <i>P</i> = 0.0006; EVC versus OFC: <i>t</i><sub>15</sub> = 4.5, <i>P</i> = 0.008; VTC versus TP: <i>t</i><sub>15</sub> = 2.6, <i>P</i> = 0.36; VTC versus OFC: <i>t</i><sub>15</sub> = 1.2, <i>P</i> = 1; TP versus OFC: <i>t</i><sub>15</sub> = âˆ’1.4, <i>P</i> = 1). For animacy DCI, we used <i>t</i> test (EVC: <i>t</i><sub>15</sub> = 3.5, <i>P</i> = 0.017; VTC: <i>t</i><sub>15</sub> = 7.8, <i>P</i> = 0.000007; TP: <i>t</i><sub>15</sub> = 0.9, <i>P</i> = 1; OFC: <i>t</i><sub>15</sub> = 3.6, <i>P</i> = 0.015) and paired <i>t</i> test (EVC versus VTC: <i>t</i><sub>15</sub> = âˆ’6.4, <i>P</i> = 0.0002; EVC versus TP: <i>t</i><sub>15</sub> = 2.4; <i>P</i> = 0.54; EVC versus OFC: <i>t</i><sub>15</sub> = 1.1, <i>P</i> = 1; VTC versus TP: <i>t</i><sub>15</sub> = 6.8, <i>P</i> = 0.0001; VTC versus OFC: <i>t</i><sub>15</sub> = 7.8, <i>P</i> = 0.00002; TP versus OFC: <i>t</i><sub>15</sub> = âˆ’2.9, <i>P</i> = 0.19). For valence DCI, we used <i>t</i> test (EVC: <i>t</i><sub>15</sub> = 1.0, <i>P</i> = 1; VTC: <i>t</i><sub>15</sub> = 2.6, <i>P</i> = 0.12; TP: <i>t</i><sub>15</sub> = 3.5, <i>P</i> = 0.019; OFC: <i>t</i><sub>15</sub> = 6.0, <i>P</i> = 0.0001) and paired <i>t</i> test (EVC versus VTC: <i>t</i><sub>15</sub> = âˆ’0.7, <i>P</i> = 1; EVC versus TP: <i>t</i><sub>15</sub> = âˆ’1.5, <i>P</i> = 1; EVC versus OFC: <i>t</i><sub>15</sub> = âˆ’3.4, <i>P</i> = 0.071; VTC versus TP: <i>t</i><sub>15</sub> = âˆ’1.6, <i>P</i> = 1; VTC versus OFC: <i>t</i><sub>15</sub> = âˆ’5.0, <i>P</i> = 0.003; TP versus OFC: <i>t</i><sub>15</sub> = âˆ’5.2, <i>P</i> = 0.002). <i>t</i> tests in a region were one-tailed and paired <i>t</i> tests were two-tailed. <i>n</i> = 16 participants. (<b>c</b>â€“<b>h</b>) Difference in mean activity magnitude and pattern in the searchlight defined regions (<b>c</b>â€“<b>e</b>: the medial OFC (mOFC) and vmPFC; <b>f</b>â€“<b>h</b>: the lateral OFC (lOFC)). (<b>c</b>,<b>f</b>) Relationship of activity magnitude and ratings for positivity and negativity (<i>n</i> = 16 participants). (<b>d</b>,<b>g</b>) Valence representation similarity matrices based on the mean activity magnitude. (<b>e</b>,<b>h</b>) Valence representation similarity matrices based on pattern activation (correlation). <i>n</i> = 16 participants. Error bars represent s.e.m. ***<i>P</i> &lt; 0.001, **<i>P</i> &lt; 0.01, *<i>P</i> &lt; 0.05. Bonferroni corrected.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3749/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>To further examine what pattern-based affect coding uniquely codes, we tested whether differences in mean activity magnitude across trials could code valence information in the regions defined by the above searchlight (that is, the medial OFC and vmPFC and lateral OFC). To test whether mean activity magnitude is capable of discrimination of valence representations, we applied the same GLM decomposing procedure to mean activity magnitude instead of activation patterns. Here, similarity-dissimilarity of neural activation was defined by difference in mean activity magnitude in the region. The medial OFC and vmPFC showed a linear increase in activation, with increases in both positive and negative valence from neutral (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Fig. 4c</a>). The mean-based GLM decomposition analysis revealed a lack of valence specificity in mean magnitude (<i>t</i><sub>15</sub> = 1.1, <i>P</i> = 0.13; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Fig. 4d</a>), whereas a pattern-based approach showed a clear separation of valence, with positive and negative valence lying on opposite ends of a continuum (<i>t</i><sub>15</sub> = 4.2, <i>P</i> = 0.0004; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Fig. 4e</a>). By contrast, the lateral OFC did not demonstrate a relationship between mean activation and positive or negative valence (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Fig. 4f</a>), confirmed by a mean activityâ€“based GLM decomposition analysis (<i>t</i><sub>15</sub> = 0.5, <i>P</i> = 0.30; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Fig. 4g</a>), whereas a pattern-based approach still yielded a clear separation of valence (<i>t</i><sub>15</sub> = 3.9, <i>P</i> = 0.0007; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Fig. 4h</a>). These results not only explain why pattern analysis is required for representational mapping of affect, but also further indicate importance of discriminating arousal and valence coding<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Dolcos, F., LaBar, K.S. &amp; Cabeza, R. Dissociable effects of arousal and valence on prefrontal activity indexing emotional evaluation and subsequent memory: an event-related fMRI study. Neuroimage 23, 64â€“74 (2004)." href="/articles/nn.3749#ref-CR39" id="ref-link-section-d30882912e1575">39</a></sup>. That is, even when regional univariate activity showed similar responses to both positive and negative valence, it may not be diagnostic of arousal coding, but rather may reveal coding of both positive and negative valence (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Fig. 4d,e,g,h</a>).</p><h3 class="c-article__sub-heading" id="Sec4">Common and distinct representations of affect in vision and taste</h3><p>To test whether valence codes are modality specific or also support an abstract, nonvisual representation of affect, we conducted a gustatory experiment on the same participants. Affective appraisals<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Lazarus, R.S. &amp; Folkman, S. Stress, Appraisal and Coping (Springer Publishing Company, 1984)." href="/articles/nn.3749#ref-CR40" id="ref-link-section-d30882912e1590">40</a></sup> of complex scenes may also require deeper and more extensive processing that is not required by simpler, chemical sensory stimuli such as taste. As such, it is important to establish the generality of the affect coding in response to visual scenes. In this experiment, four different taste solutions matched for each participant in terms of intensity (sour, sweet, bitter, salty) and a tasteless solution were delivered 20 times each across 100 trials during fMRI. Paralleling our analysis of responses to scenes, representational similarity matrices were constructed from correlations of activation patterns across trials (100 Ã— 99/2) for each region. To visualize the representation maps from the gustatory experiment, we created a valence representational similarity matrix of the OFC, which revealed higher correlations across taste experiences of similar valenceâ€”along the main diagonal (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig5">Fig. 5a</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 1</a>). Valence DCIs in the OFC were computed for each participant and submitted to one-sample <i>t</i> tests, which revealed a significant relation between activation pattern similarity and valence distance (<i>t</i><sub>15</sub> = 2.9, <i>P</i> = 0.018). Valence DCIs in the VTC also achieved significance (<i>t</i><sub>15</sub> = 3.3, <i>P</i> = 0.007), but not the EVC (<i>t</i><sub>15</sub> = 1.5, <i>P</i> = 0.23). Thus, in addition to the OFC, the VTC represents affect information even when evoked by taste. Similar results were found when excluding regions that demonstrated a significant change (<i>P</i> &lt; 0.05, uncorrected) in mean activity to taste valence (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Visual, gustatory and cross-modal affect codes."><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 5: Visual, gustatory and cross-modal affect codes.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3749/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig5_HTML.jpg?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig5_HTML.jpg" alt="figure 5" loading="lazy" width="685" height="534"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>(<b>a</b>,<b>b</b>) OFC voxel activity pattern correlations across trials in the gustatory experiment (<b>a</b>) and across visual and gustatory experiments (<b>b</b>) were rank-ordered in each participant and then averaged on the basis of valence combinations (13 Ã— 13). Correlations across trials were sorted into five bins of increasing distance in valence. OFC correlations corresponded to valence distance, both within tastes and across tastes and visual scenes (<i>n</i> = 15 participants). (<b>c</b>) Multivariate searchlight results revealed subregions coding modality-specific (visual = red, taste = yellow) and modality-independent (green) valence. (<b>d</b>) GLM coefficients (DCI) represent the extent to which correlations were predicted by valence. Averaged DCI in the visual (top), taste (middle) and visual Ã— gustatory (bottom) valence subregions. In TP, we used <i>t</i> test (visual valence (V): <i>t</i><sub>15</sub> = 4.3, <i>P</i> = 0.0003; gustatory valence (G): <i>t</i><sub>15</sub> = 0.23, <i>P</i> = 0.41; visual Ã— gustatory valence (V Ã— G): <i>t</i><sub>15</sub> = 0.71, <i>P</i> = 0.24). In VTC1, we used <i>t</i> test (V: <i>t</i><sub>15</sub> = 4.9, <i>P</i> = 0.00009; G: <i>t</i><sub>15</sub> = âˆ’0.43, <i>P</i> = 1; V Ã— G: <i>t</i><sub>15</sub> = 0.10, <i>P</i> = 0.46). In striatum (STR), we used <i>t</i> test (V: <i>t</i><sub>15</sub> = 3.9, <i>P</i> = 0.0007; G: <i>t</i><sub>15</sub> = 0.23, <i>P</i> = 0.41; V Ã— G: <i>t</i><sub>15</sub> = 1.2, <i>P</i> = 0.12). In anterior insula (aINS), we used <i>t</i> test (V: <i>t</i><sub>15</sub> = 1.2, <i>P</i> = 0.12; G: <i>t</i><sub>15</sub> = 4.0, <i>P</i> = 0.0006; V Ã— G: <i>t</i><sub>15</sub> = âˆ’1.2, <i>P</i> = 1). In VTC2, we used <i>t</i> test (V: <i>t</i><sub>15</sub> = 0.62, <i>P</i> = 0.27; G: <i>t</i><sub>15</sub> = 4.8, <i>P</i> = 0.0001; V Ã— G: <i>t</i><sub>15</sub> = âˆ’1.2, <i>P</i> = 1). In posterior OFC (pOFC), we used <i>t</i> test (V: <i>t</i><sub>15</sub> = 0.40, <i>P</i> = 0.34; G: <i>t</i><sub>15</sub> = 3.7, <i>P</i> = 0.0010; V Ã— G: <i>t</i><sub>15</sub> = 0.78, <i>P</i> = 0.22). In medial OFC, we used <i>t</i> test (V: <i>t</i><sub>15</sub> = 6.3, <i>P</i> = 0.000007; G: <i>t</i><sub>15</sub> = 2.6, <i>P</i> = 0.010; V Ã— G: <i>t</i><sub>15</sub> = 3.9, <i>P</i> = 0.0008). In lateral OFC, we used <i>t</i> test (V: <i>t</i><sub>15</sub> = 5.2, <i>P</i> = 0.00005; G: <i>t</i><sub>15</sub> = 2.8, <i>P</i> = 0.007; V Ã— G: <i>t</i><sub>15</sub> = 4.1, <i>P</i> = 0.0005). In midcingulate cortex (MCC), we used <i>t</i> test (V: <i>t</i><sub>15</sub> = 3.8, <i>P</i> = 0.0008; G: <i>t</i><sub>15</sub> = 3.8, <i>P</i> = 0.0009; V Ã— G: <i>t</i><sub>15</sub> = 4.0, <i>P</i> = 0.0005). <i>P</i> values were uncorrected. <i>n</i> = 16 participants. Error bars represent s.e.m. ***<i>P</i> &lt; 0.001, **<i>P</i> &lt; 0.01.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3749/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Evidence of affective coding for pictures and tastes is not singularly diagnostic of an underlying common valence population code, as each sensory modality coding may be independently represented in the same regions. To directly examine a cross-modal commonality, we examined the representations of valence in the OFC on the basis of trials across visual and gustatory experiments. We first computed new cross-modal representational similarity matrices correlating activation patterns across the 128 visual Ã— 100 taste trials. Then, to visualize the cross-modal representation map, we created a valence representational similarity matrix of the OFC, which revealed higher correlations across visual and taste experiences of similar valence, along the main diagonal (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig5">Fig. 5b</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 1</a>). DCI revealed increasing similarity of OFC activation patterns between visual and gustatory trials, as affect was more similar (<i>t</i><sub>15</sub> = 3.0, <i>P</i> = 0.013; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig5">Fig. 5b</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 1</a>). Notably, the same analysis revealed no such relation in the VTC (<i>t</i><sub>15</sub> = 0.5 <i>P</i> = 0.99). That is, although we found modality-specific valence coding in the VTC (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig3">Figs. 3b,c</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">4a,b</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 1</a>), we found modality-independent valence coding only in the OFC. Similar results were obtained when excluding regions that demonstrated a significant change (<i>P</i> &lt; 0.05, uncorrected) in mean activity to taste or visual valence (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 1</a>).</p><p>To investigate whether specific subregions support modality-specific versus supramodal affect, we performed three independent cubic searchlight analyses on the basis of trials within visual, within gustatory, and across visual and gustatory experiments. In a given cube, correlation of activation patterns of each cross-trial combination (128 Ã— 127/2 for visual, 100 Ã— 99/2 for gustatory, and 128 Ã— 100 across visual and gustatory trials) were subject to the same GLM decomposition procedure (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig9">Supplementary Fig. 3</a>). We defined a region as representing supramodal affect if it was discovered in all three independent searchlights, whereas a region was defined as representing visual-specific affect if it was discovered only in the visual searchlight, but not the other two (analogously for gustatory-specific affect). This revealed that the anteroventral insula and posterior OFC (putative primary and secondary gustatory cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Small, D.M. et al. Human cortical gustatory areas: a review of functional neuroimaging data. Neuroreport 10, 7â€“14 (1999)." href="/articles/nn.3749#ref-CR41" id="ref-link-section-d30882912e1968">41</a></sup>) represented gustatory valence, and adjacent, but distinct, regions in the VTC represented gustatory and visual valence separately (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig5">Fig. 5c,d</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 4</a>). In contrast with this sensory specific affect coding, the medial OFC and vmPFC, as well as the lateral OFC and midcingulate cortex, contained supramodal representations of valence (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig5">Fig. 5c,d</a>). These searchlight results were not only exploratory, but also confirmatory, as they survived multiple comparison correction (Online Methods).</p><h3 class="c-article__sub-heading" id="Sec5">Classification of affect brain states across participants</h3><p>Lastly, we assessed whether valence in a specific individual corresponded to affect representations in others' brains. As previous work has demonstrated that representational geometry of object categories in the VTC can be shared across participants<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Raizada, R.D. &amp; Connolly, A.C. What makes different people's representations alike: neural similarity space solves the problem of across-subject fMRI decoding. J. Cogn. Neurosci. 24, 868â€“877 (2012)." href="/articles/nn.3749#ref-CR35" id="ref-link-section-d30882912e1989">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Haxby, J.V. et al. A common, high-dimensional model of the representational space in human ventral temporal cortex. Neuron 72, 404â€“416 (2011)." href="/articles/nn.3749#ref-CR36" id="ref-link-section-d30882912e1992">36</a></sup>, we first examined whether item-level (that is, by picture) classification is possible by comparing each participant's item-based representational similarity matrices to that estimated from all other participants in a leave-one-out procedure. We calculated the classification performance for each target picture as the percentage that its representation was more similar to its estimate, compared pairwise to all other picture representations (50% chance; Online Methods and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig13">Supplementary Fig. 7</a>). We found that item-specific representations in the VTC were predicted very highly by the other participants' representational map (80.1 Â± 1.4% accuracy, <i>t</i><sub>15</sub> = 21.4, <i>P</i> = 2.4 Ã— 10<sup>âˆ’12</sup>; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig6">Fig. 6a</a>). Cross-participant classification accuracy was also statistically significant in the OFC (54.7 Â± 0.8% accuracy, <i>t</i><sub>15</sub> = 5.7, <i>P</i> = 0.00008); however, it was substantially reduced compared with the VTC (<i>t</i><sub>15</sub> = 15.9, <i>P</i> = 8.4 Ã— 10<sup>âˆ’11</sup>), suggesting that item-specific information is more robustly represented and translatable across participants in the VTC compared with the OFC.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Cross-participant classification of items and affect."><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 6: Cross-participant classification of items and affect.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3749/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig6_HTML.jpg?as=webp"><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig6_HTML.jpg" alt="figure 6" loading="lazy" width="685" height="261"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>(<b>a</b>) Classification accuracies of cross-participant multivoxel patterns for specific items and subjective valence in the VTC (gray) and OFC (white). Each target item or valence was estimated by all other participants' representation in a leave-one-out procedure. Performance was calculated by the target's similarity to its estimate compared with all other trials in pairwise comparison (50% chance). For item classification, <i>t</i> test (OFC: <i>t</i><sub>15</sub> = 5.7, <i>P</i> = 0.00008, VTC: <i>t</i><sub>15</sub> = 21.4, <i>P</i> = 2.4 Ã— 10<sup>âˆ’12</sup>), paired <i>t</i> test (OFC versus VTC: <sub>15</sub> = âˆ’15.9, <i>P</i> = 8.4 Ã— 10<sup>âˆ’11</sup>). For valence classification, <i>t</i> test (OFC: <i>t</i><sub>15</sub> = 6.4, <i>P</i> = 0.00002, VTC: <i>t</i><sub>15</sub> = 2.0, <i>P</i> = 0.13), paired <i>t</i> test (OFC versus VTC: <i>t</i><sub>15</sub> = 4.2, <i>P</i> = 0.0007). Bonferroni correction was applied, based on number of comparisons for each ROI (2 (ROI: OFC and VTC)). <i>t</i> tests in a region were one-tailed and paired <i>t</i> tests were two-tailed. <i>n</i> = 16 participants. (<b>b</b>) Relationship between classification accuracies and valence distance in the OFC. Accuracies increased monotonically as experienced valence across trials became more clearly differentiated for all conditions. ANOVA (visual: <i>F</i><sub>1.4, 20.3</sub> = 37.4, <i>P</i> = 5.6 Ã— 10<sup>âˆ’6</sup>; gustatory: <i>F</i><sub>1.3, 18.9</sub> = 4.7, <i>P</i> = 0.033; visual Ã— gustatory: <i>F</i><sub>1.2, 18.6</sub> = 9.7, <i>P</i> = 0.004; gustatory Ã— visual: <i>F</i><sub>1.4, 19.6</sub> = 4.3, <i>P</i> = 0.04). Greenhouse-Geisser correction was applied, as Mauchly's test revealed violation of assumption of sphericity. For visual and visual by gustatory, <i>n</i> = 16 participants. For gustatory and gustatory Ã— visual, <i>n</i> = 15 participants. Error bars represent s.e.m. ***<i>P</i> &lt; 0.001, **<i>P</i> &lt; 0.01.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3749/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>We next examined whether a person's affect representations toward these visual items could be predicted by others' affect representations. After transforming representations of items into subjective affect and conducting a similar leave-one-out procedure (Online Methods and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig14">Supplementary Fig. 8</a>), although overall much lower than item representations in the VTC, we found cross-participant classification of valence in the OFC (55.6 Â± 0.9% accuracy, <i>t</i><sub>15</sub> = 6.4, <i>P</i> = 0.00002) (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig6">Fig. 6a</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 5</a>). Valence classification did not achieve significance in the VTC (51.7 Â± 0.8% accuracy, <i>t</i><sub>15</sub> = 2.0, <i>P</i> = 0.13), with a paired <i>t</i> test between the OFC and VTC revealing greater classification accuracy in the OFC than in the VTC (<i>t</i><sub>15</sub> = 4.2, <i>P</i> = 0.0007). A two-way repeated-measures ANOVA revealed a highly significant interaction (<i>F</i><sub>1,15</sub> = 278.1, <i>P</i> = 4.3 Ã— 10<sup>âˆ’11</sup>; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig6">Fig. 6a</a>) between region and representation type (item versus affect). This interaction revealed that, although stimulus specific representations were shared in the VTC, subjective affect representations are similarly structured across people in the OFC, even when the specific stimuli evoking affect may vary. Furthermore, the continuous representation of valence information was also revealed here, as increases in valence distance decreased the confusability between two affect representations, thereby increasing classification rates (<i>F</i><sub>1.4, 20.3</sub> = 37.4, <i>P</i> = 5.6 Ã— 10<sup>âˆ’6</sup>; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig6">Fig. 6b</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 5</a>).</p><p>As an even stronger test of cross participant translation of affect, we asked whether OFC affect representations toward pictures could predict other participants' affect representations in response to tastes (visual Ã— gustatory) and vice versa (gustatory Ã— visual). These analyses revealed classification accuracies that were significantly higher than chance level (visual Ã— gustatory, 54.2 Â± 0.7%, <i>t</i><sub>15</sub> = 5.8, <i>P</i> &lt; 0.001; gustatory Ã— visual, 54.6 Â± 1.2%, <i>t</i><sub>14</sub> = 3.8, <i>P</i> &lt; 0.001), with increases in valence distance between stimuli again increasing classification accuracy (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig6">Fig. 6b</a>). Even without an overlap in objective stimulus features (that is, vision versus taste), the OFC supported classification of the affective contents of subjective experience across individuals.</p></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Discussion</h2><div class="c-article-section__content" id="Sec6-content"><p>Representational mapping revealed that a complex scene is transformed from basic perceptual features and higher level object categories into affective population representations. Furthermore, rather than specialized regions designated to represent what is good or bad, population activity in a region supported a continuous dimension of positive-to-negative valence. Population codes also revealed that there are multiple representations of valence to the same event, both sensory specific and sensory independent. Posterior cortical representations in the temporal lobe and insular cortices were unique to the sensory modality of origin, whereas more anterior cortical representations in the medial and lateral OFC afforded a translation across distinct stimuli and modalities. This shared affect population code demonstrated correspondence across participants. Taken together, these data indicate that the neural population vector in a region may represent the affective coloring of experience, whether between objects, modalities or people.</p><h3 class="c-article__sub-heading" id="Sec7">Population coding of affect</h3><p>As suggested by monkey electrophysiological studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Morrison, S.E. &amp; Salzman, C.D. The convergence of information about rewarding and aversive stimuli in single neurons. J. Neurosci. 29, 11471â€“11483 (2009)." href="/articles/nn.3749#ref-CR32" id="ref-link-section-d30882912e2272">32</a></sup>, positivity- and negativity-sensitive neurons are likely interspersed in various sectors of the human OFC and vmPFC. Consistent with these single-cell recording data, our present univariate parametric modulation analysis did not show clear separation of positivity- and negativity-sensitive voxels in the OFC, with much greater overlap than separation. Prior studies typically assume a mathematical inversion of affective coding in the brain (for example, positivity is the inverse of negativity)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Wilson-Mendenhall, C.D., Barrett, L.F. &amp; Barsalou, L.W. Neural evidence that human emotions share core affective properties. Psychol. Sci. 24, 947â€“956 (2013)." href="/articles/nn.3749#ref-CR30" id="ref-link-section-d30882912e2276">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Lim, S.L., O'Doherty, J.P. &amp; Rangel, A. Stimulus value signals in ventromedial PFC reflect the integration of attribute value signals computed in fusiform gyrus and posterior superior temporal gyrus. J. Neurosci. 33, 8729â€“8741 (2013)." href="/articles/nn.3749#ref-CR42" id="ref-link-section-d30882912e2279">42</a></sup>. We were able to test this assumption directly as participants indicated their experience of positive and negative valence independently on each trial. Using these independent parameters, we found that regions such as the medial OFC and vmPFC, which have been associated with increasing positive value<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="O'Doherty, J., Kringelbach, M.L., Rolls, E.T., Hornak, J. &amp; Andrews, C. Abstract reward and punishment representations in the human orbitofrontal cortex. Nat. Neurosci. 4, 95â€“102 (2001)." href="/articles/nn.3749#ref-CR23" id="ref-link-section-d30882912e2283">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Montague, P.R. &amp; Berns, G.S. Neural economics and the biological substrates of valuation. Neuron 36, 265â€“284 (2002)." href="/articles/nn.3749#ref-CR29" id="ref-link-section-d30882912e2286">29</a></sup>, responded equally to negative valence (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig1">Fig. 1</a>). This bivalent association is often taken to indicate a coding of arousalâ€”the activating aspect of emotional experience<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Russell, J.A. A circumplex model of affect. J. Pers. Soc. Psychol. 39, 1161â€“1178 (1980)." href="/articles/nn.3749#ref-CR20" id="ref-link-section-d30882912e2293">20</a></sup>, rather than separate coding of the opposing ends of valence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Anderson, A.K. et al. Dissociated neural representations of intensity and valence in human olfaction. Nat. Neurosci. 6, 196â€“202 (2003)." href="/articles/nn.3749#ref-CR22" id="ref-link-section-d30882912e2298">22</a></sup>. However, our results indicate that regional activity magnitude in the medial OFC and vmPFC could not differentiate between opposing experiences of positive and negative valence, whereas population coding of the same voxels distinguished them as maximally distant (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Fig. 4d,e,g,h</a>), suggesting the distinct coding of unique valence experiences.</p><p>Although pattern analysis may be able to capture the difference in distribution of positivity- and negativity-sensitive neurons in the local structure, what the patterns exactly reflect is still a matter of debate<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Haynes, J.D. Decoding and predicting intentions. Ann. NY Acad. Sci. 1224, 9â€“21 (2011)." href="/articles/nn.3749#ref-CR11" id="ref-link-section-d30882912e2308">11</a></sup>. With regard to its underlying physiological bases, the interdigitation of single cell specialization for either positive or negative valence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Morrison, S.E. &amp; Salzman, C.D. The convergence of information about rewarding and aversive stimuli in single neurons. J. Neurosci. 29, 11471â€“11483 (2009)." href="/articles/nn.3749#ref-CR32" id="ref-link-section-d30882912e2312">32</a></sup> need not suggest the utilization of a pattern code. Given the averaging of many hundreds of thousands of neurons in a voxel in fMRI, it may be that pattern analysis sensitive to voxel-level biases in valence tuned neurons<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Morrison, S.E. &amp; Salzman, C.D. The convergence of information about rewarding and aversive stimuli in single neurons. J. Neurosci. 29, 11471â€“11483 (2009)." href="/articles/nn.3749#ref-CR32" id="ref-link-section-d30882912e2316">32</a></sup> is required to reveal valence coding in BOLD imaging. It remains to be determined whether the coding of valence is best captured by a distributed population level code across cells with distinct valence tuning properties. Evidence of colocalization of distinct valenced tuned neurons<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Morrison, S.E. &amp; Salzman, C.D. The convergence of information about rewarding and aversive stimuli in single neurons. J. Neurosci. 29, 11471â€“11483 (2009)." href="/articles/nn.3749#ref-CR32" id="ref-link-section-d30882912e2320">32</a></sup> may suggest the importance of rapid within-region computation of mixed valence responses, whereby the overall affective response is derived from a population level code across individual neurons.</p><h3 class="c-article__sub-heading" id="Sec8">Sensory-specific affect codes in the perceptual cortices</h3><p>Wundt's proposal of affect as an additional dimension of perceptual experience<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Wundt, W. Grundriss der Psychologie, von Wilhelm Wundt (W. Engelmann, Leipzig, 1897)." href="/articles/nn.3749#ref-CR1" id="ref-link-section-d30882912e2332">1</a></sup> may suggest that these subjective qualia are represented in posterior sensory cortices, binding affect to specific sensory events. Although altered mean activity in perceptual cortices associated with valence has been found, including reward-related activity in the VTC in monkeys<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Mogami, T. &amp; Tanaka, K. Reward association affects neuronal responses to visual stimuli in macaque te and perirhinal cortices. J. Neurosci. 26, 6761â€“6770 (2006)." href="/articles/nn.3749#ref-CR43" id="ref-link-section-d30882912e2336">43</a></sup> and humans<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Lim, S.L., O'Doherty, J.P. &amp; Rangel, A. Stimulus value signals in ventromedial PFC reflect the integration of attribute value signals computed in fusiform gyrus and posterior superior temporal gyrus. J. Neurosci. 33, 8729â€“8741 (2013)." href="/articles/nn.3749#ref-CR42" id="ref-link-section-d30882912e2340">42</a></sup>, it is unclear whether these regions contain valence information. Population-level activity revealed modally bound affect codes, consistent with Wundt's thesis, and evidence of sensory-specific hedonic habituation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Poellinger, A. et al. Activation and habituation in olfaction: an fMRI study. Neuroimage 13, 547â€“560 (2001)." href="/articles/nn.3749#ref-CR44" id="ref-link-section-d30882912e2344">44</a></sup>. Activity patterns in the VTC not only represented visual features and object information<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Haxby, J.V. et al. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science 293, 2425â€“2430 (2001)." href="/articles/nn.3749#ref-CR4" id="ref-link-section-d30882912e2348">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Kriegeskorte, N. et al. Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron 60, 1126â€“1141 (2008)." href="/articles/nn.3749#ref-CR16" id="ref-link-section-d30882912e2351">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Misaki, M., Kim, Y., Bandettini, P.A. &amp; Kriegeskorte, N. Comparison of multivariate classifiers and response normalizations for pattern-information fMRI. Neuroimage 53, 103â€“118 (2010)." href="/articles/nn.3749#ref-CR45" id="ref-link-section-d30882912e2354">45</a></sup>, but also corresponded to the representational geometry of an individual person's subjective affect. Low-level visual, object and affect properties, however, did not arise from the same activity patterns, but were largely anatomically and functionally dissociated in the VTC. In vision, posterior regions supported representations that were descriptions of the external visual stimulus, whereas more anterior association cortices, including the anterior ventral and temporal polar cortices, the latter of which was densely interconnected with the OFC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Olson, I.R., Plotzker, A. &amp; Ezzyat, Y. The enigmatic temporal pole: a review of findings on social and emotional processing. Brain 130, 1718â€“1731 (2007)." href="/articles/nn.3749#ref-CR46" id="ref-link-section-d30882912e2359">46</a></sup>, supported the internal affective coloring of visual scene perception. Consistent with the hedonic primacy of chemical sensing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Lapid, H. et al. Neural activity at the human olfactory epithelium reflects olfactory perception. Nat. Neurosci. 14, 1455â€“1461 (2011)." href="/articles/nn.3749#ref-CR47" id="ref-link-section-d30882912e2363">47</a></sup>, taste-evoked affect codes were found in the anteroventral insula and posterior OFC (putative primary and secondary gustatory cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Small, D.M. et al. Human cortical gustatory areas: a review of functional neuroimaging data. Neuroreport 10, 7â€“14 (1999)." href="/articles/nn.3749#ref-CR41" id="ref-link-section-d30882912e2367">41</a></sup>), suggesting that higher level appraisal processes may not be as necessary for the extraction of their valence properties.</p><p>The lack of correspondence in activity patterns across modalities, despite both coding valence, suggests that modality specific processes are involved in extracting valence information. The role of these modality-specific valence representations may be to allow differential weighting of distinct features in determining one's overall judgment of value<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Lim, S.L., O'Doherty, J.P. &amp; Rangel, A. Stimulus value signals in ventromedial PFC reflect the integration of attribute value signals computed in fusiform gyrus and posterior superior temporal gyrus. J. Neurosci. 33, 8729â€“8741 (2013)." href="/articles/nn.3749#ref-CR42" id="ref-link-section-d30882912e2374">42</a></sup> or subjective valence experience. Fear conditioning renders once indiscriminable odors perceptually discriminable, supported by divergence of ensemble activity patterns in primary olfactory (piriform) cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Li, W., Howard, J.D., Parrish, T.B. &amp; Gottfried, J.A. Aversive learning enhances perceptual and cortical discrimination of indiscriminable odor cues. Science 319, 1842â€“1845 (2008)." href="/articles/nn.3749#ref-CR48" id="ref-link-section-d30882912e2378">48</a></sup>. Rather than only the domain of specialized circuits outside of perceptual systems, valence coding may also be central to perceptual encoding, affording sensory-specific hedonic weightings. It remains to be determined whether valance codes embodied in a sensory system support distinct subjective qualia, as well as their relation to sensory-independent affect representations.</p><h3 class="c-article__sub-heading" id="Sec9">Supramodal affect codes in the OFC</h3><p>It has been proposed that a common scale is required for organisms to assess the relative value of computationally and qualitatively different events, such as drinking water, smelling food, scanning predators and so forth<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Montague, P.R. &amp; Berns, G.S. Neural economics and the biological substrates of valuation. Neuron 36, 265â€“284 (2002)." href="/articles/nn.3749#ref-CR29" id="ref-link-section-d30882912e2390">29</a></sup>. To decide on an appropriate behavior, the nervous system must convert the value of events into a common scale. By using food or monetary reward, previous studies of monkey electrophysiology<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Padoa-Schioppa, C. &amp; Assad, J.A. Neurons in the orbitofrontal cortex encode economic value. Nature 441, 223â€“226 (2006)." href="/articles/nn.3749#ref-CR9" id="ref-link-section-d30882912e2394">9</a></sup>, neuroimaging<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="O'Doherty, J., Kringelbach, M.L., Rolls, E.T., Hornak, J. &amp; Andrews, C. Abstract reward and punishment representations in the human orbitofrontal cortex. Nat. Neurosci. 4, 95â€“102 (2001)." href="/articles/nn.3749#ref-CR23" id="ref-link-section-d30882912e2398">23</a></sup> and neuropsychology<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="OngÃ¼r, D. &amp; Price, J.L. The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans. Cereb. Cortex 10, 206â€“219 (2000)." href="/articles/nn.3749#ref-CR25" id="ref-link-section-d30882912e2402">25</a></sup> have found that the OFC is critical for generating this kind of currency-like common scale. However, without investigating its microstructure, overlap in mean activity in a region is insufficient to reveal an underlying commonality in representation space. By examining multi-voxel patterns, a recent fMRI study demonstrated that the vmPFC commonly represents the monetary value (that is, how much one was willing to pay) of different visual goal objects (pictures of food, money and trinkets)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="McNamee, D., Rangel, A. &amp; O'Doherty, J.P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479â€“485 (2013)." href="/articles/nn.3749#ref-CR17" id="ref-link-section-d30882912e2406">17</a></sup>. However, such studies of 'common currency'<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="McNamee, D., Rangel, A. &amp; O'Doherty, J.P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479â€“485 (2013)." href="/articles/nn.3749#ref-CR17" id="ref-link-section-d30882912e2411">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Lim, S.L., O'Doherty, J.P. &amp; Rangel, A. Stimulus value signals in ventromedial PFC reflect the integration of attribute value signals computed in fusiform gyrus and posterior superior temporal gyrus. J. Neurosci. 33, 8729â€“8741 (2013)." href="/articles/nn.3749#ref-CR42" id="ref-link-section-d30882912e2414">42</a></sup> employ only visual cues denoting associated value of different types, rather than physical stimulus modalities. By delivering pleasant and unpleasant gustatory stimuli (for example, sweet and bitter liquids) instead of presenting visual stimuli that denote gustatory reward in the future (goal value), as well as complex scenes that varied across the entire valence spectrum, including highly negative valence, we found that, even when stimuli were delivered via vision or taste, modality-independent codes were projected into the same representation space whose coordinates were defined as subjective positive-to-negative affective experience in the OFC. This provides strong evidence that some part of affect representation space in the OFC is not only stimulus, but also modality, independent.</p><p>The exploratory searchlight analysis revealed that affect representations across modality were found in the lateral OFC, the medial OFC and the vmPFC. This finding is important, as most previous studies of value representations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="McNamee, D., Rangel, A. &amp; O'Doherty, J.P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479â€“485 (2013)." href="/articles/nn.3749#ref-CR17" id="ref-link-section-d30882912e2421">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Montague, P.R. &amp; Berns, G.S. Neural economics and the biological substrates of valuation. Neuron 36, 265â€“284 (2002)." href="/articles/nn.3749#ref-CR29" id="ref-link-section-d30882912e2424">29</a></sup> have mainly focused on the medial OFC and vmPFC, and not on the lateral OFC. Although both may support supramodal valence codes, the processes that work on these representations are likely different<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Noonan, M.P. et al. Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 107, 20547â€“20552 (2010)." href="/articles/nn.3749#ref-CR49" id="ref-link-section-d30882912e2428">49</a></sup>. The medial OFC may represent approach tendencies, whereas the more inhibitory functions associated with the lateral OFC sectors may use the same valence information to suppress desire to approach a stimulus, such as consume an appetizing, yet unhealthy, food.</p><p>Beyond examining valence representations across complex visual scenes and its correspondence across pictures to tastes, we also examined commonality of representations across the brains of individuals. To do so, we extended previous application of cross-participant MVPA in representing object types<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Raizada, R.D. &amp; Connolly, A.C. What makes different people's representations alike: neural similarity space solves the problem of across-subject fMRI decoding. J. Cogn. Neurosci. 24, 868â€“877 (2012)." href="/articles/nn.3749#ref-CR35" id="ref-link-section-d30882912e2435">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Haxby, J.V. et al. A common, high-dimensional model of the representational space in human ventral temporal cortex. Neuron 72, 404â€“416 (2011)." href="/articles/nn.3749#ref-CR36" id="ref-link-section-d30882912e2438">36</a></sup> to the domain of subjective affect. Although item-specific population responses were highly similar in the VTC, affording classification of what particular scene was being viewed, these patterns captured experienced affect across people to a much lesser degree. In contrast, population codes in the OFC were less able to code the specific item being viewed, but demonstrated similarity among people even if affective responses to individual items varied. Cross-participant classification of affect across items was lower in the OFC compared with item-specific coding in the VTC. This cross-region difference may be a characteristic of the neural representations of external items versus internal affective responses: the need to abstract from physical appearance to invisible affect. Notwithstanding, such cross-participant commonality may allow a common scaling of value and valence experience across individuals. In sum, these findings suggest that there exists a common affect code across people, underlying a wide range of stimuli and object categories, and even when originating from the eye or tongue.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Methods</h2><div class="c-article-section__content" id="Sec10-content"><h3 class="c-article__sub-heading" id="Sec11">Subjects and imaging procedures.</h3><p>16 healthy adults (10 male, ages 26.1 Â± 2.1) provided informed consent to participate in the visual (experiment 1) and gustatory (experiment 2) experiments in the same session (that is, without leaving the MRI scanner). Exclusion criteria included significant psychiatric or neurological history. This study was approved by University of Toronto Research Ethics Board and Sick Kids hospital Research Ethics Board. No statistical test was run to determine sample size a priori. The sample sizes that we chose are similar to those used in previous publications<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="McNamee, D., Rangel, A. &amp; O'Doherty, J.P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479â€“485 (2013)." href="/articles/nn.3749#ref-CR17" id="ref-link-section-d30882912e2455">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Raizada, R.D. &amp; Connolly, A.C. What makes different people's representations alike: neural similarity space solves the problem of across-subject fMRI decoding. J. Cogn. Neurosci. 24, 868â€“877 (2012)." href="/articles/nn.3749#ref-CR35" id="ref-link-section-d30882912e2458">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Haxby, J.V. et al. A common, high-dimensional model of the representational space in human ventral temporal cortex. Neuron 72, 404â€“416 (2011)." href="/articles/nn.3749#ref-CR36" id="ref-link-section-d30882912e2461">36</a></sup>. The experiments were conducted using a 3.0-T fMRI system (Siemens Trio) during daytime. Localizer images were first collected to align the field of view centered on each participant's brain. T1-weighted anatomical images were obtained (1 mm<sup>3</sup>, 256 Ã— 256 FOV; MPRAGE sequence) before the experimental echo planar imaging (EPI) runs. For functional imaging, a gradient echo-planar sequence was used (repetition time (TR) = 2,000 ms, echo time (TE) = 27 ms, flip angle = 70 degrees). Each functional run consisted of 292 (experiment 1) or 263 (experiment 2) whole brain acquisitions (40- Ã— 3.5-mm slices, interleaved acquisition, field of view = 192 mm, matrix size = 64 Ã— 64, in-plane resolution of 3 mm). The first four functional images in each run were excluded from analysis to allow for the equilibration of longitudinal magnetization.</p><h3 class="c-article__sub-heading" id="Sec12">Experiment 1 (visual).</h3><p>Visual stimuli were delivered via goggles, using CinemaVision AV system (Resonance Technology), displayed at a resolution of 800 Ã— 600, 60 Hz. Affect ratings were collected by magnet-compatible button during scanning. All 128 pictures were selected from the International Affective Picture System<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Noonan, M.P. et al. Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 107, 20547â€“20552 (2010)." href="/articles/nn.3749#ref-CR49" id="ref-link-section-d30882912e2475">49</a></sup>. In each trial, a picture was presented for 3 s, then a blank screen for 5 s, then separate scaling bars to rate positivity (3 s) and negativity (3 s) of the picture. After a 4-s inter-trial interval, the next picture was presented. Trial order was pseudorandomized within emotion category, balanced across four runs of 32 trials each. Four runs were administered to each subject.</p><h3 class="c-article__sub-heading" id="Sec13">Experiment 2 (gustatory).</h3><p>Gustatory stimuli were delivered by plastic tubes converging at a plastic manifold, whose nozzle dripped the taste solutions into the mouth. 100 taste solution trials were randomized and balanced across five runs. In each trial, 0.5 ml of taste solution was delivered over 1,244 ms. When liquid delivery ended, a screen instructed participants to swallow the liquid (1 s). After 7,756 ms, the same scaling bars from experiment 1 appeared to rate positivity (3 s) and then negativity (3 s) of the liquid. This was followed by 0.5 ml of the tasteless liquid delivery during 1,244 ms for rinsing, followed by the 1-s swallow instruction. After a 7,756 ms inter-trial-interval, the next trial began. Five runs were administered to each subject. Both experiments 1 and 2 were conducted in the same session, which took approximately 2 h. To decrease the need for a bathroom break during scanning, participants were instructed not to drink liquids before the experiment.</p><h3 class="c-article__sub-heading" id="Sec14">Pre-experimental session.</h3><p>To account for individual differences in their subjective experiences of different tastes, participants were asked to taste a wider range of intensities (as measured by molar concentrations) of the different taste solutions (sour, salty, bitter, sweet). In this pre-experimental session, participants were tested for one trial (2 ml) of each of the 16 taste solutions: sour/citric acid (1 Ã— 10<sup>âˆ’1</sup> M, 3.2 Ã— 10<sup>âˆ’2</sup> M, 1.8 Ã— 10<sup>âˆ’2</sup> M and 1.0 Ã— 10<sup>âˆ’2</sup> M), salty/table salt (5.6 Ã— 10<sup>âˆ’1</sup> M, 2.5 Ã— 10<sup>âˆ’1</sup> M, 1.8 Ã— 10<sup>âˆ’1</sup> M and 1.0 Ã— 10<sup>âˆ’1</sup> M), bitter/quinine sulfate (1.0 Ã— 10<sup>âˆ’3</sup> M, 1.8 Ã— 10<sup>âˆ’4</sup> M, 3.2 Ã— 10<sup>âˆ’5</sup> M and 7.8 Ã— 10<sup>âˆ’5</sup> M) and sweet/sucrose (1.0 M, 0.56 M, 0.32 M and 0.18 M). The order of presentation was randomized by taste and then by concentration in each taste. After drinking each solution, participants rinsed and swallowed 5 ml of water, then rated the intensity and pleasantness (valence) of the solution's experience on separate scales of 1â€“9. The concentrations for each taste that matched in intensity were selected. Previous work<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Chapman, H.A., Kim, D.A., Susskind, J.M. &amp; Anderson, A.K. In bad taste: evidence for the oral origins of moral disgust. Science 323, 1222â€“1226 (2009)." href="/articles/nn.3749#ref-CR50" id="ref-link-section-d30882912e2521">50</a></sup> had shown that participants have different rating baselines and the concentrations most reliably selected are above medium self-reported intensity. All solutions were mixed using pharmaceutical grade chemical compounds from Sigma-Aldrich, safe for consumption.</p><h3 class="c-article__sub-heading" id="Sec15">ROI definition.</h3><p>ROIs were determined on the basis of AAL template<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Tzourio-Mazoyer, N. et al. Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. Neuroimage 15, 273â€“289 (2002)." href="/articles/nn.3749#ref-CR51" id="ref-link-section-d30882912e2534">51</a></sup> and anatomy toolbox<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Eickhoff, S.B. et al. A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data. Neuroimage 25, 1325â€“1335 (2005)." href="/articles/nn.3749#ref-CR52" id="ref-link-section-d30882912e2538">52</a></sup>. The EVC ROI was defined by bilateral BA 17 in the anatomy toolbox. The VTC ROI consisted of lingual gyrus, parahippocampal gyrus, fusiform gyrus and inferior temporal cortices in the bilateral hemispheres. The OFC ROI consisted of the superior, middle, inferior and medial OFC in the bilateral hemispheres. White-matter voxels were excluded on the basis of the result of segmentation implemented in SPM8 (<a href="http://www.fil.ion.ucl.ac.uk/spm/">http://www.fil.ion.ucl.ac.uk/spm/</a>), performed on each participant's imaging data.</p><h3 class="c-article__sub-heading" id="Sec16">Data analysis.</h3><p>Data were analyzed using SPM8 software. Functional images were realigned, slice timing corrected, and normalized to the MNI template (ICBM 152) with interpolation to a 2- Ã— 2- Ã— 2-mm space. The registration was performed by matching the whole of the individual's T1 image to the template T1 image (ICBM152), using 12-parameter affine transformation. This was followed by estimating nonlinear deformations, whereby, the deformations are defined by a linear combination of three-dimensional discrete cosine transform basis functions. The same transformation matrix was applied to EPI images. Data was spatially smoothed (full width half maximum = 6 mm) for univariate parametric modulation analysis, but not for MVPA, as it may impair MVPA performance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Kamitani, Y. &amp; Tong, F. Decoding the visual and subjective contents of the human brain. Nat. Neurosci. 8, 679â€“685 (2005)." href="/articles/nn.3749#ref-CR12" id="ref-link-section-d30882912e2557">12</a></sup>. Each stimulus presentation was modeled as a separate event, using the canonical function in SPM8. For the first level GLM analyses, motion regressors were included to regress out motion-related effects. For each voxel, <i>t</i> values of individual trials were demeaned by subtracting the mean value across trials. To visualize the results, xjview software (<a href="http://www.alivelearn.net/xjview8">http://www.alivelearn.net/xjview8</a>) was used.</p><h3 class="c-article__sub-heading" id="Sec17">Representational similarity analysis.</h3><p>For each participant, a vector was created containing the spatial pattern of BOLD-MRI signal related to a particular event (normalized <i>t</i> values per voxel) in each ROI. These <i>t</i> values were further normalized by subtracting mean values across trials. Pairwise Pearson correlations were calculated between all vectors of all single trials, resulting in a RSM containing correlations among all trials for each participant in each ROI for the visual experiment.</p><p>Low-level visual features (local contrast, luminance, hue, number of edges and visual salience) were computed using the Image Processing Toolbox packaged with Matlab 7.0. Local contrast was defined as the s.d. of the pixel intensities. Luminance was calculated as the average log luminance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Reinhard, E.S.M., Shirley, P. &amp; Ferwerda, J. Photographic tone reproduction for digital images. ACM Trans. Graph. 21, 267â€“276 (2002)." href="/articles/nn.3749#ref-CR53" id="ref-link-section-d30882912e2588">53</a></sup>. Hue was calculated using Matlab's rgb2hsv function. Edges were detected using a Canny edge detector with a threshold of 0.5. Lines were detected by using a Hough transform and the number of detected lines was calculated for each image. Visual salience has been defined as those basic visual properties, such as color, intensity and orientation, that preferentially bias competition for rapid, bottom-up attentional selection<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Itti, L. &amp; Koch, C. Computational modelling of visual attention. Nat. Rev. Neurosci. 2, 194â€“203 (2001)." href="/articles/nn.3749#ref-CR54" id="ref-link-section-d30882912e2592">54</a></sup>. Visual saliency map for each image was computed using the Saliency Toolbox<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Walther, D. &amp; Koch, C. Modeling attention to salient proto-objects. Neural Netw. 19, 1395â€“1407 (2006)." href="/articles/nn.3749#ref-CR55" id="ref-link-section-d30882912e2596">55</a></sup>. Saliency maps were transformed into vectors and correlations of these vectors across images were calculated. These correlations represent similarity of saliency maps. Then, all the visual feature values were standardized and compressed into a single representative score for each visual stimulus using principal component analysis. Visual feature scores were sorted into 13 bins for symmetric comparison to valence distance. Distance in visual feature space was estimated by Euclidian distance in five-dimensional visual feature space (local contrast, hue, number of edges, luminance and saliency). Animacy scores were determined by a separate group of participants (<i>n</i> = 16) who judged the stimuli as animate (0/16 to 16/16), which were also sorted into 13 bins. We chose object animacy as a higher order object property because the animate-inanimate dimension has been shown to be one of the most discriminative features for object representation in the VTC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Kriegeskorte, N. et al. Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron 60, 1126â€“1141 (2008)." href="/articles/nn.3749#ref-CR16" id="ref-link-section-d30882912e2603">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Misaki, M., Kim, Y., Bandettini, P.A. &amp; Kriegeskorte, N. Comparison of multivariate classifiers and response normalizations for pattern-information fMRI. Neuroimage 53, 103â€“118 (2010)." href="/articles/nn.3749#ref-CR45" id="ref-link-section-d30882912e2606">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Naselaris, T., Stansbury, D.E. &amp; Gallant, J.L. Cortical representation of animate and inanimate objects in complex natural scenes. J. Physiol. Paris 106, 239â€“249 (2012)." href="/articles/nn.3749#ref-CR56" id="ref-link-section-d30882912e2609">56</a></sup>.</p><p>These RSMs were submitted to MDS for visualization. Stimulus arrangements computed by MDS are data driven and serve an important exploratory function: they can reveal the properties that dominate the representation of our stimuli in the population code without any prior hypotheses. Correlation between projections on the best-fitting axis (line in each MDS plot) and property values (visual feature, animacy and valence).</p><p>To compute the valence representational maps, we took the trial-based RSM (of correlation ranks) and regressed out the other properties, distance in low-level visual features and animacy, as well as regressors of no interest (differences in basic emotions, auto-correlation and sessions). Note that we employed rank-ordered correlations, instead of correlation coefficients, for all the analyses which resulted in little assumptions for distribution of correlation coefficients. This left an RSM of residual correlation ranks predicted by valence distance, which was then sorted according to 13 Ã— 13 bins (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig9">Supplementary Fig. 3b</a>). To calculate the value for each valence <i>m</i> and valence <i>n</i> (Valence<sub><i>m</i>Ã—<i>n</i></sub>) cell of the representational map, we computed the average of [Valence<sub><i>mâ€“</i>1 Ã— <i>n</i></sub>, Valence<sub><i>m</i>+1 Ã— <i>n</i></sub>, Valence<sub><i>m</i> Ã— <i>nâ€“</i>1</sub>, Valence<sub><i>m</i> Ã— <i>n</i>+1</sub> and Valence<sub><i>m</i>Ã—<i>n</i></sub>]. Visual feature and animacy representational maps were computed in an analogous manner. This decomposition approach treats the RSM in each ROI as explained by the linear summation of multiple contributing properties. We took this approach to take into consideration the possibility that the same region may simultaneously represent qualitatively different features (for example, the VTC, which represents not only highly abstract features such as animacy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Kriegeskorte, N. et al. Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron 60, 1126â€“1141 (2008)." href="/articles/nn.3749#ref-CR16" id="ref-link-section-d30882912e2673">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Misaki, M., Kim, Y., Bandettini, P.A. &amp; Kriegeskorte, N. Comparison of multivariate classifiers and response normalizations for pattern-information fMRI. Neuroimage 53, 103â€“118 (2010)." href="/articles/nn.3749#ref-CR45" id="ref-link-section-d30882912e2676">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Naselaris, T., Stansbury, D.E. &amp; Gallant, J.L. Cortical representation of animate and inanimate objects in complex natural scenes. J. Physiol. Paris 106, 239â€“249 (2012)." href="/articles/nn.3749#ref-CR56" id="ref-link-section-d30882912e2679">56</a></sup>, but also low-level visual features<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Op de Beeck, H., Wagemans, J. &amp; Vogels, R. Inferotemporal neurons represent low-dimensional configurations of parameterized shapes. Nat. Neurosci. 4, 1244â€“1252 (2001)." href="/articles/nn.3749#ref-CR57" id="ref-link-section-d30882912e2683">57</a></sup>).</p><p>For statistical analysis of the representational maps, we computed a DCI as a measure of the relationship between activation similarity and distance in each representational property. DCI was computed using a similar GLM regression of the trial-based RSM that included all three predictors, corresponding to distance in visual features, animacy and valence, and including regressors of no interest (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig9">Supplementary Fig. 3a,b</a>). These regression coefficients represent the extent that RSMs were predicted by the distance in each of the three properties, and were thus termed distance-similarity index (final DCIs were calculated by multiplying GLM regression coefficients by âˆ’1). DCIs for each property in each ROI was computed for each participant, then submitted to statistical analysis. All the DCI analyses used one-sided tests, since negative DCIs do not make sense while other analyses used two-sided tests. This was validated retrospectively, as we did not observe any significant voxels in the searchlight analysis.</p><p>To examine cross-modal commonality of OFC affect representation, similar GLM regressions were performed using either trial correlations in the gustatory experiment or trial correlations across visual and gustatory experiments with responses and valence distance as predictor. Regressors of no interest coded differences in basic emotions, tastes, auto-correlation and sessions. To directly illustrate the decrease in similarity with valence distance, we sorted the rank-ordered correlations into five bins of valence distance for each participant (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig6">Fig. 6a,b</a>). One participant was excluded from these gustatory and gustatory Ã— visual analyses due to the lack of data for the fifth bin of valence distance in gustatory experiment. However, this participant was included in all the other analyses including DCI analyses.</p><h3 class="c-article__sub-heading" id="Sec18">Searchlight analysis.</h3><p>For information-based searchlight analyses, we used a (5 Ã— 5 Ã— 5 voxels) searchlight. In a given cube, correlation coefficients of activation patterns of each trial combination (128 Ã— 127/2) were calculated and subject to GLM analysis with correlations as the responses and differences in visual features, animacy and valence scores as predictors (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig5">Fig. 5a,b</a>). Searchlight analysis of individual visual features (for example, local contrast, hue, number of edges) used distances of a single feature as predictors (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM3">Supplementary Table 3</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig11">Supplementary Fig. 5</a>). Individual participants' data were spatially smoothed (8-mm full width half maximum) and were subject to a random effects group analysis.</p><p>Searchlight analyses examining modality-specific and supramodal valence information was conducted on within-gustatory and across-visual-and-gustatory data. In a given cube, correlation coefficients of activation patterns of each trial combination (128 Ã— 127/2 for visual, 100 Ã— 99/2 for gustatory, and 128 Ã— 100 across visual and gustatory) were calculated and subject to GLM analysis with correlations as the responses and differences in valence as predictors. Based on these three searchlight results (within visual, within gustatory, and across visual and gustatory), we explored brain regions representing visual-specific (<i>P</i> &lt; 0.001 uncorrected and FDR â‰¤ 0.05 for visual, <i>P</i> &gt; 0.05 for gustatory and across visual and gustatory), gustatory-specific (<i>P</i> &lt; 0.001 uncorrected and FDR â‰¤ 0.05 for gustatory, <i>P</i> &gt; 0.05 for visual and across visual and gustatory) and modality-independent valence (<i>P</i> &lt; 0.01 uncorrected for all the three conditions and cleared a threshold of <i>P</i> &lt; 0.05 (familywise error (FWE)) when assuming independence of these 3 conditions).</p><h3 class="c-article__sub-heading" id="Sec19">Cross-participant classification.</h3><p>We examined cross-participant commonality of visual items by comparing each participant's trial-based RSM to a trial-based RSM estimated by averaging across all other participants' RSM. Thus, each target picture was represented by 127 values that related it to all other picture trials. We then compared whether the target picture representation was more similar to its estimate than all other picture representations (similarity was computed as the correlation of the <i>r</i> values; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig12">Supplementary Fig. 6</a>). Classification performance was calculated as the percentage success of all pairwise comparisons (50% chance).</p><p>For cross-participant commonality of affect representations of visual items, we used a similar leave-one-out procedure, except that a target picture's 127-score relationship to other pictures was now treated as 127 scores related in valence space. Taking an example, consider target picture <i>j</i>, which was rated as positive = 5, negative = 1 for one participant. The first of picture <i>j</i>'s 127 scores, <i>r</i><sub>(j,1)</sub>, relates it to picture 1, but because we are interested in valence, this score cannot be directly compared to the same <i>r</i><sub>(j,1)</sub> score in another participant, as that participant's valence ratings to the same two pictures are different. Thus, to estimate the valence representation of picture <i>j</i> using other participants' data directly, we computed valence-based RSMs for both positivity and negativity, in which effects of no interest were regressed out. That is, the remaining participants' trial-based RSMs were first submitted to GLM decomposition to regress out effects of no interest, and then organized by their positive and negative valence scores, then separately combined into 7 Ã— 7 positive and 7 Ã— 7 negative valence RSMs, where each (<i>m</i>, <i>n</i>) cell was computed as the average of the cells: [(<i>m</i> âˆ’ 1, <i>n</i>), (<i>m</i> + 1, <i>n</i>), (<i>m</i>, <i>n</i> âˆ’ 1), (<i>m</i>, <i>n</i> + 1), and (<i>m</i>, <i>n</i>)]. The classification of picture <i>j</i>'s valence was then tested by looking up the 127 scores in the valence RSMs corresponding to the valence mapping. If the correlation of these scores was higher for picture <i>j</i>'s valence than another picture <i>k</i>'s valence, the classification was successful (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig13">Supplementary Fig. 7</a>). Classification performance was calculated as the percentage success of all pairwise comparisons (50% chance). Given that the across-participant MVPA that we employed cannot discriminate trials with the same valence, classification accuracies for the closest distance were always 50%.</p><p>For commonality of valence representations for the gustatory experiment (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig6">Fig. 6b</a>), we applied the same procedure as above on the gustatory Ã— gustatory similarity scores and their valence ratings. We further investigated the cross-modality commonality of the OFC affective representations by testing whether affect representations in the visual experiment can be predicted by other participants' affect representations in the gustatory experiment (visual Ã— gustatory) or vice versa (gustatory Ã— visual) (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig6">Fig. 6b</a>).</p><h3 class="c-article__sub-heading" id="Sec20">Calculation of arousal.</h3><p>Following previously described methods<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Kron, A., Goldstein, A., Lee, D.H., Gardhouse, K. &amp; Anderson, A.K. How are you feeling? Revisiting the quantification of emotional qualia. Psychol. Sci. 24, 1503â€“1511 (2013)." href="/articles/nn.3749#ref-CR37" id="ref-link-section-d30882912e2841">37</a></sup>, self-reported and autonomic indices of arousal can be estimated through the addition of independent unipolar positive and negative valence responses. Valence categories were defined from the distribution in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig7">Supplementary Figure 1</a> (negative âˆ’6 to âˆ’2, neutral âˆ’1 to 1, positive = 2 to 6). According to these definitions, positive (mean = 5.1, s.d. = 1.4) and negative (mean = 4.8, s.d. = 1.4) stimuli were similarly arousing compared to neutral (mean = 2.5, s.d. = 1.7) in experiment 1. Similar arousal values were obtained in experiment 2 (positive (mean = 5.4, s.d. = 1.5), negative (mean = 5.3, s.d. = 1.6) and neutral (mean = 2.5, s.d. = 1.5).</p><h3 class="c-article__sub-heading" id="Sec21">Statistics.</h3><p>We analyzed the data, assuming normal distribution. To examine whether DCIs are significantly above zero, we used one sample <i>t</i> test. To examine difference in DCIs, we used paired <i>t</i> test. A Shapiro-Wilk test was applied to examine whether samples had a normal distribution. In case of a non-normal distribution, a nonparametric test (Wilcoxon signed-rank test) was applied to confirm whether similar results were obtained. For ANOVA, we also examined sphericity by Mauchly's test. Where the assumption of sphericity was violated, we applied Greenhouse-Geisser correction. Multiple comparison corrections were applied to within-ROI and between-ROIs analyses, using Bonferroni correction. For <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig3">Figure 3c</a>, multiple comparison correction was applied to within-ROI (3 (feature) Ã— 3 (ROI) = 9) and between-ROI (3 (feature) Ã— 3 (ROI-pair) = 9) comparisons. For <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig4">Figure 4b</a>, multiple comparison correction was applied based on within-ROI (3 (feature) Ã— 4 (ROI) = 12) and between-ROIs (3 (feature) Ã— 6 (ROI-pair) = 18) comparisons. For <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig5">Figure 5d</a>, further multiple comparison correction was not applied, as the data survived whole brain multiple comparison.</p><p>A <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3749#MOESM4">Supplementary Methods Checklist</a> is available.</p></div></div></section>
                </div>
            

            <div>
                <div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1"><p class="c-article-references__text" id="ref-CR1">Wundt, W. <i>Grundriss der Psychologie, von Wilhelm Wundt</i> (W. Engelmann, Leipzig, 1897).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2"><p class="c-article-references__text" id="ref-CR2">Penfield, W. &amp; Boldrey, E. Somatic motor and sensory representation in the cerebral cortex of man as studies by electrical stimulation. <i>Brain</i> <b>60</b>, 389â€“443 (1937).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/brain/60.4.389" data-track-action="article reference" href="https://doi.org/10.1093%2Fbrain%2F60.4.389" aria-label="Article reference 2" data-doi="10.1093/brain/60.4.389">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Somatic%20motor%20and%20sensory%20representation%20in%20the%20cerebral%20cortex%20of%20man%20as%20studies%20by%20electrical%20stimulation&amp;journal=Brain&amp;doi=10.1093%2Fbrain%2F60.4.389&amp;volume=60&amp;pages=389-443&amp;publication_year=1937&amp;author=Penfield%2CW&amp;author=Boldrey%2CE">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3"><p class="c-article-references__text" id="ref-CR3">Huth, A.G., Nishimoto, S., Vu, A.T. &amp; Gallant, J.L. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. <i>Neuron</i> <b>76</b>, 1210â€“1224 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2012.10.014" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2012.10.014" aria-label="Article reference 3" data-doi="10.1016/j.neuron.2012.10.014">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhvVKqu77F" aria-label="CAS reference 3">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20continuous%20semantic%20space%20describes%20the%20representation%20of%20thousands%20of%20object%20and%20action%20categories%20across%20the%20human%20brain&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2012.10.014&amp;volume=76&amp;pages=1210-1224&amp;publication_year=2012&amp;author=Huth%2CAG&amp;author=Nishimoto%2CS&amp;author=Vu%2CAT&amp;author=Gallant%2CJL">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4"><p class="c-article-references__text" id="ref-CR4">Haxby, J.V. et al. Distributed and overlapping representations of faces and objects in ventral temporal cortex. <i>Science</i> <b>293</b>, 2425â€“2430 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1063736" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1063736" aria-label="Article reference 4" data-doi="10.1126/science.1063736">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3MXntlGqsbs%3D" aria-label="CAS reference 4">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Distributed%20and%20overlapping%20representations%20of%20faces%20and%20objects%20in%20ventral%20temporal%20cortex&amp;journal=Science&amp;doi=10.1126%2Fscience.1063736&amp;volume=293&amp;pages=2425-2430&amp;publication_year=2001&amp;author=Haxby%2CJV">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5"><p class="c-article-references__text" id="ref-CR5">Kanwisher, N., McDermott, J. &amp; Chun, M.M. The fusiform face area: a module in human extrastriate cortex specialized for face perception. <i>J. Neurosci.</i> <b>17</b>, 4302â€“4311 (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.17-11-04302.1997" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.17-11-04302.1997" aria-label="Article reference 5" data-doi="10.1523/JNEUROSCI.17-11-04302.1997">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK2sXjtl2mur0%3D" aria-label="CAS reference 5">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20fusiform%20face%20area%3A%20a%20module%20in%20human%20extrastriate%20cortex%20specialized%20for%20face%20perception&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.17-11-04302.1997&amp;volume=17&amp;pages=4302-4311&amp;publication_year=1997&amp;author=Kanwisher%2CN&amp;author=McDermott%2CJ&amp;author=Chun%2CMM">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6"><p class="c-article-references__text" id="ref-CR6">Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. <i>Front. Syst. Neurosci.</i> <b>2</b>, 4 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3389/neuro.01.016.2008" data-track-action="article reference" href="https://doi.org/10.3389%2Fneuro.01.016.2008" aria-label="Article reference 6" data-doi="10.3389/neuro.01.016.2008">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Representational%20similarity%20analysis%20-%20connecting%20the%20branches%20of%20systems%20neuroscience&amp;journal=Front.%20Syst.%20Neurosci.&amp;doi=10.3389%2Fneuro.01.016.2008&amp;volume=2&amp;publication_year=2008&amp;author=Kriegeskorte%2CN&amp;author=Mur%2CM&amp;author=Bandettini%2CP">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7"><p class="c-article-references__text" id="ref-CR7">Hinton, G.E., McClelland, J.L. &amp; Rumelhart, D.E. Distributed representations. in <i>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</i> (eds. Rumelhart, D.E. &amp; McClelland, J.L.) 77â€“109 (The MIT Press, Cambridge, Massachusetts, 1986).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8"><p class="c-article-references__text" id="ref-CR8">Lewis, P.A., Critchley, H.D., Rotshtein, P. &amp; Dolan, R.J. Neural correlates of processing valence and arousal in affective words. <i>Cereb. Cortex</i> <b>17</b>, 742â€“748 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/bhk024" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhk024" aria-label="Article reference 8" data-doi="10.1093/cercor/bhk024">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD2s%2FovFOmsg%3D%3D" aria-label="CAS reference 8">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20correlates%20of%20processing%20valence%20and%20arousal%20in%20affective%20words&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhk024&amp;volume=17&amp;pages=742-748&amp;publication_year=2007&amp;author=Lewis%2CPA&amp;author=Critchley%2CHD&amp;author=Rotshtein%2CP&amp;author=Dolan%2CRJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9"><p class="c-article-references__text" id="ref-CR9">Padoa-Schioppa, C. &amp; Assad, J.A. Neurons in the orbitofrontal cortex encode economic value. <i>Nature</i> <b>441</b>, 223â€“226 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature04676" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature04676" aria-label="Article reference 9" data-doi="10.1038/nature04676">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28XksVGnsrk%3D" aria-label="CAS reference 9">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Neurons%20in%20the%20orbitofrontal%20cortex%20encode%20economic%20value&amp;journal=Nature&amp;doi=10.1038%2Fnature04676&amp;volume=441&amp;pages=223-226&amp;publication_year=2006&amp;author=Padoa-Schioppa%2CC&amp;author=Assad%2CJA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10"><p class="c-article-references__text" id="ref-CR10">Kriegeskorte, N. &amp; Kievit, R.A. Representational geometry: integrating cognition, computation and the brain. <i>Trends Cogn. Sci.</i> <b>17</b>, 401â€“412 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2013.06.007" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2013.06.007" aria-label="Article reference 10" data-doi="10.1016/j.tics.2013.06.007">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Representational%20geometry%3A%20integrating%20cognition%2C%20computation%20and%20the%20brain&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2013.06.007&amp;volume=17&amp;pages=401-412&amp;publication_year=2013&amp;author=Kriegeskorte%2CN&amp;author=Kievit%2CRA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11"><p class="c-article-references__text" id="ref-CR11">Haynes, J.D. Decoding and predicting intentions. <i>Ann. NY Acad. Sci.</i> <b>1224</b>, 9â€“21 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1111/j.1749-6632.2011.05994.x" data-track-action="article reference" href="https://doi.org/10.1111%2Fj.1749-6632.2011.05994.x" aria-label="Article reference 11" data-doi="10.1111/j.1749-6632.2011.05994.x">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20and%20predicting%20intentions&amp;journal=Ann.%20NY%20Acad.%20Sci.&amp;doi=10.1111%2Fj.1749-6632.2011.05994.x&amp;volume=1224&amp;pages=9-21&amp;publication_year=2011&amp;author=Haynes%2CJD">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12"><p class="c-article-references__text" id="ref-CR12">Kamitani, Y. &amp; Tong, F. Decoding the visual and subjective contents of the human brain. <i>Nat. Neurosci.</i> <b>8</b>, 679â€“685 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn1444" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn1444" aria-label="Article reference 12" data-doi="10.1038/nn1444">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2MXjsFyktLo%3D" aria-label="CAS reference 12">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20the%20visual%20and%20subjective%20contents%20of%20the%20human%20brain&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn1444&amp;volume=8&amp;pages=679-685&amp;publication_year=2005&amp;author=Kamitani%2CY&amp;author=Tong%2CF">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13"><p class="c-article-references__text" id="ref-CR13">Freeman, J., Brouwer, G.J., Heeger, D.J. &amp; Merriam, E.P. Orientation decoding depends on maps, not columns. <i>J. Neurosci.</i> <b>31</b>, 4792â€“4804 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.5160-10.2011" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.5160-10.2011" aria-label="Article reference 13" data-doi="10.1523/JNEUROSCI.5160-10.2011">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXkslSlt7g%3D" aria-label="CAS reference 13">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Orientation%20decoding%20depends%20on%20maps%2C%20not%20columns&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.5160-10.2011&amp;volume=31&amp;pages=4792-4804&amp;publication_year=2011&amp;author=Freeman%2CJ&amp;author=Brouwer%2CGJ&amp;author=Heeger%2CDJ&amp;author=Merriam%2CEP">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14"><p class="c-article-references__text" id="ref-CR14">Sasaki, Y. et al. The radial bias: a different slant on visual orientation sensitivity in human and nonhuman primates. <i>Neuron</i> <b>51</b>, 661â€“670 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2006.07.021" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2006.07.021" aria-label="Article reference 14" data-doi="10.1016/j.neuron.2006.07.021">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28XhtVWntLrJ" aria-label="CAS reference 14">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20radial%20bias%3A%20a%20different%20slant%20on%20visual%20orientation%20sensitivity%20in%20human%20and%20nonhuman%20primates&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2006.07.021&amp;volume=51&amp;pages=661-670&amp;publication_year=2006&amp;author=Sasaki%2CY">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15"><p class="c-article-references__text" id="ref-CR15">Alink, A., Krugliak, A., Walther, A. &amp; Kriegeskorte, N. fMRI orientation decoding in V1 does not require global maps or globally coherent orientation stimuli. <i>Front. Psychol.</i> <b>4</b>, 493 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3389/fpsyg.2013.00493" data-track-action="article reference" href="https://doi.org/10.3389%2Ffpsyg.2013.00493" aria-label="Article reference 15" data-doi="10.3389/fpsyg.2013.00493">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=fMRI%20orientation%20decoding%20in%20V1%20does%20not%20require%20global%20maps%20or%20globally%20coherent%20orientation%20stimuli&amp;journal=Front.%20Psychol.&amp;doi=10.3389%2Ffpsyg.2013.00493&amp;volume=4&amp;publication_year=2013&amp;author=Alink%2CA&amp;author=Krugliak%2CA&amp;author=Walther%2CA&amp;author=Kriegeskorte%2CN">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16"><p class="c-article-references__text" id="ref-CR16">Kriegeskorte, N. et al. Matching categorical object representations in inferior temporal cortex of man and monkey. <i>Neuron</i> <b>60</b>, 1126â€“1141 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2008.10.043" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2008.10.043" aria-label="Article reference 16" data-doi="10.1016/j.neuron.2008.10.043">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXlsVKjtA%3D%3D" aria-label="CAS reference 16">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Matching%20categorical%20object%20representations%20in%20inferior%20temporal%20cortex%20of%20man%20and%20monkey&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2008.10.043&amp;volume=60&amp;pages=1126-1141&amp;publication_year=2008&amp;author=Kriegeskorte%2CN">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17"><p class="c-article-references__text" id="ref-CR17">McNamee, D., Rangel, A. &amp; O'Doherty, J.P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. <i>Nat. Neurosci.</i> <b>16</b>, 479â€“485 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.3337" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.3337" aria-label="Article reference 17" data-doi="10.1038/nn.3337">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXis1ahtLg%3D" aria-label="CAS reference 17">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Category-dependent%20and%20category-independent%20goal-value%20codes%20in%20human%20ventromedial%20prefrontal%20cortex&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.3337&amp;volume=16&amp;pages=479-485&amp;publication_year=2013&amp;author=McNamee%2CD&amp;author=Rangel%2CA&amp;author=O%27Doherty%2CJP">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18"><p class="c-article-references__text" id="ref-CR18">Brouwer, G.J. &amp; Heeger, D.J. Decoding and reconstructing color from responses in human visual cortex. <i>J. Neurosci.</i> <b>29</b>, 13992â€“14003 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.3577-09.2009" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.3577-09.2009" aria-label="Article reference 18" data-doi="10.1523/JNEUROSCI.3577-09.2009">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXhsVSmsbnK" aria-label="CAS reference 18">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20and%20reconstructing%20color%20from%20responses%20in%20human%20visual%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.3577-09.2009&amp;volume=29&amp;pages=13992-14003&amp;publication_year=2009&amp;author=Brouwer%2CGJ&amp;author=Heeger%2CDJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19"><p class="c-article-references__text" id="ref-CR19">Osgood, C.E., May, W.H. &amp; Miron, M.S. <i>Cross-Cultural Universals of Affective Meaning</i> (University of Illinois Press, Urbana, Illinois, 1975).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20"><p class="c-article-references__text" id="ref-CR20">Russell, J.A. A circumplex model of affect. <i>J. Pers. Soc. Psychol.</i> <b>39</b>, 1161â€“1178 (1980).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/h0077714" data-track-action="article reference" href="https://doi.org/10.1037%2Fh0077714" aria-label="Article reference 20" data-doi="10.1037/h0077714">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20circumplex%20model%20of%20affect&amp;journal=J.%20Pers.%20Soc.%20Psychol.&amp;doi=10.1037%2Fh0077714&amp;volume=39&amp;pages=1161-1178&amp;publication_year=1980&amp;author=Russell%2CJA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21"><p class="c-article-references__text" id="ref-CR21">Grill-Spector, K. &amp; Malach, R. The human visual cortex. <i>Annu. Rev. Neurosci.</i> <b>27</b>, 649â€“677 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1146/annurev.neuro.27.070203.144220" data-track-action="article reference" href="https://doi.org/10.1146%2Fannurev.neuro.27.070203.144220" aria-label="Article reference 21" data-doi="10.1146/annurev.neuro.27.070203.144220">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2cXmslantLc%3D" aria-label="CAS reference 21">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20human%20visual%20cortex&amp;journal=Annu.%20Rev.%20Neurosci.&amp;doi=10.1146%2Fannurev.neuro.27.070203.144220&amp;volume=27&amp;pages=649-677&amp;publication_year=2004&amp;author=Grill-Spector%2CK&amp;author=Malach%2CR">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22"><p class="c-article-references__text" id="ref-CR22">Anderson, A.K. et al. Dissociated neural representations of intensity and valence in human olfaction. <i>Nat. Neurosci.</i> <b>6</b>, 196â€“202 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn1001" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn1001" aria-label="Article reference 22" data-doi="10.1038/nn1001">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3sXmsFGgtQ%3D%3D" aria-label="CAS reference 22">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissociated%20neural%20representations%20of%20intensity%20and%20valence%20in%20human%20olfaction&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn1001&amp;volume=6&amp;pages=196-202&amp;publication_year=2003&amp;author=Anderson%2CAK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23"><p class="c-article-references__text" id="ref-CR23">O'Doherty, J., Kringelbach, M.L., Rolls, E.T., Hornak, J. &amp; Andrews, C. Abstract reward and punishment representations in the human orbitofrontal cortex. <i>Nat. Neurosci.</i> <b>4</b>, 95â€“102 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/82959" data-track-action="article reference" href="https://doi.org/10.1038%2F82959" aria-label="Article reference 23" data-doi="10.1038/82959">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3MXitVensA%3D%3D" aria-label="CAS reference 23">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Abstract%20reward%20and%20punishment%20representations%20in%20the%20human%20orbitofrontal%20cortex&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2F82959&amp;volume=4&amp;pages=95-102&amp;publication_year=2001&amp;author=O%27Doherty%2CJ&amp;author=Kringelbach%2CML&amp;author=Rolls%2CET&amp;author=Hornak%2CJ&amp;author=Andrews%2CC">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24"><p class="c-article-references__text" id="ref-CR24">Small, D.M. et al. Dissociation of neural representation of intensity and affective valuation in human gustation. <i>Neuron</i> <b>39</b>, 701â€“711 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0896-6273(03)00467-7" data-track-action="article reference" href="https://doi.org/10.1016%2FS0896-6273%2803%2900467-7" aria-label="Article reference 24" data-doi="10.1016/S0896-6273(03)00467-7">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3sXmslOmu7g%3D" aria-label="CAS reference 24">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissociation%20of%20neural%20representation%20of%20intensity%20and%20affective%20valuation%20in%20human%20gustation&amp;journal=Neuron&amp;doi=10.1016%2FS0896-6273%2803%2900467-7&amp;volume=39&amp;pages=701-711&amp;publication_year=2003&amp;author=Small%2CDM">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25"><p class="c-article-references__text" id="ref-CR25">OngÃ¼r, D. &amp; Price, J.L. The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans. <i>Cereb. Cortex</i> <b>10</b>, 206â€“219 (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/10.3.206" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2F10.3.206" aria-label="Article reference 25" data-doi="10.1093/cercor/10.3.206">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20organization%20of%20networks%20within%20the%20orbital%20and%20medial%20prefrontal%20cortex%20of%20rats%2C%20monkeys%20and%20humans&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2F10.3.206&amp;volume=10&amp;pages=206-219&amp;publication_year=2000&amp;author=Ong%C3%BCr%2CD&amp;author=Price%2CJL">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26"><p class="c-article-references__text" id="ref-CR26">Shenhav, A., Barrett, L.F. &amp; Bar, M. Affective value and associative processing share a cortical substrate. <i>Cogn. Affect. Behav. Neurosci.</i> <b>13</b>, 46â€“59 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3758/s13415-012-0128-4" data-track-action="article reference" href="https://doi.org/10.3758%2Fs13415-012-0128-4" aria-label="Article reference 26" data-doi="10.3758/s13415-012-0128-4">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Affective%20value%20and%20associative%20processing%20share%20a%20cortical%20substrate&amp;journal=Cogn.%20Affect.%20Behav.%20Neurosci.&amp;doi=10.3758%2Fs13415-012-0128-4&amp;volume=13&amp;pages=46-59&amp;publication_year=2013&amp;author=Shenhav%2CA&amp;author=Barrett%2CLF&amp;author=Bar%2CM">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27"><p class="c-article-references__text" id="ref-CR27">Gottfried, J.A., O'Doherty, J. &amp; Dolan, R.J. Appetitive and aversive olfactory learning in humans studied using event-related functional magnetic resonance imaging. <i>J. Neurosci.</i> <b>22</b>, 10829â€“10837 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.22-24-10829.2002" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.22-24-10829.2002" aria-label="Article reference 27" data-doi="10.1523/JNEUROSCI.22-24-10829.2002">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD38XpslCqt7o%3D" aria-label="CAS reference 27">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Appetitive%20and%20aversive%20olfactory%20learning%20in%20humans%20studied%20using%20event-related%20functional%20magnetic%20resonance%20imaging&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.22-24-10829.2002&amp;volume=22&amp;pages=10829-10837&amp;publication_year=2002&amp;author=Gottfried%2CJA&amp;author=O%27Doherty%2CJ&amp;author=Dolan%2CRJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28"><p class="c-article-references__text" id="ref-CR28">Rolls, E.T., Kringelbach, M.L. &amp; de Araujo, I.E. Different representations of pleasant and unpleasant odours in the human brain. <i>Eur. J. Neurosci.</i> <b>18</b>, 695â€“703 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1046/j.1460-9568.2003.02779.x" data-track-action="article reference" href="https://doi.org/10.1046%2Fj.1460-9568.2003.02779.x" aria-label="Article reference 28" data-doi="10.1046/j.1460-9568.2003.02779.x">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Different%20representations%20of%20pleasant%20and%20unpleasant%20odours%20in%20the%20human%20brain&amp;journal=Eur.%20J.%20Neurosci.&amp;doi=10.1046%2Fj.1460-9568.2003.02779.x&amp;volume=18&amp;pages=695-703&amp;publication_year=2003&amp;author=Rolls%2CET&amp;author=Kringelbach%2CML&amp;author=de%20Araujo%2CIE">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29"><p class="c-article-references__text" id="ref-CR29">Montague, P.R. &amp; Berns, G.S. Neural economics and the biological substrates of valuation. <i>Neuron</i> <b>36</b>, 265â€“284 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0896-6273(02)00974-1" data-track-action="article reference" href="https://doi.org/10.1016%2FS0896-6273%2802%2900974-1" aria-label="Article reference 29" data-doi="10.1016/S0896-6273(02)00974-1">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD38XotlansLs%3D" aria-label="CAS reference 29">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20economics%20and%20the%20biological%20substrates%20of%20valuation&amp;journal=Neuron&amp;doi=10.1016%2FS0896-6273%2802%2900974-1&amp;volume=36&amp;pages=265-284&amp;publication_year=2002&amp;author=Montague%2CPR&amp;author=Berns%2CGS">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30"><p class="c-article-references__text" id="ref-CR30">Wilson-Mendenhall, C.D., Barrett, L.F. &amp; Barsalou, L.W. Neural evidence that human emotions share core affective properties. <i>Psychol. Sci.</i> <b>24</b>, 947â€“956 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1177/0956797612464242" data-track-action="article reference" href="https://doi.org/10.1177%2F0956797612464242" aria-label="Article reference 30" data-doi="10.1177/0956797612464242">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20evidence%20that%20human%20emotions%20share%20core%20affective%20properties&amp;journal=Psychol.%20Sci.&amp;doi=10.1177%2F0956797612464242&amp;volume=24&amp;pages=947-956&amp;publication_year=2013&amp;author=Wilson-Mendenhall%2CCD&amp;author=Barrett%2CLF&amp;author=Barsalou%2CLW">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31"><p class="c-article-references__text" id="ref-CR31">Lindquist, K.A., Wager, T.D., Kober, H., Bliss-Moreau, E. &amp; Barrett, L.F. The brain basis of emotion: a meta-analytic review. <i>Behav. Brain Sci.</i> <b>35</b>, 121â€“143 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1017/S0140525X11000446" data-track-action="article reference" href="https://doi.org/10.1017%2FS0140525X11000446" aria-label="Article reference 31" data-doi="10.1017/S0140525X11000446">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20brain%20basis%20of%20emotion%3A%20a%20meta-analytic%20review&amp;journal=Behav.%20Brain%20Sci.&amp;doi=10.1017%2FS0140525X11000446&amp;volume=35&amp;pages=121-143&amp;publication_year=2012&amp;author=Lindquist%2CKA&amp;author=Wager%2CTD&amp;author=Kober%2CH&amp;author=Bliss-Moreau%2CE&amp;author=Barrett%2CLF">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32"><p class="c-article-references__text" id="ref-CR32">Morrison, S.E. &amp; Salzman, C.D. The convergence of information about rewarding and aversive stimuli in single neurons. <i>J. Neurosci.</i> <b>29</b>, 11471â€“11483 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.1815-09.2009" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.1815-09.2009" aria-label="Article reference 32" data-doi="10.1523/JNEUROSCI.1815-09.2009">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXht1SrtbbK" aria-label="CAS reference 32">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20convergence%20of%20information%20about%20rewarding%20and%20aversive%20stimuli%20in%20single%20neurons&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.1815-09.2009&amp;volume=29&amp;pages=11471-11483&amp;publication_year=2009&amp;author=Morrison%2CSE&amp;author=Salzman%2CCD">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33"><p class="c-article-references__text" id="ref-CR33">Todd, R.M., Talmi, D., Schmitz, T.W., Susskind, J. &amp; Anderson, A.K. Psychophysical and neural evidence for emotion-enhanced perceptual vividness. <i>J. Neurosci.</i> <b>32</b>, 11201â€“11212 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.0155-12.2012" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.0155-12.2012" aria-label="Article reference 33" data-doi="10.1523/JNEUROSCI.0155-12.2012">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38Xht1Ors7%2FL" aria-label="CAS reference 33">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Psychophysical%20and%20neural%20evidence%20for%20emotion-enhanced%20perceptual%20vividness&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.0155-12.2012&amp;volume=32&amp;pages=11201-11212&amp;publication_year=2012&amp;author=Todd%2CRM&amp;author=Talmi%2CD&amp;author=Schmitz%2CTW&amp;author=Susskind%2CJ&amp;author=Anderson%2CAK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34"><p class="c-article-references__text" id="ref-CR34">Grabenhorst, F., D'Souza, A.A., Parris, B.A., Rolls, E.T. &amp; Passingham, R.E. A common neural scale for the subjective pleasantness of different primary rewards. <i>Neuroimage</i> <b>51</b>, 1265â€“1274 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2010.03.043" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2010.03.043" aria-label="Article reference 34" data-doi="10.1016/j.neuroimage.2010.03.043">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20common%20neural%20scale%20for%20the%20subjective%20pleasantness%20of%20different%20primary%20rewards&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2010.03.043&amp;volume=51&amp;pages=1265-1274&amp;publication_year=2010&amp;author=Grabenhorst%2CF&amp;author=D%27Souza%2CAA&amp;author=Parris%2CBA&amp;author=Rolls%2CET&amp;author=Passingham%2CRE">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35"><p class="c-article-references__text" id="ref-CR35">Raizada, R.D. &amp; Connolly, A.C. What makes different people's representations alike: neural similarity space solves the problem of across-subject fMRI decoding. <i>J. Cogn. Neurosci.</i> <b>24</b>, 868â€“877 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1162/jocn_a_00189" data-track-action="article reference" href="https://doi.org/10.1162%2Fjocn_a_00189" aria-label="Article reference 35" data-doi="10.1162/jocn_a_00189">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20makes%20different%20people%27s%20representations%20alike%3A%20neural%20similarity%20space%20solves%20the%20problem%20of%20across-subject%20fMRI%20decoding&amp;journal=J.%20Cogn.%20Neurosci.&amp;doi=10.1162%2Fjocn_a_00189&amp;volume=24&amp;pages=868-877&amp;publication_year=2012&amp;author=Raizada%2CRD&amp;author=Connolly%2CAC">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36"><p class="c-article-references__text" id="ref-CR36">Haxby, J.V. et al. A common, high-dimensional model of the representational space in human ventral temporal cortex. <i>Neuron</i> <b>72</b>, 404â€“416 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2011.08.026" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2011.08.026" aria-label="Article reference 36" data-doi="10.1016/j.neuron.2011.08.026">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXhtlKrtLzJ" aria-label="CAS reference 36">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20common%2C%20high-dimensional%20model%20of%20the%20representational%20space%20in%20human%20ventral%20temporal%20cortex&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2011.08.026&amp;volume=72&amp;pages=404-416&amp;publication_year=2011&amp;author=Haxby%2CJV">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37"><p class="c-article-references__text" id="ref-CR37">Kron, A., Goldstein, A., Lee, D.H., Gardhouse, K. &amp; Anderson, A.K. How are you feeling? Revisiting the quantification of emotional qualia. <i>Psychol. Sci.</i> <b>24</b>, 1503â€“1511 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1177/0956797613475456" data-track-action="article reference" href="https://doi.org/10.1177%2F0956797613475456" aria-label="Article reference 37" data-doi="10.1177/0956797613475456">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20are%20you%20feeling%3F%20Revisiting%20the%20quantification%20of%20emotional%20qualia&amp;journal=Psychol.%20Sci.&amp;doi=10.1177%2F0956797613475456&amp;volume=24&amp;pages=1503-1511&amp;publication_year=2013&amp;author=Kron%2CA&amp;author=Goldstein%2CA&amp;author=Lee%2CDH&amp;author=Gardhouse%2CK&amp;author=Anderson%2CAK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38"><p class="c-article-references__text" id="ref-CR38">Kriegeskorte, N., Goebel, R. &amp; Bandettini, P. Information-based functional brain mapping. <i>Proc. Natl. Acad. Sci. USA</i> <b>103</b>, 3863â€“3868 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.0600244103" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.0600244103" aria-label="Article reference 38" data-doi="10.1073/pnas.0600244103">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28XivFWisbw%3D" aria-label="CAS reference 38">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Information-based%20functional%20brain%20mapping&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.0600244103&amp;volume=103&amp;pages=3863-3868&amp;publication_year=2006&amp;author=Kriegeskorte%2CN&amp;author=Goebel%2CR&amp;author=Bandettini%2CP">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39"><p class="c-article-references__text" id="ref-CR39">Dolcos, F., LaBar, K.S. &amp; Cabeza, R. Dissociable effects of arousal and valence on prefrontal activity indexing emotional evaluation and subsequent memory: an event-related fMRI study. <i>Neuroimage</i> <b>23</b>, 64â€“74 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2004.05.015" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2004.05.015" aria-label="Article reference 39" data-doi="10.1016/j.neuroimage.2004.05.015">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissociable%20effects%20of%20arousal%20and%20valence%20on%20prefrontal%20activity%20indexing%20emotional%20evaluation%20and%20subsequent%20memory%3A%20an%20event-related%20fMRI%20study&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2004.05.015&amp;volume=23&amp;pages=64-74&amp;publication_year=2004&amp;author=Dolcos%2CF&amp;author=LaBar%2CKS&amp;author=Cabeza%2CR">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40"><p class="c-article-references__text" id="ref-CR40">Lazarus, R.S. &amp; Folkman, S. <i>Stress, Appraisal and Coping</i> (Springer Publishing Company, 1984).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41"><p class="c-article-references__text" id="ref-CR41">Small, D.M. et al. Human cortical gustatory areas: a review of functional neuroimaging data. <i>Neuroreport</i> <b>10</b>, 7â€“14 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1097/00001756-199901180-00002" data-track-action="article reference" href="https://doi.org/10.1097%2F00001756-199901180-00002" aria-label="Article reference 41" data-doi="10.1097/00001756-199901180-00002">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK1M7ptF2isA%3D%3D" aria-label="CAS reference 41">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20cortical%20gustatory%20areas%3A%20a%20review%20of%20functional%20neuroimaging%20data&amp;journal=Neuroreport&amp;doi=10.1097%2F00001756-199901180-00002&amp;volume=10&amp;pages=7-14&amp;publication_year=1999&amp;author=Small%2CDM">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42"><p class="c-article-references__text" id="ref-CR42">Lim, S.L., O'Doherty, J.P. &amp; Rangel, A. Stimulus value signals in ventromedial PFC reflect the integration of attribute value signals computed in fusiform gyrus and posterior superior temporal gyrus. <i>J. Neurosci.</i> <b>33</b>, 8729â€“8741 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.4809-12.2013" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.4809-12.2013" aria-label="Article reference 42" data-doi="10.1523/JNEUROSCI.4809-12.2013">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXotFantL8%3D" aria-label="CAS reference 42">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Stimulus%20value%20signals%20in%20ventromedial%20PFC%20reflect%20the%20integration%20of%20attribute%20value%20signals%20computed%20in%20fusiform%20gyrus%20and%20posterior%20superior%20temporal%20gyrus&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.4809-12.2013&amp;volume=33&amp;pages=8729-8741&amp;publication_year=2013&amp;author=Lim%2CSL&amp;author=O%27Doherty%2CJP&amp;author=Rangel%2CA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43"><p class="c-article-references__text" id="ref-CR43">Mogami, T. &amp; Tanaka, K. Reward association affects neuronal responses to visual stimuli in macaque te and perirhinal cortices. <i>J. Neurosci.</i> <b>26</b>, 6761â€“6770 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.4924-05.2006" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.4924-05.2006" aria-label="Article reference 43" data-doi="10.1523/JNEUROSCI.4924-05.2006">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28Xms1WltLo%3D" aria-label="CAS reference 43">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Reward%20association%20affects%20neuronal%20responses%20to%20visual%20stimuli%20in%20macaque%20te%20and%20perirhinal%20cortices&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.4924-05.2006&amp;volume=26&amp;pages=6761-6770&amp;publication_year=2006&amp;author=Mogami%2CT&amp;author=Tanaka%2CK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44"><p class="c-article-references__text" id="ref-CR44">Poellinger, A. et al. Activation and habituation in olfaction: an fMRI study. <i>Neuroimage</i> <b>13</b>, 547â€“560 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1006/nimg.2000.0713" data-track-action="article reference" href="https://doi.org/10.1006%2Fnimg.2000.0713" aria-label="Article reference 44" data-doi="10.1006/nimg.2000.0713">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD3MzlsVOltQ%3D%3D" aria-label="CAS reference 44">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Activation%20and%20habituation%20in%20olfaction%3A%20an%20fMRI%20study&amp;journal=Neuroimage&amp;doi=10.1006%2Fnimg.2000.0713&amp;volume=13&amp;pages=547-560&amp;publication_year=2001&amp;author=Poellinger%2CA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45"><p class="c-article-references__text" id="ref-CR45">Misaki, M., Kim, Y., Bandettini, P.A. &amp; Kriegeskorte, N. Comparison of multivariate classifiers and response normalizations for pattern-information fMRI. <i>Neuroimage</i> <b>53</b>, 103â€“118 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2010.05.051" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2010.05.051" aria-label="Article reference 45" data-doi="10.1016/j.neuroimage.2010.05.051">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparison%20of%20multivariate%20classifiers%20and%20response%20normalizations%20for%20pattern-information%20fMRI&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2010.05.051&amp;volume=53&amp;pages=103-118&amp;publication_year=2010&amp;author=Misaki%2CM&amp;author=Kim%2CY&amp;author=Bandettini%2CPA&amp;author=Kriegeskorte%2CN">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46"><p class="c-article-references__text" id="ref-CR46">Olson, I.R., Plotzker, A. &amp; Ezzyat, Y. The enigmatic temporal pole: a review of findings on social and emotional processing. <i>Brain</i> <b>130</b>, 1718â€“1731 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/brain/awm052" data-track-action="article reference" href="https://doi.org/10.1093%2Fbrain%2Fawm052" aria-label="Article reference 46" data-doi="10.1093/brain/awm052">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20enigmatic%20temporal%20pole%3A%20a%20review%20of%20findings%20on%20social%20and%20emotional%20processing&amp;journal=Brain&amp;doi=10.1093%2Fbrain%2Fawm052&amp;volume=130&amp;pages=1718-1731&amp;publication_year=2007&amp;author=Olson%2CIR&amp;author=Plotzker%2CA&amp;author=Ezzyat%2CY">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47"><p class="c-article-references__text" id="ref-CR47">Lapid, H. et al. Neural activity at the human olfactory epithelium reflects olfactory perception. <i>Nat. Neurosci.</i> <b>14</b>, 1455â€“1461 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.2926" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.2926" aria-label="Article reference 47" data-doi="10.1038/nn.2926">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXht1anu7bI" aria-label="CAS reference 47">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20activity%20at%20the%20human%20olfactory%20epithelium%20reflects%20olfactory%20perception&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.2926&amp;volume=14&amp;pages=1455-1461&amp;publication_year=2011&amp;author=Lapid%2CH">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48"><p class="c-article-references__text" id="ref-CR48">Li, W., Howard, J.D., Parrish, T.B. &amp; Gottfried, J.A. Aversive learning enhances perceptual and cortical discrimination of indiscriminable odor cues. <i>Science</i> <b>319</b>, 1842â€“1845 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1152837" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1152837" aria-label="Article reference 48" data-doi="10.1126/science.1152837">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1cXjs12ktr0%3D" aria-label="CAS reference 48">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Aversive%20learning%20enhances%20perceptual%20and%20cortical%20discrimination%20of%20indiscriminable%20odor%20cues&amp;journal=Science&amp;doi=10.1126%2Fscience.1152837&amp;volume=319&amp;pages=1842-1845&amp;publication_year=2008&amp;author=Li%2CW&amp;author=Howard%2CJD&amp;author=Parrish%2CTB&amp;author=Gottfried%2CJA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49"><p class="c-article-references__text" id="ref-CR49">Noonan, M.P. et al. Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex. <i>Proc. Natl. Acad. Sci. USA</i> <b>107</b>, 20547â€“20552 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1012246107" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1012246107" aria-label="Article reference 49" data-doi="10.1073/pnas.1012246107">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3cXhsFaisr%2FL" aria-label="CAS reference 49">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Separate%20value%20comparison%20and%20learning%20mechanisms%20in%20macaque%20medial%20and%20lateral%20orbitofrontal%20cortex&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1012246107&amp;volume=107&amp;pages=20547-20552&amp;publication_year=2010&amp;author=Noonan%2CMP">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50"><p class="c-article-references__text" id="ref-CR50">Chapman, H.A., Kim, D.A., Susskind, J.M. &amp; Anderson, A.K. In bad taste: evidence for the oral origins of moral disgust. <i>Science</i> <b>323</b>, 1222â€“1226 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1165565" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1165565" aria-label="Article reference 50" data-doi="10.1126/science.1165565">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXit1elurs%3D" aria-label="CAS reference 50">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=In%20bad%20taste%3A%20evidence%20for%20the%20oral%20origins%20of%20moral%20disgust&amp;journal=Science&amp;doi=10.1126%2Fscience.1165565&amp;volume=323&amp;pages=1222-1226&amp;publication_year=2009&amp;author=Chapman%2CHA&amp;author=Kim%2CDA&amp;author=Susskind%2CJM&amp;author=Anderson%2CAK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51"><p class="c-article-references__text" id="ref-CR51">Tzourio-Mazoyer, N. et al. Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. <i>Neuroimage</i> <b>15</b>, 273â€“289 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1006/nimg.2001.0978" data-track-action="article reference" href="https://doi.org/10.1006%2Fnimg.2001.0978" aria-label="Article reference 51" data-doi="10.1006/nimg.2001.0978">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD38%2FltFCntw%3D%3D" aria-label="CAS reference 51">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20anatomical%20labeling%20of%20activations%20in%20SPM%20using%20a%20macroscopic%20anatomical%20parcellation%20of%20the%20MNI%20MRI%20single-subject%20brain&amp;journal=Neuroimage&amp;doi=10.1006%2Fnimg.2001.0978&amp;volume=15&amp;pages=273-289&amp;publication_year=2002&amp;author=Tzourio-Mazoyer%2CN">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52"><p class="c-article-references__text" id="ref-CR52">Eickhoff, S.B. et al. A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data. <i>Neuroimage</i> <b>25</b>, 1325â€“1335 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2004.12.034" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2004.12.034" aria-label="Article reference 52" data-doi="10.1016/j.neuroimage.2004.12.034">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20SPM%20toolbox%20for%20combining%20probabilistic%20cytoarchitectonic%20maps%20and%20functional%20imaging%20data&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2004.12.034&amp;volume=25&amp;pages=1325-1335&amp;publication_year=2005&amp;author=Eickhoff%2CSB">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53"><p class="c-article-references__text" id="ref-CR53">Reinhard, E.S.M., Shirley, P. &amp; Ferwerda, J. Photographic tone reproduction for digital images. <i>ACM Trans. Graph.</i> <b>21</b>, 267â€“276 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1145/566654.566575" data-track-action="article reference" href="https://doi.org/10.1145%2F566654.566575" aria-label="Article reference 53" data-doi="10.1145/566654.566575">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Photographic%20tone%20reproduction%20for%20digital%20images&amp;journal=ACM%20Trans.%20Graph.&amp;doi=10.1145%2F566654.566575&amp;volume=21&amp;pages=267-276&amp;publication_year=2002&amp;author=Reinhard%2CESM&amp;author=Shirley%2CP&amp;author=Ferwerda%2CJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54"><p class="c-article-references__text" id="ref-CR54">Itti, L. &amp; Koch, C. Computational modelling of visual attention. <i>Nat. Rev. Neurosci.</i> <b>2</b>, 194â€“203 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/35058500" data-track-action="article reference" href="https://doi.org/10.1038%2F35058500" aria-label="Article reference 54" data-doi="10.1038/35058500">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3MXitVyqsLs%3D" aria-label="CAS reference 54">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20modelling%20of%20visual%20attention&amp;journal=Nat.%20Rev.%20Neurosci.&amp;doi=10.1038%2F35058500&amp;volume=2&amp;pages=194-203&amp;publication_year=2001&amp;author=Itti%2CL&amp;author=Koch%2CC">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55"><p class="c-article-references__text" id="ref-CR55">Walther, D. &amp; Koch, C. Modeling attention to salient proto-objects. <i>Neural Netw.</i> <b>19</b>, 1395â€“1407 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neunet.2006.10.001" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neunet.2006.10.001" aria-label="Article reference 55" data-doi="10.1016/j.neunet.2006.10.001">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Modeling%20attention%20to%20salient%20proto-objects&amp;journal=Neural%20Netw.&amp;doi=10.1016%2Fj.neunet.2006.10.001&amp;volume=19&amp;pages=1395-1407&amp;publication_year=2006&amp;author=Walther%2CD&amp;author=Koch%2CC">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56"><p class="c-article-references__text" id="ref-CR56">Naselaris, T., Stansbury, D.E. &amp; Gallant, J.L. Cortical representation of animate and inanimate objects in complex natural scenes. <i>J. Physiol. Paris</i> <b>106</b>, 239â€“249 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.jphysparis.2012.02.001" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.jphysparis.2012.02.001" aria-label="Article reference 56" data-doi="10.1016/j.jphysparis.2012.02.001">Article</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Cortical%20representation%20of%20animate%20and%20inanimate%20objects%20in%20complex%20natural%20scenes&amp;journal=J.%20Physiol.%20Paris&amp;doi=10.1016%2Fj.jphysparis.2012.02.001&amp;volume=106&amp;pages=239-249&amp;publication_year=2012&amp;author=Naselaris%2CT&amp;author=Stansbury%2CDE&amp;author=Gallant%2CJL">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57"><p class="c-article-references__text" id="ref-CR57">Op de Beeck, H., Wagemans, J. &amp; Vogels, R. Inferotemporal neurons represent low-dimensional configurations of parameterized shapes. <i>Nat. Neurosci.</i> <b>4</b>, 1244â€“1252 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn767" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn767" aria-label="Article reference 57" data-doi="10.1038/nn767">Article</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3MXovVCqu7s%3D" aria-label="CAS reference 57">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Inferotemporal%20neurons%20represent%20low-dimensional%20configurations%20of%20parameterized%20shapes&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn767&amp;volume=4&amp;pages=1244-1252&amp;publication_year=2001&amp;author=Op%20de%20Beeck%2CH&amp;author=Wagemans%2CJ&amp;author=Vogels%2CR">
                    Google Scholar</a>Â 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/nn.3749?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank T. Schmitz, M. Taylor, D. Hamilton and K. Gardhouse for technical collaboration and discussion. This work was funded by Canadian Institutes of Health Research grants to A.K.A. J.C. was supported by the Japan Society for the Promotion of Science Postdoctoral Fellowships for Research Abroad (H23).</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Human Development, Human Neuroscience Institute, College of Human Ecology, Cornell University, Ithaca, New York, USA</p><p class="c-article-author-affiliation__authors-list">Junichi ChikazoeÂ &amp;Â Adam K Anderson</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Psychology, University of Toronto, Toronto, Ontario, Canada</p><p class="c-article-author-affiliation__authors-list">Daniel H LeeÂ &amp;Â Adam K Anderson</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Medical Research Council, Cognition and Brain Sciences Unit, Cambridge, UK</p><p class="c-article-author-affiliation__authors-list">Nikolaus Kriegeskorte</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Junichi-Chikazoe-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Junichi Chikazoe</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Junichi%20Chikazoe" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Junichi%20Chikazoe" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">Â </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Junichi%20Chikazoe%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Daniel_H-Lee-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Daniel H Lee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Daniel%20H%20Lee" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Daniel%20H%20Lee" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">Â </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daniel%20H%20Lee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Nikolaus-Kriegeskorte-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Nikolaus Kriegeskorte</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Nikolaus%20Kriegeskorte" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nikolaus%20Kriegeskorte" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">Â </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nikolaus%20Kriegeskorte%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Adam_K-Anderson-Aff1-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Adam K Anderson</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Adam%20K%20Anderson" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Adam%20K%20Anderson" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">Â </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Adam%20K%20Anderson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>J.C. and A.K.A. designed the experiments. J.C. and D.H.L. built the experimental apparatus and performed the experiments. J.C. analyzed the data. J.C., D.H.L., N.K. and A.K.A. wrote the paper. N.K. and A.K.A. supervised the study.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:j.chikazoe@gmail.com">Junichi Chikazoe</a> or <a id="corresp-c2" href="mailto:aka47@cornell.edu">Adam K Anderson</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading">Competing interests</h3>
                <p>The authors declare no competing financial interests.</p>
              
            </div></div></section><section data-title="Integrated supplementary information"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Integrated supplementary information</h2><div class="c-article-section__content" id="Sec22-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig7"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 1 distribution of properties" href="/articles/nn.3749/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig7_ESM.jpg">Supplementary Figure 1 Distribution of properties</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(a) Distribution of individual low-level visual features (saliency, luminance, hue, number of edges and contrast). Data were sorted based on standardized metric (<i>z</i>). (b) Distribution of visual feature, animacy, and valence in 13 bins.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig8"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 2 multidimensional scaling (m" href="/articles/nn.3749/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig8_ESM.jpg">Supplementary Figure 2 Multidimensional scaling (MDS) plots of early visual cortex (EVC), ventral temporal cortex (VTC), and orbitofrontal cortex (OFC) with optimally fit property scales.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Color denotes scores of visual feature (top), animacy (middle) and valence (bottom). Correlation between projections on the best-fitting axis (line in each MDS plot) and property values is shown below each MDS plot. <i>P</i> values for correlations for visual features in EVC, VTC and OFC are 0.00001, 0.45 and 0.02, respectively. <i>P</i> values for correlations for animacy in EVC, VTC and OFC are 0.16, 7.7 Ã— 10<sup>-23</sup> and 0.0000002, respectively. <i>P</i> values for correlations for valence in EVC, VTC and OFC are 0.46, 0.001 and 0.00000005 respectively. <i>n</i> =128 trials.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig9"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 3 glm decomposition of multiv" href="/articles/nn.3749/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig9_ESM.jpg">Supplementary Figure 3 GLM decomposition of multivoxel activity patterns</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Relationship between distance in visual, animacy, and valence predictors and activation similarity was examined by GLM regression with rank-ordered correlations as the dependent variable. Resulting GLM coefficients (multiplied by minus 1) provided a Distance Correspondence Index (DCI) between activity patterns and property types, and were subjected to one-sample <i>t</i>-tests across participants. DCI is calculated by multiplying minus 1 and beta coefficient in the GLM decomposition formula.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig10"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 4 construction of representat" href="/articles/nn.3749/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig10_ESM.jpg">Supplementary Figure 4 Construction of representational similarity matrices (RSM) for each property.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Representational similarity matrices were constructed to visualize how each distinct variable (i.e. visual features, animacy, and valence) was associated with multivoxel activation patterns. Illustrated is the steps taken to create a valence RSM. First, other properties (visual features and animacy) were regressed out, along with regressors of no interest from the rank-ordered correlations. The residual correlations, now predicted only by valence, were then sorted based valence scores (13x13). The ideal valence RSM is illustrated at the bottom, where the main diagonal (top left to bottom right) indicates greater activity pattern correlations with valence similarity and the off diagonal reveals lower activity pattern correlations with valence dissimilarity.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig11"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 5 representational similarity" href="/articles/nn.3749/figures/11" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig11_ESM.jpg">Supplementary Figure 5 Representational similarity matrices by region and feature type</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Averaged activity pattern correlations in the EVC, VTC and OFC were rank-ordered with respect to visual features (top), animacy (middle), and valence (bottom).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig12"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 6 multivariate searchlight re" href="/articles/nn.3749/figures/12" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig12_ESM.jpg">Supplementary Figure 6 Multivariate searchlight results for individual visual features (saliency, luminance, hue, number of edges, and contrast).</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Visual features were primarily coded by occipital, temporal, and parahippocampal cortices.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig13"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 7 cross-participant classific" href="/articles/nn.3749/figures/13" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig13_ESM.jpg">Supplementary Figure 7 Cross-participant classification of picture item representations in the VTC and OFC.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Each participant's trial x trial RSM of <i>r</i>-coefficients was compared against a trial x trial RSM of <i>r</i>-coefficients estimated from all other participants' RSM, in a leave-one-out procedure. Each target picture was represented by 127 values that related it to all other picture trials (this is analogous to each picture being represented as coordinates in 127-dimension space). We then compared whether the target picture representation was more similar to the cross-participant estimate than all other picture representations, with similarity computed as the correlation of the <i>r</i>-coefficients. Classification performance was calculated as the percentage success of all pairwise comparisons (50% chance).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig14"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 8 cross-participant classific" href="/articles/nn.3749/figures/14" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_Article_BFnn3749_Fig14_ESM.jpg">Supplementary Figure 8 Cross-participant classification of valence.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>We used a similar leave-one-out procedure as shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3749#Fig12">Supplementary Fig. 6</a>, however, this time comparing each target picture's valence representation to its cross-participant prediction, estimated by computing all other participants' positivity and negativity correlation matrices, which were combined across items into separate positive (7 x7) and negative (7 x7) similarity matrices. As an example, consider a target picture <i>j</i>, which was rated as positive = 5, negative = 1 for one participant. It has 127 scores relating it to all other pictures, each rated with its own valence. The first of picture <i>j</i>'s 127 scores, <i>r</i><sub>(j,1)</sub>, relates it to picture 1, which was rated as positive = 2, negative = 2. These scores (positive 5, 2; negative 1, 2) were looked up in the estimated positive and negative similarity matrices and averaged. This process was repeated for all of picture <i>j</i>'s 127 scores. Then if the correlation of these scores was higher for picture <i>j</i>'s valence than another picture <i>k</i>'s valence representation, the valence classification was successful. Classification performance calculated as the percentage success of all pairwise comparisons (50 % chance).</p></div></div></div></div></div></section><section data-title="Supplementary information"><div class="c-article-section" id="Sec23-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec23">Supplementary information</h2><div class="c-article-section__content" id="Sec23-content"><div data-test="supplementary-info"><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM3"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary text and figures" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_BFnn3749_MOESM3_ESM.pdf" data-supp-info-image="">Supplementary Text and Figures</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Supplementary Figures 1â€“8 and Supplementary Tables 1â€“5 (PDF 2659 kb)</p></div></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM4"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary methods checklist (pdf 499 kb)" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3749/MediaObjects/41593_2014_BFnn3749_MOESM4_ESM.pdf" data-supp-info-image="">Supplementary Methods Checklist (PDF 499 kb)</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Population%20coding%20of%20affect%20across%20stimuli%2C%20modalities%20and%20individuals&amp;author=Junichi%20Chikazoe%20et%20al&amp;contentID=10.1038%2Fnn.3749&amp;copyright=Springer%20Nature%20America%2C%20Inc.&amp;publication=1097-6256&amp;publicationDate=2014-06-22&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/nn.3749" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/nn.3749" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Chikazoe, J., Lee, D., Kriegeskorte, N. <i>et al.</i> Population coding of affect across stimuli, modalities and individuals.
                    <i>Nat Neurosci</i> <b>17</b>, 1114â€“1122 (2014). https://doi.org/10.1038/nn.3749</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/nn.3749?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-01-19">19 January 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-05-23">23 May 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-06-22">22 June 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-08">August 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/nn.3749</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>

            
        <section>
            <div class="c-article-section js-article-section" id="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Genetic effects on variability in visual aesthetic evaluations are partially shared across visual domains" href="https://doi.org/10.1038/s42003-023-05710-4">
                                        Genetic effects on variability in visual aesthetic evaluations are partially shared across visual domains
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Giacomo Bignardi</li><li>Dirk J. A. Smit</li><li>Tinca J. C. Polderman</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Communications Biology</i> (2024)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:A mesocorticolimbic signature of pleasure in the human brain" href="https://doi.org/10.1038/s41562-023-01639-0">
                                        A mesocorticolimbic signature of pleasure in the human brain
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Philip A. Kragel</li><li>Michael T. Treadway</li><li>Emma C. Hahn</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Human Behaviour</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Common and stimulus-type-specific brain representations of negative affect" href="https://doi.org/10.1038/s41593-022-01082-w">
                                        Common and stimulus-type-specific brain representations of negative affect
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Marta ÄŒeko</li><li>Philip A. Kragel</li><li>Tor D. Wager</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Neuroscience</i> (2022)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Refining the negative into general and specific" href="https://doi.org/10.1038/s41593-022-01077-7">
                                        Refining the negative into general and specific
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Junichi Chikazoe</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Neuroscience</i> (2022)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:A corticostriatal pathway mediating self-efficacy enhancement" href="https://doi.org/10.1038/s44184-022-00006-7">
                                        A corticostriatal pathway mediating self-efficacy enhancement
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Ofir Shany</li><li>Guy Gurevitch</li><li>Talma Hendler</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>npj Mental Health Research</i> (2022)</p>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </section>
    

            
        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="js-context-bar-sticky-point-desktop">
        

        
            <noscript>
                
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3749.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </noscript>
            <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
                <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-right-column
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3749.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </div>
        
    </div>

    
        
    

    
    
        <div class="c-article-associated-content__container">
            <section>
                <h2 class="c-article-associated-content__title u-mb-24">Associated content</h2>
                
                    
                    
                        <div class="u-full-height u-mb-24">
                            
    <article class="u-full-height c-card c-card--flush">
        <div class="c-card__layout u-full-height">
            <div class="c-card__body">
                <h3 class="c-card__title">
                    <a href="https://www.nature.com/articles/nn0814-1021"
                       class="c-card__link u-link-inherit"
                       data-track="click"
                       data-track-action="view article"
                       data-track-category="associated content"
                       
                       data-track-label="news_and_views">A common affective code</a>
                </h3>
                
<ul data-test="author-list" class="c-author-list c-author-list--compact">
    <li>P. Alexander Arguello</li>
</ul>

                
    <div class="c-card__section c-meta">
        
            <span class="c-meta__item">Nature Neuroscience</span>
        
        <span class="c-meta__item" data-test="article.type"><span class="c-meta__type">News &amp; Views</span></span>
        
        
            <time class="c-meta__item" datetime="2014-07-28">28 Jul 2014</time>
        
    </div>

            </div>
        </div>
    </article>


                        </div>
                    
                
            </section>
        </div>
        <script>
            window.dataLayer = window.dataLayer || [];
            window.dataLayer[0] = window.dataLayer[0] || {};
            window.dataLayer[0].content = window.dataLayer[0].content || {};
            window.dataLayer[0].content.associatedContentTypes = "news_and_views";
        </script>
    

    

    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>
                            
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=nn.3749;doi=10.1038/nn.3749;techmeta=36,59;subjmeta=1457,1945,378,631;kwrd=Prefrontal+cortex">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=67137106&amp;t=pos%3Dright%26type%3Darticle%26artid%3Dnn.3749%26doi%3D10.1038/nn.3749%26techmeta%3D36,59%26subjmeta%3D1457,1945,378,631%26kwrd%3DPrefrontal+cortex">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=67137106&amp;t=pos%3Dright%26type%3Darticle%26artid%3Dnn.3749%26doi%3D10.1038/nn.3749%26techmeta%3D36,59%26subjmeta%3D1457,1945,378,631%26kwrd%3DPrefrontal+cortex"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>


    
        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-and-analysis"
                                   data-track="click"
                                   data-track-action="reviews &amp; analysis"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Reviews &amp; Analysis
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/video"
                                   data-track="click"
                                   data-track-action="videos"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Videos
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/current-issue"
                                   data-track="click"
                                   data-track-action="current issue"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Current issue
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>
                        
                    
                </ul>
                <ul class="c-header__list c-header__list--js-stack">
                    
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/natureneuro"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on Twitter
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;6"
                               rel="nofollow"
                               data-track="click"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/neuro.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </nav>
    
    
        
            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/aims"
                                   data-track="click"
                                   data-track-action="aims &amp; scope"
                                   data-track-label="link">
                                    Aims &amp; Scope
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-information"
                                   data-track="click"
                                   data-track-action="journal information"
                                   data-track-label="link">
                                    Journal Information
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-impact"
                                   data-track="click"
                                   data-track-action="journal metrics"
                                   data-track-label="link">
                                    Journal Metrics
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editors"
                                   data-track="click"
                                   data-track-action="about the editors"
                                   data-track-label="link">
                                    About the Editors
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/our-publishing-models"
                                   data-track="click"
                                   data-track-action="our publishing models"
                                   data-track-label="link">
                                    Our publishing models
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-values-statement"
                                   data-track="click"
                                   data-track-action="editorial values statement"
                                   data-track-label="link">
                                    Editorial Values Statement
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-policies"
                                   data-track="click"
                                   data-track-action="editorial policies"
                                   data-track-label="link">
                                    Editorial Policies
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/content"
                                   data-track="click"
                                   data-track-action="content types"
                                   data-track-label="link">
                                    Content Types
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/web-feeds"
                                   data-track="click"
                                   data-track-action="web feeds"
                                   data-track-label="link">
                                    Web Feeds
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/posters"
                                   data-track="click"
                                   data-track-action="posters"
                                   data-track-label="link">
                                    Posters
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="research cross-journal editorial team"
                                   data-track-label="link">
                                    Research Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="reviews cross-journal editorial team"
                                   data-track-label="link">
                                    Reviews Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        

        
            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/submission-guidelines"
                                   data-track="click"
                                   data-track-action="submission guidelines"
                                   data-track-label="link">
                                    Submission Guidelines
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/for-reviewers"
                                   data-track="click"
                                   data-track-action="for reviewers"
                                   data-track-label="link">
                                    For Reviewers
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="click"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://mts-nn.nature.com/cgi-bin/main.plex?form_type&#x3D;home&amp;from_idp&#x3D;1"
                                   data-track="click"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external>Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        
    


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">
                            
                                
                                    <option value="" selected>All journals</option>
                                    <option value="neuro">This journal</option>
                                
                            
                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        

        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
            

            <p class="c-meta u-ma-0 u-flex-shrink">
                <span class="c-meta__item">
                    Nature Neuroscience (<i>Nat Neurosci</i>)
                </span>
                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="onlineIssn">1546-1726</span> (online)
    </span>
    


                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="printIssn">1097-6256</span> (print)
    </span>
    

            </p>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/protocolexchange/"
                                                  data-track="click" data-track-action="protocol exchange"
                                                  data-track-label="link">Protocol Exchange</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natitaly"
                                                  data-track="click" data-track-action="nature Italy"
                                                  data-track-label="link">Nature Italy</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ko-kr"
                                                  data-track="click" data-track-action="nature korea"
                                                  data-track-label="link">Nature Korea</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>
            
        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-ea63208b81.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2024 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path id="shape" fill-rule="evenodd" clip-rule="evenodd" d="M1 3.78571C1 2.75867 1.85698 2 2.8209 2H6.1791C7.14302 2 8 2.75867 8 3.78571V4H11.1668C11.885 4 12.5585 4.42017 12.8494 5.07033C12.9893 4.98169 13.1425 4.91101 13.3056 4.86206L16.5222 3.89704C17.4454 3.62005 18.4843 4.10046 18.7794 5.08419L22.9256 18.9042C23.2207 19.8878 22.618 20.8608 21.6947 21.1378L18.4781 22.1029C17.5548 22.3799 16.516 21.8993 16.2209 20.9157L13.0001 10.1804V20.2143C13.0001 21.255 12.1231 22 11.1668 22H7.83346C7.54206 22 7.25803 21.9308 7.00392 21.8052C6.75263 21.9305 6.47077 22 6.1791 22H2.8209C1.85693 22 1 21.2412 1 20.2143V3.78571ZM3 4V15H6V4H3ZM3 20V17H6V20H3ZM18.0749 20.1358L17.2129 17.2623L20.0863 16.4002L20.9484 19.2737L18.0749 20.1358ZM19.5116 14.4846L16.6381 15.3466L14.0519 6.72624L16.9254 5.86416L19.5116 14.4846ZM8.00012 20L8.00012 6H11.0001L11.0001 20H8.00012Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</footer>




    

    

<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">

    
    <div class="c-site-messages__banner-large">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing" src="/static/images/logos/nature-briefing-logo-n150-white-d81c9da3ec.svg" width="250" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing</em> newsletter â€” what matters in science, free to your inbox daily.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/briefing" method="post" data-location="banner" data-track="submit||nature_briefing_sign_up" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="MainBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick">
                        <input type="hidden" value="false" name="marketing" id="marketing">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick">
                        <input type="hidden" value="MainBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">
                            
                            <input type="hidden" value="true" name="N:nature_briefing_daily" id="defaultNewsletter">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>

    
    <div class="c-site-messages__banner-small">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner">Sign up for Nature Briefing
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/nn.3749&amp;format=js&amp;last_modified=2014-08-01" async></script>
<img src="/a7fjn4y6/article/nn.3749" width="1" height="1" alt="" class="u-visually-hidden">
</body>
</html>