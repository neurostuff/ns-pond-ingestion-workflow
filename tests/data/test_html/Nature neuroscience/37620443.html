<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>High-precision mapping reveals the structure of odor coding in the human brain | Nature Neuroscience</title>
    
        
<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/neuro.rss"/>


    
        

        <script id="save-data-connection-testing">
            function hasConnection() {
                return navigator.connection || navigator.mozConnection || navigator.webkitConnection || navigator.msConnection;
            }

            function createLink(src) {
                var preloadLink = document.createElement("link");
                preloadLink.rel = "preload";
                preloadLink.href = src;
                preloadLink.as = "font";
                preloadLink.type = "font/woff2";
                preloadLink.crossOrigin = "";
                document.head.insertBefore(preloadLink, document.head.firstChild);
            }

            var connectionDetail = {
                saveDataEnabled: false,
                slowConnection: false
            };

            var connection = hasConnection();
            if (connection) {
                connectionDetail.saveDataEnabled = connection.saveData;
                if (/\slow-2g|2g/.test(connection.effectiveType)) {
                    connectionDetail.slowConnection = true;
                }
            }

            if (!(connectionDetail.saveDataEnabled || connectionDetail.slowConnection)) {
                createLink("/static/fonts/HardingText-Regular-Web-cecd90984f.woff2");
            } else {
                document.documentElement.classList.add('save-data');
            }
        </script>
    

<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"olfactory-cortex;perception","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Nature Neuroscience","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article"}},"article":{"doi":"10.1038/s41593-023-01414-4"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["Vivek Sagar","Laura K. Shanahan","Christina M. Zelano","Jay A. Gottfried","Thorsten Kahnt"],"publishedAt":1692835200,"publishedAtString":"2023-08-24","title":"High-precision mapping reveals the structure of odor coding in the human brain","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"neuro","title":"nature neuroscience","volume":"26","issue":"9"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"nature-onwards-journey","active":false}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"US","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://collect.nature.com',
        imprint: 'nature'
    });
</script>

<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
 



     
    
    
        
    
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}button:focus{outline:3px solid #fece3e;will-change:transform}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:8px}.c-recommendations-list__item .c-card{flex-basis:100%}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 0 0 16px;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #cedbe0;height:auto;min-height:0;position:static}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0 0 8px;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:inherit}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}.c-card__title-recommendation .MathJax_Display{display:inline!important}.c-card__link:not(.c-card__link--no-block-link):before{z-index:1}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a,.c-article-recommendations-card__link{color:inherit}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-article-recommendations-card__meta-type,.c-meta .c-meta__item:first-child{font-weight:700}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:539px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px 13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.c-pdf-download__link{padding:13px 24px} } </style>




    
        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-nature-branded-68c4876c28.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    
    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-nature-branded-68c4876c28.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-122346e276.css" media="print">
    



<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">


<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h.indexOf('preview-www.nature.com') > -1) return;

            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('nature.com') > -1) {
                if (h.indexOf('test-www.nature.com') > -1) {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-26e142e9c6.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    
                        {src: '/static/js/global-article-es6-bundle-22e9088d18.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-be699ef0bc.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-4841faf3e2.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-f72f566c73.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-1fe07484e5.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;
                        
                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-8fc1a30809.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-9389823142.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-cb0ea70df9.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">





    
    <script type="application/ld+json">{"mainEntity":{"headline":"High-precision mapping reveals the structure of odor coding in the human brain","description":"Odor perception is inherently subjective. Previous work has shown that odorous molecules evoke distributed activity patterns in olfactory cortices, but how these patterns map on to subjective odor percepts remains unclear. In the present study, we collected neuroimaging responses to 160 odors from 3 individual subjects (18â€‰h per subject) to probe the neural coding scheme underlying idiosyncratic odor perception. We found that activity in the orbitofrontal cortex (OFC) represents the fine-grained perceptual identity of odors over and above coarsely defined percepts, whereas this difference is less pronounced in the piriform cortex (PirC) and amygdala. Furthermore, the implementation of perceptual encoding models enabled us to predict olfactory functional magnetic resonance imaging responses to new odors, revealing that the dimensionality of the encoded perceptual spaces increases from the PirC to the OFC. Whereas encoding of lower-order dimensions generalizes across subjects, encoding of higher-order dimensions is idiosyncratic. These results provide new insights into cortical mechanisms of odor coding and suggest that subjective olfactory percepts reside in the OFC. The authors used precision functional imaging and computational modeling to uncover the structure of perceptual odor coding in the human brain. Olfactory areas differ in the granularity, dimensionality and subjectivity of perceptual coding.","datePublished":"2023-08-24T00:00:00Z","dateModified":"2023-08-24T00:00:00Z","pageStart":"1595","pageEnd":"1602","sameAs":"https://doi.org/10.1038/s41593-023-01414-4","keywords":["Olfactory cortex","Perception","Biomedicine","general","Neurosciences","Behavioral Sciences","Biological Techniques","Neurobiology","Animal Genetics and Genomics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig4_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig5_HTML.png"],"isPartOf":{"name":"Nature Neuroscience","issn":["1546-1726","1097-6256"],"volumeNumber":"26","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Vivek Sagar","url":"http://orcid.org/0000-0003-1301-2687","affiliation":[{"name":"Northwestern University","address":{"name":"Department of Neurology, Feinberg School of Medicine, Northwestern University, Chicago, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Laura K. Shanahan","affiliation":[{"name":"Rhodes College","address":{"name":"Department of Psychology, Rhodes College, Memphis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Christina M. Zelano","affiliation":[{"name":"Northwestern University","address":{"name":"Department of Neurology, Feinberg School of Medicine, Northwestern University, Chicago, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Jay A. Gottfried","affiliation":[{"name":"University of Pennsylvania","address":{"name":"Department of Neurology, University of Pennsylvania, Philadelphia, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Pennsylvania","address":{"name":"Department of Psychology, University of Pennsylvania, Philadelphia, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Thorsten Kahnt","url":"http://orcid.org/0000-0002-3575-2670","affiliation":[{"name":"National Institute on Drug Abuse Intramural Research Program","address":{"name":"National Institute on Drug Abuse Intramural Research Program, Baltimore, USA","@type":"PostalAddress"},"@type":"Organization"}],"email":"thorsten.kahnt@nih.gov","@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>




    
    
    


    
    <link rel="canonical" href="https://www.nature.com/articles/s41593-023-01414-4">
    
    
    <meta name="journal_id" content="41593"/>
    <meta name="dc.title" content="High-precision mapping reveals the structure of odor coding in the human brain"/>
    <meta name="dc.source" content="Nature Neuroscience 2023 26:9"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2023-08-24"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2023 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply"/>
    <meta name="dc.rights" content="2023 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="Odor perception is inherently subjective. Previous work has shown that odorous molecules evoke distributed activity patterns in olfactory cortices, but how these patterns map on to subjective odor percepts remains unclear. In the present study, we collected neuroimaging responses to 160 odors from 3 individual subjects (18&#8201;h per subject) to probe the neural coding scheme underlying idiosyncratic odor perception. We found that activity in the orbitofrontal cortex (OFC) represents the fine-grained perceptual identity of odors over and above coarsely defined percepts, whereas this difference is less pronounced in the piriform cortex (PirC) and amygdala. Furthermore, the implementation of perceptual encoding models enabled us to predict olfactory functional magnetic resonance imaging responses to new odors, revealing that the dimensionality of the encoded perceptual spaces increases from the PirC to the OFC. Whereas encoding of lower-order dimensions generalizes across subjects, encoding of higher-order dimensions is idiosyncratic. These results provide new insights into cortical mechanisms of odor coding and suggest that subjective olfactory percepts reside in the OFC. The authors used precision functional imaging and computational modeling to uncover the structure of perceptual odor coding in the human brain. Olfactory areas differ in the granularity, dimensionality and subjectivity of perceptual coding."/>
    <meta name="prism.issn" content="1546-1726"/>
    <meta name="prism.publicationName" content="Nature Neuroscience"/>
    <meta name="prism.publicationDate" content="2023-08-24"/>
    <meta name="prism.volume" content="26"/>
    <meta name="prism.number" content="9"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="1595"/>
    <meta name="prism.endingPage" content="1602"/>
    <meta name="prism.copyright" content="2023 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/s41593-023-01414-4"/>
    <meta name="prism.doi" content="doi:10.1038/s41593-023-01414-4"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/s41593-023-01414-4.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/s41593-023-01414-4"/>
    <meta name="citation_journal_title" content="Nature Neuroscience"/>
    <meta name="citation_journal_abbrev" content="Nat Neurosci"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="1546-1726"/>
    <meta name="citation_title" content="High-precision mapping reveals the structure of odor coding in the human brain"/>
    <meta name="citation_volume" content="26"/>
    <meta name="citation_issue" content="9"/>
    <meta name="citation_publication_date" content="2023/09"/>
    <meta name="citation_online_date" content="2023/08/24"/>
    <meta name="citation_firstpage" content="1595"/>
    <meta name="citation_lastpage" content="1602"/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/s41593-023-01414-4"/>
    <meta name="DOI" content="10.1038/s41593-023-01414-4"/>
    <meta name="size" content="289429"/>
    <meta name="citation_doi" content="10.1038/s41593-023-01414-4"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/s41593-023-01414-4&amp;api_key="/>
    <meta name="description" content="Odor perception is inherently subjective. Previous work has shown that odorous molecules evoke distributed activity patterns in olfactory cortices, but how these patterns map on to subjective odor percepts remains unclear. In the present study, we collected neuroimaging responses to 160 odors from 3 individual subjects (18&#8201;h per subject) to probe the neural coding scheme underlying idiosyncratic odor perception. We found that activity in the orbitofrontal cortex (OFC) represents the fine-grained perceptual identity of odors over and above coarsely defined percepts, whereas this difference is less pronounced in the piriform cortex (PirC) and amygdala. Furthermore, the implementation of perceptual encoding models enabled us to predict olfactory functional magnetic resonance imaging responses to new odors, revealing that the dimensionality of the encoded perceptual spaces increases from the PirC to the OFC. Whereas encoding of lower-order dimensions generalizes across subjects, encoding of higher-order dimensions is idiosyncratic. These results provide new insights into cortical mechanisms of odor coding and suggest that subjective olfactory percepts reside in the OFC. The authors used precision functional imaging and computational modeling to uncover the structure of perceptual odor coding in the human brain. Olfactory areas differ in the granularity, dimensionality and subjectivity of perceptual coding."/>
    <meta name="dc.creator" content="Sagar, Vivek"/>
    <meta name="dc.creator" content="Shanahan, Laura K."/>
    <meta name="dc.creator" content="Zelano, Christina M."/>
    <meta name="dc.creator" content="Gottfried, Jay A."/>
    <meta name="dc.creator" content="Kahnt, Thorsten"/>
    <meta name="dc.subject" content="Olfactory cortex"/>
    <meta name="dc.subject" content="Perception"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Stable perception of visually ambiguous patterns; citation_author=DA Leopold, M Wilke, A Maier, NK Logothetis; citation_volume=5; citation_publication_date=2002; citation_pages=605-609; citation_id=CR1"/>
    <meta name="citation_reference" content="Walsh, V. &amp; Kulikowski, J. Perceptual Constancy: Why Things Look As They Do (Cambridge Univ. Press, 1998)."/>
    <meta name="citation_reference" content="citation_journal_title=J. Comp. Physiol. A; citation_title=From molecule to mind: the role of experience in shaping olfactory function; citation_author=R Hudson; citation_volume=185; citation_publication_date=1999; citation_pages=297-304; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=Angew. Chem. Int. Ed.; citation_title=On the unpredictability of odor; citation_author=CS Sell; citation_volume=45; citation_publication_date=2006; citation_pages=6254-6261; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=The missense of smell: functional variability in the human odorant receptor repertoire; citation_author=JD Mainland; citation_volume=17; citation_publication_date=2014; citation_pages=114-120; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Humans can discriminate more than 1 trillion olfactory stimuli; citation_author=C Bushdid, MO Magnasco, LB Vosshall, A Keller; citation_volume=343; citation_publication_date=2014; citation_pages=1370-1372; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Syst. Neurosci.; citation_title=In search of the structure of human olfactory space; citation_author=A Koulakov, BE Kolterman, A Enikolopov, D Rinberg; citation_volume=5; citation_publication_date=2011; citation_pages=65; citation_id=CR7"/>
    <meta name="citation_reference" content="Shepherd, G. M. &amp; Greer, C. A. in The Synaptic Organization of the Brain (ed. Shepherd, G. M.) 159&#8211;203 (Oxford Univ. Press, 1998)."/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Odor representations in olfactory cortex: distributed rate coding and decorrelated population activity; citation_author=K Miura, ZF Mainen, N Uchida; citation_volume=74; citation_publication_date=2012; citation_pages=1087-1098; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Structure and flexibility in cortical representations of odour space; citation_author=SL Pashkovski; citation_volume=583; citation_publication_date=2020; citation_pages=253-258; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Representations of odor in the piriform cortex; citation_author=DD Stettler, R Axel; citation_volume=63; citation_publication_date=2009; citation_pages=854-864; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Distinct representations of olfactory information in different cortical centres; citation_author=DL Sosulski, ML Bloom, T Cutforth, R Axel, SR Datta; citation_volume=472; citation_publication_date=2011; citation_pages=213-216; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Cortical representations of olfactory input by trans-synaptic tracing; citation_author=K Miyamichi; citation_volume=472; citation_publication_date=2011; citation_pages=191-196; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=eLife; citation_title=Odor identity coding by distributed ensembles of neurons in the mouse olfactory cortex; citation_author=B Roland, T Deneux, KM Franks, B Bathellier, A Fleischmann; citation_volume=6; citation_publication_date=2017; citation_pages=e26337; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Odor quality coding and categorization in human posterior piriform cortex; citation_author=JD Howard, J Plailly, M Grueschow, JD Haynes, JA Gottfried; citation_volume=12; citation_publication_date=2009; citation_pages=932-938; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=eLife; citation_title=Complementary codes for odor identity and intensity in olfactory cortex; citation_author=KA Bolding, KM Franks; citation_volume=6; citation_publication_date=2017; citation_pages=e22630; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Predicting human olfactory perception from chemical features of odor molecules; citation_author=A Keller; citation_volume=355; citation_publication_date=2017; citation_pages=820-826; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS ONE; citation_title=Categorical dimensions of human odor descriptor space revealed by non-negative matrix factorization; citation_author=JB Castro, A Ramanathan, CS Chennubhotla; citation_volume=8; citation_publication_date=2013; citation_pages=e73289; citation_id=CR18"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Rev. Neurosci.; citation_title=The what, where and how of auditory-object perception; citation_author=JK Bizley, YE Cohen; citation_volume=14; citation_publication_date=2013; citation_pages=693-707; citation_id=CR19"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Human scene-selective areas represent 3D configurations of surfaces; citation_author=MD Lescroart, JL Gallant; citation_volume=101; citation_publication_date=2019; citation_pages=178-192. e177; citation_id=CR20"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=The integration of multiple stimulus features by V1 neurons; citation_author=A Grunewald, EK Skoumbourdis; citation_volume=24; citation_publication_date=2004; citation_pages=9185-9194; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Resolving human object recognition in space and time; citation_author=RM Cichy, D Pantazis, A Oliva; citation_volume=17; citation_publication_date=2014; citation_pages=455-462; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Syst. Neurosci.; citation_title=Representational similarity analysis&#8212;connecting the branches of systems neuroscience; citation_author=N Kriegeskorte, M Mur, P Bandettini; citation_volume=2; citation_publication_date=2008; citation_pages=4; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=Annu. Rev. Neurosci.; citation_title=Complete functional characterization of sensory neurons by system identification; citation_author=MC Wu, SV David, JL Gallant; citation_volume=29; citation_publication_date=2006; citation_pages=477-505; citation_id=CR24"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence; citation_author=EJ Allen; citation_volume=25; citation_publication_date=2022; citation_pages=116-126; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Bayesian reconstruction of natural images from human brain activity; citation_author=T Naselaris, RJ Prenger, KN Kay, M Oliver, JL Gallant; citation_volume=63; citation_publication_date=2009; citation_pages=902-915; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Identifying natural images from human brain activity; citation_author=KN Kay, T Naselaris, RJ Prenger, JL Gallant; citation_volume=452; citation_publication_date=2008; citation_pages=352-355; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=Predicting odor perceptual similarity from odor structure; citation_author=K Snitz; citation_volume=9; citation_publication_date=2013; citation_pages=e1003184; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Annu. Rev. Psychol.; citation_title=An odor is not worth a thousand words: from multidimensional odors to unidimensional odor objects; citation_author=Y Yeshurun, N Sobel; citation_volume=61; citation_publication_date=2010; citation_pages=219-241; citation_id=CR29"/>
    <meta name="citation_reference" content="Sirotin, Y. B., Shusterman, R. &amp; Rinberg, D. Neural coding of perceived odor intensity. eNeuro 
                  https://doi.org/10.1523/ENEURO.0083-15.2015
                  
                 (2015)."/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Integrated neural representations of odor intensity and affective valence in human amygdala; citation_author=JS Winston, JA Gottfried, JM Kilner, RJ Dolan; citation_volume=25; citation_publication_date=2005; citation_pages=8903-8907; citation_id=CR31"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Dissociated neural representations of intensity and valence in human olfaction; citation_author=AK Anderson; citation_volume=6; citation_publication_date=2003; citation_pages=196-202; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Neural activity at the human olfactory epithelium reflects olfactory perception; citation_author=H Lapid; citation_volume=14; citation_publication_date=2011; citation_pages=1455-1461; citation_id=CR33"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Brain-behavior correlations: two paths toward reliability; citation_author=C Gratton, SM Nelson, EM Gordon; citation_volume=110; citation_publication_date=2022; citation_pages=1446-1449; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=How to establish robust brain&#8211;behavior relationships without thousands of individuals; citation_author=MD Rosenberg, ES Finn; citation_volume=25; citation_publication_date=2022; citation_pages=835-837; citation_id=CR35"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Dissociable codes of odor quality and odorant structure in human piriform cortex; citation_author=JA Gottfried, JS Winston, RJ Dolan; citation_volume=49; citation_publication_date=2006; citation_pages=467-479; citation_id=CR36"/>
    <meta name="citation_reference" content="citation_journal_title=Hum. Brain Mapp.; citation_title=Multidimensional representation of odors in the human olfactory cortex; citation_author=A Fournel, C Ferdenzi, C Sezille, C Rouby, M Bensafi; citation_volume=37; citation_publication_date=2016; citation_pages=2161-2172; citation_id=CR37"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Olfactory predictive codes and stimulus templates in piriform cortex; citation_author=C Zelano, A Mohanty, JA Gottfried; citation_volume=72; citation_publication_date=2011; citation_pages=178-187; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Sniffing and smelling: separate subsystems in the human olfactory cortex; citation_author=N Sobel; citation_volume=392; citation_publication_date=1998; citation_pages=282-286; citation_id=CR39"/>
    <meta name="citation_reference" content="citation_journal_title=NeuroImage; citation_title=Olfactory system activation from sniffing: effects in piriform and orbitofrontal cortex; citation_author=DA Kareken; citation_volume=22; citation_publication_date=2004; citation_pages=456-465; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Multisensory integration of natural odors and sounds in the auditory cortex; citation_author=L Cohen, G Rothschild, A Mizrahi; citation_volume=72; citation_publication_date=2011; citation_pages=357-369; citation_id=CR41"/>
    <meta name="citation_reference" content="Olofsson, J. K., Zhou, G., East, B. S., Zelano, C. &amp; Wilson, D. A. Odor identification in rats: behavioral and electrophysiological evidence of learned olfactory-auditory associations. eNeuro 
                  https://doi.org/10.1523/ENEURO.0102-19.2019
                  
                 (2019)."/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Aversive learning enhances perceptual and cortical discrimination of indiscriminable odor cues; citation_author=W Li, JD Howard, TB Parrish, JA Gottfried; citation_volume=319; citation_publication_date=2008; citation_pages=1842-1845; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS ONE; citation_title=Odor quality profile is partially influenced by verbal cues; citation_author=J Bae, J-Y Yi, C Moon; citation_volume=14; citation_publication_date=2019; citation_pages=e0226385; citation_id=CR44"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Transient and persistent representations of odor value in prefrontal cortex; citation_author=PY Wang; citation_volume=108; citation_publication_date=2020; citation_pages=209-224. e206; citation_id=CR45"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Opin. Neurobiol.; citation_title=Interpreting encoding and decoding models; citation_author=N Kriegeskorte, PK Douglas; citation_volume=55; citation_publication_date=2019; citation_pages=167-179; citation_id=CR46"/>
    <meta name="citation_reference" content="citation_journal_title=Sci. Rep.; citation_title=Preprocessing of emotional visual information in the human piriform cortex; citation_author=P Schulze, A-K Bestgen, RK Lech, L Kuchinke, B Suchan; citation_volume=7; citation_publication_date=2017; citation_pages=9191; citation_id=CR47"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Commun.; citation_title=A primacy code for odor identity; citation_author=CD Wilson, GO Serrano, AA Koulakov, D Rinberg; citation_volume=8; citation_publication_date=2017; citation_pages=1477; citation_id=CR48"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Encoding predictive reward value in human amygdala and orbitofrontal cortex; citation_author=JA Gottfried, J O&#8217;Doherty, RJ Dolan; citation_volume=301; citation_publication_date=2003; citation_pages=1104-1107; citation_id=CR49"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Temporal integration of olfactory perceptual evidence in human orbitofrontal cortex; citation_author=NE Bowman, KP Kording, JA Gottfried; citation_volume=75; citation_publication_date=2012; citation_pages=916-927; citation_id=CR50"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Biol.; citation_title=Rat orbitofrontal ensemble activity contains multiplexed but dissociable representations of value and task structure in an odor sequence task; citation_author=J Zhou; citation_volume=29; citation_publication_date=2019; citation_pages=897-907. e893; citation_id=CR51"/>
    <meta name="citation_reference" content="citation_journal_title=eLife; citation_title=The role of piriform associative connections in odor categorization; citation_author=X Bao, LL Raguet, SM Cole, JD Howard, JA Gottfried; citation_volume=5; citation_publication_date=2016; citation_pages=e13732; citation_id=CR52"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Odorant category profile selectivity of olfactory cortex neurons; citation_author=I Yoshida, K Mori; citation_volume=27; citation_publication_date=2007; citation_pages=9105-9114; citation_id=CR53"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Cognitive modulation of olfactory processing; citation_author=IE Araujo, ET Rolls, MI Velazco, C Margot, I Cayeux; citation_volume=46; citation_publication_date=2005; citation_pages=671-679; citation_id=CR54"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Psychol.; citation_title=Attention and olfactory consciousness; citation_author=A Keller; citation_volume=2; citation_publication_date=2011; citation_pages=380; citation_id=CR55"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Sci.; citation_title=Right orbitofrontal cortex mediates conscious olfactory perception; citation_author=W Li; citation_volume=21; citation_publication_date=2010; citation_pages=1454-1463; citation_id=CR56"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Discrimination of odors in olfactory bulb, pyriform-amygdaloid areas, and orbitofrontal cortex of the monkey; citation_author=T Tanabe, M Iino, S Takagi; citation_volume=38; citation_publication_date=1975; citation_pages=1284-1296; citation_id=CR57"/>
    <meta name="citation_reference" content="citation_journal_title=Neuropsychologia; citation_title=An assessment of olfactory deficits in patients with damage to prefrontal cortex; citation_author=H Potter, N Butters; citation_volume=18; citation_publication_date=1980; citation_pages=621-628; citation_id=CR58"/>
    <meta name="citation_reference" content="citation_journal_title=Neuropsychologia; citation_title=Olfactory identification deficits in patients with focal cerebral excision; citation_author=M Jones-Gotman, RJ Zatorre; citation_volume=26; citation_publication_date=1988; citation_pages=387-400; citation_id=CR59"/>
    <meta name="citation_reference" content="citation_journal_title=BMC Neurosci.; citation_title=Olfactory perception of chemically diverse molecules; citation_author=A Keller, LB Vosshall; citation_volume=17; citation_publication_date=2016; citation_pages=55; citation_id=CR60"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=Representational structure or task structure? Bias in neural representational similarity analysis and a Bayesian method for reducing bias; citation_author=MB Cai, NW Schuck, JW Pillow, Y Niv; citation_volume=15; citation_publication_date=2019; citation_pages=e1006299; citation_id=CR61"/>
    <meta name="citation_reference" content="citation_journal_title=eLife; citation_title=Characterizing functional pathways of the human olfactory system; citation_author=G Zhou, G Lane, SL Cooper, T Kahnt, C Zelano; citation_volume=8; citation_publication_date=2019; citation_pages=e47177; citation_id=CR62"/>
    <meta name="citation_reference" content="Echevarria-Cooper, S. L. et al. Mapping the microstructure and striae of the human olfactory tract with diffusion MRI. J. Neurosci. 42, 58&#8211;68 (2021)."/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Emotion, olfaction, and the human amygdala: amygdala activation during aversive olfactory stimulation; citation_author=DH Zald, JV Pardo; citation_volume=94; citation_publication_date=1997; citation_pages=4119-4124; citation_id=CR64"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Functional localization and lateralization of human olfactory cortex; citation_author=RJ Zatorre, M Jones-Gotman, AC Evans, E Meyer; citation_volume=360; citation_publication_date=1992; citation_pages=339-340; citation_id=CR65"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Combined lesions of hippocampus and subiculum do not produce deficits in a nonspatial social olfactory memory task; citation_author=S Burton, D Murphy, U Qureshi, P Sutton, J O&#8217;Keefe; citation_volume=20; citation_publication_date=2000; citation_pages=5468-5475; citation_id=CR66"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS ONE; citation_title=On the definition of signal-to-noise ratio and contrast-to-noise ratio for fMRI data; citation_author=M Welvaert, Y Rosseel; citation_volume=8; citation_publication_date=2013; citation_pages=e77089; citation_id=CR67"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain; citation_author=N Tzourio-Mazoyer; citation_volume=15; citation_publication_date=2002; citation_pages=273-289; citation_id=CR68"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Commun.; citation_title=Identity prediction errors in the human midbrain update reward-identity expectations in the orbitofrontal cortex; citation_author=JD Howard, T Kahnt; citation_volume=9; citation_publication_date=2018; citation_pages=1611; citation_id=CR69"/>
    <meta name="citation_reference" content="citation_journal_title=eLife; citation_title=Sensory prediction errors in the human midbrain signal identity violations independent of perceptual distance; citation_author=JA Suarez, JD Howard, G Schoenbaum, T Kahnt; citation_volume=8; citation_publication_date=2019; citation_pages=e43962; citation_id=CR70"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Biol.; citation_title=Olfactory perceptual decision-making is biased by motivational state; citation_author=LK Shanahan, S Bhutani, T Kahnt; citation_volume=19; citation_publication_date=2021; citation_pages=e3001374; citation_id=CR71"/>
    <meta name="citation_reference" content="citation_journal_title=Chem. Senses; citation_title=Automated analysis of breathing waveforms using BreathMetrics: a respiratory signal processing toolbox; citation_author=T Noto, G Zhou, S Schuele, J Templer, C Zelano; citation_volume=43; citation_publication_date=2018; citation_pages=583-597; citation_id=CR72"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=A toolbox for representational similarity analysis; citation_author=H Nili; citation_volume=10; citation_publication_date=2014; citation_pages=e1003553; citation_id=CR73"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=High-amplitude cofluctuations in cortical activity drive functional connectivity; citation_author=FZ Esfahlani; citation_volume=117; citation_publication_date=2020; citation_pages=28393-28401; citation_id=CR74"/>
    <meta name="citation_reference" content="Gusfield, D. &amp; Irving, R. W. The Stable Marriage Problem: Structure and Algorithms (MIT Press, 1989)."/>
    <meta name="citation_reference" content="Sagar, V., Shanahan, L. K., Zelano, C. M., Gottfried, J. A. &amp; Kahnt, T. Neural encoding models of olfaction (NEMO) dataset. Zenodo 
                  https://doi.org/10.5281/zenodo.7636722
                  
                 (2023)."/>
    <meta name="citation_author" content="Sagar, Vivek"/>
    <meta name="citation_author_institution" content="Department of Neurology, Feinberg School of Medicine, Northwestern University, Chicago, USA"/>
    <meta name="citation_author" content="Shanahan, Laura K."/>
    <meta name="citation_author_institution" content="Department of Psychology, Rhodes College, Memphis, USA"/>
    <meta name="citation_author" content="Zelano, Christina M."/>
    <meta name="citation_author_institution" content="Department of Neurology, Feinberg School of Medicine, Northwestern University, Chicago, USA"/>
    <meta name="citation_author" content="Gottfried, Jay A."/>
    <meta name="citation_author_institution" content="Department of Neurology, University of Pennsylvania, Philadelphia, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of Pennsylvania, Philadelphia, USA"/>
    <meta name="citation_author" content="Kahnt, Thorsten"/>
    <meta name="citation_author_institution" content="National Institute on Drug Abuse Intramural Research Program, Baltimore, USA"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@natureneuro"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="High-precision mapping reveals the structure of odor coding in the human brain"/>
    <meta name="twitter:description" content="Nature Neuroscience - The authors used precision functional imaging and computational modeling to uncover the structure of perceptual odor coding in the human brain. Olfactory areas differ in the..."/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig1_HTML.png"/>
    

    
    
    <meta property="og:url" content="https://www.nature.com/articles/s41593-023-01414-4"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="High-precision mapping reveals the structure of odor coding in the human brain - Nature Neuroscience"/>
    <meta property="og:description" content="The authors used precision functional imaging and computational modeling to uncover the structure of perceptual odor coding in the human brain. Olfactory areas differ in the granularity, dimensionality and subjectivity of perceptual coding."/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig1_HTML.png"/>
    

    <script>
        window.eligibleForRa21 = 'true'; 
    </script>
</head>
<body class="article-page">

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>



<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
        
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>

    

    <div class="u-lazy-ad-wrapper u-mbs-0">
            <div class="deferred-placeholder" data-replace="true"
                 data-placeholder="/placeholder/v1/institutionalBanner?bpids=[bpids] #institutional-banner-container"></div>
            <aside class="c-ad c-ad--728x90">
                <div class="c-ad__inner" data-container-type="banner-advert">
                    <p class="c-ad__label">Advertisement</p>
                    
        
            
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=s41593-023-01414-4;doi=10.1038/s41593-023-01414-4;techmeta=36,59;subjmeta=1704,1723,2624,2649,378,631;kwrd=Olfactory+cortex,Perception">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-328985134&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-023-01414-4%26doi%3D10.1038/s41593-023-01414-4%26techmeta%3D36,59%26subjmeta%3D1704,1723,2624,2649,378,631%26kwrd%3DOlfactory+cortex,Perception">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-328985134&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-023-01414-4%26doi%3D10.1038/s41593-023-01414-4%26techmeta%3D36,59%26subjmeta%3D1704,1723,2624,2649,378,631%26kwrd%3DOlfactory+cortex,Perception"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

        
    
                </div>
            </aside>
        </div>
    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#00928c">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">
                    
                    
                    <div class="c-header__logo-container">
                        
                        <a href="/neuro"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-0ccc487532906d646419e51f647ce79a.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-880e5942f43b9213989c58a04ab5c8e6.svg" height="32" alt="Nature Neuroscience">
                            </picture>
                        </a>
                    
                    </div>
                    
                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">
                            
                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41593-023-01414-4'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
                            
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        
            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">
                            
                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span><span class="c-header__show-text">Explore</span> content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                                
                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>
                                
                            
                            
                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">
                            
                                <li class="c-header__item">
                                    <a class="c-header__link"
                                       href="https://idp.nature.com/auth/personal/springernature?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Fmy-account%2Falerts%2Fsubscribe-journal%3Flist-id%3D6%26journal-link%3Dhttps%253A%252F%252Fwww.nature.com%252Fneuro%252F"
                                       rel="nofollow"
                                       data-track="click"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/neuro.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        
    </header>


    
    
        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature neuroscience"><span itemprop="name">nature neuroscience</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro/articles?type&#x3D;article" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="name">articles</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>
    



    

</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
        
            <div class="c-context-bar u-hide"
                 data-test="context-bar"
                 data-context-bar
                 aria-hidden="true">
                <div class="c-context-bar__container u-container">
                    <div class="c-context-bar__title">
                        High-precision mapping reveals the structure of odor coding in the human brain
                    </div>
                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01414-4.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                </div>
            </div>
        
        <article lang="en">
            
            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Article</li>
    
    

                        <li class="c-article-identifiers__item">Published: <time datetime="2023-08-24">24 August 2023</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">High-precision mapping reveals the structure of odor coding in the human brain</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Vivek-Sagar-Aff1" data-author-popup="auth-Vivek-Sagar-Aff1" data-author-search="Sagar, Vivek">Vivek Sagar</a><span class="u-js-hide">Â 
            <a class="js-orcid" href="http://orcid.org/0000-0003-1301-2687"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-1301-2687</a></span><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Laura_K_-Shanahan-Aff2" data-author-popup="auth-Laura_K_-Shanahan-Aff2" data-author-search="Shanahan, Laura K.">Laura K. Shanahan</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Christina_M_-Zelano-Aff1" data-author-popup="auth-Christina_M_-Zelano-Aff1" data-author-search="Zelano, Christina M.">Christina M. Zelano</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jay_A_-Gottfried-Aff3-Aff4" data-author-popup="auth-Jay_A_-Gottfried-Aff3-Aff4" data-author-search="Gottfried, Jay A.">Jay A. Gottfried</a><sup class="u-js-hide"><a href="#Aff3">3</a>,<a href="#Aff4">4</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 5 authors for this article" title="Show all 5 authors for this article">â€¦</li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Thorsten-Kahnt-Aff5" data-author-popup="auth-Thorsten-Kahnt-Aff5" data-author-search="Kahnt, Thorsten" data-corresp-id="c1">Thorsten Kahnt<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><span class="u-js-hide">Â 
            <a class="js-orcid" href="http://orcid.org/0000-0002-3575-2670"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-3575-2670</a></span><sup class="u-js-hide"><a href="#Aff5">5</a></sup>Â </li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>

                    

                    <p class="c-article-info-details" data-container-section="info">
                        
    <a data-test="journal-link" href="/neuro" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Nature Neuroscience</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span>Â 26</b>,Â <span class="u-visually-hidden">pages </span>1595â€“1602 (<span data-test="article-publication-year">2023</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>
                    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5144 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">72 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/articles/s41593-023-01414-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
                    
                </header>

                
    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/olfactory-cortex" data-track="click" data-track-action="view subject" data-track-label="link">Olfactory cortex</a></li><li class="c-article-subject-list__subject"><a href="/subjects/perception" data-track="click" data-track-action="view subject" data-track-label="link">Perception</a></li>
        </ul>
    </div>

                
    
    

    
    

                
            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Odor perception is inherently subjective. Previous work has shown that odorous molecules evoke distributed activity patterns in olfactory cortices, but how these patterns map on to subjective odor percepts remains unclear. In the present study, we collected neuroimaging responses to 160 odors from 3 individual subjects (18â€‰h per subject) to probe the neural coding scheme underlying idiosyncratic odor perception. We found that activity in the orbitofrontal cortex (OFC) represents the fine-grained perceptual identity of odors over and above coarsely defined percepts, whereas this difference is less pronounced in the piriform cortex (PirC) and amygdala. Furthermore, the implementation of perceptual encoding models enabled us to predict olfactory functional magnetic resonance imaging responses to new odors, revealing that the dimensionality of the encoded perceptual spaces increases from the PirC to the OFC. Whereas encoding of lower-order dimensions generalizes across subjects, encoding of higher-order dimensions is idiosyncratic. These results provide new insights into cortical mechanisms of odor coding and suggest that subjective olfactory percepts reside in the OFC.</p></div></div></section>

            <noscript>
                
                    
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01414-4.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                
            </noscript>

            
                <div class="js-context-bar-sticky-point-mobile">
                    
                        <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-entitled-mobile
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01414-4.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                    
                </div>
            

            
                
                    
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41586-020-2451-1/MediaObjects/41586_2020_2451_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41586-020-2451-1?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1038/s41586-020-2451-1">Structure and flexibility in cortical representations of odour space
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">01 July 2020</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41562-024-01849-0/MediaObjects/41562_2024_1849_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41562-024-01849-0?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1038/s41562-024-01849-0">Decomposition of an odorant in olfactory perception and neural representation
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">18 March 2024</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41583-024-00822-0/MediaObjects/41583_2024_822_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41583-024-00822-0?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1038/s41583-024-00822-0">Common principles for odour coding across vertebrates and invertebrates
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">28 May 2024</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'speedy-BootstrappedUCB',
                        timestamp: 1717144745,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                
                
                <div class="main-content">
                    <section data-title="Main"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Main</h2><div class="c-article-section__content" id="Sec1-content"><p>A rose is a rose is a rose, except when itâ€™s not. In the case of vision, the connection between an object and its percept is highly stable<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Leopold, D. A., Wilke, M., Maier, A. &amp; Logothetis, N. K. Stable perception of visually ambiguous patterns. Nat. Neurosci. 5, 605â€“609 (2002)." href="/articles/s41593-023-01414-4#ref-CR1" id="ref-link-section-d54222645e502">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Walsh, V. &amp; Kulikowski, J. Perceptual Constancy: Why Things Look As They Do (Cambridge Univ. Press, 1998)." href="/articles/s41593-023-01414-4#ref-CR2" id="ref-link-section-d54222645e505">2</a></sup> and an experimenter presenting the picture of a rose flower can trust that their subjects will correctly identify the stimulus. In the olfactory system, by contrast, the mapping between the same object and its percept is far more flexible<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Hudson, R. From molecule to mind: the role of experience in shaping olfactory function. J. Comp. Physiol. A 185, 297â€“304 (1999)." href="/articles/s41593-023-01414-4#ref-CR3" id="ref-link-section-d54222645e509">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Sell, C. S. On the unpredictability of odor. Angew. Chem. Int. Ed. 45, 6254â€“6261 (2006)." href="/articles/s41593-023-01414-4#ref-CR4" id="ref-link-section-d54222645e512">4</a></sup>. The same odor can smell â€˜fruityâ€™ and â€˜floralâ€™ to one person and â€˜muskyâ€™ and â€˜decayedâ€™ to another. This perceptual ambiguity favors the formation of idiosyncratic olfactory percepts (that is, the unique and personal experience of perceiving a given odor), such that the same volatile molecule may smell different to different people<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Mainland, J. D. et al. The missense of smell: functional variability in the human odorant receptor repertoire. Nat. Neurosci. 17, 114â€“120 (2014)." href="/articles/s41593-023-01414-4#ref-CR5" id="ref-link-section-d54222645e516">5</a></sup>.</p><p>Given the challenge of representing a vast number of odorants and odor percepts<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bushdid, C., Magnasco, M. O., Vosshall, L. B. &amp; Keller, A. Humans can discriminate more than 1 trillion olfactory stimuli. Science 343, 1370â€“1372 (2014)." href="/articles/s41593-023-01414-4#ref-CR6" id="ref-link-section-d54222645e523">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Koulakov, A., Kolterman, B. E., Enikolopov, A. &amp; Rinberg, D. In search of the structure of human olfactory space. Front. Syst. Neurosci. 5, 65 (2011)." href="/articles/s41593-023-01414-4#ref-CR7" id="ref-link-section-d54222645e526">7</a></sup>, the olfactory system cannot employ a simple one-to-one mapping between an odor stimulus and a localized neural response. Instead, as demonstrated in rodents, odor stimuli evoke distributed patterns of activity at the level of the olfactory bulb and cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shepherd, G. M. &amp; Greer, C. A. in The Synaptic Organization of the Brain (ed. Shepherd, G. M.) 159â€“203 (Oxford Univ. Press, 1998)." href="#ref-CR8" id="ref-link-section-d54222645e530">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Miura, K., Mainen, Z. F. &amp; Uchida, N. Odor representations in olfactory cortex: distributed rate coding and decorrelated population activity. Neuron 74, 1087â€“1098 (2012)." href="#ref-CR9" id="ref-link-section-d54222645e530_1">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pashkovski, S. L. et al. Structure and flexibility in cortical representations of odour space. Nature 583, 253â€“258 (2020)." href="#ref-CR10" id="ref-link-section-d54222645e530_2">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Stettler, D. D. &amp; Axel, R. Representations of odor in the piriform cortex. Neuron 63, 854â€“864 (2009)." href="#ref-CR11" id="ref-link-section-d54222645e530_3">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Sosulski, D. L., Bloom, M. L., Cutforth, T., Axel, R. &amp; Datta, S. R. Distinct representations of olfactory information in different cortical centres. Nature 472, 213â€“216 (2011)." href="#ref-CR12" id="ref-link-section-d54222645e530_4">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Miyamichi, K. et al. Cortical representations of olfactory input by trans-synaptic tracing. Nature 472, 191â€“196 (2011)." href="/articles/s41593-023-01414-4#ref-CR13" id="ref-link-section-d54222645e533">13</a></sup>. Work in both rodents and humans has shown that these distributed ensemble patterns discriminate among different odors and that odor stimuli can be robustly decoded from neural activity in the PirC, amygdala (AMY) and OFC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Pashkovski, S. L. et al. Structure and flexibility in cortical representations of odour space. Nature 583, 253â€“258 (2020)." href="/articles/s41593-023-01414-4#ref-CR10" id="ref-link-section-d54222645e537">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Roland, B., Deneux, T., Franks, K. M., Bathellier, B. &amp; Fleischmann, A. Odor identity coding by distributed ensembles of neurons in the mouse olfactory cortex. eLife 6, e26337 (2017)." href="#ref-CR14" id="ref-link-section-d54222645e540">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Howard, J. D., Plailly, J., Grueschow, M., Haynes, J. D. &amp; Gottfried, J. A. Odor quality coding and categorization in human posterior piriform cortex. Nat. Neurosci. 12, 932â€“938 (2009)." href="#ref-CR15" id="ref-link-section-d54222645e540_1">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Bolding, K. A. &amp; Franks, K. M. Complementary codes for odor identity and intensity in olfactory cortex. eLife 6, e22630 (2017)." href="/articles/s41593-023-01414-4#ref-CR16" id="ref-link-section-d54222645e543">16</a></sup>.</p><p>Although the ensemble nature of olfactory coding has been well established across species, we lack a fundamental understanding of the relationship between neural patterns and subjective odor perception. The human system is ideal for addressing this question, because it allows access to subjective perception in a way that is difficult to achieve in animals. Accordingly, in the present study, we collected high-resolution functional magnetic resonance imaging (fMRI) responses to 160 odors across 4,320 trials for each of 3 human subjects (that is, high-precision imaging), who also provided detailed perceptual ratings for these stimuli.</p><p>We conceptualized odor percepts in a multidimensional space defined by subjective perceptual descriptors (for example, fruity, floral, fishy)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Koulakov, A., Kolterman, B. E., Enikolopov, A. &amp; Rinberg, D. In search of the structure of human olfactory space. Front. Syst. Neurosci. 5, 65 (2011)." href="/articles/s41593-023-01414-4#ref-CR7" id="ref-link-section-d54222645e553">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Keller, A. et al. Predicting human olfactory perception from chemical features of odor molecules. Science 355, 820â€“826 (2017)." href="/articles/s41593-023-01414-4#ref-CR17" id="ref-link-section-d54222645e556">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Castro, J. B., Ramanathan, A. &amp; Chennubhotla, C. S. Categorical dimensions of human odor descriptor space revealed by non-negative matrix factorization. PLoS ONE 8, e73289 (2013)." href="/articles/s41593-023-01414-4#ref-CR18" id="ref-link-section-d54222645e559">18</a></sup>. Similar to other sensory systems<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bizley, J. K. &amp; Cohen, Y. E. The what, where and how of auditory-object perception. Nat. Rev. Neurosci. 14, 693â€“707 (2013)." href="#ref-CR19" id="ref-link-section-d54222645e563">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lescroart, M. D. &amp; Gallant, J. L. Human scene-selective areas represent 3D configurations of surfaces. Neuron 101, 178â€“192. e177 (2019)." href="#ref-CR20" id="ref-link-section-d54222645e563_1">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Grunewald, A. &amp; Skoumbourdis, E. K. The integration of multiple stimulus features by V1 neurons. J. Neurosci. 24, 9185â€“9194 (2004)." href="/articles/s41593-023-01414-4#ref-CR21" id="ref-link-section-d54222645e566">21</a></sup>, we hypothesized that olfactory brain areas employ a perceptual coding scheme, that is, activity patterns and odor percepts should be systematically related, such that the similarity between the activity patterns evoked by different odors is determined by their proximity in the perceptual space. We tested for such perceptual coding in olfactory areas using representational similarity analysis (RSA)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Cichy, R. M., Pantazis, D. &amp; Oliva, A. Resolving human object recognition in space and time. Nat. Neurosci. 17, 455â€“462 (2014)." href="/articles/s41593-023-01414-4#ref-CR22" id="ref-link-section-d54222645e570">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysisâ€”connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/s41593-023-01414-4#ref-CR23" id="ref-link-section-d54222645e573">23</a></sup> and compared this coding scheme with the encoding of molecular structure.</p><p>Importantly, we further hypothesized that this mapping between activity patterns and odor percepts results from the neural encoding of the dimensions defining the odor space. We tested for the encoding of specific dimensions using computational encoding models for neural system identification<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Wu, M. C., David, S. V. &amp; Gallant, J. L. Complete functional characterization of sensory neurons by system identification. Annu. Rev. Neurosci. 29, 477â€“505 (2006)." href="/articles/s41593-023-01414-4#ref-CR24" id="ref-link-section-d54222645e581">24</a></sup>. Importantly, rather than decoding the stimulus from neural activity, encoding models predict neural responses from explicitly hypothesized characteristics of the stimulus, yielding a description of the content of the neural code. Encoding models can be implemented at the level of fMRI, given a collection of neural responses to a sufficiently large set of stimuli<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Allen, E. J. et al. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nat. Neurosci. 25, 116â€“126 (2022)." href="/articles/s41593-023-01414-4#ref-CR25" id="ref-link-section-d54222645e585">25</a></sup>, to reveal population-level encoding across cortical areas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Lescroart, M. D. &amp; Gallant, J. L. Human scene-selective areas represent 3D configurations of surfaces. Neuron 101, 178â€“192. e177 (2019)." href="/articles/s41593-023-01414-4#ref-CR20" id="ref-link-section-d54222645e589">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Naselaris, T., Prenger, R. J., Kay, K. N., Oliver, M. &amp; Gallant, J. L. Bayesian reconstruction of natural images from human brain activity. Neuron 63, 902â€“915 (2009)." href="/articles/s41593-023-01414-4#ref-CR26" id="ref-link-section-d54222645e592">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Kay, K. N., Naselaris, T., Prenger, R. J. &amp; Gallant, J. L. Identifying natural images from human brain activity. Nature 452, 352â€“355 (2008)." href="/articles/s41593-023-01414-4#ref-CR27" id="ref-link-section-d54222645e595">27</a></sup>. We found that neural activity patterns represent idiosyncratic odor percepts by encoding the principal dimensions of perceptual odor spaces that reflect increasing levels of complexity and subjectivity across the major olfactory areas.</p></div></div></section><section data-title="Results"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Results</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Ensemble responses in olfactory areas represent odor stimuli</h3><p>We implemented a high-precision imaging approach to uncover the neural coding scheme underlying idiosyncratic odor percepts. Specifically, we collected odor-evoked fMRI responses and perceptual ratings for a large set of olfactory stimuli (160 monomolecular odors) in 3 human subjects. We scanned each subject for 18â€‰h across 12 fMRI sessions, resulting in 4,320 trials per subject (27â€“30 trials per odor). On each trial, subjects were presented with one of the odorants and then rated it on 1 of 18 perceptual descriptors (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig1">1a,b</a>). For subjects S2 and S3, we collected additional perceptual ratings across 5,760 trials outside of the MRI scanner to further characterize each subjectâ€™s multidimensional odor percepts. These ratings were significantly correlated across independent sessions, demonstrating their reliability within each subject (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig6">1a</a>). Moreover, each odor was represented by a unique profile of perceptual descriptors (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig6">1b</a>), allowing us to reconstruct the olfactory perceptual space for each individual subject.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Neural activity patterns in olfactory brain areas represent odor stimuli."><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1: Neural activity patterns in olfactory brain areas represent odor stimuli.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="610"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p><b>a</b>, Trial structure. During fMRI, subjects were cued to sniff on each trial. If they reported detecting an odor (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig7">2</a>), they rated the odor on one of the perceptual descriptors listed in <b>b</b>. Odors were presented 27â€“30 times in pseudo-randomized order across multiple sessions and only one descriptor rating was obtained on each trial (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-023-01414-4#Sec8">Methods</a>). <b>b</b>, Perceptual ratings for two example odors (methyl tributyrate and 2-methyl-1-butanol). Subjects rated odors on 18 perceptual descriptors (note that these were drawn from a total of 21 descriptors; see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-023-01414-4#Sec8">Methods</a> for details). S1 rated 2-methyl-1-butanol as sweaty and decayed, but S2 found the same odor to be pleasant and floral, highlighting the substantial variability in odor perception across individuals. <b>c</b>, Anatomical ROIs shown for subject S1. In each of the olfactory ROIs, significant odor-evoked activity was observed with similar temporal signal:noise ratio in the voxel time series (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig7">2</a>). The shaded area shows the field of view for scanning. <b>d</b>, Difference between pattern correlation (âˆ†<i>r</i>) among activity patterns evoked by the same minus different odors in different fMRI sessions. Multi-voxel patterns were more similar (across sessions) when comparing responses evoked by the same odor versus different odors in all four ROIs (âˆ†<i>r</i>â€‰&gt;â€‰0, <i>P</i>â€‰=â€‰0.0000 in all areas, <i>n</i>â€‰=â€‰3 subjects, 12,720 odor pairs per subject, two-tailed percentile bootstrap; <i>P</i>â€‰=â€‰0.0000 in all areas, <i>n</i>â€‰=â€‰3 subjects, 12,720 odor pairs per subject, two-sample Studentâ€™s <i>t</i>-test). Bars depict mean correlation difference and error bars depict 95% CIs. S1, S2 and S3 indicate subjects 1, 2 and 3. ***<i>P</i>â€‰&lt;â€‰0.001.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>In a first step, we aimed to replicate previous findings<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Pashkovski, S. L. et al. Structure and flexibility in cortical representations of odour space. Nature 583, 253â€“258 (2020)." href="/articles/s41593-023-01414-4#ref-CR10" id="ref-link-section-d54222645e695">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Stettler, D. D. &amp; Axel, R. Representations of odor in the piriform cortex. Neuron 63, 854â€“864 (2009)." href="/articles/s41593-023-01414-4#ref-CR11" id="ref-link-section-d54222645e698">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Roland, B., Deneux, T., Franks, K. M., Bathellier, B. &amp; Fleischmann, A. Odor identity coding by distributed ensembles of neurons in the mouse olfactory cortex. eLife 6, e26337 (2017)." href="/articles/s41593-023-01414-4#ref-CR14" id="ref-link-section-d54222645e701">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Howard, J. D., Plailly, J., Grueschow, M., Haynes, J. D. &amp; Gottfried, J. A. Odor quality coding and categorization in human posterior piriform cortex. Nat. Neurosci. 12, 932â€“938 (2009)." href="/articles/s41593-023-01414-4#ref-CR15" id="ref-link-section-d54222645e704">15</a></sup> showing that distributed patterns of odor-evoked activity in the frontal PirC (PirF), temporal PirC (PirT), AMY and OFC (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig1">1c</a>) discriminate between different odor stimuli. In each of these brain areas, we used data from independent scanning sessions and compared correlations among the fMRI activity patterns evoked by â€˜sameâ€™ and â€˜differentâ€™ odors (that is, âˆ†<i>r</i>â€‰=â€‰sameâ€‰âˆ’â€‰different odors). We found that multi-voxel activity patterns evoked by the same odor stimulus were significantly more similar (across sessions) than those evoked by different odor stimuli (âˆ†<i>r</i>â€‰&gt;â€‰0, <i>P</i>â€‰&lt;â€‰0.001, percentile bootstrap; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig1">1d</a>). These initial findings demonstrate that odor stimuli are robustly represented in olfactory areas and the OFC.</p><h3 class="c-article__sub-heading" id="Sec4">Ensemble responses represent fine-grained odor percepts</h3><p>Having confirmed that activity patterns represent odor stimuli, we next examined whether these patterns reflect the molecular features of the odor stimuli or their evoked olfactory percepts. To this end, we used RSA<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Cichy, R. M., Pantazis, D. &amp; Oliva, A. Resolving human object recognition in space and time. Nat. Neurosci. 17, 455â€“462 (2014)." href="/articles/s41593-023-01414-4#ref-CR22" id="ref-link-section-d54222645e732">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysisâ€”connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/s41593-023-01414-4#ref-CR23" id="ref-link-section-d54222645e735">23</a></sup> to test whether the similarity between odor-evoked activity patterns (that is, neural similarity) mirrored the perceptual or molecular similarity among odor pairs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig2">2a</a>). This analysis revealed a significant correlation between neural and perceptual similarity in the PirT, AMY and OFC, but not the PirF (PirF, <i>P</i>â€‰=â€‰0.280; PirT, AMY, OFC, <i>P</i>â€‰=â€‰0.0000; percentile bootstrap; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig2">2b</a>), which was significantly larger than the correlation with molecular similarity in the PirT and OFC (PirF, <i>P</i>â€‰=â€‰0.177; PirT, <i>P</i>â€‰=â€‰0.030; AMY, <i>P</i>â€‰=â€‰0.084; OFC, <i>P</i>â€‰=â€‰0.0000; two-tailed bootstrap comparison; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig2">2b</a>). These findings demonstrate that odor-evoked activity patterns in the PirT and OFC predominantly represent odor percepts over molecular structure.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Neural activity patterns represent odor percepts."><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2: Neural activity patterns represent odor percepts.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="300"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p><b>a</b>, RSA schematic. For each subject, we computed similarity matrices comparing each odor pair in neural and perceptual spaces. The representational similarity for a given ROI is measured as Spearmanâ€™s rank correlation (<i>r</i>) between the off-diagonal entries of these matrices. <b>b</b>, RSA results (control areas: A1 and wm). Bars depict the correlation between neural and perceptual similarity (<i>r</i><sub>p</sub>â€‰, dark bars) or neural and molecular similarity (<i>r</i><sub>m</sub>, light bars); <i>r</i><sub>p</sub>â€‰is significant in all areas except the PirF and wm (PirF, <i>r</i><sub>p</sub>â€‰=â€‰0.005, <i>P</i>â€‰=â€‰0.184; PirT, <i>r</i><sub>p</sub>â€‰=â€‰0.035, <i>P</i>â€‰=â€‰0.0000; AMY, <i>r</i><sub>p</sub>â€‰=â€‰0.059, <i>P</i>â€‰=â€‰0.0000; OFC, <i>r</i><sub>p</sub>â€‰=â€‰0.120, <i>P</i>â€‰=â€‰0.0000; A1, <i>r</i><sub>p</sub>â€‰=â€‰0.015, <i>P</i>â€‰=â€‰0.0003; wm, <i>r</i><sub>p</sub>â€‰=â€‰0.001, <i>P</i>â€‰=â€‰0.696; two-tailed bootstrap comparison). The <i>r</i><sub>m</sub> is significant in all areas except wm (PirF, <i>r</i><sub>m</sub>â€‰=â€‰0.012, <i>P</i>â€‰=â€‰0.001; PirT, <i>r</i><sub>m</sub>â€‰=â€‰0.024, <i>P</i>â€‰=â€‰0.0000; AMY, <i>r</i><sub>m</sub>â€‰=â€‰0.050, <i>P</i>â€‰=â€‰0.0000; OFC, <i>r</i><sub>m</sub>â€‰=â€‰0.077, <i>P</i>â€‰=â€‰0.0000; A1, <i>r</i><sub>m</sub>â€‰=â€‰0.015, <i>P</i>â€‰=â€‰0.0004; wm, <i>r</i><sub>m</sub>â€‰=â€‰0.004, <i>P</i>â€‰=â€‰0.334; two-tailed bootstrap comparison). The <i>r</i><sub>p</sub> exceeds <i>r</i><sub>m</sub> in the PirT and OFC but not in other areas (<i>r</i><sub>p</sub>â€‰&gt;â€‰<i>r</i><sub>m</sub>: PirF, <i>P</i>â€‰=â€‰0.172; PirT, <i>P</i>â€‰=â€‰0.031; AMY, <i>P</i>â€‰=â€‰0.074; OFC, <i>P</i>â€‰=â€‰0.0000; A1, <i>P</i>â€‰=â€‰0.972; wm, <i>P</i>â€‰=â€‰0.671; two-tailed bootstrap comparison). NS, not significant. For all tests, <i>n</i>â€‰=â€‰3 subjects, 12,720 odor pairs per subject. Bars indicate mean correlation and error bars depict 95% CIs. S1, S2 and S3 indicate subjects 1, 2 and 3. We did not observe significant effects when perceptual or molecular descriptors were randomly shuffled (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig10">5a</a>). *<i>P</i>â€‰&lt;â€‰0.05; ***<i>P</i>â€‰&lt;â€‰0.001.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>To explore perceptual representations in a nonolfactory primary sensory area and to rule out potential biases in our analysis, we repeated these analyses in the auditory cortex (A1) and frontal white matter (wm), respectively. We obtained a small but significant effect in A1 (<i>P</i>â€‰=â€‰0.0003; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig2">2b</a>) that was driven by odor intensity (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig11">6a,b</a>), suggesting that there is rudimentary olfactory information in other sensory areas. However, no significant effects were observed in the wm (<i>P</i>â€‰=â€‰0.696, percentile bootstrap; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig2">2b</a>), ruling out biases in our analysis. Moreover, perceptual and molecular representations did not differ in these areas (A1, <i>P</i>â€‰=â€‰0.972; wm, <i>P</i>â€‰=â€‰0.671, two-tailed bootstrap comparison; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig2">2b</a>).</p><p>In a next step, we examined the granularity of perceptual information represented in these patterns. The perceptual similarity among odors can be defined at a coarse and a fine-grained level. At the coarse level, perceptual similarity is defined based on a small set of dominant descriptors. In contrast, at the fine-grained level, similarity is defined based on a larger set of descriptors, even if they are not dominant features of the percept. For example, peppermint and spearmint are similar at a coarse level because they are both rated highly on the â€˜coolâ€™ dimension, but these odors differ at a fine-grained level because peppermint has an additional (but nondominant) â€˜spicyâ€™ note. Our goal was to test whether neural representations in different olfactory brain areas were more consistent with coarse or fine-grained perceptual similarity. To that end, we compared two perceptual similarity models. In the first model, we defined perceptual similarity in a coarse perceptual space, capturing only the extent to which two odors were rated highly on one common dominant perceptual descriptor (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3a</a>) (for example, the product of ratings for peppermint and spearmint on the most common â€˜coolâ€™ dimension). In the second model, we defined perceptual similarity in a fine-grained perceptual space that included 16 descriptors (for example, the correlation between the perceptual ratings for peppermint and spearmint). Note, we excluded intensity and pleasantness from both analyses to ensure that results were not driven by these two valence-related descriptors. To illustrate this point: if the analysis included pleasantness, two odors such as apple and cinnamon would be considered coarsely similar because they are both highly pleasant, even though their percepts are considerably different. We also examined neural representations of pleasantness and intensity separately, as well as perceptual representations after regressing out intensity and pleasantness from the perceptual descriptors (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig11">6a,b</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Neural activity patterns represent fine-grained odor percepts."><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3: Neural activity patterns represent fine-grained odor percepts.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="588"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p><b>a</b>, Perceptual ratings (excluding intensity and pleasantness) for two odors (ethyl propionate and ethyl butanoate) and the element-wise product of their ratings (green), which is maximal for the fruity dimension. Coarse similarity between two odors was defined as the element-wise product of their most dominant perceptual descriptor ratings. <b>b</b>, Coarse versus fine-grained RSA. Neural representation of coarse perceptual similarity (<i>r</i><sub>c</sub>, hatched bars) and fine-grained perceptual similarity (<i>r</i><sub>f</sub>, solid bars) is significant in the PirT, AMY and OFC but not in the PirF, A1 or wm (PirF, <i>r</i><sub>c</sub>â€‰=â€‰0.004, <i>P</i>â€‰=â€‰0.290; PirT, <i>r</i><sub>c</sub>â€‰=â€‰0.015, <i>P</i>â€‰=â€‰0.0012; AMY, <i>r</i><sub>c</sub>â€‰=â€‰0.024, <i>P</i>â€‰=â€‰0.0000; OFC, <i>r</i><sub>c</sub>â€‰=â€‰0.063, <i>P</i>â€‰=â€‰0.0000; A1, <i>r</i><sub>c</sub>â€‰=â€‰0.005, <i>P</i>â€‰=â€‰0.162; wm, <i>r</i><sub>c</sub>â€‰=â€‰0.002, <i>P</i>â€‰=â€‰0.510; PirF, <i>r</i><sub>f</sub>â€‰=â€‰âˆ’0.0003, <i>P</i>â€‰=â€‰0.965; PirT, <i>r</i><sub>f</sub>â€‰=â€‰0.021, <i>P</i>â€‰=â€‰0.0000; AMY, <i>r</i><sub>f</sub>â€‰=â€‰0.032, <i>P</i>â€‰=â€‰0.0000; OFC, <i>r</i><sub>f</sub>â€‰=â€‰0.083, <i>P</i>â€‰=â€‰0.0000; A1, <i>r</i><sub>f</sub>â€‰=â€‰0.007, <i>P</i>â€‰=â€‰0.085; wm, <i>r</i><sub>f</sub>â€‰=â€‰âˆ’0.002, <i>P</i>â€‰=â€‰0.572; two-tailed percentile bootstrap). Furthermore, <i>r</i><sub>f</sub> is significantly higher than <i>r</i><sub>c</sub> in the PirT, AMY and OFC, but not in the PirF, A1 or wm (PirF, <i>P</i>â€‰=â€‰0.104; PirT, <i>P</i>â€‰=â€‰0.012; AMY, <i>P</i>â€‰=â€‰0.002; OFC, <i>P</i>â€‰=â€‰0.0000; A1, <i>P</i>â€‰=â€‰0.556; wm, <i>P</i>â€‰=â€‰0.060; two-tailed percentile bootstrap). The <i>r</i><sub>f</sub> is also significantly higher than <i>r</i><sub>c</sub> in the PirT (<i>P</i>â€‰=â€‰0.040), AMY (<i>P</i>â€‰=â€‰0.020) and OFC (<i>P</i>â€‰=â€‰0.0000) when corrected for multiple comparisons across areas (FDR correction). For subject-wise data, see Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig8">3</a> and Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01414-4#MOESM1">2</a>. <b>c</b>, Difference between the neural representation of fine-grained and coarse perceptual similarity is significantly larger in the OFC than in the PirF, PirT and AMY (<i>P</i>â€‰=â€‰0.0000, two-tailed bootstrap comparison). <b>d</b>, The <i>t</i>-value of the difference between the neural representation of fine-grained and coarse perceptual similarity depicted in <b>c</b> for the OFC, computed for odor sets of different sizes. The difference is significant when at least 90 odors are included (threshold <i>P</i>â€‰=â€‰0.05, two-tailed bootstrap comparison, solid line). In all panels, bars depict mean effects and error bars depict 95% CIs (<i>n</i>â€‰=â€‰3 subjects, 12,720 odor pairs per subject). S1, S2 and S3 indicate subjects 1, 2 and 3. We obtained identical results from additional shuffle tests for representational similarities (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig10">5b</a>). Coarse and fine-grained representational similarities in A1 are significantly smaller than the effects found in the PirT, AMY and OFC (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig10">5c</a>) and <i>r</i><sub>p</sub> in A1 is driven exclusively by odor intensity (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig11">6b</a>). *<i>P</i>â€‰&lt;â€‰0.05; **<i>P</i>â€‰&lt;â€‰0.01; ***<i>P</i>â€‰&lt;â€‰0.001.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Comparing coarse and fine-grained perceptual representations revealed that fMRI responses in the PirT, AMY and OFC were significantly better explained by fine-grained compared with coarse perceptual similarity (<i>P</i>â€‰&lt;â€‰0.005, bootstrap comparison; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3b</a>). This effect was most prominent in the OFC, where the difference was significantly larger compared with the other areas (<i>P</i>â€‰=â€‰0.0000, bootstrap comparison; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3c</a> and Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig8">3</a>), suggesting a key role for the OFC in representing rich olfactory percepts. In fact, the OFC was the only area in which the representation of fine-grained similarity significantly exceeded the coarse similarity in all three subjects (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig8">3</a>). This result remained significant when controlling for various factors (for example, differences in ROI sizes, odor similarity in the molecular space, inclusion of intensity and pleasantness, exclusion of undetectable odors and differences in hemodynamic responses; Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4</a> and Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01414-4#MOESM1">2</a>). Moreover, representational similarity in the OFC increased linearly when perceptual similarity was gradually transformed from coarse to fine-grained by progressively increasing the number of perceptual dimensions (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig12">7</a>).</p><p>A key motivation for our high-precision approach was the expectation that discriminating between odor coding at different levels of granularity requires many odors. To quantify the number of odors needed, we compared neural representations of coarse and fine-grained perceptual similarity in the OFC in several randomly drawn subsets of our data, each with a different number of odors. This revealed that dissociating between neural representations of coarse and fine-grained perceptual similarity was only possible when at least 90 odors were included in the analysis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3d</a>). This highlights the unique advantage of our approach to image responses to an extensive odor set, because smaller datasets would be insufficient to distinguish between neural representations of olfactory percepts with different levels of granularity.</p><h3 class="c-article__sub-heading" id="Sec5">Modeling odor responses using individual perceptual spaces</h3><p>The previous analyses demonstrate that olfactory cortices represent odor percepts at different levels of granularity, such that fine-grained percepts are most prominently represented in the OFC. Taking this a step further, we sought to characterize the coding scheme that maps activity patterns on to odor percepts in the olfactory areas. To test our hypothesis that neural activity encodes the dimensions of perceptual odor spaces, we constructed individual encoding models with perceptual descriptor ratings as the basic functions (that is, garlic, mint, floral, fish). We then tested whether these models could accurately predict a given subjectâ€™s fMRI responses to new odor stimuli, based on their perceptual ratings (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4a</a>). To train the model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4a</a>, left), we used ridge regression to estimate voxel-wise encoding weights for individual perceptual features, such that features were weighted to fit fMRI activity optimally. For testing the model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4a</a>, right), we used those encoding weights (derived from the training model) to predict fMRI responses in an out-of-sample set of test odors. We found that average prediction accuracy was significantly above chance in the PirF, PirT, AMY and OFC (<i>P</i>â€‰&lt;â€‰0.05, false discovery rate (FDR) corrected, one-sample Studentâ€™s <i>t</i>-test; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4bâ€“d</a>), demonstrating that neural responses in these areas do represent the perceptual features included in the computational models.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Modeling odor-evoked activity using individual perceptual spaces."><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4: Modeling odor-evoked activity using individual perceptual spaces.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="552"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p><b>a</b>, Schematic of the voxel-wise encoding model. The model predicts voxel-wise fMRI activity based on olfactory perceptual features (for example, garlic (white), mint (green), fish (blue), and so on for both training odor on the left and test odor on the right). In the model training step (left), voxel-wise encoding weights for perceptual features are estimated to optimally fit fMRI activity (fMRI response, black bars; model fits, magenta bars; individual odors denoted by shapes). In model testing (right), estimated encoding weights are used to predict fMRI responses to an out-of-sample set of test odors using olfactory perpetual ratings as input. Prediction accuracy is defined as Pearsonâ€™s correlation between the predicted and observed fMRI responses. <b>b</b>, Voxels in olfactory ROIs with significant out-of-sample prediction accuracy for individual subjects (threshold <i>P</i>â€‰=â€‰0.05, one-tailed, one-sample Studentâ€™s <i>t</i>-test, FDR corrected). <b>c</b>, Average prediction accuracy in odor-responsive gray matter voxels by ROI. <b>d</b>, Percentage of odor-responsive gray matter voxels with significant prediction accuracy (threshold <i>P</i>â€‰=â€‰0.05, one-tailed, one-sample Studentâ€™s <i>t</i>-test, FDR corrected). <b>e</b>, Magnitude of absolute encoding weights averaged across significant voxels by ROI. Dark lines illustrate significant encoding weights (threshold <i>P</i>â€‰=â€‰0.05, FWE-corrected, two-tailed percentile bootstrap). <b>f</b>, Cumulative percentage of explained variance in the voxel-wise encoding weights as a function of the number of PCs, averaged across subjects by ROI. <b>g</b>, Dimensionality (<i>Îº</i>) is proportional to the area under the curves depicted in <b>f</b> and reflects the number of PCs required to explain a given percentage of variance. The dimensionality of perceptual encoding increases from the PirF and PirT to the AMY, and from the AMY to the OFC (<i>P</i>â€‰=â€‰0.0000, for all pairs except PirFâ€“PirT, <i>P</i>â€‰=â€‰0.102, two-tailed bootstrap comparison). This increase in dimensionality was consistently observed in all subjects individually and was robust when accounting for differences in ROI size (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig14">9</a>). Encoding models in the control areas revealed only low prediction accuracies (A1, mean <i>r</i>â€‰=â€‰0.027; wm, mean <i>r</i>â€‰=â€‰0.045; Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig13">8i,j</a>). In all panels, bars denote mean effects and error bars depict 95% CIs (percentile bootstrap) based on <i>n</i>â€‰=â€‰3 subjects, 160 odors per subject. ***<i>P</i>â€‰&lt;â€‰0.001.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>To examine the degree to which neural encoding was specific for idiosyncratic perceptual spaces, we evaluated encoding models using perceptual ratings provided by different subjects. The prediction accuracy of the encoding model was higher when subject-specific rather than cross-subject descriptor ratings were used in the model (<i>F</i><sub>1,15</sub>â€‰=â€‰12.58, <i>P</i>â€‰=â€‰0.016, repeated-measures, two-way analysis of variance (ANOVA); Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig15">10</a>), suggesting that neural encoding reflects subjective rather than generic perceptual features.</p><p>Moreover, to reveal which brain areas encoded which perceptual features, we examined the distribution of encoding weights across brain areas. It is interesting that the feature weights of the encoding models differed substantially across brain areas, indicating that perceptual dimensions were not uniformly encoded. Specifically, the PirF and PirT encoded very few perceptual features, most prominently intensity and chemical-like. In contrast, the AMY and OFC encoded several additional features, including acidic, sweaty, fruity and bakery (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4e</a>). This suggests a progression in the complexity of odor percept encoding from the PirC to the OFC, and implicates the OFC as a candidate region for representing detailed and subjective odor percepts.</p><p>To gain more insight into the architecture of the perceptual spaces encoded in different olfactory areas, we next sought to quantify their dimensionality. As perceptual descriptor ratings are often correlated (for example, sweet and fruity are often rated similarly), the number of significant encoding weights may overestimate the dimensionality of the encoded perceptual space. We therefore determined the number of orthogonal perceptual dimensions encoded in each area by conducting a principal component analysis (PCA) on the voxel-wise encoding weights. We then examined the number of components required to explain a given percentage of variance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4f</a>). We defined the dimensionality of encoding (<i>Îº</i>) based on the area under the resulting curve. This analysis revealed that the dimensionality of the encoded perceptual space differed significantly across areas, increasing from the PirC to the AMY to the OFC (<i>P</i>â€‰&lt;â€‰0.05, FWE (family-wise error)-corrected, bootstrap comparison; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4g</a>). This finding shows that, whereas the PirC and AMY represent low-dimensional perceptual spaces, the OFC represents a high-dimensional space that is capable of encoding a large number of unique and detailed odor percepts.</p><h3 class="c-article__sub-heading" id="Sec6">Encoding of idiosyncratic perceptual spaces in the OFC</h3><p>The previous findings suggest that detailed olfactory experiences may be represented in high-dimensional perceptual odor spaces encoded in the OFC. In a final step, we sought to explicitly determine whether these perceptual spaces also reflect the idiosyncratic nature of odor perception. To address this, we compared perceptual encoding weights across all three subjects for each brain region of interest (ROI) separately. We found that encoding weights were consistent across subjects in the PirC and AMY, but more idiosyncratic in the OFC. Specifically, encoding weights in the OFC generalized significantly less compared with those observed in the other olfactory ROIs (<i>P</i>â€‰&lt;â€‰0.05, FWE-corrected, bootstrap comparison; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig5">5a</a>). It is interesting that this lack of generalizability in the OFC varied by the order of the perceptual dimensions. When considering cross-subject correlations for the first four principal components (PCs) in this region (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig5">5b</a>), we found that the first PC was significantly more consistent across subjects compared with the subsequent components (<i>F</i><sub>3,8</sub>â€‰=â€‰13.41, <i>P</i>â€‰=â€‰0.002; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig5">5c</a>). Notably, the first PC primarily reflected odor intensity (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig5">5d</a>), indicating that, although low-level perceptual dimensions in the OFC were consistently encoded across subjects, higher-order dimensions reflected a higher level of individuality.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Encoding of idiosyncratic perceptual spaces in the OFC."><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5: Encoding of idiosyncratic perceptual spaces in the OFC.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="546"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p><b>a</b>, Box plots of correlation coefficients between voxel-wise encoding weights across subjects in each ROI. Encoding weights in the PirF, PirT and AMY are significantly more similar across subjects than encoding weights in the OFC (<i>r</i><sup>2</sup> (OFC)â€‰&lt;â€‰<i>r</i><sup>2</sup> (PirF), <i>P</i>â€‰=â€‰0.0000; <i>r</i><sup>2</sup> (OFC)â€‰&lt;â€‰<i>r</i><sup>2</sup> (PirT), <i>P</i>â€‰=â€‰0.0002; <i>r</i><sup>2</sup> (OFC)â€‰&lt;â€‰<i>r</i><sup>2</sup> (AMY), <i>P</i>â€‰=â€‰0.030, two-tailed bootstrap comparison). The same number of voxel pairs was selected in all areas. The center lines correspond to the median, the box limits are upper and lower quartiles, the whiskers denote 1.5Ã— interquartile range and points are outliers. <b>b</b>, Intersubject correlation matrix for the first four PCs of encoding weights in the OFC. The correlations of PCA coefficients across subjects for matching PCs (matched using the â€˜stable marriageâ€™ algorithm) are highlighted in magenta triads. <b>c</b>, Average intersubject correlation of different PCs in the OFC. Bars denote mean effects and error bars depict 95% CIs. PC1 is significantly more consistent across subjects than PC2â€“PC4 (<i>F</i><sub>3,8</sub>â€‰=â€‰13.41, <i>P</i>â€‰=â€‰0.002, one-way ANOVA). Lines show cross-subject correlation for individual subject pairs. <b>d</b>, Average PCA coefficients of perceptual feature weights for different PCs. PC1 is primarily driven by intensity, whereas subsequent components are more heterogeneous. S1, S2 and S3 indicate subjects 1, 2 and 3. PCA coefficients for the PirF, PirT and AMY are shown in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig14">9e</a>. In all panels, effects are based on <i>n</i>â€‰=â€‰3 subjects, 160 odors per subject. *<i>P</i>â€‰&lt;â€‰0.05.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01414-4/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Discussion</h2><div class="c-article-section__content" id="Sec7-content"><p>The olfactory system is tasked with synthesizing subjective odor percepts from the objective physiochemical properties of volatile molecules<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Keller, A. et al. Predicting human olfactory perception from chemical features of odor molecules. Science 355, 820â€“826 (2017)." href="/articles/s41593-023-01414-4#ref-CR17" id="ref-link-section-d54222645e1517">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Snitz, K. et al. Predicting odor perceptual similarity from odor structure. PLoS Comput. Biol. 9, e1003184 (2013)." href="/articles/s41593-023-01414-4#ref-CR28" id="ref-link-section-d54222645e1520">28</a></sup>, but the neural coding scheme underlying these percepts has remained elusive. Using a combination of computational modeling and high-precision functional mapping, we demonstrate that olfactory brain areas represent odors using a perceptual coding scheme. Most importantly, we show that this coding scheme increases in dimensionality and subjectivity from the PirC to the OFC.</p><p>Although neural ensemble coding of odors has been a prime focus of several studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Miura, K., Mainen, Z. F. &amp; Uchida, N. Odor representations in olfactory cortex: distributed rate coding and decorrelated population activity. Neuron 74, 1087â€“1098 (2012)." href="#ref-CR9" id="ref-link-section-d54222645e1527">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pashkovski, S. L. et al. Structure and flexibility in cortical representations of odour space. Nature 583, 253â€“258 (2020)." href="#ref-CR10" id="ref-link-section-d54222645e1527_1">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Stettler, D. D. &amp; Axel, R. Representations of odor in the piriform cortex. Neuron 63, 854â€“864 (2009)." href="#ref-CR11" id="ref-link-section-d54222645e1527_2">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Sosulski, D. L., Bloom, M. L., Cutforth, T., Axel, R. &amp; Datta, S. R. Distinct representations of olfactory information in different cortical centres. Nature 472, 213â€“216 (2011)." href="#ref-CR12" id="ref-link-section-d54222645e1527_3">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Miyamichi, K. et al. Cortical representations of olfactory input by trans-synaptic tracing. Nature 472, 191â€“196 (2011)." href="#ref-CR13" id="ref-link-section-d54222645e1527_4">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Roland, B., Deneux, T., Franks, K. M., Bathellier, B. &amp; Fleischmann, A. Odor identity coding by distributed ensembles of neurons in the mouse olfactory cortex. eLife 6, e26337 (2017)." href="#ref-CR14" id="ref-link-section-d54222645e1527_5">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Howard, J. D., Plailly, J., Grueschow, M., Haynes, J. D. &amp; Gottfried, J. A. Odor quality coding and categorization in human posterior piriform cortex. Nat. Neurosci. 12, 932â€“938 (2009)." href="/articles/s41593-023-01414-4#ref-CR15" id="ref-link-section-d54222645e1530">15</a></sup>, a critical gap remains in our understanding of how subjective odor percepts are represented in the brain. Bridging this gap requires capturing perceptual and neural odor responses at a high level of granularity in individual subjects. However, pioneering work in the past has tended to (1) probe individual perceptual properties rather than multiple perceptual dimensions in parallel<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Bolding, K. A. &amp; Franks, K. M. Complementary codes for odor identity and intensity in olfactory cortex. eLife 6, e22630 (2017)." href="/articles/s41593-023-01414-4#ref-CR16" id="ref-link-section-d54222645e1534">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Yeshurun, Y. &amp; Sobel, N. An odor is not worth a thousand words: from multidimensional odors to unidimensional odor objects. Annu. Rev. Psychol. 61, 219â€“241 (2010)." href="/articles/s41593-023-01414-4#ref-CR29" id="ref-link-section-d54222645e1537">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Sirotin, Y. B., Shusterman, R. &amp; Rinberg, D. Neural coding of perceived odor intensity. eNeuro &#xA;                  https://doi.org/10.1523/ENEURO.0083-15.2015&#xA;                  &#xA;                 (2015)." href="/articles/s41593-023-01414-4#ref-CR30" id="ref-link-section-d54222645e1540">30</a></sup>; (2) use small numbers of stimuli, often limited to a few unique odorants<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Winston, J. S., Gottfried, J. A., Kilner, J. M. &amp; Dolan, R. J. Integrated neural representations of odor intensity and affective valence in human amygdala. J. Neurosci. 25, 8903â€“8907 (2005)." href="/articles/s41593-023-01414-4#ref-CR31" id="ref-link-section-d54222645e1544">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Anderson, A. K. et al. Dissociated neural representations of intensity and valence in human olfaction. Nat. Neurosci. 6, 196â€“202 (2003)." href="/articles/s41593-023-01414-4#ref-CR32" id="ref-link-section-d54222645e1547">32</a></sup>; and (3) collect a relatively small number of neural responses across a large number of participants<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Howard, J. D., Plailly, J., Grueschow, M., Haynes, J. D. &amp; Gottfried, J. A. Odor quality coding and categorization in human posterior piriform cortex. Nat. Neurosci. 12, 932â€“938 (2009)." href="/articles/s41593-023-01414-4#ref-CR15" id="ref-link-section-d54222645e1551">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Lapid, H. et al. Neural activity at the human olfactory epithelium reflects olfactory perception. Nat. Neurosci. 14, 1455â€“1461 (2011)." href="/articles/s41593-023-01414-4#ref-CR33" id="ref-link-section-d54222645e1554">33</a></sup>. In the present study, we address this gap by implementing a high-precision mapping approach<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Gratton, C., Nelson, S. M. &amp; Gordon, E. M. Brain-behavior correlations: two paths toward reliability. Neuron 110, 1446â€“1449 (2022)." href="/articles/s41593-023-01414-4#ref-CR34" id="ref-link-section-d54222645e1558">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Rosenberg, M. D. &amp; Finn, E. S. How to establish robust brainâ€“behavior relationships without thousands of individuals. Nat. Neurosci. 25, 835â€“837 (2022)." href="/articles/s41593-023-01414-4#ref-CR35" id="ref-link-section-d54222645e1561">35</a></sup>, which involved collecting detailed perceptual ratings and fMRI responses to a large set of 160 unique odorants in individual subjects over several experimental sessions. We used these ratings to characterize subjective odor percepts, which we subsequently mapped on to neural responses using computational modeling. This approach allowed us to make several important conceptual advances in our understanding of the human cortical mechanisms of olfactory coding.</p><p>First, we show that olfactory brain areas represent odor stimuli using a perceptual code. Although information about the molecular structure was significantly represented in the PirC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gottfried, J. A., Winston, J. S. &amp; Dolan, R. J. Dissociable codes of odor quality and odorant structure in human piriform cortex. Neuron 49, 467â€“479 (2006)." href="/articles/s41593-023-01414-4#ref-CR36" id="ref-link-section-d54222645e1568">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Fournel, A., Ferdenzi, C., Sezille, C., Rouby, C. &amp; Bensafi, M. Multidimensional representation of odors in the human olfactory cortex. Hum. Brain Mapp. 37, 2161â€“2172 (2016)." href="/articles/s41593-023-01414-4#ref-CR37" id="ref-link-section-d54222645e1571">37</a></sup>, this was exceeded by encoding of perceptual information in the PirT and OFC (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig2">2a,b</a>). Second, we examined encoding of information about the fine-grained perceptual identity of odors that is not otherwise explained by perceptual coding at a coarse level. Representing the fine-grained perceptual identity requires high-dimensional perceptual spaces and was more prominently found in the OFC, suggesting that this brain region encodes richer and more subjective olfactory percepts (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3aâ€“d</a>). Third, computational analysis using encoding modelsâ€”by which perceptual features were directly mapped on to neural responsesâ€”suggests that the brain encodes odor percepts as a composition of principal perceptual dimensions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4aâ€“e</a>). Last, the dimensionality of the perceptual spaces increased from the PirC and AMY to the OFC (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4f,g</a>), with higher-order dimensions reflecting the subjective nature of olfactory perception (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig5">5</a>).</p><p>It is worth noting that the conceptualization of coarse and fine-grained perceptual identity mirrors related ideas in the olfactory literature about odor category and identity. Indeed, odor stimuli can be grouped into perceptual classes or categories (for example, citric, woody), based on their perceptual similarity along a small number of dominant perceptual notes that are broadly applicable across a large number of odors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Roland, B., Deneux, T., Franks, K. M., Bathellier, B. &amp; Fleischmann, A. Odor identity coding by distributed ensembles of neurons in the mouse olfactory cortex. eLife 6, e26337 (2017)." href="/articles/s41593-023-01414-4#ref-CR14" id="ref-link-section-d54222645e1594">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Howard, J. D., Plailly, J., Grueschow, M., Haynes, J. D. &amp; Gottfried, J. A. Odor quality coding and categorization in human posterior piriform cortex. Nat. Neurosci. 12, 932â€“938 (2009)." href="/articles/s41593-023-01414-4#ref-CR15" id="ref-link-section-d54222645e1597">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Zelano, C., Mohanty, A. &amp; Gottfried, J. A. Olfactory predictive codes and stimulus templates in piriform cortex. Neuron 72, 178â€“187 (2011)." href="/articles/s41593-023-01414-4#ref-CR38" id="ref-link-section-d54222645e1600">38</a></sup>. In the present study, our conceptualization of coarse odor similarity probes the representation of odor percepts in a low-dimensional space defined by their dominant perceptual notes, akin to odor category. In contrast, high-dimensional odor representations capture the percept specifically pertaining to the identity of the odor object. We found that the difference between the neural representation of coarse and fine-grained perceptual identity increased from the PirC to the OFC. In addition, and compatible with previous work<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Howard, J. D., Plailly, J., Grueschow, M., Haynes, J. D. &amp; Gottfried, J. A. Odor quality coding and categorization in human posterior piriform cortex. Nat. Neurosci. 12, 932â€“938 (2009)." href="/articles/s41593-023-01414-4#ref-CR15" id="ref-link-section-d54222645e1604">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gottfried, J. A., Winston, J. S. &amp; Dolan, R. J. Dissociable codes of odor quality and odorant structure in human piriform cortex. Neuron 49, 467â€“479 (2006)." href="/articles/s41593-023-01414-4#ref-CR36" id="ref-link-section-d54222645e1607">36</a></sup>, we observed a functional dissociation in the PirC along the anteroposterior axis, such that the posterior PirC (PirT) contained distinguishable representations of coarse and fine-grained odor percepts, whereas the anterior PirC (PirF), despite being physically nearer to the OFC, did not. Of note, we found that representations of odor intensity were comparable in the PirF and A1 (Extended Data Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig10">5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig11">6</a>). Given that odor intensity was related to sniff parameters (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig11">6c,d</a>), it is possible that these intensity-related fMRI signals reflect residual sniff-related activity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Sobel, N. et al. Sniffing and smelling: separate subsystems in the human olfactory cortex. Nature 392, 282â€“286 (1998)." href="/articles/s41593-023-01414-4#ref-CR39" id="ref-link-section-d54222645e1621">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Kareken, D. A. et al. Olfactory system activation from sniffing: effects in piriform and orbitofrontal cortex. NeuroImage 22, 456â€“465 (2004)." href="/articles/s41593-023-01414-4#ref-CR40" id="ref-link-section-d54222645e1624">40</a></sup>. However, accounting for sniff parameters did not change our fMRI results in the A1 or elsewhere (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig11">6e,f</a>), suggesting that rudimentary olfactory perceptual information is transmitted to other sensory areas, presumably to facilitate multisensory integration and associative learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Cohen, L., Rothschild, G. &amp; Mizrahi, A. Multisensory integration of natural odors and sounds in the auditory cortex. Neuron 72, 357â€“369 (2011)." href="/articles/s41593-023-01414-4#ref-CR41" id="ref-link-section-d54222645e1631">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Olofsson, J. K., Zhou, G., East, B. S., Zelano, C. &amp; Wilson, D. A. Odor identification in rats: behavioral and electrophysiological evidence of learned olfactory-auditory associations. eNeuro &#xA;                  https://doi.org/10.1523/ENEURO.0102-19.2019&#xA;                  &#xA;                 (2019)." href="/articles/s41593-023-01414-4#ref-CR42" id="ref-link-section-d54222645e1634">42</a></sup>. The detailed description of how olfactory information is conveyed to other brain areas remains to be explored in future studies.</p><p>Moreover, we found that lower-order perceptual dimensions in the PirC were stable across individuals, which may allow for generalization of basic odor categories and features. In contrast, higher-order dimensions in the OFC differed across individuals, suggesting that this region could facilitate fine discrimination and individuation, that is, higher-order perceptual dimensions could allow odor perception to be shaped by experience, learning and context<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Li, W., Howard, J. D., Parrish, T. B. &amp; Gottfried, J. A. Aversive learning enhances perceptual and cortical discrimination of indiscriminable odor cues. Science 319, 1842â€“1845 (2008)." href="#ref-CR43" id="ref-link-section-d54222645e1642">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bae, J., Yi, J.-Y. &amp; Moon, C. Odor quality profile is partially influenced by verbal cues. PLoS ONE 14, e0226385 (2019)." href="#ref-CR44" id="ref-link-section-d54222645e1642_1">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Wang, P. Y. et al. Transient and persistent representations of odor value in prefrontal cortex. Neuron 108, 209â€“224. e206 (2020)." href="/articles/s41593-023-01414-4#ref-CR45" id="ref-link-section-d54222645e1645">45</a></sup> without abandoning the core character of the percept. This would allow us to perceive our olfactory environment in an individualized and dynamic way, while still maintaining a basic understanding of how the world smells to others.</p><p>A limitation of any encoding experiment is that neural encoding of the hypothesized stimulus features cannot be fully dissociated from other correlated variables<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Kriegeskorte, N. &amp; Douglas, P. K. Interpreting encoding and decoding models. Curr. Opin. Neurobiol. 55, 167â€“179 (2019)." href="/articles/s41593-023-01414-4#ref-CR46" id="ref-link-section-d54222645e1652">46</a></sup>. For instance, perceptual features such as perceived pleasantness are associated with specific behaviors (for example, approach or avoidance), and neural encoding may thus be driven by the percept, the behavior or a combination of the two. The implication is that some brain areas may encode high-dimensional perceptual spaces because they also encode for the associated behaviors. Although perceptual representations in our data remained significant after regressing out features with high behavioral salience (such as intensity and pleasantness; Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4e</a>), we are unable to rule out whether higher dimensions encoded in the OFC could still be partially driven by associated behaviors. Relatedly, it remains to be tested whether similar perceptual representational forms exist for other sensory modalities in areas such as the OFC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Schulze, P., Bestgen, A.-K., Lech, R. K., Kuchinke, L. &amp; Suchan, B. Preprocessing of emotional visual information in the human piriform cortex. Sci. Rep. 7, 9191 (2017)." href="/articles/s41593-023-01414-4#ref-CR47" id="ref-link-section-d54222645e1659">47</a></sup>. In addition, although it is difficult to quantify the extent to which semantics and verbal labels contribute to neural activity, we minimized such variability by training subjects extensively on the perceptual descriptors before the scanning sessions (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-023-01414-4#Sec8">Methods</a>).</p><p>Intriguingly, our results challenge the view that perceptual odor identity, at its finest level of granularity, is generated in the olfactory bulb<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Wilson, C. D., Serrano, G. O., Koulakov, A. A. &amp; Rinberg, D. A primacy code for odor identity. Nat. Commun. 8, 1477 (2017)." href="/articles/s41593-023-01414-4#ref-CR48" id="ref-link-section-d54222645e1669">48</a></sup> or PirC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Pashkovski, S. L. et al. Structure and flexibility in cortical representations of odour space. Nature 583, 253â€“258 (2020)." href="/articles/s41593-023-01414-4#ref-CR10" id="ref-link-section-d54222645e1673">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Roland, B., Deneux, T., Franks, K. M., Bathellier, B. &amp; Fleischmann, A. Odor identity coding by distributed ensembles of neurons in the mouse olfactory cortex. eLife 6, e26337 (2017)." href="/articles/s41593-023-01414-4#ref-CR14" id="ref-link-section-d54222645e1676">14</a></sup>, whereas the OFC contributes to olfaction indirectly by supporting secondary cognitive functions related to reward, context and decision-making<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gottfried, J. A., Oâ€™Doherty, J. &amp; Dolan, R. J. Encoding predictive reward value in human amygdala and orbitofrontal cortex. Science 301, 1104â€“1107 (2003)." href="#ref-CR49" id="ref-link-section-d54222645e1680">49</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bowman, N. E., Kording, K. P. &amp; Gottfried, J. A. Temporal integration of olfactory perceptual evidence in human orbitofrontal cortex. Neuron 75, 916â€“927 (2012)." href="#ref-CR50" id="ref-link-section-d54222645e1680_1">50</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Zhou, J. et al. Rat orbitofrontal ensemble activity contains multiplexed but dissociable representations of value and task structure in an odor sequence task. Curr. Biol. 29, 897â€“907. e893 (2019)." href="/articles/s41593-023-01414-4#ref-CR51" id="ref-link-section-d54222645e1683">51</a></sup>. Instead, our findings suggest that, although PirC encodes basic odor category<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Howard, J. D., Plailly, J., Grueschow, M., Haynes, J. D. &amp; Gottfried, J. A. Odor quality coding and categorization in human posterior piriform cortex. Nat. Neurosci. 12, 932â€“938 (2009)." href="/articles/s41593-023-01414-4#ref-CR15" id="ref-link-section-d54222645e1687">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Bolding, K. A. &amp; Franks, K. M. Complementary codes for odor identity and intensity in olfactory cortex. eLife 6, e22630 (2017)." href="/articles/s41593-023-01414-4#ref-CR16" id="ref-link-section-d54222645e1690">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Bao, X., Raguet, L. L., Cole, S. M., Howard, J. D. &amp; Gottfried, J. A. The role of piriform associative connections in odor categorization. eLife 5, e13732 (2016)." href="/articles/s41593-023-01414-4#ref-CR52" id="ref-link-section-d54222645e1693">52</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Yoshida, I. &amp; Mori, K. Odorant category profile selectivity of olfactory cortex neurons. J. Neurosci. 27, 9105â€“9114 (2007)." href="/articles/s41593-023-01414-4#ref-CR53" id="ref-link-section-d54222645e1696">53</a></sup>, it is the OFCâ€”at its coreâ€”that represents the subjective identity of unique odor percepts. In other words, the OFCâ€™s contribution to olfaction is not limited to cognitive and affective operations when confronted with a given odor percept<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="De Araujo, I. E., Rolls, E. T., Velazco, M. I., Margot, C. &amp; Cayeux, I. Cognitive modulation of olfactory processing. Neuron 46, 671â€“679 (2005)." href="#ref-CR54" id="ref-link-section-d54222645e1700">54</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Keller, A. Attention and olfactory consciousness. Front. Psychol. 2, 380 (2011)." href="#ref-CR55" id="ref-link-section-d54222645e1700_1">55</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Li, W. et al. Right orbitofrontal cortex mediates conscious olfactory perception. Psychol. Sci. 21, 1454â€“1463 (2010)." href="/articles/s41593-023-01414-4#ref-CR56" id="ref-link-section-d54222645e1703">56</a></sup>. Rather, the OFC may meaningfully and actively shape the very odor percept itself. This proposal is in line with classic nonhuman primate work suggesting that individuation of odor coding increases from the olfactory bulb to the olfactory cortex to the OFC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Tanabe, T., Iino, M. &amp; Takagi, S. Discrimination of odors in olfactory bulb, pyriform-amygdaloid areas, and orbitofrontal cortex of the monkey. J. Neurophysiol. 38, 1284â€“1296 (1975)." href="/articles/s41593-023-01414-4#ref-CR57" id="ref-link-section-d54222645e1708">57</a></sup>, and may explain why the OFC lesions diminish odor discrimination but not the simple detection of odors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Potter, H. &amp; Butters, N. An assessment of olfactory deficits in patients with damage to prefrontal cortex. Neuropsychologia 18, 621â€“628 (1980)." href="/articles/s41593-023-01414-4#ref-CR58" id="ref-link-section-d54222645e1712">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Jones-Gotman, M. &amp; Zatorre, R. J. Olfactory identification deficits in patients with focal cerebral excision. Neuropsychologia 26, 387â€“400 (1988)." href="/articles/s41593-023-01414-4#ref-CR59" id="ref-link-section-d54222645e1715">59</a></sup>. By revealing this perceptual coding scheme, our findings provide fundamental insights into the cortical mechanisms of odor processing and suggest that how we perceive our olfactory environment critically depends on the architecture of the olfactory spaces encoded in the OFC.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Methods</h2><div class="c-article-section__content" id="Sec8-content"><h3 class="c-article__sub-heading" id="Sec9">Subjects</h3><p>The study was approved by Northwestern Universityâ€™s Institutional Review Board. Data from three healthy human subjects (two women aged 23â€“24, one man aged 24) were included in the present study. The subjects were right-handed, native English speakers with normal or corrected-to-normal vision. Subjects provided informed consent to participate in the study and reported no history of prior psychiatric or neurological disorders, no significant medical disorders and no smell and taste dysfunction. In addition, subjects did not have a history of sinusitis or allergic rhinitis and were not using medications that could affect alertness. Data from one additional subject were not included due to a psychiatric disorder that was disclosed after data collection had been completed. No statistical methods were used to predetermine sample sizes but our sample sizes are similar to those reported in previous publications on encoding models of sensory perception<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Naselaris, T., Prenger, R. J., Kay, K. N., Oliver, M. &amp; Gallant, J. L. Bayesian reconstruction of natural images from human brain activity. Neuron 63, 902â€“915 (2009)." href="/articles/s41593-023-01414-4#ref-CR26" id="ref-link-section-d54222645e1731">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Kay, K. N., Naselaris, T., Prenger, R. J. &amp; Gallant, J. L. Identifying natural images from human brain activity. Nature 452, 352â€“355 (2008)." href="/articles/s41593-023-01414-4#ref-CR27" id="ref-link-section-d54222645e1734">27</a></sup>. Subjects received monetary compensation amounting to US$40 per h for sessions involving fMRI (18â€‰h), US$20 per h for behavioral sessions outside the scanner (12â€“16â€‰h) and a study completion bonus of US$300.</p><h3 class="c-article__sub-heading" id="Sec10">Odor stimuli and presentation</h3><p>We used a total of 160 unique monomolecular odor stimuli per subject in the present study. We selected the odor stimuli based on the previously published database used in the DREAM olfaction challenge<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Keller, A. et al. Predicting human olfactory perception from chemical features of odor molecules. Science 355, 820â€“826 (2017)." href="/articles/s41593-023-01414-4#ref-CR17" id="ref-link-section-d54222645e1746">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Keller, A. &amp; Vosshall, L. B. Olfactory perception of chemically diverse molecules. BMC Neurosci. 17, 55 (2016)." href="/articles/s41593-023-01414-4#ref-CR60" id="ref-link-section-d54222645e1749">60</a></sup>. Odors were prepared with mineral oil or water as the solvent and varied in concentrations ranging from 0.001% to 10% (v:v) for liquid compounds and 0.1â€‰M for solid compounds. Odor stimuli were delivered directly to the nose using a customized, computer-controlled olfactometer equipped with two mass flow controllers (Alicat). Odors were further diluted with odorless air during delivery and presented through nasal masks (Phillips Respironics) at a constant flow rate of 4.8â€‰lâ€‰min<sup>âˆ’1</sup>. The breathing rate was monitored through breathing belts affixed to the chest, in subject 1 or through a pneumotachograph device and spirometer, in subjects 2 and 3 (AD instruments) (see â€˜<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-023-01414-4#Sec15">Odor-evoked responses and nuisance regressors</a>â€™ for details about sniff measurements).</p><h3 class="c-article__sub-heading" id="Sec11">Experimental design</h3><p>On their first visit, we tested subjectsâ€™ olfactory sensitivity using the Sniffinâ€™ Sticks threshold test (Burghardt). Subsequently, all subjects participated in 12 fMRI sessions (separate visits). Subjects S2 and S3 also participated in 10 additional behavioral sessions during which they rated the odors outside the scanner (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig7">2a</a>). During the first two of these behavioral sessions, S2 and S3 were presented with a list of perceptual descriptors used in the study (for example, fishy, fruity), and they smelled 80 training odors (flagged in the behavioral data table (Data availability)) and rated each of them on 1 of the 16 descriptors (5 odors per descriptor). Specifically, for each descriptor (except edibility and familiarity), we chose 3 odors that were rated high and 2 odors that were rated low for that descriptor by S1 (14 odors were selected based on perceptual ratings in the DREAM dataset). This training assured that subjects were well acquainted with perceptual descriptors before they were scanned and that the descriptors were interpreted similarly across subjects. In each of the remaining 8 behavioral sessions, subjects rated a set of 40 odors on all perceptual descriptors. Subjects rated odors that they could detect using a self-paced rating task. Behavioral ratings outside the scanner were not acquired for subject S1.</p><p>The 160 odors used per subject were divided into 4 odor sets, each containing 40 odors. Each set was presented in 3 different fMRI sessions, resulting in a total of 12. Each fMRI session consisted of four separate fMRI runs (20.5â€‰min each), during which ten odors were presented at least nine times each. Thus, the experiment consisted of a total of at least 4,320 odor trials per subject. The composition of odor sets was kept constant across sessions but, within each set, the assignment of odors to fMRI runs was randomized across sessions (that is, a different subset of 10 out of 40 odors was presented in each fMRI run across sessions). This ensured that, in each session, a given odor was presented in the context of different odors. Furthermore, in each fMRI run, odors were presented in a pseudo-randomized order to reduce any systematic bias in odor ratings based on preceding odors and biases in neural similarity due to task structure<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Cai, M. B., Schuck, N. W., Pillow, J. W. &amp; Niv, Y. Representational structure or task structure? Bias in neural representational similarity analysis and a Bayesian method for reducing bias. PLoS Comput. Biol. 15, e1006299 (2019)." href="/articles/s41593-023-01414-4#ref-CR61" id="ref-link-section-d54222645e1772">61</a></sup>.</p><p>On each trial of the first and second sessions of a given odor set, subjects were instructed to fixate on a central cross and make a consistent sniff when the cross changed color. Subjects reported whether they could smell the odor by pressing a button. If they indicated that they could smell the odor, they were asked to rate it on one of the perceptual descriptors. Only one perceptual descriptor was rated on each trial. Subjects reported the rating on a horizontal scale with a button press. For S2 and S3, the starting position and orientation of the scale were randomized at every trial to minimize confounds related to motor responses. In total, up to three ratings per descriptor and odor were acquired in each subject. In the third session of a given odor set, S2 and S3 were not asked to provide any ratings.</p><p>The percentage of odors with low detectability (undetectable in &gt;80% of trials) were 28% for S1, 0% for S2 and 13% for S3, respectively. The rated intensity of the undetectable odors for S1 was significantly lower than that of the detectable odors (<i>P</i>â€‰&lt;â€‰0.001, Studentâ€™s <i>t</i>-test). As odors were not rated on undetectable trials, ratings from behavioral sessions outside the scanner were used for subjects S2 and S3. Ratings acquired inside the scanner were significantly correlated with those obtained in the behavioral sessions for S2 and S3 (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig6">1b</a>). For S1, as behavioral ratings were not acquired outside the scanner, ratings were interpolated from the publicly available DREAM dataset for undetectable odors. The average correlation of ratings for detectable odors for S1 with that in the DREAM dataset was <i>r</i>â€‰=â€‰0.405 (<i>P</i>â€‰&lt;â€‰0.001, Studentâ€™s <i>t</i>-test). Excluding odors with low detectability had no qualitative impact on the results from the coarse versus fine RSA and the encoding model (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4d</a>). For all subjects, the final set of behavioral ratings used in the fMRI analyses was the average of all ratings acquired during the study (that is, ratings acquired during scanning sessions for S1 and ratings acquired during scanning and behavioral sessions for S2 and S3). The same experimental setup (olfactometer and odor-delivery apparatus) was used inside and outside the scanner. Subjects were blinded with respect to the order of the conditions. Double blinding was not relevant to the present study because the investigatorâ€™s knowledge of which odor was presented in each trial could not influence the behavioral or neural data or any subsequent analyses based on those data.</p><h3 class="c-article__sub-heading" id="Sec12">Perceptual descriptors</h3><p>Eighteen descriptors were used to characterize the perceptual feature space for each subject. The descriptors were selected based on recent studies in which chemical properties of molecules were mapped to perceptual properties of odors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Keller, A. et al. Predicting human olfactory perception from chemical features of odor molecules. Science 355, 820â€“826 (2017)." href="/articles/s41593-023-01414-4#ref-CR17" id="ref-link-section-d54222645e1812">17</a></sup>. The chosen perceptual dimensions were: pleasantness, intensity, fishy, burnt, sour, decayed, musky, fruity, sweaty, cool, floral, sweet, warm, bakery and spicy. In addition to these common descriptors, S1 rated acidic, garlic and chemical-like, whereas S2 and S3 rated ammonia, edibility and familiarity. Each subject therefore rated odors on 18 out of 21 perceptual descriptors. Ratings were normalized across [âˆ’1, 1] where the bounds correspond to the limits of the rating scale. We quantified the discriminability, reliability and generalizability of perceptual odor ratings (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig6">1</a>). We quantified the discriminability for each odor pair in this perceptual feature space by computing the distance (supremum norm) between each odor pair along the most dissimilar dimension and computed the percentage of odor pairs for which the distance exceeded 1â€‰s.d. Reliability for descriptors was quantified by computing Pearsonâ€™s <i>r</i> across different sessions of descriptor ratings for each subject. For the RSAs (see below) and to study the generalizability of perceptual ratings across subjects, we computed perceptual similarity matrices consisting of the correlations among the perceptual ratings for all odor pairs. The off-diagonal upper triangle entries of the similarity matrices were extracted to obtain vectors of perceptual similarities, <b>p</b>, for all pairs of distinct odors. We obtained the generalizability of perceptual ratings by calculating the correlation of perceptual similarity <b>p</b> across different subjects (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig6">1c,d</a>).</p><h3 class="c-article__sub-heading" id="Sec13">MRI data acquisition and preprocessing</h3><p>We used a 3-tesla PRISMA system (Siemens) with a 64-channel head/neck coil to acquire gradient echo T2*-weighted echoplanar images. The imaging sequence was optimized for signal recovery in olfactory areas (repetition timeâ€‰=â€‰1.4â€‰s, echo timeâ€‰=â€‰22â€‰ms, matrix sizeâ€‰=â€‰104â€‰Ã—â€‰96â€‰voxels<sup>2</sup>, flip angleâ€‰=â€‰80Â°, slices per imageâ€‰=â€‰42, in-plane resolutionâ€‰=â€‰2â€‰Ã—â€‰2â€‰mm<sup>2</sup>, slice thicknessâ€‰=â€‰2â€‰mm, acquisition angleâ€‰=â€‰30Â° rostral to the intercommissural line, multiband factorâ€‰=â€‰2). To further optimize the spatiotemporal coverage in S2 and S3, small adjustments to the scanning sequence were made (repetition timeâ€‰=â€‰1.4â€‰s, echo timeâ€‰=â€‰4â€‰ms, flip angleâ€‰=â€‰70Â°, matrix sizeâ€‰=â€‰122â€‰Ã—â€‰102â€‰voxels<sup>2</sup>, slices per imageâ€‰=â€‰38, in-plane resolutionâ€‰=â€‰1.7â€‰Ã—â€‰1.7â€‰mm<sup>2</sup>, slice thicknessâ€‰=â€‰2â€‰mm, acquisition angleâ€‰=â€‰30Â° rostral to the intercommissural line, multiband factorâ€‰=â€‰3). A set of high-resolution T1-weighed anatomical images (1â€‰mm<sup>3</sup> isotropic) were acquired using an MP-RAGE sequence to identify anatomical ROIs. Ten whole-brain echoplanar images were also acquired in each session to optimize the coregistration of functional and anatomical scans. All subjects wore customized 3D-milled Styrofoam headcases (Caseforge Co.) to minimize head motion during imaging. Headcases conformed to the MR coil on the outside and the shape of the subjectâ€™s head on the inside.</p><p>Images were preprocessed using statistical parametric mapping software (SPM12) in MATLAB. Functional images were realigned and coregistered to the average T1 anatomical image, and then images were smoothed using a 2-mm<sup>3</sup> isotropic Gaussian filter. All analyses were performed in the subjectsâ€™ native space.</p><h3 class="c-article__sub-heading" id="Sec14">Anatomical ROIs</h3><p>T1 anatomical images obtained from all scanning sessions were coregistered and averaged to generate an average structural image for each subject. We constructed masks of anatomical ROIs based on the Montreal Neurological Institute (MNI) masks used in previous studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Zhou, G., Lane, G., Cooper, S. L., Kahnt, T. &amp; Zelano, C. Characterizing functional pathways of the human olfactory system. eLife 8, e47177 (2019)." href="/articles/s41593-023-01414-4#ref-CR62" id="ref-link-section-d54222645e1864">62</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Echevarria-Cooper, S. L. et al. Mapping the microstructure and striae of the human olfactory tract with diffusion MRI. J. Neurosci. 42, 58â€“68 (2021)." href="/articles/s41593-023-01414-4#ref-CR63" id="ref-link-section-d54222645e1867">63</a></sup>, including the PirF, PirT, AMY and OFC. The PirC, AMY and OFC are widely known to be involved in olfactory perception<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Zald, D. H. &amp; Pardo, J. V. Emotion, olfaction, and the human amygdala: amygdala activation during aversive olfactory stimulation. Proc. Natl Acad. Sci. USA 94, 4119â€“4124 (1997)." href="/articles/s41593-023-01414-4#ref-CR64" id="ref-link-section-d54222645e1871">64</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Zatorre, R. J., Jones-Gotman, M., Evans, A. C. &amp; Meyer, E. Functional localization and lateralization of human olfactory cortex. Nature 360, 339â€“340 (1992)." href="/articles/s41593-023-01414-4#ref-CR65" id="ref-link-section-d54222645e1874">65</a></sup>. Although the hippocampus and entorhinal cortex may play important roles in olfactory memory, in contrast to the PirC, AMY and OFC, lesions to the hippocampus are less likely to produce olfactory deficits<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Burton, S., Murphy, D., Qureshi, U., Sutton, P. &amp; Oâ€™Keefe, J. Combined lesions of hippocampus and subiculum do not produce deficits in a nonspatial social olfactory memory task. J. Neurosci. 20, 5468â€“5475 (2000)." href="/articles/s41593-023-01414-4#ref-CR66" id="ref-link-section-d54222645e1878">66</a></sup> and so we decided not to focus on these regions for the present study. The MNI masks were inverse normalized to native space using each subjectâ€™s average T1 anatomical image and the resulting masks were manually refined to fit the anatomical boundaries in native space using ITK snap. All further analysis of the fMRI data was restricted to gray matter voxels (derived using segmentation of the T1 anatomical image) in these ROIs. We confirmed that the temporal signal:noise ratio<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Welvaert, M. &amp; Rosseel, Y. On the definition of signal-to-noise ratio and contrast-to-noise ratio for fMRI data. PLoS ONE 8, e77089 (2013)." href="/articles/s41593-023-01414-4#ref-CR67" id="ref-link-section-d54222645e1882">67</a></sup> was comparable across areas (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig7">2d</a>). As a control, we repeated the analyses in the primary auditory cortex (Heschlâ€™s area in the AAL Atlas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Tzourio-Mazoyer, N. et al. Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. Neuroimage 15, 273â€“289 (2002)." href="/articles/s41593-023-01414-4#ref-CR68" id="ref-link-section-d54222645e1890">68</a></sup>) and voxels in the white matter lateral to the anterior cingulate cortex. Sizes of all the ROIs are listed in Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01414-4#MOESM1">1</a>.</p><h3 class="c-article__sub-heading" id="Sec15">Odor-evoked responses and nuisance regressors</h3><p>To obtain the mean odor-evoked activity for each voxel, we first constructed a set of single-subject general linear models (GLMs). All fMRI volumes from a given subject were concatenated in a single design matrix that included a single covariate of event-related odor onsets (that is, time points where subjects were instructed to make a sniff response to odors). Translation and rotation parameters estimated during the realignment procedure were used as nuisance regressors to account for motion-related effects. We also included additional nuisance regressors to account for steep fluctuations in signal quality across volumes. Following previous work<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Howard, J. D. &amp; Kahnt, T. Identity prediction errors in the human midbrain update reward-identity expectations in the orbitofrontal cortex. Nat. Commun. 9, 1611 (2018)." href="#ref-CR69" id="ref-link-section-d54222645e1905">69</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Suarez, J. A., Howard, J. D., Schoenbaum, G. &amp; Kahnt, T. Sensory prediction errors in the human midbrain signal identity violations independent of perceptual distance. eLife 8, e43962 (2019)." href="#ref-CR70" id="ref-link-section-d54222645e1905_1">70</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Shanahan, L. K., Bhutani, S. &amp; Kahnt, T. Olfactory perceptual decision-making is biased by motivational state. PLoS Biol. 19, e3001374 (2021)." href="/articles/s41593-023-01414-4#ref-CR71" id="ref-link-section-d54222645e1908">71</a></sup>, we computed the mean difference between the average signal in odd and even slices (interleaved slice order), as well as the variance of the average signal across slices for each volume. Both of these measures are highly sensitive to head motion that occurs within a single volume and volumes with high values in these measures are probably corrupted by head motion. We flagged volumes for which the slice difference and variability measures exceeded 5â€‰units of signal:noise ratio (mean and s.d.) and then added volume-specific dummy regressors to the GLM, effectively excluding those volumes from the GLM estimation.</p><p>To account for sniff-related effects, sniff traces obtained from breathing measurements were included in the GLM as nuisance regressors. The breathing rate was monitored through breathing belts affixed to the chest in S1 or through a pneumotachograph device and spirometer (AD instruments) in S2 and S3. For all subjects, we used three sets of breathing regressors: breathing trace (volume of air in the chest), sniff trace (airflow at the nasal mask) and the squared amplitude of sniff trace. The breathing trace was obtained directly from the breathing belt (S1) or by computing the temporal integral of the trace obtained from the spirometer (S2 and S3). Similarly, the sniff trace was obtained directly from the spirometer (S2 and S3) or by computing the temporal derivative of the trace obtained from the breathing belt (S1). The sniff covariates were appended as regressors to the GLM and voxel-wise responses were estimated using the AR(1) regression model with nonsphericity correction in SPM12. We computed trial-wise sniff volumes and sniff durations for further analyses using the BreathMetrics toolbox<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Noto, T., Zhou, G., Schuele, S., Templer, J. &amp; Zelano, C. Automated analysis of breathing waveforms using BreathMetrics: a respiratory signal processing toolbox. Chem. Senses 43, 583â€“597 (2018)." href="/articles/s41593-023-01414-4#ref-CR72" id="ref-link-section-d54222645e1915">72</a></sup>.</p><p>For all gray matter voxels in each olfactory ROI, we computed a <i>t</i>-contrast on the sniff onset regressor and all the following analyses were restricted to voxels with significant odor-evoked activity (two-tailed, paired Studentâ€™s <i>t</i>-test, <i>t</i><sub>39991</sub>â€‰=â€‰3.09, <i>P</i>â€‰&lt;â€‰0.001). We computed the across-session reliability (that is, correlation) of odor-evoked fMRI activity in each ROI (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig7">2f</a>). In all ROIs except the PirF in S3, reliability was significant. We excluded the PirF of S3 from dimensionality analyses. Data distribution was assumed to be normal but this was not formally tested.</p><h3 class="c-article__sub-heading" id="Sec16">Finite impulse response model and HRF estimation</h3><p>To estimate a scalar response for each odor and voxel, while accounting for voxel-specific hemodynamic differences, we constructed a finite impulse response (FIR) model. For each subject, a single regressor per odor with onset times at odor delivery was convolved with 11 FIR kernels spanning 11â€‰s. Nuisance regressors, as defined in the average odor-evoked GLM, were included in the design matrix. An AR(1) regression model was computed with SPM12. Odor-evoked voxel traces obtained from the FIR model were temporally smoothed with a moving window of 1â€‰s. For each ROI <i>a</i> in a subject, we obtained a <i>V</i><sub><i>a</i></sub>â€‰<i>Ã—</i>â€‰<i>N</i>â€‰<i>Ã—</i>â€‰<i>T</i> matrix of odor responses from the FIR model, where <i>V</i><sub><i>a</i></sub> is the number of voxels in region <i>a</i>, <i>N</i> the total number of odors (that is, 160) and <i>T</i> the total FIR components evaluated per odor and voxel.</p><p>The FIR traces for ROIs are shown in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig7">2b</a>. For all analyses except the decoding analysis in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig1">1d</a>, we used the time bin corresponding to the peak of the FIR response. In all areas, the peak time bins were at least 4â€‰s after the odor onset. We restricted further analyses to 6â€‰s after odor onset to avoid confounding odor-evoked responses with responses to the rating task, which started 5.5â€‰s after odor onset (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig7">2a</a>). To obtain independent estimates for the pattern correlation analysis in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig1">1d</a>, we estimated odor-evoked responses from one-third of the data. To reduce the degrees of freedom in analyzing this reduced dataset and to avoid introducing biases due to potentially different optimal time bins across different sessions, we used the same canonical hemodynamic response function (HRF) for all sessions in the pattern correlation analysis.</p><h3 class="c-article__sub-heading" id="Sec17">Pattern correlation analysis</h3><p>We performed a pattern-based correlation analysis to test for odor-specific information in the neural activity patterns in each brain area. We first computed the correlation between the activity patterns of all odor pairs (including the correlation of an odor to itself) across two sessions, resulting in session-specific neural similarity matrices <i>Î·</i><sub><i>a,ij</i></sub> for ROI <i>a</i> and sessions <i>i</i> and <i>j</i>. The matrix consists of correlations between responses to the same odor in session <i>i</i> versus session <i>j</i> on the diagonal entries and correlations between responses to different odors on the off-diagonal entries. We Fisherâ€™s <i>z</i>-transformed <span class="mathjax-tex">\(\left(\frac{1}{2}\mathrm{ln}\left(\frac{1+r}{1-r}\right)\right)\)</span> and averaged these matrices across all session pairs <i>i,j</i> to obtain an average session-wise neural similarity matrix <i>Î·</i><sub><i>a</i>.</sub> We then estimated two quantities: <i>R</i><sub>on</sub>â€‰=â€‰<i>z</i>-transformed Pearsonâ€™s <i>r</i> between odor responses in two sessions for the same odor (the on-diagonal entries of <i>Î·</i><sub><i>a</i></sub>) and <i>R</i><sub>off</sub>â€‰=â€‰<i>z</i>-transformed Pearsonâ€™s <i>r</i> between odor responses in two sessions for different odors (the off-diagonal entries of <i>Î·</i><sub><i>a</i></sub>). Pattern correlation difference was defined as <i>âˆ†r</i>â€‰=â€‰mean (<i>R</i><sub>on</sub>)â€‰âˆ’â€‰mean (<i>R</i><sub>off</sub>), averaged across sessions and subjects, and tested for significance using a two-tailed percentile bootstrap (10,000 samples). We tested the average correlation difference against the null hypothesis that the pattern correlation difference was equal to 0.</p><h3 class="c-article__sub-heading" id="Sec18">Neural similarity matrix</h3><p>As input to the RSA (see below), we computed a neural similarity matrix <i>Î¼</i><sub><i>a</i></sub> for ROI <i>a</i>, using odor responses estimated from all odor trials. This neural similarity matrix consisted of all odor responses across all sessions. Specifically, we first identified the peak FIR component between 4 and 6â€‰s (see above) in all areas and subjects. We then computed Pearsonâ€™s correlation between responses at the peak FIR time bin, across voxels in area <i>a</i> for each pair of odors (12,720 unique pairs total). This resulted in a symmetrical correlation matrix <i>Î¼</i><sub><i>a</i></sub>. We obtained pairwise similarity vectors, <b>Î¼</b><sub><i>a</i></sub>, from the off-diagonal entries in the upper triangle of the correlation matrix. For visualization purposes (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig7">2e</a>), a <i>k</i>-means algorithm was used to sort odors into four clusters based on odor responses as features. To minimize biases in the neural similarity matrix imposed by the task structure, experimental conditions (odors presented) were randomized across runs and sessions. In addition, we regressed out two task-based similarity matrices from <i>Î¼</i><sub><i>a</i></sub>. In the first binary matrix, an element was assigned the value of 1 if two odors belonged to the same scanning session and 0 otherwise. The second matrix quantified the number of runs in which two odors were presented within the same run.</p><h3 class="c-article__sub-heading" id="Sec19">Perceptual and molecular RSA</h3><p>To quantify the neural representation of odor percepts and molecular odor structure, we implemented different RSAs. The perceptual representational similarity in an area <i>a</i> (<i>r</i><sub>p,<i>a</i></sub>) was defined as the correlation between pairwise odor similarity measured in the neural space and the perceptual space. More specifically, for each subject and ROI, we computed the perceptual representational similarity <i>r</i><sub>p,<i>a</i></sub> as Spearmanâ€™s rank correlation <i>r</i> between the neural similarity <b>Î¼</b><sub><i>a</i></sub> and perceptual similarity p across all odor pairs. We used Spearmanâ€™s rank correlation instead of Pearsonâ€™s <i>r</i> following recommendations in the literature<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Nili, H. et al. A toolbox for representational similarity analysis. PLoS Comput. Biol. 10, e1003553 (2014)." href="/articles/s41593-023-01414-4#ref-CR73" id="ref-link-section-d54222645e2235">73</a></sup>, because the relationship between perceptual and neural similarities can be assumed to be only monotonic but not strictly linear. Similar to computing the perceptual similarity of odors, we quantified similarity in the molecular space. For this, we used the 4,869 physiochemical descriptors from the DREAM olfaction challenge<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Keller, A. et al. Predicting human olfactory perception from chemical features of odor molecules. Science 355, 820â€“826 (2017)." href="/articles/s41593-023-01414-4#ref-CR17" id="ref-link-section-d54222645e2239">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Keller, A. &amp; Vosshall, L. B. Olfactory perception of chemically diverse molecules. BMC Neurosci. 17, 55 (2016)." href="/articles/s41593-023-01414-4#ref-CR60" id="ref-link-section-d54222645e2242">60</a></sup>. Following Keller et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Keller, A. et al. Predicting human olfactory perception from chemical features of odor molecules. Science 355, 820â€“826 (2017)." href="/articles/s41593-023-01414-4#ref-CR17" id="ref-link-section-d54222645e2246">17</a></sup>, we used a PCA to reduce the full log(transformed) chemical space into a reduced space of 40 PCs. To define molecular similarity <i>m</i> for the molecular RSA, we constructed a matrix of pairwise Pearsonâ€™s correlations of these 40 PCs for all pairs of odors. We evaluated molecular representational similarity <i>r</i><sub>m,<i>a</i></sub> as the Spearmanâ€™s rank correlation between <i>Î¼</i><sub><i>a</i></sub> and m. Then, <i>r</i><sub>m,<i>a</i></sub> and <i>r</i><sub>p,<i>a</i></sub> were computed separately for each ROI <i>a</i> and in each subject. The <i>r</i><sub>m,<i>a</i></sub> and <i>r</i><sub>p,<i>a</i></sub> were averaged across subjects and statistical tests were performed using bootstrap analyses (â€˜<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-023-01414-4#Sec21">Statistical analyses for RSA methods</a>â€™).</p><h3 class="c-article__sub-heading" id="Sec20">Coarse and fine-grained RSAs</h3><p>To further probe whether perceptual representations in olfactory ROIs were driven by coarse or fine-grained odor percepts, we constructed another perceptual representational similarity model, but with two modifications. First, to probe fine-grained perceptual representations, we computed <i>p</i><sub>f</sub> odor similarity in the perceptual space without intensity and pleasantness as perceptual dimensions. Second, to compute coarse similarity, <i>p</i><sub>c</sub>, we assumed that, if two odors belong to the same coarse dimension, then the odor ratings on the feature most related to their mutual dimension would have a high product. Along those lines, and based on similar ideas in other studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="Esfahlani, F. Z. et al. High-amplitude cofluctuations in cortical activity drive functional connectivity. Proc. Natl Acad. Sci. USA 117, 28393â€“28401 (2020)." href="/articles/s41593-023-01414-4#ref-CR74" id="ref-link-section-d54222645e2319">74</a></sup>, we unwrapped Pearsonâ€™s correlation of odor ratings in the perceptual space in the form of an element-wise product of ratings. We then identified the perceptual feature for which the product of perceptual ratings for the two odors was the highest. We defined <i>p</i><sub>c</sub> as the product of ratings for the identified feature. We then evaluated coarse (or fine-grained) representational similarity <i>r</i><sub>c,<i>a</i></sub> (or <i>r</i><sub>f,<i>a</i></sub>) as Spearmanâ€™s rank correlation <i>r</i> between neural similarity for region <i>a</i>, <i>Î¼</i><sub><i>a</i></sub> and <i>p</i><sub>c</sub> (or <i>p</i><sub>f</sub>). We used the bootstrap procedure to test the null hypothesis <i>r</i><sub>c,<i>a</i></sub>â€‰=â€‰<i>r</i><sub>f,<i>a</i></sub> (in each ROI <i>a</i>). Similar to the RSA comprising perceptual and molecular similarities, we used Spearmanâ€™s rank correlation. However, use of Pearsonâ€™s <i>r</i> for <i>r</i><sub>c,<i>a</i></sub> (or <i>r</i><sub>f,<i>a</i></sub>) leads to qualitatively similar results (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4f</a> and Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01414-4#MOESM1">2</a>).</p><p>Furthermore, to quantify the extent to which the neural similarity is explained by <i>p</i><sub>f</sub>: if <i>r</i><sub>f,<i>a</i></sub> exceeds <i>r</i><sub>c,<i>a</i></sub>, we computed <i>r</i><sub>fc,<i>a</i></sub>â€‰=â€‰<i>r</i><sub>f,<i>a</i></sub>â€‰âˆ’â€‰<i>r</i><sub>c,<i>a</i></sub> in each ROI <i>a</i> and tested the null hypothesis, <i>r</i><sub><i>f</i>c,<i>a</i></sub>â€‰=â€‰<i>r</i><sub>f,<i>a</i>â€²</sub>, for two ROIs <i>a</i> and <i>a</i>â€² (for example, PirF versus PirT), using the bootstrap procedure. In addition, to determine the minimum number of odors needed to observe representational similarity differences in the OFC, we computed <i>r</i><sub>fc,<i>a</i></sub> and its corresponding <i>t</i>-value (<i>r</i><sub>fc,<i>a</i></sub>â€‰&gt;â€‰0) for different datasets with randomly drawn odors (ranging from 5 odors to 160 odors). To test whether <i>r</i><sub>c,<i>a</i></sub> increased relative to <i>r</i><sub>f,<i>a</i></sub> when more descriptors were added, we implemented an additional analysis in which <i>r</i><sub>c,<i>a</i></sub> was computed using increasing numbers of descriptors. We then quantified the linear increase in <i>r</i><sub>c,<i>a</i></sub> as a function of the number of included descriptors (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig12">7</a>).</p><p>We conducted further analyses to rule out alternative explanations for our RSA results. First, we repeated the analyses after including intensity and pleasantness to examine why results are not explained by the exclusion of these features (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4a</a>). We also studied the representation of intensity and pleasantness separately and quantified the effect of removing these descriptors from the analyses (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig11">6</a>). Second, to account for potential statistical biases due to differences in ROI size, we repeated these analyses based on the same number of voxels per ROI. In each subject and in each bootstrap sample, neural similarity <i>Î¼</i><sub><i>a</i></sub> was computed based on a fixed number of voxels. This generated a conservative estimate of RSA effects, adjusted for differences in size, based on restricted sampling of 70â€‰voxels in all areas and subjects (the smallest ROI with significant odor-evoked activity in our data). The PirT in S2 and the PirF in S3 were not considered for further analyses due to poor across-session reliability (Extended Data Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig7">2f</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4b</a>). Third, we examined the possibility that differences in coarse and fine-grained identity RSA reflected the differences in odor encoding not in the perceptual but in the correlated molecular space. Similar to the molecular RSA, we used 4,869 physiochemical descriptors used in the DREAM olfaction challenge<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Keller, A. et al. Predicting human olfactory perception from chemical features of odor molecules. Science 355, 820â€“826 (2017)." href="/articles/s41593-023-01414-4#ref-CR17" id="ref-link-section-d54222645e2547">17</a></sup> and computed odor similarities in the physiochemical space. We then regressed out molecular similarity information from <i>p</i><sub>c</sub> and <i>p</i><sub>f</sub> and computed the RSA using the residual perceptual similarities (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4c</a>). Fourth, we performed an analysis excluding any odors that were not reliably detected by subjects (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4d</a>). Fifth, to account for differences due to hemodynamic responses, we repeated the analysis using the same time bin (5â€‰s after stimulus onset) in all areas and subjects to extract odor responses (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4e</a>). Last, we computed representational similarity based on Pearsonâ€™s <i>r</i> instead of Spearmanâ€™s rank correlation (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig9">4f</a>).</p><p>We observed a correlation of sniff volume and duration with odor intensity for some subjects (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig11">6c,d</a>). We therefore performed additional analyses to ensure that the results are not driven by sniff-related effects. Specifically, we regressed out similarities in sniff volume and duration from perceptual similarities. We observed similar results when the sniff effects were regressed from intensity, pleasantness, coarse and fine-grained similarities (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig11">6eâ€“h</a>).</p><h3 class="c-article__sub-heading" id="Sec21">Statistical analyses for RSA results</h3><p>We computed the confidence intervals (CIs) for all RSA results using a percentile bootstrap approach. This included molecular (<i>r</i><sub>m,<i>a</i></sub>), perceptual (<i>r</i><sub>p,<i>a</i></sub>), coarse perceptual (<i>r</i><sub>c,<i>a</i></sub>) and fine-grained perceptual (<i>r</i><sub>f,<i>a</i></sub>) similarities. Given that there were 160 odors per subject, there were 12,720 unique odor pairs. To create the sampling distribution of the mean correlation between the neural and perceptual/molecular similarities, we drew 10,000 bootstrap samples. For each of these 10,000 iterations, we randomly sampled 12,720 odor pairs with replacement and computed the correlation between neural and chemical/perceptual similarity.</p><p>We computed 95% CIs for all RSA results using the bootstrap distribution. For coarse (<i>r</i><sub>c,<i>a</i></sub>) and fine-grained (<i>r</i><sub>f,<i>a</i></sub>) representational similarities, we also tested the null hypotheses that <i>r</i><sub>c,<i>a</i></sub>â€‰=â€‰<i>r</i><sub>f,<i>a</i></sub> (two-tailed percentile bootstrap comparison). We combined the bootstrap samples across subjects to obtain the mean <i>r</i><sub>c,<i>a</i></sub> and <i>r</i><sub>f,<i>a</i></sub>. To compare region-wise differences, we quantified the increase in fine-grained perceptual similarity beyond coarse perceptual similarity as <i>r</i><sub>fc,<i>a</i></sub>â€‰=â€‰<i>r</i><sub>f,<i>a</i></sub>â€‰âˆ’â€‰<i>r</i><sub>c,<i>a</i></sub> in each ROI <i>a</i> and tested the null hypothesis (<i>r</i><sub>c,<i>a</i></sub>â€‰=â€‰<i>r</i><sub>fc,<i>a</i>â€²</sub>) for two ROIs <i>a</i> and <i>a</i>â€² using a two-tailed percentile bootstrap comparison. To ensure that our results were not affected by biases in the percentile bootstrap approach, we also performed permutation tests (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig10">5</a>).</p><h3 class="c-article__sub-heading" id="Sec22">Encoding models</h3><p>We implemented voxel-wise encoding models to examine the specific dimensions of perceptual odor encoding. We focused on gray matter voxels in the previously described anatomically defined ROIs, and further constrained the analysis to voxels that displayed significant odor-evoked activity (<i>t</i><sub>39991</sub>â€‰=â€‰3.09, <i>P</i>â€‰&lt;â€‰0.001). We modeled the voxel-wise activity as a linear combination of perceptual features as basic functions. Specifically, we used the FIR component at the peak of the average odor response for each voxel (<i>i</i>) to estimate the vector of odor (o) responses, <b>v</b><sub>o<i>i</i></sub>. We then trained a ridge regularized GLM to estimate <b>v</b><sub>o<i>i</i></sub> using the perceptual bases (<i>b</i><sub><i>j</i>o</sub>) of the odors. More specifically, we used leave-one-out crossvalidation to predict the voxel activity in response to odors that were not included in model training. To optimize the estimation of training weights for dimensionality and subjectivity analyses (see below), we trained the model in 160 folds, using 159 odors for odor training and one left-out odor as a test odor. Thus, data from a subset of the training odors were partially acquired within the same scanning run as data from the test odor. In each fold of crossvalidation, the model trained a set of weights <i>w</i><sub><i>ij</i></sub> for the <i>j</i>th perceptual feature per voxel <i>i</i>, with the residual Î¶<sub><i>i</i></sub>:</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\mathbf{v}}_{{\mathrm{o}}i}={\sum }_{j}{w}_{ij}\,{b}_{j{\mathrm{o}}}+{\zeta }_{i}.$$</span></div></div><p>We renormalized the response to the test odor based on the mean and s.d. of odor activity from the training data. We used Pearsonâ€™s <i>r</i> to correlate the model predictions with the matched voxel response to obtain a prediction accuracy score <i>r</i> for each voxel. Encoding model performance was computed using Fisherâ€™s <i>z</i>-transform of prediction accuracies. We then used <i>t</i>-values (one-tailed, one-sample Studentâ€™s <i>t</i>-test) of the prediction accuracies to obtained <i>P</i> values pertaining to the null hypothesis that mean (<i>r</i>)â€‰â‰¤â€‰0. Then, we performed a voxel-wise FDR correction of the <i>P</i> values using the Benjaminiâ€“Hochberg procedure. Finally, we generated voxel-wise maps of prediction accuracy, with a threshold of <i>P</i>â€‰&lt;â€‰0.05 (one-sample Studentâ€™s <i>t</i>-test, FDR corrected). We also computed the mean prediction accuracy and the fraction of voxels with significant prediction accuracy for each ROI on an individual subject basis.</p><p>To account for potential biases introduced by the task structure, we performed control analyses with PCs of subjective perceptual spaces. We constructed encoding models with 14 PCs that together explained at least 90% of the variance in the perceptual space. Similar to the original encoding model, we computed the prediction accuracy and dimensionality estimation for our ROIs (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig13">8aâ€“c</a>).</p><p>To account for potential confounds due to the fact that fMRI responses to a subset of the training odors were partially acquired in the same fMRI run as the test odors, we performed additional control analyses in which training and test odors were chosen from entirely independent scanning sessions. Specifically, we implemented a fourfold crossvalidation in which three sets of odors (forty odors per set) were used to train the encoding model. The left-out odor set, which was collected in independent scanning sessions, was used as the test set. Similar to the previous encoding model, we computed the prediction accuracy and dimensionality estimation for our ROIs (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig13">8dâ€“f</a>).</p><h3 class="c-article__sub-heading" id="Sec23">Significance test for model coefficients</h3><p>For voxels with significant prediction accuracy, we sought to examine whether the perceptual feature <i>j</i> was significantly encoded in a given ROI. For voxel <i>i</i> and perceptual feature <i>j</i> each <i>N-</i>fold of crossvalidation resulted in one set of <i>w</i><sub><i>ij</i></sub>. We therefore obtained model weights <i>w</i><sub><i>ijN</i></sub>. from all folds of crossvalidation. We defined the functional profile <i>W</i><sub><i>ij</i></sub> for each voxel <i>i</i> and feature <i>j</i> as the mean(<i>w</i><sub><i>ijN</i></sub>) divided by the s.d. (<i>w</i><sub><i>ijN</i></sub>), where the mean and s.d. were taken across the <i>N</i>-folds. We assumed that the quantitative impact of perceptual feature encoding on neural activity would be determined by the absolute value of <i>W</i><sub><i>ij</i></sub> and not by its sign. We therefore did not discriminate whether perceptual features were mapped to an increase or decrease in the BOLD response. In each ROI <i>a</i>, we determined whether the absolute weight for descriptor <i>j</i>, averaged across voxels in <i>a</i>, was significantly greater than the absolute weight assigned to the smallest descriptor <i>j</i><sub>0,<i>a</i></sub>. We used bootstrapping across voxels to test the null hypothesis for the average subject: mean (<i>W</i><sub><i>ij</i></sub>)â€‰â‰¤â€‰mean (<i>W</i><sub><i>i</i></sub> <i>j</i><sub>0,<i>a</i></sub>) (two-tailed, percentile bootstrap comparison). We then applied an FWE correction to account for multiple tests of pairwise comparisons for any two arbitrary perceptual features. However, as the explicit quantification depends on the precise distribution of <i>W</i><sub><i>ij</i></sub>, number of voxels in an area and collinearity of descriptors, this method could provide only a qualitative description of dimensionality.</p><h3 class="c-article__sub-heading" id="Sec24">Dimensionality of encoding</h3><p>To quantify the dimensionality of encoding for each ROI, we performed a PCA of <span class="mathjax-tex">\({W}_{{ij}}\)</span> where voxels were included as observations and model features were included as variables. We restricted the PCA to voxels with significant prediction accuracy (<i>P</i>â€‰&lt;â€‰0.05, uncorrected). We computed the cumulative percentage of variance explained by the first 12 PCs of <i>W</i><sub><i>ij</i></sub>. To compare differences in dimensionality across areas, we examined the number of PCs needed to explain a fixed amount of variance. To that end, we considered the cumulative percentage of variance explained as a function of the number of PCs. From this curve, we defined a dimensionality parameter, <i>Îº</i>, proportional to the area under the curve and normalized to 0, corresponding to the theoretical case when the first component explains 100% of the variance (<i>Îº</i><sub>0</sub>), and 1, corresponding to the case when all components explain equal variance (<i>Îº</i><sub>1</sub>). Thus:</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\kappa =\frac{{{\kappa}}_{0}-{\kappa}}{{{\kappa}}_{0}-{{\kappa}}_{1}}.$$</span></div></div><p>For each subject, the statistical difference in <i>Îº</i> across ROIs was tested for significance using a two-tailed bootstrap comparison. Specifically, we obtained cumulative variance and <i>Îº</i> for the mean subject from the combined subject-wise bootstraps with 10,000 samples. As bootstrapped estimates on PCA are not always stable, we made two adjustments. First, we computed only the cumulative percentage of variance explained by the PCs because cumulative quantification is robust to small changes in the order of PCs. Second, we ensured that the rank of the bootstrapped <span class="mathjax-tex">\({W}_{{ij}}\)</span> exceeded the number of PCs computed for each bootstrap estimation. We also excluded PirF in S3 because this was the only ROI where the number of voxels with significant prediction accuracy was fewer than the 18 perceptual features in the model.</p><p>To account for the possibility that the results were biased by the differences in the number of voxels in each ROI, we computed another dimensionality parameter adjusted for size, <i>Îº</i><sub>adj</sub>. <i>Îº</i><sub>adj</sub> was based on PCA of <i>W</i><sub><i>ij</i></sub> with the same number of voxels drawn with replacement from each ROI and subject. We restricted the size of sampling to 25, based on the minimum number of gray matter voxels in an ROI with significant odor-evoked activity and significant encoding prediction accuracy in our data (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig14">9</a>).</p><h3 class="c-article__sub-heading" id="Sec25">Generalizability of perceptual encoding</h3><p>We examined the similarity in encoding profiles across subjects to quantify idiosyncrasy in perceptual encoding. We probed whether intersubject correlations of encoding weights in one area were more consistent across subjects than that in other areas. To that end, we computed Pearsonâ€™s correlation between the weight profile in a voxel for one subject and that of voxels in the same ROI in the remaining two subjects. We obtained a bootstrapped estimate of this correlation. The bootstrapping was restricted to the same number of voxel pairs in all ROIs (that is, 2,500 voxel pairs) to minimize bias due to size differences across ROIs. As encoding weights were the least generalizable in the OFC, we performed post-hoc analyses to further examine whether a subset of encoding dimensions drove idiosyncrasies in the OFC. Specifically, we measured the intersubject generalizability of specific PCs of encoding weights in the OFC. To align the PCs from different subjects, we sorted them using the stable marriage algorithm<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 75" title="Gusfield, D. &amp; Irving, R. W. The Stable Marriage Problem: Structure and Algorithms (MIT Press, 1989)." href="/articles/s41593-023-01414-4#ref-CR75" id="ref-link-section-d54222645e3235">75</a></sup>. We then evaluated intersubject Pearsonâ€™s correlations between OFC PCs and tested for component-specific differences in generalizability. We also examined the coefficients of the PCs based on the perceptual descriptors to qualitatively highlight the extent to which each perceptual feature contributed to the generalizability of encoding in the OFC.</p><h3 class="c-article__sub-heading" id="Sec26">Computing resources</h3><p>All analyses were performed in MATLAB R2016b and R2020b. The task was designed using COGENT 2000. This research was supported in part through the computational resources and staff contributions provided for the Quest high-performance computing facility at Northwestern University, which is jointly supported by the Office of the Provost, the Office for Research and Northwestern University Information Technology.</p><h3 class="c-article__sub-heading" id="Sec27">Reporting summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01414-4#MOESM2">Nature Portfolio Reporting Summary</a> linked to this article.</p></div></div></section>
                </div>
            

            <div>
                <section data-title="Data availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data availability</h2><div class="c-article-section__content" id="data-availability-content">
              
              <p>Data to reproduce the main findings presented in this manuscript (odor database, perceptual ratings, ROIs and odor-evoked responses) are available at <a href="https://github.com/viveksgr/NEMO_scripts">https://github.com/viveksgr/NEMO_scripts</a>. Raw data are available upon access request at <a href="https://doi.org/10.5281/zenodo.7636722">https://doi.org/10.5281/zenodo.7636722</a> (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Sagar, V., Shanahan, L. K., Zelano, C. M., Gottfried, J. A. &amp; Kahnt, T. Neural encoding models of olfaction (NEMO) dataset. Zenodo &#xA;                  https://doi.org/10.5281/zenodo.7636722&#xA;                  &#xA;                 (2023)." href="/articles/s41593-023-01414-4#ref-CR76" id="ref-link-section-d54222645e3372">76</a></sup>). The timeframe for response to requests is approximately 10 business days. Molecular information of odors is accessible from a publicly available dataset from previous studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Keller, A. et al. Predicting human olfactory perception from chemical features of odor molecules. Science 355, 820â€“826 (2017)." href="/articles/s41593-023-01414-4#ref-CR17" id="ref-link-section-d54222645e3376">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Keller, A. &amp; Vosshall, L. B. Olfactory perception of chemically diverse molecules. BMC Neurosci. 17, 55 (2016)." href="/articles/s41593-023-01414-4#ref-CR60" id="ref-link-section-d54222645e3379">60</a></sup>.</p>
            </div></div></section><section data-title="Code availability"><div class="c-article-section" id="code-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="code-availability">Code availability</h2><div class="c-article-section__content" id="code-availability-content">
              
              <p>Code for preprocessing and reproducing the results presented in this manuscript is available at <a href="https://github.com/viveksgr/NEMO_scripts">https://github.com/viveksgr/NEMO_scripts</a>.</p>
            </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Leopold, D. A., Wilke, M., Maier, A. &amp; Logothetis, N. K. Stable perception of visually ambiguous patterns. <i>Nat. Neurosci.</i> <b>5</b>, 605â€“609 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD38XjvVygu7o%3D" aria-label="CAS reference 1">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11992115" aria-label="PubMed reference 1">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Stable%20perception%20of%20visually%20ambiguous%20patterns&amp;journal=Nat.%20Neurosci.&amp;volume=5&amp;pages=605-609&amp;publication_year=2002&amp;author=Leopold%2CDA&amp;author=Wilke%2CM&amp;author=Maier%2CA&amp;author=Logothetis%2CNK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Walsh, V. &amp; Kulikowski, J. <i>Perceptual Constancy: Why Things Look As They Do</i> (Cambridge Univ. Press, 1998).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Hudson, R. From molecule to mind: the role of experience in shaping olfactory function. <i>J. Comp. Physiol. A</i> <b>185</b>, 297â€“304 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD3c%2Fit12ltA%3D%3D" aria-label="CAS reference 3">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10555266" aria-label="PubMed reference 3">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=From%20molecule%20to%20mind%3A%20the%20role%20of%20experience%20in%20shaping%20olfactory%20function&amp;journal=J.%20Comp.%20Physiol.%20A&amp;volume=185&amp;pages=297-304&amp;publication_year=1999&amp;author=Hudson%2CR">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Sell, C. S. On the unpredictability of odor. <i>Angew. Chem. Int. Ed.</i> <b>45</b>, 6254â€“6261 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28XhtVKqs7bK" aria-label="CAS reference 4">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20unpredictability%20of%20odor&amp;journal=Angew.%20Chem.%20Int.%20Ed.&amp;volume=45&amp;pages=6254-6261&amp;publication_year=2006&amp;author=Sell%2CCS">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Mainland, J. D. et al. The missense of smell: functional variability in the human odorant receptor repertoire. <i>Nat. Neurosci.</i> <b>17</b>, 114â€“120 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhvV2htrrI" aria-label="CAS reference 5">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24316890" aria-label="PubMed reference 5">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20missense%20of%20smell%3A%20functional%20variability%20in%20the%20human%20odorant%20receptor%20repertoire&amp;journal=Nat.%20Neurosci.&amp;volume=17&amp;pages=114-120&amp;publication_year=2014&amp;author=Mainland%2CJD">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Bushdid, C., Magnasco, M. O., Vosshall, L. B. &amp; Keller, A. Humans can discriminate more than 1 trillion olfactory stimuli. <i>Science</i> <b>343</b>, 1370â€“1372 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXktl2gsL4%3D" aria-label="CAS reference 6">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24653035" aria-label="PubMed reference 6">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4483192" aria-label="PubMed Central reference 6">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Humans%20can%20discriminate%20more%20than%201%20trillion%20olfactory%20stimuli&amp;journal=Science&amp;volume=343&amp;pages=1370-1372&amp;publication_year=2014&amp;author=Bushdid%2CC&amp;author=Magnasco%2CMO&amp;author=Vosshall%2CLB&amp;author=Keller%2CA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Koulakov, A., Kolterman, B. E., Enikolopov, A. &amp; Rinberg, D. In search of the structure of human olfactory space. <i>Front. Syst. Neurosci.</i> <b>5</b>, 65 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21954378" aria-label="PubMed reference 7">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3173711" aria-label="PubMed Central reference 7">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=In%20search%20of%20the%20structure%20of%20human%20olfactory%20space&amp;journal=Front.%20Syst.%20Neurosci.&amp;volume=5&amp;publication_year=2011&amp;author=Koulakov%2CA&amp;author=Kolterman%2CBE&amp;author=Enikolopov%2CA&amp;author=Rinberg%2CD">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Shepherd, G. M. &amp; Greer, C. A. in <i>T</i><i>he Synaptic Organization of the Brain</i> (ed. Shepherd, G. M.) 159â€“203 (Oxford Univ. Press, 1998).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Miura, K., Mainen, Z. F. &amp; Uchida, N. Odor representations in olfactory cortex: distributed rate coding and decorrelated population activity. <i>Neuron</i> <b>74</b>, 1087â€“1098 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XptVKiu7c%3D" aria-label="CAS reference 9">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22726838" aria-label="PubMed reference 9">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3383608" aria-label="PubMed Central reference 9">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Odor%20representations%20in%20olfactory%20cortex%3A%20distributed%20rate%20coding%20and%20decorrelated%20population%20activity&amp;journal=Neuron&amp;volume=74&amp;pages=1087-1098&amp;publication_year=2012&amp;author=Miura%2CK&amp;author=Mainen%2CZF&amp;author=Uchida%2CN">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Pashkovski, S. L. et al. Structure and flexibility in cortical representations of odour space. <i>Nature</i> <b>583</b>, 253â€“258 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXhtlShtL%2FJ" aria-label="CAS reference 10">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32612230" aria-label="PubMed reference 10">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7450987" aria-label="PubMed Central reference 10">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Structure%20and%20flexibility%20in%20cortical%20representations%20of%20odour%20space&amp;journal=Nature&amp;volume=583&amp;pages=253-258&amp;publication_year=2020&amp;author=Pashkovski%2CSL">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Stettler, D. D. &amp; Axel, R. Representations of odor in the piriform cortex. <i>Neuron</i> <b>63</b>, 854â€“864 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXhsVCjsrbI" aria-label="CAS reference 11">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19778513" aria-label="PubMed reference 11">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Representations%20of%20odor%20in%20the%20piriform%20cortex&amp;journal=Neuron&amp;volume=63&amp;pages=854-864&amp;publication_year=2009&amp;author=Stettler%2CDD&amp;author=Axel%2CR">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Sosulski, D. L., Bloom, M. L., Cutforth, T., Axel, R. &amp; Datta, S. R. Distinct representations of olfactory information in different cortical centres. <i>Nature</i> <b>472</b>, 213â€“216 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXjvFOntro%3D" aria-label="CAS reference 12">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21451525" aria-label="PubMed reference 12">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3354569" aria-label="PubMed Central reference 12">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinct%20representations%20of%20olfactory%20information%20in%20different%20cortical%20centres&amp;journal=Nature&amp;volume=472&amp;pages=213-216&amp;publication_year=2011&amp;author=Sosulski%2CDL&amp;author=Bloom%2CML&amp;author=Cutforth%2CT&amp;author=Axel%2CR&amp;author=Datta%2CSR">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Miyamichi, K. et al. Cortical representations of olfactory input by trans-synaptic tracing. <i>Nature</i> <b>472</b>, 191â€“196 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXhtVCrtg%3D%3D" aria-label="CAS reference 13">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21179085" aria-label="PubMed reference 13">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Cortical%20representations%20of%20olfactory%20input%20by%20trans-synaptic%20tracing&amp;journal=Nature&amp;volume=472&amp;pages=191-196&amp;publication_year=2011&amp;author=Miyamichi%2CK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Roland, B., Deneux, T., Franks, K. M., Bathellier, B. &amp; Fleischmann, A. Odor identity coding by distributed ensembles of neurons in the mouse olfactory cortex. <i>eLife</i> <b>6</b>, e26337 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28489003" aria-label="PubMed reference 14">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438249" aria-label="PubMed Central reference 14">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Odor%20identity%20coding%20by%20distributed%20ensembles%20of%20neurons%20in%20the%20mouse%20olfactory%20cortex&amp;journal=eLife&amp;volume=6&amp;publication_year=2017&amp;author=Roland%2CB&amp;author=Deneux%2CT&amp;author=Franks%2CKM&amp;author=Bathellier%2CB&amp;author=Fleischmann%2CA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Howard, J. D., Plailly, J., Grueschow, M., Haynes, J. D. &amp; Gottfried, J. A. Odor quality coding and categorization in human posterior piriform cortex. <i>Nat. Neurosci.</i> <b>12</b>, 932â€“938 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXms1alu7g%3D" aria-label="CAS reference 15">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19483688" aria-label="PubMed reference 15">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2834563" aria-label="PubMed Central reference 15">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Odor%20quality%20coding%20and%20categorization%20in%20human%20posterior%20piriform%20cortex&amp;journal=Nat.%20Neurosci.&amp;volume=12&amp;pages=932-938&amp;publication_year=2009&amp;author=Howard%2CJD&amp;author=Plailly%2CJ&amp;author=Grueschow%2CM&amp;author=Haynes%2CJD&amp;author=Gottfried%2CJA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Bolding, K. A. &amp; Franks, K. M. Complementary codes for odor identity and intensity in olfactory cortex. <i>eLife</i> <b>6</b>, e22630 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28379135" aria-label="PubMed reference 16">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5438247" aria-label="PubMed Central reference 16">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Complementary%20codes%20for%20odor%20identity%20and%20intensity%20in%20olfactory%20cortex&amp;journal=eLife&amp;volume=6&amp;publication_year=2017&amp;author=Bolding%2CKA&amp;author=Franks%2CKM">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Keller, A. et al. Predicting human olfactory perception from chemical features of odor molecules. <i>Science</i> <b>355</b>, 820â€“826 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2sXjtVaisb0%3D" aria-label="CAS reference 17">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28219971" aria-label="PubMed reference 17">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455768" aria-label="PubMed Central reference 17">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Predicting%20human%20olfactory%20perception%20from%20chemical%20features%20of%20odor%20molecules&amp;journal=Science&amp;volume=355&amp;pages=820-826&amp;publication_year=2017&amp;author=Keller%2CA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Castro, J. B., Ramanathan, A. &amp; Chennubhotla, C. S. Categorical dimensions of human odor descriptor space revealed by non-negative matrix factorization. <i>PLoS ONE</i> <b>8</b>, e73289 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhsFeitb7N" aria-label="CAS reference 18">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24058466" aria-label="PubMed reference 18">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3776812" aria-label="PubMed Central reference 18">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Categorical%20dimensions%20of%20human%20odor%20descriptor%20space%20revealed%20by%20non-negative%20matrix%20factorization&amp;journal=PLoS%20ONE&amp;volume=8&amp;publication_year=2013&amp;author=Castro%2CJB&amp;author=Ramanathan%2CA&amp;author=Chennubhotla%2CCS">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Bizley, J. K. &amp; Cohen, Y. E. The what, where and how of auditory-object perception. <i>Nat. Rev. Neurosci.</i> <b>14</b>, 693â€“707 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhsVymsrjL" aria-label="CAS reference 19">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24052177" aria-label="PubMed reference 19">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4082027" aria-label="PubMed Central reference 19">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20what%2C%20where%20and%20how%20of%20auditory-object%20perception&amp;journal=Nat.%20Rev.%20Neurosci.&amp;volume=14&amp;pages=693-707&amp;publication_year=2013&amp;author=Bizley%2CJK&amp;author=Cohen%2CYE">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Lescroart, M. D. &amp; Gallant, J. L. Human scene-selective areas represent 3D configurations of surfaces. <i>Neuron</i> <b>101</b>, 178â€“192. e177 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1cXitlalurrK" aria-label="CAS reference 20">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30497771" aria-label="PubMed reference 20">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20scene-selective%20areas%20represent%203D%20configurations%20of%20surfaces&amp;journal=Neuron&amp;volume=101&amp;pages=178-192.%20e177&amp;publication_year=2019&amp;author=Lescroart%2CMD&amp;author=Gallant%2CJL">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Grunewald, A. &amp; Skoumbourdis, E. K. The integration of multiple stimulus features by V1 neurons. <i>J. Neurosci.</i> <b>24</b>, 9185â€“9194 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2cXpsVWku7c%3D" aria-label="CAS reference 21">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15483137" aria-label="PubMed reference 21">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6730054" aria-label="PubMed Central reference 21">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20integration%20of%20multiple%20stimulus%20features%20by%20V1%20neurons&amp;journal=J.%20Neurosci.&amp;volume=24&amp;pages=9185-9194&amp;publication_year=2004&amp;author=Grunewald%2CA&amp;author=Skoumbourdis%2CEK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Cichy, R. M., Pantazis, D. &amp; Oliva, A. Resolving human object recognition in space and time. <i>Nat. Neurosci.</i> <b>17</b>, 455â€“462 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXht1ymt7k%3D" aria-label="CAS reference 22">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24464044" aria-label="PubMed reference 22">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4261693" aria-label="PubMed Central reference 22">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Resolving%20human%20object%20recognition%20in%20space%20and%20time&amp;journal=Nat.%20Neurosci.&amp;volume=17&amp;pages=455-462&amp;publication_year=2014&amp;author=Cichy%2CRM&amp;author=Pantazis%2CD&amp;author=Oliva%2CA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysisâ€”connecting the branches of systems neuroscience. <i>Front. Syst. Neurosci.</i> <b>2</b>, 4 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19104670" aria-label="PubMed reference 23">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405" aria-label="PubMed Central reference 23">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Representational%20similarity%20analysis%E2%80%94connecting%20the%20branches%20of%20systems%20neuroscience&amp;journal=Front.%20Syst.%20Neurosci.&amp;volume=2&amp;publication_year=2008&amp;author=Kriegeskorte%2CN&amp;author=Mur%2CM&amp;author=Bandettini%2CP">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Wu, M. C., David, S. V. &amp; Gallant, J. L. Complete functional characterization of sensory neurons by system identification. <i>Annu. Rev. Neurosci.</i> <b>29</b>, 477â€“505 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28XosVeit78%3D" aria-label="CAS reference 24">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16776594" aria-label="PubMed reference 24">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Complete%20functional%20characterization%20of%20sensory%20neurons%20by%20system%20identification&amp;journal=Annu.%20Rev.%20Neurosci.&amp;volume=29&amp;pages=477-505&amp;publication_year=2006&amp;author=Wu%2CMC&amp;author=David%2CSV&amp;author=Gallant%2CJL">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Allen, E. J. et al. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. <i>Nat. Neurosci.</i> <b>25</b>, 116â€“126 (2022).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3MXislOnsLrE" aria-label="CAS reference 25">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34916659" aria-label="PubMed reference 25">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20massive%207T%20fMRI%20dataset%20to%20bridge%20cognitive%20neuroscience%20and%20artificial%20intelligence&amp;journal=Nat.%20Neurosci.&amp;volume=25&amp;pages=116-126&amp;publication_year=2022&amp;author=Allen%2CEJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Naselaris, T., Prenger, R. J., Kay, K. N., Oliver, M. &amp; Gallant, J. L. Bayesian reconstruction of natural images from human brain activity. <i>Neuron</i> <b>63</b>, 902â€“915 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXhsVCjsrbE" aria-label="CAS reference 26">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19778517" aria-label="PubMed reference 26">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5553889" aria-label="PubMed Central reference 26">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Bayesian%20reconstruction%20of%20natural%20images%20from%20human%20brain%20activity&amp;journal=Neuron&amp;volume=63&amp;pages=902-915&amp;publication_year=2009&amp;author=Naselaris%2CT&amp;author=Prenger%2CRJ&amp;author=Kay%2CKN&amp;author=Oliver%2CM&amp;author=Gallant%2CJL">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Kay, K. N., Naselaris, T., Prenger, R. J. &amp; Gallant, J. L. Identifying natural images from human brain activity. <i>Nature</i> <b>452</b>, 352â€“355 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1cXjsFCnsL8%3D" aria-label="CAS reference 27">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18322462" aria-label="PubMed reference 27">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3556484" aria-label="PubMed Central reference 27">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Identifying%20natural%20images%20from%20human%20brain%20activity&amp;journal=Nature&amp;volume=452&amp;pages=352-355&amp;publication_year=2008&amp;author=Kay%2CKN&amp;author=Naselaris%2CT&amp;author=Prenger%2CRJ&amp;author=Gallant%2CJL">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Snitz, K. et al. Predicting odor perceptual similarity from odor structure. <i>PLoS Comput. Biol.</i> <b>9</b>, e1003184 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhsFOjsr7E" aria-label="CAS reference 28">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24068899" aria-label="PubMed reference 28">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3772038" aria-label="PubMed Central reference 28">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Predicting%20odor%20perceptual%20similarity%20from%20odor%20structure&amp;journal=PLoS%20Comput.%20Biol.&amp;volume=9&amp;publication_year=2013&amp;author=Snitz%2CK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Yeshurun, Y. &amp; Sobel, N. An odor is not worth a thousand words: from multidimensional odors to unidimensional odor objects. <i>Annu. Rev. Psychol.</i> <b>61</b>, 219â€“241 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19958179" aria-label="PubMed reference 29">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20odor%20is%20not%20worth%20a%20thousand%20words%3A%20from%20multidimensional%20odors%20to%20unidimensional%20odor%20objects&amp;journal=Annu.%20Rev.%20Psychol.&amp;volume=61&amp;pages=219-241&amp;publication_year=2010&amp;author=Yeshurun%2CY&amp;author=Sobel%2CN">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Sirotin, Y. B., Shusterman, R. &amp; Rinberg, D. Neural coding of perceived odor intensity. <i>eNeuro</i> <a href="https://doi.org/10.1523/ENEURO.0083-15.2015" data-track="click" data-track-action="external reference" data-track-label="10.1523/ENEURO.0083-15.2015">https://doi.org/10.1523/ENEURO.0083-15.2015</a> (2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Winston, J. S., Gottfried, J. A., Kilner, J. M. &amp; Dolan, R. J. Integrated neural representations of odor intensity and affective valence in human amygdala. <i>J. Neurosci.</i> <b>25</b>, 8903â€“8907 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2MXhtVOqs7rF" aria-label="CAS reference 31">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16192380" aria-label="PubMed reference 31">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6725588" aria-label="PubMed Central reference 31">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Integrated%20neural%20representations%20of%20odor%20intensity%20and%20affective%20valence%20in%20human%20amygdala&amp;journal=J.%20Neurosci.&amp;volume=25&amp;pages=8903-8907&amp;publication_year=2005&amp;author=Winston%2CJS&amp;author=Gottfried%2CJA&amp;author=Kilner%2CJM&amp;author=Dolan%2CRJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Anderson, A. K. et al. Dissociated neural representations of intensity and valence in human olfaction. <i>Nat. Neurosci.</i> <b>6</b>, 196â€“202 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3sXmsFGgtQ%3D%3D" aria-label="CAS reference 32">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12536208" aria-label="PubMed reference 32">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissociated%20neural%20representations%20of%20intensity%20and%20valence%20in%20human%20olfaction&amp;journal=Nat.%20Neurosci.&amp;volume=6&amp;pages=196-202&amp;publication_year=2003&amp;author=Anderson%2CAK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Lapid, H. et al. Neural activity at the human olfactory epithelium reflects olfactory perception. <i>Nat. Neurosci.</i> <b>14</b>, 1455â€“1461 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXht1anu7bI" aria-label="CAS reference 33">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21946326" aria-label="PubMed reference 33">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20activity%20at%20the%20human%20olfactory%20epithelium%20reflects%20olfactory%20perception&amp;journal=Nat.%20Neurosci.&amp;volume=14&amp;pages=1455-1461&amp;publication_year=2011&amp;author=Lapid%2CH">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Gratton, C., Nelson, S. M. &amp; Gordon, E. M. Brain-behavior correlations: two paths toward reliability. <i>Neuron</i> <b>110</b>, 1446â€“1449 (2022).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB38Xht1SrsLnO" aria-label="CAS reference 34">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=35512638" aria-label="PubMed reference 34">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Brain-behavior%20correlations%3A%20two%20paths%20toward%20reliability&amp;journal=Neuron&amp;volume=110&amp;pages=1446-1449&amp;publication_year=2022&amp;author=Gratton%2CC&amp;author=Nelson%2CSM&amp;author=Gordon%2CEM">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Rosenberg, M. D. &amp; Finn, E. S. How to establish robust brainâ€“behavior relationships without thousands of individuals. <i>Nat. Neurosci.</i> <b>25</b>, 835â€“837 (2022).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB38XhsFCku7fK" aria-label="CAS reference 35">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=35710985" aria-label="PubMed reference 35">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20to%20establish%20robust%20brain%E2%80%93behavior%20relationships%20without%20thousands%20of%20individuals&amp;journal=Nat.%20Neurosci.&amp;volume=25&amp;pages=835-837&amp;publication_year=2022&amp;author=Rosenberg%2CMD&amp;author=Finn%2CES">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Gottfried, J. A., Winston, J. S. &amp; Dolan, R. J. Dissociable codes of odor quality and odorant structure in human piriform cortex. <i>Neuron</i> <b>49</b>, 467â€“479 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28Xhs1eiu70%3D" aria-label="CAS reference 36">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16446149" aria-label="PubMed reference 36">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissociable%20codes%20of%20odor%20quality%20and%20odorant%20structure%20in%20human%20piriform%20cortex&amp;journal=Neuron&amp;volume=49&amp;pages=467-479&amp;publication_year=2006&amp;author=Gottfried%2CJA&amp;author=Winston%2CJS&amp;author=Dolan%2CRJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Fournel, A., Ferdenzi, C., Sezille, C., Rouby, C. &amp; Bensafi, M. Multidimensional representation of odors in the human olfactory cortex. <i>Hum. Brain Mapp.</i> <b>37</b>, 2161â€“2172 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BC28fhsVWjsw%3D%3D" aria-label="CAS reference 37">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26991044" aria-label="PubMed reference 37">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6867239" aria-label="PubMed Central reference 37">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Multidimensional%20representation%20of%20odors%20in%20the%20human%20olfactory%20cortex&amp;journal=Hum.%20Brain%20Mapp.&amp;volume=37&amp;pages=2161-2172&amp;publication_year=2016&amp;author=Fournel%2CA&amp;author=Ferdenzi%2CC&amp;author=Sezille%2CC&amp;author=Rouby%2CC&amp;author=Bensafi%2CM">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Zelano, C., Mohanty, A. &amp; Gottfried, J. A. Olfactory predictive codes and stimulus templates in piriform cortex. <i>Neuron</i> <b>72</b>, 178â€“187 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXht12kt7nE" aria-label="CAS reference 38">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21982378" aria-label="PubMed reference 38">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3190127" aria-label="PubMed Central reference 38">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Olfactory%20predictive%20codes%20and%20stimulus%20templates%20in%20piriform%20cortex&amp;journal=Neuron&amp;volume=72&amp;pages=178-187&amp;publication_year=2011&amp;author=Zelano%2CC&amp;author=Mohanty%2CA&amp;author=Gottfried%2CJA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Sobel, N. et al. Sniffing and smelling: separate subsystems in the human olfactory cortex. <i>Nature</i> <b>392</b>, 282â€“286 (1998).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK1cXitFait78%3D" aria-label="CAS reference 39">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9521322" aria-label="PubMed reference 39">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Sniffing%20and%20smelling%3A%20separate%20subsystems%20in%20the%20human%20olfactory%20cortex&amp;journal=Nature&amp;volume=392&amp;pages=282-286&amp;publication_year=1998&amp;author=Sobel%2CN">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Kareken, D. A. et al. Olfactory system activation from sniffing: effects in piriform and orbitofrontal cortex. <i>NeuroImage</i> <b>22</b>, 456â€“465 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15110039" aria-label="PubMed reference 40">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Olfactory%20system%20activation%20from%20sniffing%3A%20effects%20in%20piriform%20and%20orbitofrontal%20cortex&amp;journal=NeuroImage&amp;volume=22&amp;pages=456-465&amp;publication_year=2004&amp;author=Kareken%2CDA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Cohen, L., Rothschild, G. &amp; Mizrahi, A. Multisensory integration of natural odors and sounds in the auditory cortex. <i>Neuron</i> <b>72</b>, 357â€“369 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXhtlKrtLzN" aria-label="CAS reference 41">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22017993" aria-label="PubMed reference 41">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Multisensory%20integration%20of%20natural%20odors%20and%20sounds%20in%20the%20auditory%20cortex&amp;journal=Neuron&amp;volume=72&amp;pages=357-369&amp;publication_year=2011&amp;author=Cohen%2CL&amp;author=Rothschild%2CG&amp;author=Mizrahi%2CA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Olofsson, J. K., Zhou, G., East, B. S., Zelano, C. &amp; Wilson, D. A. Odor identification in rats: behavioral and electrophysiological evidence of learned olfactory-auditory associations. <i>eNeuro</i> <a href="https://doi.org/10.1523/ENEURO.0102-19.2019" data-track="click" data-track-action="external reference" data-track-label="10.1523/ENEURO.0102-19.2019">https://doi.org/10.1523/ENEURO.0102-19.2019</a> (2019).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Li, W., Howard, J. D., Parrish, T. B. &amp; Gottfried, J. A. Aversive learning enhances perceptual and cortical discrimination of indiscriminable odor cues. <i>Science</i> <b>319</b>, 1842â€“1845 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1cXjs12ktr0%3D" aria-label="CAS reference 43">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18369149" aria-label="PubMed reference 43">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2756335" aria-label="PubMed Central reference 43">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Aversive%20learning%20enhances%20perceptual%20and%20cortical%20discrimination%20of%20indiscriminable%20odor%20cues&amp;journal=Science&amp;volume=319&amp;pages=1842-1845&amp;publication_year=2008&amp;author=Li%2CW&amp;author=Howard%2CJD&amp;author=Parrish%2CTB&amp;author=Gottfried%2CJA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Bae, J., Yi, J.-Y. &amp; Moon, C. Odor quality profile is partially influenced by verbal cues. <i>PLoS ONE</i> <b>14</b>, e0226385 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXitV2ktLg%3D" aria-label="CAS reference 44">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31830119" aria-label="PubMed reference 44">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6907808" aria-label="PubMed Central reference 44">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Odor%20quality%20profile%20is%20partially%20influenced%20by%20verbal%20cues&amp;journal=PLoS%20ONE&amp;volume=14&amp;publication_year=2019&amp;author=Bae%2CJ&amp;author=Yi%2CJ-Y&amp;author=Moon%2CC">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Wang, P. Y. et al. Transient and persistent representations of odor value in prefrontal cortex. <i>Neuron</i> <b>108</b>, 209â€“224. e206 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXhs1GqtbzK" aria-label="CAS reference 45">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32827456" aria-label="PubMed reference 45">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7886003" aria-label="PubMed Central reference 45">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Transient%20and%20persistent%20representations%20of%20odor%20value%20in%20prefrontal%20cortex&amp;journal=Neuron&amp;volume=108&amp;pages=209-224.%20e206&amp;publication_year=2020&amp;author=Wang%2CPY">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Kriegeskorte, N. &amp; Douglas, P. K. Interpreting encoding and decoding models. <i>Curr. Opin. Neurobiol.</i> <b>55</b>, 167â€“179 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1MXnslOktL8%3D" aria-label="CAS reference 46">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31039527" aria-label="PubMed reference 46">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6705607" aria-label="PubMed Central reference 46">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Interpreting%20encoding%20and%20decoding%20models&amp;journal=Curr.%20Opin.%20Neurobiol.&amp;volume=55&amp;pages=167-179&amp;publication_year=2019&amp;author=Kriegeskorte%2CN&amp;author=Douglas%2CPK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Schulze, P., Bestgen, A.-K., Lech, R. K., Kuchinke, L. &amp; Suchan, B. Preprocessing of emotional visual information in the human piriform cortex. <i>Sci. Rep.</i> <b>7</b>, 9191 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28835658" aria-label="PubMed reference 47">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5569091" aria-label="PubMed Central reference 47">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Preprocessing%20of%20emotional%20visual%20information%20in%20the%20human%20piriform%20cortex&amp;journal=Sci.%20Rep.&amp;volume=7&amp;publication_year=2017&amp;author=Schulze%2CP&amp;author=Bestgen%2CA-K&amp;author=Lech%2CRK&amp;author=Kuchinke%2CL&amp;author=Suchan%2CB">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">Wilson, C. D., Serrano, G. O., Koulakov, A. A. &amp; Rinberg, D. A primacy code for odor identity. <i>Nat. Commun.</i> <b>8</b>, 1477 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29133907" aria-label="PubMed reference 48">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5684307" aria-label="PubMed Central reference 48">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20primacy%20code%20for%20odor%20identity&amp;journal=Nat.%20Commun.&amp;volume=8&amp;publication_year=2017&amp;author=Wilson%2CCD&amp;author=Serrano%2CGO&amp;author=Koulakov%2CAA&amp;author=Rinberg%2CD">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">Gottfried, J. A., Oâ€™Doherty, J. &amp; Dolan, R. J. Encoding predictive reward value in human amygdala and orbitofrontal cortex. <i>Science</i> <b>301</b>, 1104â€“1107 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3sXmsFWisbk%3D" aria-label="CAS reference 49">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12934011" aria-label="PubMed reference 49">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Encoding%20predictive%20reward%20value%20in%20human%20amygdala%20and%20orbitofrontal%20cortex&amp;journal=Science&amp;volume=301&amp;pages=1104-1107&amp;publication_year=2003&amp;author=Gottfried%2CJA&amp;author=O%E2%80%99Doherty%2CJ&amp;author=Dolan%2CRJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">Bowman, N. E., Kording, K. P. &amp; Gottfried, J. A. Temporal integration of olfactory perceptual evidence in human orbitofrontal cortex. <i>Neuron</i> <b>75</b>, 916â€“927 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhtlSktLnO" aria-label="CAS reference 50">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22958830" aria-label="PubMed reference 50">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3441053" aria-label="PubMed Central reference 50">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Temporal%20integration%20of%20olfactory%20perceptual%20evidence%20in%20human%20orbitofrontal%20cortex&amp;journal=Neuron&amp;volume=75&amp;pages=916-927&amp;publication_year=2012&amp;author=Bowman%2CNE&amp;author=Kording%2CKP&amp;author=Gottfried%2CJA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">Zhou, J. et al. Rat orbitofrontal ensemble activity contains multiplexed but dissociable representations of value and task structure in an odor sequence task. <i>Curr. Biol.</i> <b>29</b>, 897â€“907. e893 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1MXktV2qsLY%3D" aria-label="CAS reference 51">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30827919" aria-label="PubMed reference 51">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9445914" aria-label="PubMed Central reference 51">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=Rat%20orbitofrontal%20ensemble%20activity%20contains%20multiplexed%20but%20dissociable%20representations%20of%20value%20and%20task%20structure%20in%20an%20odor%20sequence%20task&amp;journal=Curr.%20Biol.&amp;volume=29&amp;pages=897-907.%20e893&amp;publication_year=2019&amp;author=Zhou%2CJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">Bao, X., Raguet, L. L., Cole, S. M., Howard, J. D. &amp; Gottfried, J. A. The role of piriform associative connections in odor categorization. <i>eLife</i> <b>5</b>, e13732 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27130519" aria-label="PubMed reference 52">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4884078" aria-label="PubMed Central reference 52">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20piriform%20associative%20connections%20in%20odor%20categorization&amp;journal=eLife&amp;volume=5&amp;publication_year=2016&amp;author=Bao%2CX&amp;author=Raguet%2CLL&amp;author=Cole%2CSM&amp;author=Howard%2CJD&amp;author=Gottfried%2CJA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">Yoshida, I. &amp; Mori, K. Odorant category profile selectivity of olfactory cortex neurons. <i>J. Neurosci.</i> <b>27</b>, 9105â€“9114 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2sXhtVSnsLbM" aria-label="CAS reference 53">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17715347" aria-label="PubMed reference 53">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6672213" aria-label="PubMed Central reference 53">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Odorant%20category%20profile%20selectivity%20of%20olfactory%20cortex%20neurons&amp;journal=J.%20Neurosci.&amp;volume=27&amp;pages=9105-9114&amp;publication_year=2007&amp;author=Yoshida%2CI&amp;author=Mori%2CK">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR54">De Araujo, I. E., Rolls, E. T., Velazco, M. I., Margot, C. &amp; Cayeux, I. Cognitive modulation of olfactory processing. <i>Neuron</i> <b>46</b>, 671â€“679 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15944134" aria-label="PubMed reference 54">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20modulation%20of%20olfactory%20processing&amp;journal=Neuron&amp;volume=46&amp;pages=671-679&amp;publication_year=2005&amp;author=Araujo%2CIE&amp;author=Rolls%2CET&amp;author=Velazco%2CMI&amp;author=Margot%2CC&amp;author=Cayeux%2CI">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55."><p class="c-article-references__text" id="ref-CR55">Keller, A. Attention and olfactory consciousness. <i>Front. Psychol.</i> <b>2</b>, 380 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22203813" aria-label="PubMed reference 55">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3241345" aria-label="PubMed Central reference 55">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20and%20olfactory%20consciousness&amp;journal=Front.%20Psychol.&amp;volume=2&amp;publication_year=2011&amp;author=Keller%2CA">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56."><p class="c-article-references__text" id="ref-CR56">Li, W. et al. Right orbitofrontal cortex mediates conscious olfactory perception. <i>Psychol. Sci.</i> <b>21</b>, 1454â€“1463 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20817780" aria-label="PubMed reference 56">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Right%20orbitofrontal%20cortex%20mediates%20conscious%20olfactory%20perception&amp;journal=Psychol.%20Sci.&amp;volume=21&amp;pages=1454-1463&amp;publication_year=2010&amp;author=Li%2CW">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57."><p class="c-article-references__text" id="ref-CR57">Tanabe, T., Iino, M. &amp; Takagi, S. Discrimination of odors in olfactory bulb, pyriform-amygdaloid areas, and orbitofrontal cortex of the monkey. <i>J. Neurophysiol.</i> <b>38</b>, 1284â€“1296 (1975).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaE28%2Fis1Kkug%3D%3D" aria-label="CAS reference 57">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=809550" aria-label="PubMed reference 57">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Discrimination%20of%20odors%20in%20olfactory%20bulb%2C%20pyriform-amygdaloid%20areas%2C%20and%20orbitofrontal%20cortex%20of%20the%20monkey&amp;journal=J.%20Neurophysiol.&amp;volume=38&amp;pages=1284-1296&amp;publication_year=1975&amp;author=Tanabe%2CT&amp;author=Iino%2CM&amp;author=Takagi%2CS">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58."><p class="c-article-references__text" id="ref-CR58">Potter, H. &amp; Butters, N. An assessment of olfactory deficits in patients with damage to prefrontal cortex. <i>Neuropsychologia</i> <b>18</b>, 621â€“628 (1980).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaL3M7itlanuw%3D%3D" aria-label="CAS reference 58">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=7465021" aria-label="PubMed reference 58">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20assessment%20of%20olfactory%20deficits%20in%20patients%20with%20damage%20to%20prefrontal%20cortex&amp;journal=Neuropsychologia&amp;volume=18&amp;pages=621-628&amp;publication_year=1980&amp;author=Potter%2CH&amp;author=Butters%2CN">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59."><p class="c-article-references__text" id="ref-CR59">Jones-Gotman, M. &amp; Zatorre, R. J. Olfactory identification deficits in patients with focal cerebral excision. <i>Neuropsychologia</i> <b>26</b>, 387â€“400 (1988).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaL1c3jtlamsA%3D%3D" aria-label="CAS reference 59">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=3374800" aria-label="PubMed reference 59">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Olfactory%20identification%20deficits%20in%20patients%20with%20focal%20cerebral%20excision&amp;journal=Neuropsychologia&amp;volume=26&amp;pages=387-400&amp;publication_year=1988&amp;author=Jones-Gotman%2CM&amp;author=Zatorre%2CRJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60."><p class="c-article-references__text" id="ref-CR60">Keller, A. &amp; Vosshall, L. B. Olfactory perception of chemically diverse molecules. <i>BMC Neurosci.</i> <b>17</b>, 55 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27502425" aria-label="PubMed reference 60">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4977894" aria-label="PubMed Central reference 60">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Olfactory%20perception%20of%20chemically%20diverse%20molecules&amp;journal=BMC%20Neurosci.&amp;volume=17&amp;publication_year=2016&amp;author=Keller%2CA&amp;author=Vosshall%2CLB">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61."><p class="c-article-references__text" id="ref-CR61">Cai, M. B., Schuck, N. W., Pillow, J. W. &amp; Niv, Y. Representational structure or task structure? Bias in neural representational similarity analysis and a Bayesian method for reducing bias. <i>PLoS Comput. Biol.</i> <b>15</b>, e1006299 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1MXhvVOisr3L" aria-label="CAS reference 61">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31125335" aria-label="PubMed reference 61">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6553797" aria-label="PubMed Central reference 61">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Representational%20structure%20or%20task%20structure%3F%20Bias%20in%20neural%20representational%20similarity%20analysis%20and%20a%20Bayesian%20method%20for%20reducing%20bias&amp;journal=PLoS%20Comput.%20Biol.&amp;volume=15&amp;publication_year=2019&amp;author=Cai%2CMB&amp;author=Schuck%2CNW&amp;author=Pillow%2CJW&amp;author=Niv%2CY">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62."><p class="c-article-references__text" id="ref-CR62">Zhou, G., Lane, G., Cooper, S. L., Kahnt, T. &amp; Zelano, C. Characterizing functional pathways of the human olfactory system. <i>eLife</i> <b>8</b>, e47177 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXhtlaltrjE" aria-label="CAS reference 62">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31339489" aria-label="PubMed reference 62">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6656430" aria-label="PubMed Central reference 62">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Characterizing%20functional%20pathways%20of%20the%20human%20olfactory%20system&amp;journal=eLife&amp;volume=8&amp;publication_year=2019&amp;author=Zhou%2CG&amp;author=Lane%2CG&amp;author=Cooper%2CSL&amp;author=Kahnt%2CT&amp;author=Zelano%2CC">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63."><p class="c-article-references__text" id="ref-CR63">Echevarria-Cooper, S. L. et al. Mapping the microstructure and striae of the human olfactory tract with diffusion MRI. <i>J. Neurosci.</i> <b>42</b>, 58â€“68 (2021).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64."><p class="c-article-references__text" id="ref-CR64">Zald, D. H. &amp; Pardo, J. V. Emotion, olfaction, and the human amygdala: amygdala activation during aversive olfactory stimulation. <i>Proc. Natl Acad. Sci. USA</i> <b>94</b>, 4119â€“4124 (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK2sXis1eisr4%3D" aria-label="CAS reference 64">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9108115" aria-label="PubMed reference 64">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC20578" aria-label="PubMed Central reference 64">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%2C%20olfaction%2C%20and%20the%20human%20amygdala%3A%20amygdala%20activation%20during%20aversive%20olfactory%20stimulation&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;volume=94&amp;pages=4119-4124&amp;publication_year=1997&amp;author=Zald%2CDH&amp;author=Pardo%2CJV">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65."><p class="c-article-references__text" id="ref-CR65">Zatorre, R. J., Jones-Gotman, M., Evans, A. C. &amp; Meyer, E. Functional localization and lateralization of human olfactory cortex. <i>Nature</i> <b>360</b>, 339â€“340 (1992).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK3s%2FnvVyntA%3D%3D" aria-label="CAS reference 65">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=1448149" aria-label="PubMed reference 65">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=Functional%20localization%20and%20lateralization%20of%20human%20olfactory%20cortex&amp;journal=Nature&amp;volume=360&amp;pages=339-340&amp;publication_year=1992&amp;author=Zatorre%2CRJ&amp;author=Jones-Gotman%2CM&amp;author=Evans%2CAC&amp;author=Meyer%2CE">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66."><p class="c-article-references__text" id="ref-CR66">Burton, S., Murphy, D., Qureshi, U., Sutton, P. &amp; Oâ€™Keefe, J. Combined lesions of hippocampus and subiculum do not produce deficits in a nonspatial social olfactory memory task. <i>J. Neurosci.</i> <b>20</b>, 5468â€“5475 (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3cXlt1Wiu7g%3D" aria-label="CAS reference 66">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10884330" aria-label="PubMed reference 66">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6772337" aria-label="PubMed Central reference 66">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 66" href="http://scholar.google.com/scholar_lookup?&amp;title=Combined%20lesions%20of%20hippocampus%20and%20subiculum%20do%20not%20produce%20deficits%20in%20a%20nonspatial%20social%20olfactory%20memory%20task&amp;journal=J.%20Neurosci.&amp;volume=20&amp;pages=5468-5475&amp;publication_year=2000&amp;author=Burton%2CS&amp;author=Murphy%2CD&amp;author=Qureshi%2CU&amp;author=Sutton%2CP&amp;author=O%E2%80%99Keefe%2CJ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67."><p class="c-article-references__text" id="ref-CR67">Welvaert, M. &amp; Rosseel, Y. On the definition of signal-to-noise ratio and contrast-to-noise ratio for fMRI data. <i>PLoS ONE</i> <b>8</b>, e77089 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhslGrurrI" aria-label="CAS reference 67">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24223118" aria-label="PubMed reference 67">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3819355" aria-label="PubMed Central reference 67">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 67" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20definition%20of%20signal-to-noise%20ratio%20and%20contrast-to-noise%20ratio%20for%20fMRI%20data&amp;journal=PLoS%20ONE&amp;volume=8&amp;publication_year=2013&amp;author=Welvaert%2CM&amp;author=Rosseel%2CY">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68."><p class="c-article-references__text" id="ref-CR68">Tzourio-Mazoyer, N. et al. Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. <i>Neuroimage</i> <b>15</b>, 273â€“289 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD38%2FltFCntw%3D%3D" aria-label="CAS reference 68">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11771995" aria-label="PubMed reference 68">PubMed</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 68" href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20anatomical%20labeling%20of%20activations%20in%20SPM%20using%20a%20macroscopic%20anatomical%20parcellation%20of%20the%20MNI%20MRI%20single-subject%20brain&amp;journal=Neuroimage&amp;volume=15&amp;pages=273-289&amp;publication_year=2002&amp;author=Tzourio-Mazoyer%2CN">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69."><p class="c-article-references__text" id="ref-CR69">Howard, J. D. &amp; Kahnt, T. Identity prediction errors in the human midbrain update reward-identity expectations in the orbitofrontal cortex. <i>Nat. Commun.</i> <b>9</b>, 1611 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29686225" aria-label="PubMed reference 69">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5913228" aria-label="PubMed Central reference 69">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 69" href="http://scholar.google.com/scholar_lookup?&amp;title=Identity%20prediction%20errors%20in%20the%20human%20midbrain%20update%20reward-identity%20expectations%20in%20the%20orbitofrontal%20cortex&amp;journal=Nat.%20Commun.&amp;volume=9&amp;publication_year=2018&amp;author=Howard%2CJD&amp;author=Kahnt%2CT">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70."><p class="c-article-references__text" id="ref-CR70">Suarez, J. A., Howard, J. D., Schoenbaum, G. &amp; Kahnt, T. Sensory prediction errors in the human midbrain signal identity violations independent of perceptual distance. <i>eLife</i> <b>8</b>, e43962 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30950792" aria-label="PubMed reference 70">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6450666" aria-label="PubMed Central reference 70">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensory%20prediction%20errors%20in%20the%20human%20midbrain%20signal%20identity%20violations%20independent%20of%20perceptual%20distance&amp;journal=eLife&amp;volume=8&amp;publication_year=2019&amp;author=Suarez%2CJA&amp;author=Howard%2CJD&amp;author=Schoenbaum%2CG&amp;author=Kahnt%2CT">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="71."><p class="c-article-references__text" id="ref-CR71">Shanahan, L. K., Bhutani, S. &amp; Kahnt, T. Olfactory perceptual decision-making is biased by motivational state. <i>PLoS Biol.</i> <b>19</b>, e3001374 (2021).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3MXhvFalur3M" aria-label="CAS reference 71">CAS</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34437533" aria-label="PubMed reference 71">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8389475" aria-label="PubMed Central reference 71">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 71" href="http://scholar.google.com/scholar_lookup?&amp;title=Olfactory%20perceptual%20decision-making%20is%20biased%20by%20motivational%20state&amp;journal=PLoS%20Biol.&amp;volume=19&amp;publication_year=2021&amp;author=Shanahan%2CLK&amp;author=Bhutani%2CS&amp;author=Kahnt%2CT">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="72."><p class="c-article-references__text" id="ref-CR72">Noto, T., Zhou, G., Schuele, S., Templer, J. &amp; Zelano, C. Automated analysis of breathing waveforms using BreathMetrics: a respiratory signal processing toolbox. <i>Chem. Senses</i> <b>43</b>, 583â€“597 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29985980" aria-label="PubMed reference 72">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6150778" aria-label="PubMed Central reference 72">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 72" href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20analysis%20of%20breathing%20waveforms%20using%20BreathMetrics%3A%20a%20respiratory%20signal%20processing%20toolbox&amp;journal=Chem.%20Senses&amp;volume=43&amp;pages=583-597&amp;publication_year=2018&amp;author=Noto%2CT&amp;author=Zhou%2CG&amp;author=Schuele%2CS&amp;author=Templer%2CJ&amp;author=Zelano%2CC">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="73."><p class="c-article-references__text" id="ref-CR73">Nili, H. et al. A toolbox for representational similarity analysis. <i>PLoS Comput. Biol.</i> <b>10</b>, e1003553 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24743308" aria-label="PubMed reference 73">PubMed</a>Â 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3990488" aria-label="PubMed Central reference 73">PubMed Central</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20toolbox%20for%20representational%20similarity%20analysis&amp;journal=PLoS%20Comput.%20Biol.&amp;volume=10&amp;publication_year=2014&amp;author=Nili%2CH">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="74."><p class="c-article-references__text" id="ref-CR74">Esfahlani, F. Z. et al. High-amplitude cofluctuations in cortical activity drive functional connectivity. <i>Proc. Natl Acad. Sci. USA</i> <b>117</b>, 28393â€“28401 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXitlWnu7vN" aria-label="CAS reference 74">CAS</a>Â 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 74" href="http://scholar.google.com/scholar_lookup?&amp;title=High-amplitude%20cofluctuations%20in%20cortical%20activity%20drive%20functional%20connectivity&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;volume=117&amp;pages=28393-28401&amp;publication_year=2020&amp;author=Esfahlani%2CFZ">
                    Google Scholar</a>Â 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="75."><p class="c-article-references__text" id="ref-CR75">Gusfield, D. &amp; Irving, R. W. <i>The Stable Marriage Problem: Structure and Algorithms</i> (MIT Press, 1989).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="76."><p class="c-article-references__text" id="ref-CR76">Sagar, V., Shanahan, L. K., Zelano, C. M., Gottfried, J. A. &amp; Kahnt, T. Neural encoding models of olfaction (NEMO) dataset. <i>Zenodo</i> <a href="https://doi.org/10.5281/zenodo.7636722" data-track="click" data-track-action="external reference" data-track-label="10.5281/zenodo.7636722">https://doi.org/10.5281/zenodo.7636722</a> (2023).</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-023-01414-4?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank T. Parrish for help with optimizing the scanning sequence; H. Siddiqui, S. Attanti, D. Smith and R. Reynolds for help with data acquisition; and the Rockefeller University and Sage Bionetworks-DREAM for the database on molecular properties of odors. This work was supported by grants from the National Institute of Mental Health (grant no. T32 MH067564 to V.S.), the National Institute of Neurological Disorders and Stroke (grant no. T32 NS047987 to V.S.) and the Intramural Research Program at the National Institute on Drug Abuse (grant no. ZIA DA000642). The opinions expressed in this work are the authorsâ€™ own and do not reflect the view of the National Institutes of Health/Department of Health and Human Services.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Neurology, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA</p><p class="c-article-author-affiliation__authors-list">Vivek SagarÂ &amp;Â Christina M. Zelano</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Psychology, Rhodes College, Memphis, TN, USA</p><p class="c-article-author-affiliation__authors-list">Laura K. Shanahan</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Neurology, University of Pennsylvania, Philadelphia, PA, USA</p><p class="c-article-author-affiliation__authors-list">Jay A. Gottfried</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Department of Psychology, University of Pennsylvania, Philadelphia, PA, USA</p><p class="c-article-author-affiliation__authors-list">Jay A. Gottfried</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">National Institute on Drug Abuse Intramural Research Program, Baltimore, MD, USA</p><p class="c-article-author-affiliation__authors-list">Thorsten Kahnt</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Vivek-Sagar-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Vivek Sagar</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Vivek%20Sagar" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Vivek%20Sagar" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">Â </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Vivek%20Sagar%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Laura_K_-Shanahan-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Laura K. Shanahan</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Laura%20K.%20Shanahan" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Laura%20K.%20Shanahan" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">Â </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Laura%20K.%20Shanahan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Christina_M_-Zelano-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Christina M. Zelano</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Christina%20M.%20Zelano" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Christina%20M.%20Zelano" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">Â </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Christina%20M.%20Zelano%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Jay_A_-Gottfried-Aff3-Aff4"><span class="c-article-authors-search__title u-h3 js-search-name">Jay A. Gottfried</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Jay%20A.%20Gottfried" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jay%20A.%20Gottfried" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">Â </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jay%20A.%20Gottfried%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Thorsten-Kahnt-Aff5"><span class="c-article-authors-search__title u-h3 js-search-name">Thorsten Kahnt</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Thorsten%20Kahnt" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Thorsten%20Kahnt" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">Â </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Thorsten%20Kahnt%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>V.S. and T.K. conceived and designed the experiment. V.S. created the odorant stimuli and performed the experiments. V.S. and T.K. conceptualized the computational analyses. V.S. performed the data analysis. All the authors discussed the results and wrote and edited the manuscript.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:thorsten.kahnt@nih.gov">Thorsten Kahnt</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading" id="FPar4">Competing interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div></section><section data-title="Peer review"><div class="c-article-section" id="peer-review-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="peer-review">Peer review</h2><div class="c-article-section__content" id="peer-review-content">
              
              
                <h3 class="c-article__sub-heading" id="FPar3">Peer review information</h3>
                <p><i>Nature Neuroscience</i> thanks Mingbo Cai, Tali Weiss and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p><b>Publisherâ€™s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Extended data"><div class="c-article-section" id="Sec29-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec29">Extended data</h2><div class="c-article-section__content" id="Sec29-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig6"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 1 perceptual odor descriptors a" href="/articles/s41593-023-01414-4/figures/6" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig6_ESM.jpg">Extended Data Fig. 1 Perceptual odor descriptors and ratings.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, Reliability of perceptual ratings. In each subject and for each descriptor, reliability of the perceptual descriptor is computed by correlating perceptual ratings for the same odor acquired in different sessions. Gray line indicates threshold for statistical significance (râ€‰&gt;â€‰0.131, threshold pâ€‰=â€‰0.05, nâ€‰=â€‰3 subjects, 160 odors/subject, one tailed t-test) and dots are individual subjects. Reliability is computed between different fMRI sessions for S1. For S2 and S3, the average ratings acquired in two behavioral sessions outside the scanner were correlated with ratings acquired inside the scanner (S2, râ€‰=â€‰0.589; S3, râ€‰=â€‰0.660, nâ€‰=â€‰3 subjects, 160 odors/subject). The correlation of odor-wise descriptor ratings (averaged across odors) between S2 and S3 was 0.377. <b>b</b>, Histogram of discriminability of odors for the average subject. Discriminability between two odors is the absolute difference (in standard deviations) of the perceptual feature with maximum difference. <b>c</b>, Perceptual similarity matrices for all subjects. Each cell in the matrix depicts the correlation between the perceptual ratings of two odors. For illustration, rows and columns are sorted using k-means, independently for each subject. <b>d</b>, Generalizability of perceptual ratings across subjects is computed as the correlation between the (off-diagonal entries of) the perceptual similarity matrices of two subjects and averaged across all subject pairs (râ€‰=â€‰0.168, pâ€‰=â€‰0.0000, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject, two-tailed t-test). Dots indicate subject pairs. The gray line indicates the threshold for statistical significance (râ€‰&gt;â€‰0.022, threshold pâ€‰=â€‰0.05, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject, two-tailed t-test). Error bars indicate 95% C.I.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig7"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 2 neural responses to odors." href="/articles/s41593-023-01414-4/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig7_ESM.jpg">Extended Data Fig. 2 Neural responses to odors.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, Task design comprising of self-paced behavioral task (top-panel) to acquire at least two sets of ratings per odor per descriptor and fMRI task (bottom panel) to rate the odors. S1 provided ratings in all fMRI sessions, whereas S2 and S3 did not rate odors in the third fMRI session. <b>b</b>, Odor-evoked fMRI response in each ROI for each subject. Shaded areas depict 95% C.I. for the mean (black lines) per subject. Peaks in all areas occurred at least 4â€‰seconds after odor presentation. Analyses were restricted to up to 6â€‰seconds to avoid confounding the neural activity with the perceptual rating task. For OFC in S3, BOLD response does not return to baseline, highlighting individual and inter-regional variability in the shape of the hemodynamic response. <b>c</b>, Mean percentage of gray matter voxels with significant odor-evoked responses for each ROI. Error bars depict 95% C.I. and lines depict individual subjects (nâ€‰=â€‰3 subjects, 160 odors/subject). <b>d</b>, Average temporal signal to noise ratio (t-snr: mean/standard deviation of the voxel time-series) in an ROI. Bars denote mean effects and errorbars are s.e.m. across subjects (nâ€‰=â€‰3 subjects, 160 odors/subject). t-snr did not differ significantly across areas (F<sub>3,8</sub>â€‰=â€‰0.39, pâ€‰=â€‰0.78, one way ANOVA). <b>e</b>, Neural similarity matrices for each ROI in each subject. Each cell in the matrix depicts the correlation between the multi-voxel response patterns of two odors. For illustration purposes, rows and columns are sorted using k-means (4 total clusters), independently for each subject. <b>f</b>, Correlation of neural activity patterns evoked by the same odor in different sessions (pattern reliability), averaged across odors and subjects. Error bars indicate s.e.m. across subjects. Pattern reliability is significant in all areas and in all subjects (râ€‰&gt;â€‰0, pâ€‰=â€‰0.000, Wilcoxon signed rank test, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject), except PirF in S3 (râ€‰=â€‰0.04, pâ€‰=â€‰0.086, Wilcoxon signed rank test, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject). <b>g</b>, Pattern reliability separately measured between sessions 1 and 2, sessions 2 and 3, and sessions 1 and 3. Pattern reliability between sessions 1 and 2 and 2 and 3 is not significantly different from pattern reliability between sessions 1 and 3 (F<sub>1,6</sub>â€‰=â€‰0.02, pâ€‰=â€‰0.90, repeated measures 2-way ANOVA with session pairs and ROI as factors). There was no significant main effect of ROI (F<sub>3,6</sub>â€‰=â€‰2.07, pâ€‰=â€‰0.206), and no significant interaction (F<sub>3,6</sub>â€‰=â€‰2.12, pâ€‰=â€‰0.198), suggesting that odor-evoked activity patterns remained stable across fMRI sessions. Error bars indicate 95% C.I. For all tests, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig8"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 3 representational similarity a" href="/articles/s41593-023-01414-4/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig8_ESM.jpg">Extended Data Fig. 3 Representational similarity analysis (RSA) for individual subjects.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>RSA analysis based on coarse and fine-grained perceptual similarity for individual subjects. Correlations were taken across 12,720 odor pairs. <b>a</b>, Bars depict the Spearman rank-correlation between neural and coarse perceptual similarity (r<sub>c</sub> hatched) or fine-grained perceptual similarity matrices (r<sub>f</sub> solid), for individual subjects. Bars indicate mean correlation and error bars depict 95% C.I. (perc. bootstrap). In all subjects, fine-grained and coarse perceptual representational similarity is significant in AMY and OFC. In subject S1, representation of fine-grained perceptual similarity is significantly higher than coarse perceptual similarity in OFC, but not in any other area (PirF, r<sub>c</sub>â€‰=â€‰0.010, pâ€‰=â€‰0.132, r<sub>f</sub>â€‰=â€‰0.012, pâ€‰=â€‰0.075 p(r<sub>f</sub>&gt;r<sub>c</sub>)=0.692; PirT, r<sub>c</sub>â€‰=â€‰0.016, pâ€‰=â€‰0.022, r<sub>f</sub>â€‰=â€‰0.017, pâ€‰=â€‰0.019, p(r<sub>f</sub>&gt;r<sub>c</sub>)=0.932; AMY, r<sub>c</sub>â€‰=â€‰0.018, pâ€‰=â€‰0.002, r<sub>f</sub>â€‰=â€‰0.025, pâ€‰=â€‰0.0000, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.127; OFC, r<sub>c</sub>â€‰=â€‰0.037, pâ€‰=â€‰0.0000, r<sub>f</sub>â€‰=â€‰0.061, pâ€‰=â€‰0.0000, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.005, pâ€‰=â€‰0.472, r<sub>f</sub>â€‰=â€‰âˆ’0.0001, pâ€‰=â€‰0.988, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.312; wm, r<sub>c</sub>â€‰=â€‰0.006, pâ€‰=â€‰0.351, r<sub>f</sub>â€‰=â€‰âˆ’0.002, pâ€‰=â€‰0.721, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.057, two-tailed bootstrap comparison). In subject S2, representation of fine-grained perceptual similarity is significantly higher than coarse perceptual similarity in OFC, but not in other areas (PirF, r<sub>c</sub>â€‰=â€‰-0.005, pâ€‰=â€‰0.442, r<sub>f</sub>â€‰=â€‰-0.013, pâ€‰=â€‰0.050, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.060; PirT, r<sub>c</sub>â€‰=â€‰0.002, pâ€‰=â€‰0.793, r<sub>f</sub>â€‰=â€‰0.009, pâ€‰=â€‰0.223, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.095; AMY, r<sub>c</sub>â€‰=â€‰0.026, pâ€‰=â€‰0.0000, r<sub>f</sub>â€‰=â€‰0.030, pâ€‰=â€‰0.0000, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.290; OFC, r<sub>c</sub>â€‰=â€‰0.051, pâ€‰=â€‰0.0000, r<sub>f</sub>â€‰=â€‰0.067, pâ€‰=â€‰0.0000, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.012, pâ€‰=â€‰0.076, r<sub>f</sub>â€‰=â€‰0.016, pâ€‰=â€‰0.015, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.290; wm, r<sub>c</sub>â€‰=â€‰-0.003, pâ€‰=â€‰0.619, r<sub>f</sub>â€‰=â€‰-0.006, pâ€‰=â€‰0.330, p(r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.463, two-tailed bootstrap comparison). In subject S3, representation of fine-grained perceptual similarity is significantly higher than coarse perceptual similarity in PirT, AMY and OFC, but not in PirF, A1 and wm (PirF, r<sub>c</sub>â€‰=â€‰0.007, pâ€‰=â€‰0.295, r<sub>f</sub>â€‰=â€‰0.0002, pâ€‰=â€‰0.960, p(r<sub>f</sub>â€‰&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.177; PirT, r<sub>c</sub>â€‰=â€‰0.026, pâ€‰=â€‰0.0000, r<sub>f</sub>â€‰=â€‰0.039, pâ€‰=â€‰0.0000, p(r<sub>f</sub>â€‰&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.009; AMY, r<sub>c</sub>â€‰=â€‰0.030, pâ€‰=â€‰0.0000, r<sub>f</sub>â€‰=â€‰0.041, pâ€‰=â€‰0.0000, p(r<sub>f</sub>â€‰&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.018; OFC, r<sub>c</sub>â€‰=â€‰0.101, pâ€‰=â€‰0.0000, r<sub>f</sub>â€‰=â€‰0.122, pâ€‰=â€‰0.0000, p(r<sub>f</sub>â€‰&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰âˆ’0.0002, pâ€‰=â€‰0.976, r<sub>f</sub>â€‰=â€‰0.005, pâ€‰=â€‰0.460, p(r<sub>f</sub>â€‰&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.282; wm, r<sub>c</sub>â€‰=â€‰0.005, pâ€‰=â€‰0.476, r<sub>f</sub>â€‰=â€‰âˆ’0.002, pâ€‰=â€‰0.771, p(r<sub>f</sub>â€‰&gt;â€‰r<sub>c</sub>)â€‰=â€‰0.553, two-tailed bootstrap comparison). Thus, OFC is the only ROI where the fine-grained RSA exceeds the coarse RSA in all three subjects. <b>b</b>, Difference between the neural representation of fine-grained and coarse perceptual similarity in <b>a</b> (r). Bars depict mean correlation difference in each subject, error bars depict 95% C.I. (perc. bootstrap). The difference is significantly larger in OFC than in PirF in all subjects (OFC-PirF all subjects, pâ€‰=â€‰0.0000), in PirT for S1 (pâ€‰=â€‰0.0012) but not S2 (pâ€‰=â€‰0.106) or S3 (pâ€‰=â€‰0.211) and in AMY for S1 (pâ€‰=â€‰0.0012) and S2 (pâ€‰=â€‰0.025) but not in S3 (pâ€‰=â€‰0.171) (two-tailed bootstrap comparison, 12720 odor pairs). The difference between the coarse and fine-grained RSA is maximum in OFC across areas for all subjects. Further, OFC is the only area where the difference between the coarse and fine-grained RSA is significant across all subjects.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig9"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 4 control analyses for rsa." href="/articles/s41593-023-01414-4/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig9_ESM.jpg">Extended Data Fig. 4 Control analyses for RSA.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>We performed control RSAs in olfactory ROIs as well as control areas A1 (primary auditory cortex) and wm (white matter voxels). For statistics on subject-wise results, see Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01414-4#MOESM1">2</a>. <b>a</b>, (Top panel) bars depict the Spearman rank-correlation between neural and coarse (r<sub>c</sub> hatched) or fine-grained perceptual similarity matrices (r<sub>f</sub> solid), averaged across subjects, adjusted to include intensity and pleasantness. r<sub>f</sub>&gt;r<sub>c</sub> in all areas except PirF, A1 and wm. All p-values are based on null hypothesis r<sub>c</sub> = r<sub>f</sub>, tested using two tailed bootstrap comparison (PirF, r<sub>c</sub>â€‰=â€‰0.005, r<sub>f</sub>â€‰=â€‰0.005, pâ€‰=â€‰0.992; PirT, r<sub>c</sub>â€‰=â€‰0.022, r<sub>f</sub>â€‰=â€‰0.035, pâ€‰=â€‰0.0000; AMY, r<sub>c</sub>â€‰=â€‰0.040, r<sub>f</sub>â€‰=â€‰0.059, pâ€‰=â€‰0.0000; OFC, r<sub>c</sub>â€‰=â€‰0.084, r<sub>f</sub>â€‰=â€‰0.120, pâ€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.014, r<sub>f</sub>â€‰=â€‰0.015, pâ€‰=â€‰0.734; wm, r<sub>c</sub>â€‰=â€‰0.008, r<sub>f</sub>â€‰=â€‰0.002, pâ€‰=â€‰0.03). Note that in wm, r<sub>c</sub> significantly exceeds r<sub>f</sub> (that is, r<sub>câ€‰</sub>&gt;â€‰r<sub>f</sub>), which is the opposite of what is expected and found in olfactory brain areas, and testing r<sub>fâ€‰</sub>&gt;â€‰r<sub>c</sub> using a one-tailed test is not significant (pâ€‰=â€‰0.97). (Bottom panel) Difference between the fine-grained and coarse representational similarity in <b>a</b>, top panel (r). Difference is significantly higher in OFC than in PirF, PirT, AMY, A1 or wm (all areas, pâ€‰=â€‰0.0000, two-tailed bootstrap comparison). <b>b</b>, (Top panel) bars depict the Spearman rank correlation between neural and coarse (r<sub>c</sub> hatched) or fine-grained perceptual similarity matrices (r<sub>f</sub> solid), averaged across subjects, adjusted to account for differences in size of the ROI. 70 voxels were chosen with replacement from each ROI and subject to construct the neural similarity matrix. r<sub>f</sub>&gt;r<sub>c</sub> only in the OFC and not other areas (PirF, r<sub>c</sub>â€‰=â€‰0.004, r<sub>f</sub>â€‰=â€‰0.000, pâ€‰=â€‰0.196; PirT, r<sub>c</sub>â€‰=â€‰0.012, r<sub>f</sub>â€‰=â€‰0.018, pâ€‰=â€‰0.080; AMY, r<sub>c</sub>â€‰=â€‰0.019, r<sub>f</sub>â€‰=â€‰0.026, pâ€‰=â€‰0.077; OFC, r<sub>c</sub>â€‰=â€‰0.049, r<sub>f</sub>â€‰=â€‰0.064, pâ€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.005, r<sub>f</sub>â€‰=â€‰0.006, pâ€‰=â€‰0.745; wm, r<sub>c</sub>â€‰=â€‰0.002, r<sub>f</sub>â€‰=â€‰âˆ’0.002, pâ€‰=â€‰0.172). (Bottom panel) Difference between the fine-grained and coarse representational similarity in <b>b</b>, top panel (r). Difference is significantly higher in OFC than in PirF, A1 or wm (pâ€‰=â€‰0.0000) and trending for PirT (pâ€‰=â€‰0.053) and AMY (pâ€‰=â€‰0.074). <b>c</b>, (Top panel) bars depict the Spearman rank correlation between neural and coarse (r<sub>c</sub> hatched) or fine-grained perceptual similarity matrices (r<sub>f</sub> solid), averaged across subjects, adjusted to account for perceptual correlations with molecular features. 4869 molecular features were used to construct the molecular similarity matrix. Molecular similarity was regressed out from both fine-grained and coarse perceptual similarity matrices. r<sub>f</sub>&gt;r<sub>c</sub> in all areas except PirF, A1 and wm (PirF, r<sub>c</sub>â€‰=â€‰0.003, r<sub>f</sub>â€‰=â€‰-0.000, pâ€‰=â€‰0.113; PirT, r<sub>c</sub>â€‰=â€‰0.013, r<sub>f</sub>â€‰=â€‰0.020, pâ€‰=â€‰0.010; AMY, r<sub>c</sub>â€‰=â€‰0.021, r<sub>f</sub>â€‰=â€‰0.029, pâ€‰=â€‰0.002; OFC, r<sub>c</sub>â€‰=â€‰0.058, r<sub>f</sub>â€‰=â€‰0.078, pâ€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.004, r<sub>f</sub>â€‰=â€‰0.006, pâ€‰=â€‰0.538; wm, r<sub>c</sub>â€‰=â€‰0.002, r<sub>f</sub> =âˆ’0.003, pâ€‰=â€‰0.060). (Bottom panel) Difference between the fine-grained and coarse representational similarity in <b>c</b>, top panel (r). Difference is significantly higher in OFC than all areas (PirF, A1, wm, pâ€‰=â€‰0.0000; PirT, pâ€‰=â€‰0.0004; AMY, pâ€‰=â€‰0.0002). <b>d</b>, (Top panel) bars depict the Spearman rank correlation between neural and coarse (r<sub>c</sub> hatched) or fine-grained perceptual similarity matrices (r<sub>f</sub> solid), averaged across subjects, after excluding odors with low detectability. r<sub>f</sub>&gt;r<sub>c</sub> all areas except wm (PirF, r<sub>c</sub>â€‰=â€‰0.002, r<sub>f</sub>â€‰=â€‰-0.007, pâ€‰=â€‰0.005; PirT, r<sub>c</sub>â€‰=â€‰0.006, r<sub>f</sub>â€‰=â€‰0.015, pâ€‰=â€‰0.008; AMY, r<sub>c</sub>â€‰=â€‰0.021, r<sub>f</sub>â€‰=â€‰0.030, pâ€‰=â€‰0.007; OFC, r<sub>c</sub>â€‰=â€‰0.051, r<sub>f</sub>â€‰=â€‰0.078, pâ€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.000, r<sub>f</sub>â€‰=â€‰0.087, pâ€‰=â€‰0.049; wm, r<sub>c</sub>â€‰=â€‰âˆ’0.001, r<sub>f</sub>â€‰=â€‰0.000, pâ€‰=â€‰0.701). (Bottom panel) Difference between the fine-grained and coarse representational similarity in <b>d</b>, top panel (r). Difference is significantly higher in OFC than all areas (PirF, AMY, A1, wm pâ€‰=â€‰0.0000; PirT, pâ€‰=â€‰0.0001;). <b>e</b>, (Top panel) bars depict the Spearman rank correlation between neural and coarse (r<sub>c</sub> hatched) or fine-grained perceptual similarity matrices (r<sub>f</sub> solid), averaged across subjects when neural responses were extracted from the same time bin (5â€‰second after odor onset) in all areas and subjects. r<sub>f</sub>â€‰&gt;â€‰r<sub>c</sub> in PirT, AMY and OFC but not other areas (PirF, r<sub>c</sub>â€‰=â€‰-0.001, r<sub>f</sub>â€‰=â€‰0.002, pâ€‰=â€‰0.184; PirT, r<sub>c</sub>â€‰=â€‰0.013, r<sub>f</sub>â€‰=â€‰0.020, pâ€‰=â€‰0.012; AMY, r<sub>c</sub>â€‰=â€‰0.030, r<sub>f</sub>â€‰=â€‰0.038, pâ€‰=â€‰0.002; OFC, r<sub>c</sub>â€‰=â€‰0.060, r<sub>f</sub>â€‰=â€‰0.081, pâ€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.002, r<sub>f</sub>â€‰=â€‰0.002, pâ€‰=â€‰0.859; wm, r<sub>c</sub>â€‰=â€‰0.000, r<sub>f</sub>â€‰=â€‰0.001, pâ€‰=â€‰0.733). (Bottom panel) Difference between the fine-grained and coarse representational similarity in <b>e</b>, top panel (r). Difference is significantly higher in OFC than all areas (all areas, pâ€‰=â€‰0.0000). <b>f</b>, (Top panel) bars depict the Pearsonâ€™s (instead of Spearman) correlation between neural and coarse (r<sub>c</sub> hatched) or fine-grained perceptual similarity matrices (r<sub>f</sub> solid), averaged across subjects. r<sub>f</sub>&gt;r<sub>c</sub> only in PirT, AMY and OFC but not other areas (PirF, r<sub>c</sub>â€‰=â€‰0.002, r<sub>f</sub>â€‰=â€‰0.002, pâ€‰=â€‰0.97; PirT, r<sub>c</sub>â€‰=â€‰0.016, r<sub>f</sub>â€‰=â€‰0.022, pâ€‰=â€‰0.046; AMY, r<sub>c</sub>â€‰=â€‰0.027, r<sub>f</sub>â€‰=â€‰0.037, pâ€‰=â€‰0.002; OFC, r<sub>c</sub>â€‰=â€‰0.070, r<sub>f</sub>â€‰=â€‰0.093, pâ€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.005, r<sub>f</sub>â€‰=â€‰0.008, pâ€‰=â€‰0.360; wm, r<sub>c</sub>â€‰=â€‰0.002, r<sub>f</sub>â€‰=â€‰0.000, pâ€‰=â€‰0.364). (Bottom panel) Difference between the fine-grained and coarse representational similarity in <b>f</b>, top panel (r). Difference is significantly higher in OFC than all areas (all areas, pâ€‰=â€‰0.0000). For all panels, error bars depict 95% C.I. (perc. bootstrap) and comparisons are based on two tailed bootstrap comparison, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig10"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 5 statistical control analyses " href="/articles/s41593-023-01414-4/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig10_ESM.jpg">Extended Data Fig. 5 Statistical control analyses for RSA.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, To account for potential statistical biases in the bootstrap procedure, we performed additional permutation tests for perceptual and molecular RSA effects (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig2">2b</a>). For this, we generated null distributions by randomly shuffling perceptual and molecular ratings across odors. Plots show the means and 95% C.I. for the null distributions of perceptual and molecular RSA effects, which were (as expected) not significantly different from zero in any area for any subject (pâ€‰&gt;â€‰0.2, all areas, all subjects). Solid lines indicate 95% C.I. for perceptual RSA and dashed lines indicate 95% C.I. (two tailed percentile bootstrap) for molecular RSA. Importantly, we used these null distributions to compute p-values for the perceptual and molecular RSA shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig2">2b</a>, confirming that r<sub>p</sub> is significant in PirT (pâ€‰=â€‰0.0000), AMY (pâ€‰=â€‰0.0000), OFC (pâ€‰=â€‰0.0000), A1 (pâ€‰=â€‰0.008) but not PirF (pâ€‰=â€‰0.308) or wm (pâ€‰=â€‰0.733). Moreover, r<sub>p</sub> significantly exceeds r<sub>m</sub> in OFC (pâ€‰=â€‰0.0000) but not in PirF (pâ€‰=â€‰0.288), PirT (pâ€‰=â€‰0.102), AMY (pâ€‰=â€‰0.173), A1 (pâ€‰=â€‰0.99) or wm (0.741, two tailed permutation test). To further test for biases in the bootstrap approach, we tested whether the number of odor pairs selected in each bootstrap affects the results. That is, we computed the correlation between the number of unique odor pairs in each bootstrap and r<sub>p</sub> and r<sub>m</sub> which was not significant in most areas and subjects (all areas, pâ€‰&gt;â€‰0.05, one sample t-test) except AMY in S1 (pâ€‰=â€‰0.035, one sample t-test). <b>b</b>, To account for potential statistical biases in the bootstrap procedure, we performed additional permutation tests for coarse and fine-grained perceptual RSA effects (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3b</a>). Similar to the analysis described in panel <b>a</b>, we generated null distributions by randomly shuffling perceptual ratings across odors. Plots show the means and 95% C.I. (two tailed percentile bootstrap) for the null distributions of coarse and fine-grained perceptual RSA effects, which were (as expected) not significantly different from zero in any area for any subject (pâ€‰&gt;â€‰0.2, all areas, all subjects). Solid lines indicate 95% C.I. for fine-grained RSA and dashed lines indicate 95% C.I. for coarse RSA. Importantly, we used these null distributions to compute p-values for the coarse and fine-grained perceptual RSA effects shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3b</a>, confirming that r<sub>c</sub> is significant in PirT (pâ€‰=â€‰0.006), AMY (pâ€‰=â€‰0.0000), OFC (pâ€‰=â€‰0.0000), but not PirF (pâ€‰=â€‰0.401), A1 (pâ€‰=â€‰0.280) or wm (pâ€‰=â€‰0.589), whereas r<sub>f</sub> is significant PirT (pâ€‰=â€‰0.0003), AMY (pâ€‰=â€‰0.0000), OFC (pâ€‰=â€‰0.0000), but not PirF (pâ€‰=â€‰0.98), A1 (pâ€‰=â€‰0.182) or wm (pâ€‰=â€‰0.660). Moreover, r<sub>f</sub> &gt; r<sub>c</sub> is significant in AMY (pâ€‰=â€‰0.0232), OFC (pâ€‰=â€‰0.0000) and trending in PirT (pâ€‰=â€‰0.051), but not significant in PirF (pâ€‰=â€‰0.198), A1 (pâ€‰=â€‰0.651) or wm (0.147, two tailed permutation test). <b>c</b>, To further validate our RSA results, we compared r<sub>c</sub> and r<sub>f</sub> in olfactory areas to r<sub>c</sub> and r<sub>f</sub> in our control area A1. All olfactory areas (except PirF) had significantly larger representational similarities for fine-grained (r<sub>f</sub>) odor percepts than A1 (difference between representational similarities in the ROI and A1 denoted by ROI-A1) (r<sub>c</sub>: PirF-A1, pâ€‰=â€‰0.794; PirT-A1, pâ€‰=â€‰0.058; AMY-A1, pâ€‰=â€‰0.0000; OFC-A1, pâ€‰=â€‰0.0000; wm-A1, pâ€‰=â€‰0.601; r<sub>f</sub>: PirF-A1, pâ€‰=â€‰0.161; PirT-A1, pâ€‰=â€‰0.002; AMY-A1, pâ€‰=â€‰0.0000; OFC-A1, pâ€‰=â€‰0.0000; wm-A1, pâ€‰=â€‰0.086, two tailed bootstrap comparison). For all panels, bars indicate mean effects and error bars depict 95% C.I. (perc. bootstrap), nâ€‰=â€‰3 subjects, 12720 odor pairs/subject.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig11"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 6 rsa control analyses for inte" href="/articles/s41593-023-01414-4/figures/11" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig11_ESM.jpg">Extended Data Fig. 6 RSA control analyses for intensity, pleasantness and sniff evoked activity.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, We examined representational similarities based exclusively on intensity or pleasantness. The intensity RSA is significant in all areas (PirF, PirT, AMY, OFC, A1, pâ€‰=â€‰0.0000; wm, pâ€‰=â€‰0.033), while the pleasantness RSA is significant only in the olfactory areas: PirF, PirT, AMY and OFC but not A1 or wm (PirF, pâ€‰=â€‰0.002; PirT, AMY, OFC, pâ€‰=â€‰0.0000; A1, pâ€‰=â€‰0.105; wm, pâ€‰=â€‰0.42, two tailed bootstrap comparison, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject). <b>b</b>, RSA results when intensity or pleasantness is regressed out of the perceptual descriptor ratings. Two RSA models were constructed: one without intensity and one without pleasantness. The RSA without intensity is significant in PirT, AMY and OFC but not PirF, A1 or wm (PirF, pâ€‰=â€‰0.345; PirT, pâ€‰=â€‰0.006; AMY, pâ€‰=â€‰0.0000; OFC, pâ€‰=â€‰0.0000; A1, pâ€‰=â€‰0.903; wm, pâ€‰=â€‰0.125). The RSA without pleasantness is significant in PirF, PirT, AMY, OFC, A1 but not wm (PirF, pâ€‰=â€‰0.039; PirT, AMY, OFC, A1, pâ€‰=â€‰0.0000; wm, pâ€‰=â€‰0.778, two tailed bootstrap comparison). This suggests that perceptual encoding does not exclusively rely on intensity and/or pleasantness in olfactory areas (PirT, AMY or OFC) and that RSA results in the A1 control area are exclusively driven by odor intensity. For all tests, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject. <b>c</b>, Pearsonâ€™s correlation of intensity ratings and sniff volumes (averaged across all trials) across 160 odors for each subject. <b>d</b>, Pearsonâ€™s correlation of intensity ratings and sniff durations (averaged across all trials) across 160 odors for each subject. <b>e</b>, Regressing odor similarity based on sniff volume from intensity and pleasantness similarity and computing the residual RSA for intensity and pleasantness (similar to <b>a</b>). The intensity RSA is significant in all areas (PirF, PirT, AMY, OFC, A1, pâ€‰=â€‰0.0000, two-tailed boostrap comparison) except wm, pâ€‰=â€‰0.128, while the pleasantness RSA is significant only in the olfactory areas: PirF, PirT, AMY and OFC but not A1 or wm (PirF, pâ€‰=â€‰0.001; PirT, pâ€‰=â€‰0.002; AMY, OFC, pâ€‰=â€‰0.0000; A1, pâ€‰=â€‰0.639; wm, pâ€‰=â€‰0.543, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject). <b>f</b>, Regressing odor similarity based on sniff duration from intensity and pleasantness similarity and computing the residual RSA for intensity and pleasantness (similar to <b>a</b>). The intensity RSA is significant in all areas (PirF, PirT, AMY, OFC, pâ€‰=â€‰0.0000; A1,pâ€‰=â€‰0.001, two tailed boostrap comparison) except wm, pâ€‰=â€‰0.392, while the pleasantness RSA is significant only in the olfactory areas: PirF, PirT, AMY and OFC but not A1 or wm (PirF, pâ€‰=â€‰0.005; PirT, pâ€‰=â€‰0.044; AMY, pâ€‰=â€‰0.004; OFC, pâ€‰=â€‰0.0000; A1, pâ€‰=â€‰0.616; wm, pâ€‰=â€‰0.792, nâ€‰=â€‰3 subjects, 12720 odor pairs/subject). <b>g</b>, We regressed odor similarity based on sniff volume from coarse and fine-grained perceptual similarity and computed the residual RSA. Results are similar to Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3b</a>. r<sub>f</sub>â€‰&gt;â€‰r<sub>c</sub> in all areas except PirF, A1 and wm (PirF, r<sub>c</sub>â€‰=â€‰0.002, r<sub>f</sub>â€‰=â€‰âˆ’0.001, pâ€‰=â€‰0.150; PirT, r<sub>c</sub>â€‰=â€‰0.012, r<sub>f</sub>â€‰=â€‰0.018, pâ€‰=â€‰0.019; AMY, r<sub>c</sub>â€‰=â€‰0.021, r<sub>f</sub>â€‰=â€‰0.029, pâ€‰=â€‰0.004; OFC, r<sub>c</sub>â€‰=â€‰0.059, r<sub>f</sub>â€‰=â€‰0.098, pâ€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.003, r<sub>f</sub>â€‰=â€‰0.004, pâ€‰=â€‰0.638; wm, r<sub>c</sub>â€‰=â€‰0.002, r<sub>f</sub>â€‰=â€‰âˆ’0.003, pâ€‰=â€‰0.055). <b>h</b>, We regressed odor similarity based on sniff duration from coarse and fine-grained perceptual similarity and computed the residual RSA. Results are similar to Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3b</a>. r<sub>f</sub>â€‰&gt;â€‰r<sub>c</sub> in all areas except PirF, A1 and wm (PirF, r<sub>c</sub>â€‰=â€‰0.003, r<sub>f</sub>â€‰=â€‰-0.001, pâ€‰=â€‰0.090; PirT, r<sub>c</sub>â€‰=â€‰0.012, r<sub>f</sub>â€‰=â€‰0.018, pâ€‰=â€‰0.006; AMY, r<sub>c</sub>â€‰=â€‰0.020, r<sub>f</sub>â€‰=â€‰0.027, pâ€‰=â€‰0.003; OFC, r<sub>c</sub>â€‰=â€‰0.057, r<sub>f</sub>â€‰=â€‰0.077, pâ€‰=â€‰0.0000; A1, r<sub>c</sub>â€‰=â€‰0.003, r<sub>f</sub>â€‰=â€‰0.004, pâ€‰=â€‰0.680; wm, r<sub>c</sub>â€‰=â€‰0.002, r<sub>f</sub>â€‰=â€‰âˆ’0.003, pâ€‰=â€‰0.063). In all panels, error bars indicate 95% C.I.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig12"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 7 rsa for increasing numbers of" href="/articles/s41593-023-01414-4/figures/12" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig12_ESM.jpg">Extended Data Fig. 7 RSA for increasing numbers of perceptual descriptors.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>A</b>, Perceptual representational similarity as a function of the number of perceptual descriptors used in estimating perceptual similarity. The case when only 1 descriptor is used corresponds to coarse representational similarity while the case when 16 descriptors are used corresponds to fine-grained representational similarity (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig3">3b</a>). <b>b</b>, Slope of perceptual representational similarity as a function of number of perceptual descriptors used. Error bars are s.e.m. across subjects. Slopes are maximal for OFC in all subjects (F<sub>3,8</sub>â€‰=â€‰6.99, pâ€‰=â€‰0.013, one way ANOVA, nâ€‰=â€‰3 subjects). This indicates that fine-grained representational similarity in the OFC increases as additional descriptors are added in the model.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig13"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 8 control analyses for encoding" href="/articles/s41593-023-01414-4/figures/13" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig13_ESM.jpg">Extended Data Fig. 8 Control analyses for encoding models.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>A</b>, Mean prediction accuracy of the encoding model using 14 orthogonal principal components (explaining at least 90% of the variance) of the perceptual descriptors as basis functions. <b>B</b>, Percentage of odor-responsive gray matter voxels with significant prediction accuracy (threshold pâ€‰=â€‰0.05, one-tailed one-sample t-test, FDR corrected) with PCA basis. <b>c</b>, Dimensionality of encoding for the encoding model with PCA basis. Dimensionality of encoding increases from PirF to OFC (pâ€‰=â€‰0.000, FWE against the null hypothesis Îº(PirF)â€‰=â€‰Îº(PirT)â€‰=â€‰Îº(AMY)â€‰=â€‰Îº(OFC), two-tailed bootstrap comparison). <b>d</b>, Mean prediction accuracy of the encoding model with 4-fold cross-validation where training and test odors came from independent scanning sessions. <b>e</b>, Percentage of odor-responsive gray matter voxels with significant prediction accuracy (threshold pâ€‰=â€‰0.05, one-tailed one-sample t-test, FDR corrected) for encoding model with 4-fold cross-validation. <b>f</b>, Dimensionality of encoding for the encoding model with 4-fold cross-validation. Dimensionality of encoding increases from PirF to OFC (pâ€‰=â€‰0.000, FWE against the null hypothesis Îº(PirF)â€‰=â€‰Îº(PirT)â€‰=â€‰Îº(AMY)â€‰=â€‰Îº(OFC), two-tailed bootstrap comparison). <b>g</b>, Prediction accuracy of encoding model with shuffled perceptual ratings is not significant for any area in any subject (pâ€‰&gt;â€‰0.1, all areas, all subjects, two tailed shuffle test). <b>h</b>, Mean prediction accuracy of the encoding model without odors with low detectability is significantly greater than zero in all ROIs and subjects (except PirF in subject S1, pâ€‰=â€‰0.65, PirF S3, pâ€‰=â€‰0.03, remaining areas/subjects pâ€‰=â€‰0.0000, two sided Wilcoxon signed rank test). These results are qualitatively similar to those obtained when odors with low detectability are included (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4c</a>). <b>i</b>, Mean prediction accuracy of encoding model in primary auditory cortex (A1) and white matter (wm) (A1, mean râ€‰=â€‰0.027; wm mean râ€‰=â€‰0.045) are much lower than those observed in olfactory areas (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01414-4#Fig4">4c</a>). <b>j</b>, Percentage of voxels in A1 and wm that show significant prediction accuracy (threshold pâ€‰=â€‰0.05, one-tailed one-sample t-test, FDR corrected). For all panels, bars indicate mean effects and error bars indicate 95% C.I. All tests were based on nâ€‰=â€‰3 subjects, 160 odors/subject.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig14"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 9 dimensionality of encoded per" href="/articles/s41593-023-01414-4/figures/14" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig14_ESM.jpg">Extended Data Fig. 9 Dimensionality of encoded perceptual spaces for individual subjects.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, Cumulative percentage of explained variance in the voxel-wise encoding weights as a function of the number of principal components, for individual subjects. <b>b</b>, Dimensionality parameter (Îº) is proportional to area under the curve in a and reflects the number of principal components required to explain a given percentage of variance explained in each subject. Bars depict mean effect and error bars depict 95% C.I. (perc. bootstrap) across nâ€‰=â€‰3 subjects. The dimensionality of perceptual encoding is maximum in OFC in each subject and significantly different across areas (pâ€‰=â€‰0.000 (FWE corrected) against the null hypothesis Îº(PirF)â€‰=â€‰Îº(PirT)â€‰=â€‰Îº(AMY)â€‰=â€‰Îº(OFC), two-tailed bootstrap comparison, nâ€‰=â€‰3 subjects, 160 odors/subject). <b>c</b>, Dimensionality estimation adjusted for differences in ROI size. 25 voxels were chosen with replacement from each ROI to estimate the principal components in each bootstrap. <b>d</b>, Adjusted dimensionality increases from PirF to PirT to AMY and to OFC. Adjusted dimensionality is maximum in OFC and significantly different across areas (pâ€‰=â€‰0.002 (FWE corrected) against the null hypothesis Îº(PirF)â€‰=â€‰Îº(PirT)â€‰=â€‰Îº(AMY)â€‰=â€‰Îº(OFC), two-tailed bootstrap comparison, nâ€‰=â€‰3 subjects, 160 odors/subject). Error bars indicate 95% C.I. <b>e</b> Average PCA coefficients of perceptual feature weights for different principal components in PirF, PirT and AMY. PC1 is primarily driven by intensity, whereas subsequent components are more heterogeneous in all ROIs.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig15"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 10 subject-specific and cross-s" href="/articles/s41593-023-01414-4/figures/15" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_Fig15_ESM.jpg">Extended Data Fig. 10 Subject-specific and cross-subject encoding model.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, Mean prediction accuracy of encoding models based on fMRI data and perceptual ratings provided by the same subject (subject-specific encoding model [EM], dark) and fMRI data and ratings provided by different subjects (cross-subject EM, light bars). Subject-specific encoding models have a significantly higher prediction accuracy compared to cross-subject encoding models (F<sub>1,15</sub>â€‰=â€‰12.58, pâ€‰=â€‰0.016, repeated measures 2-way ANOVA with subjective-specific vs. cross-subject and ROI as factors). There was no significant main effect of ROI (F<sub>3,15</sub>â€‰=â€‰0.62, pâ€‰=â€‰0.615), and no significant interaction (F<sub>3,15</sub>â€‰=â€‰0.84, pâ€‰=â€‰0.494). <b>b</b>, Differences between the prediction accuracy of subject-specific and cross-subject encoding models. All encoding models were based on 14 principal components of perceptual ratings that explained at least 90% of variance. Lines depict individual subject pairs. Error bars are s.e.m. across all six subject pairs.</p></div></div></div></div></div></section><section data-title="Supplementary information"><div class="c-article-section" id="Sec30-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec30">Supplementary information</h2><div class="c-article-section__content" id="Sec30-content"><div data-test="supplementary-info"><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary information" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Information</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Supplementary Tables 1 and 2.</p></div></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="reporting summary" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-023-01414-4/MediaObjects/41593_2023_1414_MOESM2_ESM.pdf" data-supp-info-image="">Reporting Summary</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=High-precision%20mapping%20reveals%20the%20structure%20of%20odor%20coding%20in%20the%20human%20brain&amp;author=Vivek%20Sagar%20et%20al&amp;contentID=10.1038%2Fs41593-023-01414-4&amp;copyright=This%20is%20a%20U.S.%20Government%20work%20and%20not%20under%20copyright%20protection%20in%20the%20US%3B%20foreign%20copyright%20protection%20may%20apply&amp;publication=1097-6256&amp;publicationDate=2023-08-24&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/s41593-023-01414-4" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41593-023-01414-4" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Sagar, V., Shanahan, L.K., Zelano, C.M. <i>et al.</i> High-precision mapping reveals the structure of odor coding in the human brain.
                    <i>Nat Neurosci</i> <b>26</b>, 1595â€“1602 (2023). https://doi.org/10.1038/s41593-023-01414-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-023-01414-4?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2022-02-26">26 February 2022</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-07-18">18 July 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-08-24">24 August 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-09">September 2023</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/s41593-023-01414-4</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>

            
        <section>
            <div class="c-article-section js-article-section" id="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Odor identification score as an alternative method for early identification of amyloidogenesis in Alzheimerâ€™s disease" href="https://doi.org/10.1038/s41598-024-54322-3">
                                        Odor identification score as an alternative method for early identification of amyloidogenesis in Alzheimerâ€™s disease
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Yukifusa Igeta</li><li>Isao Hemmi</li><li>Yasuyoshi Ouchi</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Scientific Reports</i> (2024)</p>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </section>
    

            
        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="js-context-bar-sticky-point-desktop">
        

        
            <noscript>
                
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01414-4.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </noscript>
            <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
                <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-right-column
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01414-4.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </div>
        
    </div>

    
        
    

    
    

    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>
                            
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=s41593-023-01414-4;doi=10.1038/s41593-023-01414-4;techmeta=36,59;subjmeta=1704,1723,2624,2649,378,631;kwrd=Olfactory+cortex,Perception">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=-1860936343&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-023-01414-4%26doi%3D10.1038/s41593-023-01414-4%26techmeta%3D36,59%26subjmeta%3D1704,1723,2624,2649,378,631%26kwrd%3DOlfactory+cortex,Perception">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=-1860936343&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-023-01414-4%26doi%3D10.1038/s41593-023-01414-4%26techmeta%3D36,59%26subjmeta%3D1704,1723,2624,2649,378,631%26kwrd%3DOlfactory+cortex,Perception"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>


    
        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-and-analysis"
                                   data-track="click"
                                   data-track-action="reviews &amp; analysis"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Reviews &amp; Analysis
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/video"
                                   data-track="click"
                                   data-track-action="videos"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Videos
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/current-issue"
                                   data-track="click"
                                   data-track-action="current issue"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Current issue
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>
                        
                    
                </ul>
                <ul class="c-header__list c-header__list--js-stack">
                    
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/natureneuro"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on Twitter
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;6"
                               rel="nofollow"
                               data-track="click"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/neuro.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </nav>
    
    
        
            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/aims"
                                   data-track="click"
                                   data-track-action="aims &amp; scope"
                                   data-track-label="link">
                                    Aims &amp; Scope
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-information"
                                   data-track="click"
                                   data-track-action="journal information"
                                   data-track-label="link">
                                    Journal Information
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-impact"
                                   data-track="click"
                                   data-track-action="journal metrics"
                                   data-track-label="link">
                                    Journal Metrics
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editors"
                                   data-track="click"
                                   data-track-action="about the editors"
                                   data-track-label="link">
                                    About the Editors
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="research cross-journal editorial team"
                                   data-track-label="link">
                                    Research Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="reviews cross-journal editorial team"
                                   data-track-label="link">
                                    Reviews Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/our-publishing-models"
                                   data-track="click"
                                   data-track-action="our publishing models"
                                   data-track-label="link">
                                    Our publishing models
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-values-statement"
                                   data-track="click"
                                   data-track-action="editorial values statement"
                                   data-track-label="link">
                                    Editorial Values Statement
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-policies"
                                   data-track="click"
                                   data-track-action="editorial policies"
                                   data-track-label="link">
                                    Editorial Policies
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/content"
                                   data-track="click"
                                   data-track-action="content types"
                                   data-track-label="link">
                                    Content Types
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/web-feeds"
                                   data-track="click"
                                   data-track-action="web feeds"
                                   data-track-label="link">
                                    Web Feeds
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/posters"
                                   data-track="click"
                                   data-track-action="posters"
                                   data-track-label="link">
                                    Posters
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        

        
            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/submission-guidelines"
                                   data-track="click"
                                   data-track-action="submission guidelines"
                                   data-track-label="link">
                                    Submission Guidelines
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/for-reviewers"
                                   data-track="click"
                                   data-track-action="for reviewers"
                                   data-track-label="link">
                                    For Reviewers
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="click||nav_language_services"
                                   data-track-context="header publish with us dropdown menu"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://mts-nn.nature.com/cgi-bin/main.plex?form_type&#x3D;home&amp;from_idp&#x3D;1"
                                   data-track="click"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external>Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        
    


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">
                            
                                
                                    <option value="" selected>All journals</option>
                                    <option value="neuro">This journal</option>
                                
                            
                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        

        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
            

            <p class="c-meta u-ma-0 u-flex-shrink">
                <span class="c-meta__item">
                    Nature Neuroscience (<i>Nat Neurosci</i>)
                </span>
                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="onlineIssn">1546-1726</span> (online)
    </span>
    


                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="printIssn">1097-6256</span> (print)
    </span>
    

            </p>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.protocols.io/"
                                                  data-track="click" data-track-action="protocols.io"
                                                  data-track-label="link">protocols.io</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natitaly"
                                                  data-track="click" data-track-action="nature Italy"
                                                  data-track-label="link">Nature Italy</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>
            
        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-ea63208b81.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2024 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path id="shape" fill-rule="evenodd" clip-rule="evenodd" d="M1 3.78571C1 2.75867 1.85698 2 2.8209 2H6.1791C7.14302 2 8 2.75867 8 3.78571V4H11.1668C11.885 4 12.5585 4.42017 12.8494 5.07033C12.9893 4.98169 13.1425 4.91101 13.3056 4.86206L16.5222 3.89704C17.4454 3.62005 18.4843 4.10046 18.7794 5.08419L22.9256 18.9042C23.2207 19.8878 22.618 20.8608 21.6947 21.1378L18.4781 22.1029C17.5548 22.3799 16.516 21.8993 16.2209 20.9157L13.0001 10.1804V20.2143C13.0001 21.255 12.1231 22 11.1668 22H7.83346C7.54206 22 7.25803 21.9308 7.00392 21.8052C6.75263 21.9305 6.47077 22 6.1791 22H2.8209C1.85693 22 1 21.2412 1 20.2143V3.78571ZM3 4V15H6V4H3ZM3 20V17H6V20H3ZM18.0749 20.1358L17.2129 17.2623L20.0863 16.4002L20.9484 19.2737L18.0749 20.1358ZM19.5116 14.4846L16.6381 15.3466L14.0519 6.72624L16.9254 5.86416L19.5116 14.4846ZM8.00012 20L8.00012 6H11.0001L11.0001 20H8.00012Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</footer>




    

    

<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">

    
    <div class="c-site-messages__banner-large">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing" src="/static/images/logos/nature-briefing-logo-n150-white-d81c9da3ec.svg" width="250" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing</em> newsletter â€” what matters in science, free to your inbox daily.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/briefing" method="post" data-location="banner" data-track="submit||nature_briefing_sign_up" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="MainBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick">
                        <input type="hidden" value="false" name="marketing" id="marketing_input">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick">
                        <input type="hidden" value="MainBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">
                            
                            <input type="hidden" value="true" name="N:nature_briefing_daily" id="defaultNewsletter">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>

    
    <div class="c-site-messages__banner-small">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner">Sign up for Nature Briefing
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/s41593-023-01414-4&amp;format=js&amp;last_modified=2023-08-24" async></script>
<img src="/w3dmyulm/article/s41593-023-01414-4" width="1" height="1" alt="" class="u-visually-hidden">
</body>
</html>