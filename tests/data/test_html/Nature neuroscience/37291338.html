<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence | Nature Neuroscience</title>
    
        
<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/neuro.rss"/>


    
        

        <script id="save-data-connection-testing">
            function hasConnection() {
                return navigator.connection || navigator.mozConnection || navigator.webkitConnection || navigator.msConnection;
            }

            function createLink(src) {
                var preloadLink = document.createElement("link");
                preloadLink.rel = "preload";
                preloadLink.href = src;
                preloadLink.as = "font";
                preloadLink.type = "font/woff2";
                preloadLink.crossOrigin = "";
                document.head.insertBefore(preloadLink, document.head.firstChild);
            }

            var connectionDetail = {
                saveDataEnabled: false,
                slowConnection: false
            };

            var connection = hasConnection();
            if (connection) {
                connectionDetail.saveDataEnabled = connection.saveData;
                if (/\slow-2g|2g/.test(connection.effectiveType)) {
                    connectionDetail.slowConnection = true;
                }
            }

            if (!(connectionDetail.saveDataEnabled || connectionDetail.slowConnection)) {
                createLink("/static/fonts/HardingText-Regular-Web-cecd90984f.woff2");
            } else {
                document.documentElement.classList.add('save-data');
            }
        </script>
    

<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"development-of-the-nervous-system;emotion;social-neuroscience","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Nature Neuroscience","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article"}},"article":{"doi":"10.1038/s41593-023-01358-9"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["M. Catalina Camacho","Ashley N. Nielsen","Dori Balser","Emily Furtado","David C. Steinberger","Leah Fruchtman","Joseph P. Culver","Chad M. Sylvester","Deanna M. Barch"],"publishedAt":1686182400,"publishedAtString":"2023-06-08","title":"Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"neuro","title":"nature neuroscience","volume":"26","issue":"7"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"nature-onwards-journey","active":false}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"US","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://collect.nature.com',
        imprint: 'nature'
    });
</script>

<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
 



     
    
    
        
    
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}button:focus{outline:3px solid #fece3e;will-change:transform}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:8px}.c-recommendations-list__item .c-card{flex-basis:100%}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 0 0 16px;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #cedbe0;height:auto;min-height:0;position:static}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0 0 8px;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:inherit}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}.c-card__title-recommendation .MathJax_Display{display:inline!important}.c-card__link:not(.c-card__link--no-block-link):before{z-index:1}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a,.c-article-recommendations-card__link{color:inherit}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-article-recommendations-card__meta-type,.c-meta .c-meta__item:first-child{font-weight:700}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:539px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px 13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.c-pdf-download__link{padding:13px 24px} } </style>




    
        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-nature-branded-68c4876c28.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    
    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-nature-branded-68c4876c28.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-122346e276.css" media="print">
    



<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">


<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h.indexOf('preview-www.nature.com') > -1) return;

            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('nature.com') > -1) {
                if (h.indexOf('test-www.nature.com') > -1) {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-26e142e9c6.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    
                        {src: '/static/js/global-article-es6-bundle-22e9088d18.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-be699ef0bc.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-4841faf3e2.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-f72f566c73.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-1fe07484e5.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;
                        
                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-8fc1a30809.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-9389823142.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-cb0ea70df9.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">





    
    <script type="application/ld+json">{"mainEntity":{"headline":"Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence","description":"Humans require a shared conceptualization of others’ emotions for adaptive social functioning. A concept is a mental blueprint that gives our brains parameters for predicting what will happen next. Emotion concepts undergo refinement with development, but it is not known whether their neural representations change in parallel. Here, in a sample of 5–15-year-old children (n = 823), we show that the brain represents different emotion concepts distinctly throughout the cortex, cerebellum and caudate. Patterns of activation to each emotion changed little across development. Using a model-free approach, we show that activation patterns were more similar between older children than between younger children. Moreover, scenes that required inferring negative emotional states elicited higher default mode network activation similarity in older children than younger children. These results suggest that representations of emotion concepts are relatively stable by mid to late childhood and synchronize between individuals during adolescence. Camacho et al. show that emotion concepts are represented throughout the brain, giving insight to how the brain perceives real-world emotions. These patterns are present before children enter school and become more standardized across adolescence.","datePublished":"2023-06-08T00:00:00Z","dateModified":"2023-07-27T00:00:00Z","pageStart":"1256","pageEnd":"1266","sameAs":"https://doi.org/10.1038/s41593-023-01358-9","keywords":["Development of the nervous system","Emotion","Social neuroscience","Biomedicine","general","Neurosciences","Behavioral Sciences","Biological Techniques","Neurobiology","Animal Genetics and Genomics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig4_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig5_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig6_HTML.png"],"isPartOf":{"name":"Nature Neuroscience","issn":["1546-1726","1097-6256"],"volumeNumber":"26","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"M. Catalina Camacho","url":"http://orcid.org/0000-0003-0457-5410","affiliation":[{"name":"Washington University School of Medicine","address":{"name":"Division of Biology and Biomedical Sciences, Washington University School of Medicine, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"}],"email":"camachoc@wustl.edu","@type":"Person"},{"name":"Ashley N. Nielsen","affiliation":[{"name":"Washington University School of Medicine","address":{"name":"Department of Psychiatry, Washington University School of Medicine, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Dori Balser","affiliation":[{"name":"Washington University in St. Louis","address":{"name":"Department of Psychological and Brain Sciences, Washington University in St. Louis, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Emily Furtado","affiliation":[{"name":"Washington University School of Medicine","address":{"name":"Department of Psychiatry, Washington University School of Medicine, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"David C. Steinberger","affiliation":[{"name":"Washington University in St. Louis","address":{"name":"Department of Psychological and Brain Sciences, Washington University in St. Louis, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Leah Fruchtman","affiliation":[{"name":"Washington University in St. Louis","address":{"name":"Department of Psychological and Brain Sciences, Washington University in St. Louis, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Joseph P. Culver","affiliation":[{"name":"Washington University School of Medicine","address":{"name":"Division of Biology and Biomedical Sciences, Washington University School of Medicine, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Washington University School of Medicine","address":{"name":"Department of Radiology, Washington University School of Medicine, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Washington University in St. Louis","address":{"name":"Department of Physics, Washington University in St. Louis, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Washington University in St. Louis","address":{"name":"Department of Biomedical Engineering, Washington University in St. Louis, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Washington University in St. Louis","address":{"name":"Department of Electrical and Systems Engineering, Washington University in St. Louis, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Chad M. Sylvester","affiliation":[{"name":"Washington University School of Medicine","address":{"name":"Division of Biology and Biomedical Sciences, Washington University School of Medicine, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Washington University School of Medicine","address":{"name":"Department of Psychiatry, Washington University School of Medicine, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Deanna M. Barch","url":"http://orcid.org/0000-0003-1693-8506","affiliation":[{"name":"Washington University School of Medicine","address":{"name":"Division of Biology and Biomedical Sciences, Washington University School of Medicine, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Washington University School of Medicine","address":{"name":"Department of Psychiatry, Washington University School of Medicine, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Washington University in St. Louis","address":{"name":"Department of Psychological and Brain Sciences, Washington University in St. Louis, St. Louis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>




    
    
    


    
    <link rel="canonical" href="https://www.nature.com/articles/s41593-023-01358-9">
    
    
    <meta name="journal_id" content="41593"/>
    <meta name="dc.title" content="Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence"/>
    <meta name="dc.source" content="Nature Neuroscience 2023 26:7"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2023-06-08"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2023 The Author(s), under exclusive licence to Springer Nature America, Inc."/>
    <meta name="dc.rights" content="2023 The Author(s), under exclusive licence to Springer Nature America, Inc."/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="Humans require a shared conceptualization of others&#8217; emotions for adaptive social functioning. A concept is a mental blueprint that gives our brains parameters for predicting what will happen next. Emotion concepts undergo refinement with development, but it is not known whether their neural representations change in parallel. Here, in a sample of 5&#8211;15-year-old children (n&#8201;=&#8201;823), we show that the brain represents different emotion concepts distinctly throughout the cortex, cerebellum and caudate. Patterns of activation to each emotion changed little across development. Using a model-free approach, we show that activation patterns were more similar between older children than between younger children. Moreover, scenes that required inferring negative emotional states elicited higher default mode network activation similarity in older children than younger children. These results suggest that representations of emotion concepts are relatively stable by mid to late childhood and synchronize between individuals during adolescence. Camacho et al. show that emotion concepts are represented throughout the brain, giving insight to how the brain perceives real-world emotions. These patterns are present before children enter school and become more standardized across adolescence."/>
    <meta name="prism.issn" content="1546-1726"/>
    <meta name="prism.publicationName" content="Nature Neuroscience"/>
    <meta name="prism.publicationDate" content="2023-06-08"/>
    <meta name="prism.volume" content="26"/>
    <meta name="prism.number" content="7"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="1256"/>
    <meta name="prism.endingPage" content="1266"/>
    <meta name="prism.copyright" content="2023 The Author(s), under exclusive licence to Springer Nature America, Inc."/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/s41593-023-01358-9"/>
    <meta name="prism.doi" content="doi:10.1038/s41593-023-01358-9"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/s41593-023-01358-9.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/s41593-023-01358-9"/>
    <meta name="citation_journal_title" content="Nature Neuroscience"/>
    <meta name="citation_journal_abbrev" content="Nat Neurosci"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="1546-1726"/>
    <meta name="citation_title" content="Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence"/>
    <meta name="citation_volume" content="26"/>
    <meta name="citation_issue" content="7"/>
    <meta name="citation_publication_date" content="2023/07"/>
    <meta name="citation_online_date" content="2023/06/08"/>
    <meta name="citation_firstpage" content="1256"/>
    <meta name="citation_lastpage" content="1266"/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/s41593-023-01358-9"/>
    <meta name="DOI" content="10.1038/s41593-023-01358-9"/>
    <meta name="size" content="272554"/>
    <meta name="citation_doi" content="10.1038/s41593-023-01358-9"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/s41593-023-01358-9&amp;api_key="/>
    <meta name="description" content="Humans require a shared conceptualization of others&#8217; emotions for adaptive social functioning. A concept is a mental blueprint that gives our brains parameters for predicting what will happen next. Emotion concepts undergo refinement with development, but it is not known whether their neural representations change in parallel. Here, in a sample of 5&#8211;15-year-old children (n&#8201;=&#8201;823), we show that the brain represents different emotion concepts distinctly throughout the cortex, cerebellum and caudate. Patterns of activation to each emotion changed little across development. Using a model-free approach, we show that activation patterns were more similar between older children than between younger children. Moreover, scenes that required inferring negative emotional states elicited higher default mode network activation similarity in older children than younger children. These results suggest that representations of emotion concepts are relatively stable by mid to late childhood and synchronize between individuals during adolescence. Camacho et al. show that emotion concepts are represented throughout the brain, giving insight to how the brain perceives real-world emotions. These patterns are present before children enter school and become more standardized across adolescence."/>
    <meta name="dc.creator" content="Camacho, M. Catalina"/>
    <meta name="dc.creator" content="Nielsen, Ashley N."/>
    <meta name="dc.creator" content="Balser, Dori"/>
    <meta name="dc.creator" content="Furtado, Emily"/>
    <meta name="dc.creator" content="Steinberger, David C."/>
    <meta name="dc.creator" content="Fruchtman, Leah"/>
    <meta name="dc.creator" content="Culver, Joseph P."/>
    <meta name="dc.creator" content="Sylvester, Chad M."/>
    <meta name="dc.creator" content="Barch, Deanna M."/>
    <meta name="dc.subject" content="Development of the nervous system"/>
    <meta name="dc.subject" content="Emotion"/>
    <meta name="dc.subject" content="Social neuroscience"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Bull.; citation_title=Is memory schematic?; citation_author=JW Alba, L Hasher; citation_volume=93; citation_publication_date=1983; citation_pages=203-231; citation_id=CR1"/>
    <meta name="citation_reference" content="citation_journal_title=eLife; citation_title=Schema representations in distinct brain networks support narrative memory during encoding and retrieval; citation_author=R Mas&#237;s-Obando, KA Norman, C Baldassano; citation_volume=11; citation_publication_date=2022; citation_pages=e70445; citation_id=CR2"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Neurosci.; citation_title=How schema and novelty augment memory formation; citation_author=MTR Kesteren, DJ Ruiter, G Fern&#225;ndez, RN Henson; citation_volume=35; citation_publication_date=2012; citation_pages=211-219; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=Am. Psychol.; citation_title=Expectancy confirmation processes arising in the social interaction sequence; citation_author=JM Darley, RH Fazio; citation_volume=35; citation_publication_date=1980; citation_pages=867-881; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Annu. Rev. Dev. Psychol.; citation_title=The development of emotion reasoning in infancy and early childhood; citation_author=AL Ruba, SD Pollak; citation_volume=2; citation_publication_date=2020; citation_pages=503-531; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Structural brain development between childhood and adulthood: convergence across four longitudinal samples; citation_author=KL Mills; citation_volume=141; citation_publication_date=2016; citation_pages=273-281; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Structural MRI of pediatric brain development: what have we learned and where are we going?; citation_author=JN Giedd, JL Rapoport; citation_volume=67; citation_publication_date=2010; citation_pages=728-734; citation_id=CR7"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Development of large-scale functional networks from birth to adulthood: a guide to the neuroimaging literature; citation_author=DS Grayson, DA Fair; citation_volume=160; citation_publication_date=2017; citation_pages=15-31; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS ONE; citation_title=Emotional facial perception development in 7, 9 and 11 year-old children: the emergence of a silent eye-tracked emotional other-race effect; citation_author=J Malsert, A Palama, E Gentaz; citation_volume=15; citation_publication_date=2020; citation_pages=e0233008; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Sci.; citation_title=The development of emotional face processing during childhood; citation_author=M Batty, MJ Taylor; citation_volume=9; citation_publication_date=2006; citation_pages=207-220; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=Child Dev.; citation_title=An integrated model of emotion processes and cognition in social information processing; citation_author=EA Lemerise, WF Arsenio; citation_volume=71; citation_publication_date=2000; citation_pages=107-118; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Bull.; citation_title=A review and reformulation of social information-processing mechanisms in children&#8217;s social adjustment; citation_author=NR Crick, KA Dodge; citation_volume=115; citation_publication_date=1994; citation_pages=74-101; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Neurosci.; citation_title=Object vision and spatial vision: two cortical pathways; citation_author=M Mishkin, LG Ungerleider, KA Macko; citation_volume=6; citation_publication_date=1983; citation_pages=414-417; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Neurosci.; citation_title=Nature and nurture in language acquisition: anatomical and functional brain-imaging studies in infants; citation_author=G Dehaene-Lambertz, L Hertz-Pannier, J Dubois; citation_volume=29; citation_publication_date=2006; citation_pages=367-373; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Rev. Neurosci.; citation_title=Control of goal-directed and stimulus-driven attention in the brain; citation_author=M Corbetta, GL Shulman; citation_volume=3; citation_publication_date=2002; citation_pages=215-229; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Functional characterization of the cingulo-opercular network in the maintenance of tonic alertness; citation_author=S Sadaghiani, M D&#8217;Esposito; citation_volume=25; citation_publication_date=2015; citation_pages=2763-2773; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Opin. Neurobiol.; citation_title=Large-scale brain networks in affective and social neuroscience: towards an integrative functional architecture of the brain; citation_author=LF Barrett, AB Satpute; citation_volume=23; citation_publication_date=2013; citation_pages=361-372; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Rev. Neurosci.; citation_title=The brain&#8217;s default network: updated anatomy, physiology and evolving insights; citation_author=RL Buckner, LM DiNicola; citation_volume=20; citation_publication_date=2019; citation_pages=593-608; citation_id=CR18"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=The default mode network&#8217;s role in discrete emotion; citation_author=AB Satpute, KA Lindquist; citation_volume=23; citation_publication_date=2019; citation_pages=851-864; citation_id=CR19"/>
    <meta name="citation_reference" content="citation_journal_title=Dialogues Clin. Neurosci.; citation_title=The frontoparietal network: function, electrophysiology, and importance of individual precision mapping; citation_author=S Marek, NUF Dosenbach; citation_volume=20; citation_publication_date=2018; citation_pages=133-140; citation_id=CR20"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=A functional architecture of the human brain: emerging insights from the science of emotion; citation_author=KA Lindquist, LF Barrett; citation_volume=16; citation_publication_date=2012; citation_pages=533-540; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=Sci. Adv.; citation_title=Emotion schemas are embedded in the human visual system; citation_author=PA Kragel, MC Reddan, KS LaBar, TD Wager; citation_volume=5; citation_publication_date=2019; citation_pages=eaaw4358; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Cogn. Neurosci.; citation_title=Early stressful experiences are associated with reduced neural responses to naturalistic emotional and social content in children; citation_author=AT Park; citation_volume=57; citation_publication_date=2022; citation_pages=101152; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Relationships between depressive symptoms and brain responses during emotional movie viewing emerge in adolescence; citation_author=DC Gruskin, MD Rosenberg, AJ Holmes; citation_volume=216; citation_publication_date=2020; citation_pages=116217; citation_id=CR24"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Neural architecture supporting active emotion processing in children: a multivariate approach; citation_author=MC Camacho, HT Karim, SB Perlman; citation_volume=188; citation_publication_date=2019; citation_pages=171-180; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=Emot. Rev.; citation_title=Children&#8217;s interpretation of facial expressions: the long path from valence-based to specific discrete categories; citation_author=SC Widen; citation_volume=5; citation_publication_date=2013; citation_pages=72-77; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Sci.; citation_title=The nonlinear development of emotion differentiation: granular emotional experience is low in adolescence; citation_author=EC Nook, SF Sasse, HK Lambert, KA McLaughlin, LH Somerville; citation_volume=29; citation_publication_date=2018; citation_pages=1346-1357; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=Emotion; citation_title=Charting the development of emotion comprehension and abstraction from childhood to adulthood using observer-rated and linguistic measures; citation_author=EC Nook; citation_volume=20; citation_publication_date=2020; citation_pages=773-792; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Sci.; citation_title=Pubertal development shapes perception of complex facial expressions; citation_author=NV Motta-Mena, KS Scherf; citation_volume=20; citation_publication_date=2017; citation_pages=e12451; citation_id=CR29"/>
    <meta name="citation_reference" content="citation_journal_title=Biol. Psychiatry; citation_title=Amygdala response to facial expressions in children and adults; citation_author=KM Thomas; citation_volume=49; citation_publication_date=2001; citation_pages=309-316; citation_id=CR30"/>
    <meta name="citation_reference" content="citation_journal_title=Soc. Cogn. Affect. Neurosci.; citation_title=Developmental differences in the neural mechanisms of facial emotion labeling; citation_author=JL Wiggins; citation_volume=11; citation_publication_date=2016; citation_pages=172-181; citation_id=CR31"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=The stimuli drive the response: an fMRI study of youth processing adult or child emotional face stimuli; citation_author=HA Marusak, JM Carr&#233;, ME Thomason; citation_volume=83; citation_publication_date=2013; citation_pages=679-689; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=Behav. Brain Res.; citation_title=Positive reinforcement modulates fronto-limbic systems subserving emotional interference in adolescents; citation_author=CD Ladouceur, MW Schlund, A-M Segreti; citation_volume=338; citation_publication_date=2018; citation_pages=109-117; citation_id=CR33"/>
    <meta name="citation_reference" content="citation_journal_title=Soc. Neurosci.; citation_title=Children&#8217;s processing of emotions expressed by peers and adults: an fMRI study; citation_author=S Hoehl, J Brauer, G Brasse, T Striano, AD Friederici; citation_volume=5; citation_publication_date=2010; citation_pages=543-559; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Cogn. Neurosci.; citation_title=Reliability of neural activation and connectivity during implicit face emotion processing in youth; citation_author=SP Haller; citation_volume=31; citation_publication_date=2018; citation_pages=67-73; citation_id=CR35"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroreport; citation_title=Children recruit distinct neural systems for implicit emotional face processing; citation_author=NJ Lobaugh, E Gibson, MJ Taylor; citation_volume=17; citation_publication_date=2006; citation_pages=215-219; citation_id=CR36"/>
    <meta name="citation_reference" content="citation_journal_title=J. Cogn. Neurosci.; citation_title=A developmental examination of amygdala response to facial expressions; citation_author=AE Guyer; citation_volume=20; citation_publication_date=2008; citation_pages=1565-1582; citation_id=CR37"/>
    <meta name="citation_reference" content="citation_journal_title=Cogn. Affect. Behav. Neurosci.; citation_title=Functional brain activation to emotional and nonemotional faces in healthy children: evidence for developmentally undifferentiated amygdala function during the school-age period; citation_author=D Pagliaccio; citation_volume=13; citation_publication_date=2013; citation_pages=771-789; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=Biol. Psychiatry; citation_title=Biological substrates of emotional reactivity and regulation in adolescence during an emotional go-nogo task; citation_author=TA Hare; citation_volume=63; citation_publication_date=2008; citation_pages=927-934; citation_id=CR39"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Neuropsychol.; citation_title=Behavioral and neural representation of emotional facial expressions across the lifespan; citation_author=LH Somerville, N Fani, EB McClure-Tone; citation_volume=36; citation_publication_date=2011; citation_pages=408-428; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=Cogn. Dev.; citation_title=Children acquire emotion categories gradually; citation_author=SC Widen, JA Russell; citation_volume=23; citation_publication_date=2008; citation_pages=291-312; citation_id=CR41"/>
    <meta name="citation_reference" content="citation_journal_title=Br. J. Dev. Psychol.; citation_title=Children&#8217;s scripts for social emotions: causes and consequences are more central than are facial expressions; citation_author=SC Widen, JA Russell; citation_volume=28; citation_publication_date=2010; citation_pages=565-581; citation_id=CR42"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Dir. Psychol. Sci.; citation_title=Emotion as information in early social learning; citation_author=Y Wu, LE Schulz, MC Frank, H Gweon; citation_volume=30; citation_publication_date=2021; citation_pages=468-475; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=The balance of rigor and reality in developmental neuroscience; citation_author=JF Cantlon; citation_volume=216; citation_publication_date=2020; citation_pages=116464; citation_id=CR44"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Cogn. Neurosci.; citation_title=Movies in the magnet: naturalistic paradigms in developmental functional neuroimaging; citation_author=T Vanderwal, J Eilbott, FX Castellanos; citation_volume=36; citation_publication_date=2019; citation_pages=100600; citation_id=CR45"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Towards clinical applications of movie fMRI; citation_author=SB Eickhoff, M Milham, T Vanderwal; citation_volume=217; citation_publication_date=2020; citation_pages=116860; citation_id=CR46"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=The minimal preprocessing pipelines for the Human Connectome Project; citation_author=MF Glasser; citation_volume=80; citation_publication_date=2013; citation_pages=105-124; citation_id=CR47"/>
    <meta name="citation_reference" content="citation_journal_title=Affect. Sci.; citation_title=EmoCodes: a standardized coding system for socio-emotional content in complex video stimuli; citation_author=MC Camacho; citation_volume=3; citation_publication_date=2022; citation_pages=168-181; citation_id=CR48"/>
    <meta name="citation_reference" content="citation_journal_title=Emot. Rev.; citation_title=An appraisal-driven componential approach to the emotional brain; citation_author=D Sander, D Grandjean, KR Scherer; citation_volume=10; citation_publication_date=2018; citation_pages=219-231; citation_id=CR49"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Opin. Behav. Sci.; citation_title=Understanding emotion with brain networks; citation_author=L Pessoa; citation_volume=19; citation_publication_date=2018; citation_pages=19-25; citation_id=CR50"/>
    <meta name="citation_reference" content="citation_journal_title=Emot. Rev.; citation_title=Comment: Constructionism is a multilevel framework for affective science; citation_author=KA Lindquist, JK MacCormack; citation_volume=6; citation_publication_date=2014; citation_pages=134-135; citation_id=CR51"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Biol.; citation_title=Neural representations of emotion are organized around abstract event features; citation_author=AE Skerry, R Saxe; citation_volume=25; citation_publication_date=2015; citation_pages=1945-1954; citation_id=CR52"/>
    <meta name="citation_reference" content="citation_journal_title=Emot. Rev.; citation_title=Four models of basic emotions: a review of Ekman and Cordaro, Izard, Levenson, and Panksepp and Watt; citation_author=JL Tracy, D Randles; citation_volume=3; citation_publication_date=2011; citation_pages=397-405; citation_id=CR53"/>
    <meta name="citation_reference" content="citation_journal_title=Emot. Rev.; citation_title=What is basic about basic emotions? Lasting lessons from affective neuroscience; citation_author=J Panksepp, D Watt; citation_volume=3; citation_publication_date=2011; citation_pages=387-396; citation_id=CR54"/>
    <meta name="citation_reference" content="citation_journal_title=Hum. Dev.; citation_title=Factors facilitating early emotion understanding development: contributions to individual differences; citation_author=M Ogren, SP Johnson; citation_volume=64; citation_publication_date=2020; citation_pages=108-118; citation_id=CR55"/>
    <meta name="citation_reference" content="citation_journal_title=Emotion; citation_title=Emotion words link faces to emotional scenarios in early childhood; citation_author=M Ogren, CM Sandhofer; citation_volume=22; citation_publication_date=2022; citation_pages=167-178; citation_id=CR56"/>
    <meta name="citation_reference" content="citation_journal_title=J. Nonverbal Behav.; citation_title=Children&#8217;s understanding of emotional facial expressions and verbal labels; citation_author=LA Camras, K Allison; citation_volume=9; citation_publication_date=1985; citation_pages=84-94; citation_id=CR57"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Psychol.; citation_title=Age, gender, and puberty influence the development of facial emotion recognition; citation_author=K Lawrence, R Campbell, D Skuse; citation_volume=6; citation_publication_date=2015; citation_pages=761; citation_id=CR58"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Neuropsychol.; citation_title=Age, sex, and pubertal phase influence mentalizing about emotions and actions in adolescents; citation_author=EHH Keulers, EAT Evers, P Stiers, J Jolles; citation_volume=35; citation_publication_date=2010; citation_pages=555-569; citation_id=CR59"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Cogn. Neurosci.; citation_title=Puberty and functional brain development in humans: convergence in findings?; citation_author=J Dai, KS Scherf; citation_volume=39; citation_publication_date=2019; citation_pages=100690; citation_id=CR60"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Psychol.; citation_title=Emotion words, emotion concepts, and emotional development in children: a constructionist hypothesis; citation_author=K Hoemann, F Xu, LF Barrett; citation_volume=55; citation_publication_date=2019; citation_pages=1830-1849; citation_id=CR61"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Commun.; citation_title=Development of the social brain from age three to twelve years; citation_author=H Richardson, G Lisandrelli, A Riobueno-Naylor, R Saxe; citation_volume=9; citation_publication_date=2018; citation_id=CR62"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Development of the default-mode network during childhood and adolescence: a longitudinal resting-state fMRI study; citation_author=F Fan; citation_volume=226; citation_publication_date=2021; citation_pages=117581; citation_id=CR63"/>
    <meta name="citation_reference" content="citation_journal_title=Cell Rep.; citation_title=Three distinct sets of connector hubs integrate human brain function; citation_author=EM Gordon; citation_volume=24; citation_publication_date=2018; citation_pages=1687-1695; citation_id=CR64"/>
    <meta name="citation_reference" content="citation_journal_title=Commun. Biol.; citation_title=The surprising role of the default mode network in naturalistic perception; citation_author=T Brandman, R Malach, E Simony; citation_volume=4; citation_publication_date=2021; citation_pages=79; citation_id=CR65"/>
    <meta name="citation_reference" content="citation_journal_title=Brain Struct. Funct.; citation_title=Non-classical behavior of the default mode network regions during an information processing task; citation_author=PHR Silva, C Rondinoni, RF Leoni; citation_volume=225; citation_publication_date=2020; citation_pages=2553-2562; citation_id=CR66"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Default-mode network streams for coupling to language and control systems; citation_author=EM Gordon; citation_volume=117; citation_publication_date=2020; citation_pages=17308-17319; citation_id=CR67"/>
    <meta name="citation_reference" content="citation_journal_title=JAMA Psychiatry; citation_title=Large-scale network dysfunction in major depressive disorder; citation_author=RH Kaiser, JR Andrews-Hanna, TD Wager, DA Pizzagalli; citation_volume=72; citation_publication_date=2015; citation_pages=603; citation_id=CR68"/>
    <meta name="citation_reference" content="citation_journal_title=J. Affect. Disord.; citation_title=A systematic literature review of resting state network&#8212;functional MRI in bipolar disorder; citation_author=C Vargas, C L&#243;pez-Jaramillo, E Vieta; citation_volume=150; citation_publication_date=2013; citation_pages=727-735; citation_id=CR69"/>
    <meta name="citation_reference" content="citation_journal_title=Neurosci. Biobehav. Rev.; citation_title=Default-mode brain dysfunction in mental disorders: a systematic review; citation_author=SJ Broyd; citation_volume=33; citation_publication_date=2009; citation_pages=279-296; citation_id=CR70"/>
    <meta name="citation_reference" content="citation_journal_title=Depress. Anxiety; citation_title=Defining biotypes for depression and anxiety based on large-scale circuit dysfunction: a theoretical review of the evidence and future directions for clinical translation; citation_author=LM Williams; citation_volume=34; citation_publication_date=2017; citation_pages=9-24; citation_id=CR71"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Commun.; citation_title=Emotionotopy in the human right temporo-parietal cortex; citation_author=G Lettieri; citation_volume=10; citation_publication_date=2019; citation_id=CR72"/>
    <meta name="citation_reference" content="citation_journal_title=Soc. Cogn. Affect. Neurosci.; citation_title=Default and control network connectivity dynamics track the stream of affect at multiple timescales; citation_author=G Lettieri; citation_volume=17; citation_publication_date=2022; citation_pages=461-469; citation_id=CR73"/>
    <meta name="citation_reference" content="citation_journal_title=Soc. Cogn. Affect. Neurosci.; citation_title=Multivariate neural biomarkers of emotional states are categorically distinct; citation_author=PA Kragel, KS LaBar; citation_volume=10; citation_publication_date=2015; citation_pages=1437-1448; citation_id=CR74"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Biol.; citation_title=Decoding spontaneous emotional states in the human brain; citation_author=PA Kragel, AR Knodt, AR Hariri, KS Labar; citation_volume=14; citation_publication_date=2016; citation_pages=e2000106; citation_id=CR75"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Decoding the nature of emotion in the brain; citation_author=PA Kragel, KS Labar; citation_volume=20; citation_publication_date=2016; citation_pages=444-455; citation_id=CR76"/>
    <meta name="citation_reference" content="Chang, L. J. et al. Endogenous variation in ventromedial prefrontal cortex state dynamics during naturalistic viewing reflects affective experience. Sci. Adv. 7, eabf7129 (2021)."/>
    <meta name="citation_reference" content="citation_journal_title=Sci. Data; citation_title=An open resource for transdiagnostic research in pediatric mental health and learning disorders; citation_author=LM Alexander; citation_volume=4; citation_publication_date=2017; citation_id=CR78"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Sci.; citation_title=The nonlinear development of emotion differentiation: granular emotional experience is low in adolescence; citation_author=EC Nook, SF Sasse, HK Lambert, KA McLaughlin, LH Somerville; citation_volume=29; citation_publication_date=2018; citation_pages=1346-1357; citation_id=CR79"/>
    <meta name="citation_reference" content="citation_journal_title=Emotion; citation_title=Charting the development of emotion comprehension and abstraction from childhood to adulthood using observer-rated and linguistic measures; citation_author=EC Nook; citation_volume=20; citation_publication_date=2020; citation_pages=773-792; citation_id=CR80"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Sci.; citation_title=Pubertal development shapes perception of complex facial expressions; citation_author=NV Motta-Mena, KS Scherf; citation_volume=20; citation_publication_date=2017; citation_pages=e12451; citation_id=CR81"/>
    <meta name="citation_reference" content="citation_journal_title=Dev. Sci.; citation_title=Development of emotional facial recognition in late childhood and adolescence; citation_author=LA Thomas, MD Bellis, R Graham, KS LaBar; citation_volume=10; citation_publication_date=2007; citation_pages=547-558; citation_id=CR82"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Child Psychol.; citation_title=The development of facial emotion recognition: the role of configural information; citation_author=K Durand, M Gallay, A Seigneuric, F Robichon, J-Y Baudouin; citation_volume=97; citation_publication_date=2007; citation_pages=14-27; citation_id=CR83"/>
    <meta name="citation_reference" content="citation_journal_title=Hum. Brain Mapp.; citation_title=Age-related changes in amygdala-frontal connectivity during emotional face processing from childhood into young adulthood; citation_author=M Wu; citation_volume=37; citation_publication_date=2016; citation_pages=1684-1695; citation_id=CR84"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Array programming with NumPy; citation_author=CR Harris; citation_volume=585; citation_publication_date=2020; citation_pages=357-362; citation_id=CR85"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Methods; citation_title=SciPy 1.0: fundamental algorithms for scientific computing in Python; citation_author=P Virtanen; citation_volume=17; citation_publication_date=2020; citation_pages=261-272; citation_id=CR86"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=An integrated approach to correction for off-resonance effects and subject movement in diffusion MR imaging; citation_author=JLR Andersson, SN Sotiropoulos; citation_volume=125; citation_publication_date=2016; citation_pages=1063-1078; citation_id=CR87"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=FreeSurfer; citation_author=B Fischl; citation_volume=62; citation_publication_date=2012; citation_pages=774-781; citation_id=CR88"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Multimodal surface matching with higher-order smoothness constraints; citation_author=EC Robinson; citation_volume=167; citation_publication_date=2018; citation_pages=453-465; citation_id=CR89"/>
    <meta name="citation_reference" content="citation_journal_title=Hum. Brain Mapp.; citation_title=Statistical improvements in functional magnetic resonance imaging analyses produced by censoring high-motion data points; citation_author=JS Siegel; citation_volume=35; citation_publication_date=2014; citation_pages=1981-1996; citation_id=CR90"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Data quality influences observed links between functional connectivity and behavior; citation_author=JS Siegel; citation_volume=27; citation_publication_date=2017; citation_pages=4492-4502; citation_id=CR91"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Behavioral interventions for reducing head motion during MRI scans in children; citation_author=DJ Greene; citation_volume=171; citation_publication_date=2018; citation_pages=234-245; citation_id=CR92"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Inscapes: a movie paradigm to improve compliance in functional magnetic resonance imaging; citation_author=T Vanderwal, C Kelly, J Eilbott, LC Mayes, FX Castellanos; citation_volume=122; citation_publication_date=2015; citation_pages=222-232; citation_id=CR93"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Generation and evaluation of a cortical area parcellation from resting-state correlations; citation_author=EM Gordon; citation_volume=26; citation_publication_date=2016; citation_pages=288-303; citation_id=CR94"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=A set of functionally-defined brain regions with improved representation of the subcortex and cerebellum; citation_author=BA Seitzman; citation_volume=206; citation_publication_date=2020; citation_pages=116290; citation_id=CR95"/>
    <meta name="citation_reference" content="citation_journal_title=Affect. Sci.; citation_title=EmoCodes: a standardized coding system for socio-emotional content in complex video stimuli; citation_author=MC Camacho; citation_volume=3; citation_publication_date=2022; citation_pages=168-181; citation_id=CR96"/>
    <meta name="citation_reference" content="McNamara, Q., De La Vega, A. &amp; Yarkoni, T. Developing a comprehensive framework for multimodal feature extraction. In Proc. 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Matwin, S. et al.) 1567&#8211;1574 (Association for Computing Machinery, 2017)."/>
    <meta name="citation_reference" content="citation_journal_title=Soc. Cogn. Affect. Neurosci.; citation_title=Measuring shared responses across subjects using intersubject correlation; citation_author=SA Nastase, V Gazzola, U Hasson, C Keysers; citation_volume=14; citation_publication_date=2019; citation_pages=669-687; citation_id=CR98"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Intersubject synchronization of cortical activity during natural vision; citation_author=U Hasson, Y Nir, I Levy, G Fuhrmann, R Malach; citation_volume=303; citation_publication_date=2004; citation_pages=1634-1640; citation_id=CR99"/>
    <meta name="citation_reference" content="citation_journal_title=Hum. Brain Mapp.; citation_title=A phase synchrony measure for quantifying dynamic functional integration in the brain; citation_author=S Aviyente, EM Bernat, WS Evans, SR Sponheim; citation_volume=32; citation_publication_date=2011; citation_pages=80-93; citation_id=CR100"/>
    <meta name="citation_reference" content="citation_journal_title=Brain Connect.; citation_title=Functional magnetic resonance imaging phase synchronization as a measure of dynamic functional connectivity; citation_author=E Glerean, J Salmi, JM Lahnakoski, IP J&#228;&#228;skel&#228;inen, M Sams; citation_volume=2; citation_publication_date=2012; citation_pages=91-101; citation_id=CR101"/>
    <meta name="citation_reference" content="citation_journal_title=J. Youth Adolesc.; citation_title=A self-report measure of pubertal status: reliability, validity, and initial norms; citation_author=AC Petersen, L Crockett, M Richards, A Boxer; citation_volume=17; citation_publication_date=1988; citation_pages=117-133; citation_id=CR102"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Idiosynchrony: from shared responses to individual differences during naturalistic neuroimaging; citation_author=ES Finn; citation_volume=215; citation_publication_date=2020; citation_pages=116828; citation_id=CR103"/>
    <meta name="citation_reference" content="citation_journal_title=Cancer Res.; citation_title=The detection of disease clustering and a generalized regression approach; citation_author=N Mantel; citation_volume=27; citation_publication_date=1967; citation_pages=209-220; citation_id=CR104"/>
    <meta name="citation_reference" content="citation_journal_title=J. R. Stat. Soc. B; citation_title=Controlling the false discovery rate: a practical and powerful approach to multiple testing; citation_author=Y Benjamini, Y Hochberg; citation_volume=57; citation_publication_date=1995; citation_pages=289-300; citation_id=CR105"/>
    <meta name="citation_author" content="Camacho, M. Catalina"/>
    <meta name="citation_author_institution" content="Division of Biology and Biomedical Sciences, Washington University School of Medicine, St. Louis, USA"/>
    <meta name="citation_author" content="Nielsen, Ashley N."/>
    <meta name="citation_author_institution" content="Department of Psychiatry, Washington University School of Medicine, St. Louis, USA"/>
    <meta name="citation_author" content="Balser, Dori"/>
    <meta name="citation_author_institution" content="Department of Psychological and Brain Sciences, Washington University in St. Louis, St. Louis, USA"/>
    <meta name="citation_author" content="Furtado, Emily"/>
    <meta name="citation_author_institution" content="Department of Psychiatry, Washington University School of Medicine, St. Louis, USA"/>
    <meta name="citation_author" content="Steinberger, David C."/>
    <meta name="citation_author_institution" content="Department of Psychological and Brain Sciences, Washington University in St. Louis, St. Louis, USA"/>
    <meta name="citation_author" content="Fruchtman, Leah"/>
    <meta name="citation_author_institution" content="Department of Psychological and Brain Sciences, Washington University in St. Louis, St. Louis, USA"/>
    <meta name="citation_author" content="Culver, Joseph P."/>
    <meta name="citation_author_institution" content="Division of Biology and Biomedical Sciences, Washington University School of Medicine, St. Louis, USA"/>
    <meta name="citation_author_institution" content="Department of Radiology, Washington University School of Medicine, St. Louis, USA"/>
    <meta name="citation_author_institution" content="Department of Physics, Washington University in St. Louis, St. Louis, USA"/>
    <meta name="citation_author_institution" content="Department of Biomedical Engineering, Washington University in St. Louis, St. Louis, USA"/>
    <meta name="citation_author_institution" content="Department of Electrical and Systems Engineering, Washington University in St. Louis, St. Louis, USA"/>
    <meta name="citation_author" content="Sylvester, Chad M."/>
    <meta name="citation_author_institution" content="Division of Biology and Biomedical Sciences, Washington University School of Medicine, St. Louis, USA"/>
    <meta name="citation_author_institution" content="Department of Psychiatry, Washington University School of Medicine, St. Louis, USA"/>
    <meta name="citation_author" content="Barch, Deanna M."/>
    <meta name="citation_author_institution" content="Division of Biology and Biomedical Sciences, Washington University School of Medicine, St. Louis, USA"/>
    <meta name="citation_author_institution" content="Department of Psychiatry, Washington University School of Medicine, St. Louis, USA"/>
    <meta name="citation_author_institution" content="Department of Psychological and Brain Sciences, Washington University in St. Louis, St. Louis, USA"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@natureneuro"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence"/>
    <meta name="twitter:description" content="Nature Neuroscience - Camacho et al. show that emotion concepts are represented throughout the brain, giving insight to how the brain perceives real-world emotions. These patterns are present..."/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig1_HTML.png"/>
    

    
    
    <meta property="og:url" content="https://www.nature.com/articles/s41593-023-01358-9"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence - Nature Neuroscience"/>
    <meta property="og:description" content="Camacho et al. show that emotion concepts are represented throughout the brain, giving insight to how the brain perceives real-world emotions. These patterns are present before children enter school and become more standardized across adolescence."/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig1_HTML.png"/>
    

    <script>
        window.eligibleForRa21 = 'true'; 
    </script>
</head>
<body class="article-page">

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>



<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
        
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>

    

    <div class="u-lazy-ad-wrapper u-mbs-0">
            <div class="deferred-placeholder" data-replace="true"
                 data-placeholder="/placeholder/v1/institutionalBanner?bpids=[bpids] #institutional-banner-container"></div>
            <aside class="c-ad c-ad--728x90">
                <div class="c-ad__inner" data-container-type="banner-advert">
                    <p class="c-ad__label">Advertisement</p>
                    
        
            
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=s41593-023-01358-9;doi=10.1038/s41593-023-01358-9;techmeta=36,59;subjmeta=1457,2571,2645,378,631;kwrd=Development+of+the+nervous+system,Emotion,Social+neuroscience">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=601525743&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-023-01358-9%26doi%3D10.1038/s41593-023-01358-9%26techmeta%3D36,59%26subjmeta%3D1457,2571,2645,378,631%26kwrd%3DDevelopment+of+the+nervous+system,Emotion,Social+neuroscience">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=601525743&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-023-01358-9%26doi%3D10.1038/s41593-023-01358-9%26techmeta%3D36,59%26subjmeta%3D1457,2571,2645,378,631%26kwrd%3DDevelopment+of+the+nervous+system,Emotion,Social+neuroscience"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

        
    
                </div>
            </aside>
        </div>
    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#00928c">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">
                    
                    
                    <div class="c-header__logo-container">
                        
                        <a href="/neuro"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-0ccc487532906d646419e51f647ce79a.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-880e5942f43b9213989c58a04ab5c8e6.svg" height="32" alt="Nature Neuroscience">
                            </picture>
                        </a>
                    
                    </div>
                    
                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">
                            
                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41593-023-01358-9'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
                            
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        
            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">
                            
                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span><span class="c-header__show-text">Explore</span> content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                                
                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>
                                
                            
                            
                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">
                            
                                <li class="c-header__item">
                                    <a class="c-header__link"
                                       href="https://idp.nature.com/auth/personal/springernature?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Fmy-account%2Falerts%2Fsubscribe-journal%3Flist-id%3D6%26journal-link%3Dhttps%253A%252F%252Fwww.nature.com%252Fneuro%252F"
                                       rel="nofollow"
                                       data-track="click"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/neuro.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        
    </header>


    
    
        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature neuroscience"><span itemprop="name">nature neuroscience</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro/articles?type&#x3D;article" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="name">articles</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>
    



    

</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
        
            <div class="c-context-bar u-hide"
                 data-test="context-bar"
                 data-context-bar
                 aria-hidden="true">
                <div class="c-context-bar__container u-container">
                    <div class="c-context-bar__title">
                        Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence
                    </div>
                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01358-9.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                </div>
            </div>
        
        <article lang="en">
            
            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Article</li>
    
    

                        <li class="c-article-identifiers__item">Published: <time datetime="2023-06-08">08 June 2023</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-M__Catalina-Camacho-Aff1" data-author-popup="auth-M__Catalina-Camacho-Aff1" data-author-search="Camacho, M. Catalina" data-corresp-id="c1">M. Catalina Camacho<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0003-0457-5410"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-0457-5410</a></span><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ashley_N_-Nielsen-Aff2" data-author-popup="auth-Ashley_N_-Nielsen-Aff2" data-author-search="Nielsen, Ashley N.">Ashley N. Nielsen</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Dori-Balser-Aff3" data-author-popup="auth-Dori-Balser-Aff3" data-author-search="Balser, Dori">Dori Balser</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Emily-Furtado-Aff2" data-author-popup="auth-Emily-Furtado-Aff2" data-author-search="Furtado, Emily">Emily Furtado</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-David_C_-Steinberger-Aff3" data-author-popup="auth-David_C_-Steinberger-Aff3" data-author-search="Steinberger, David C.">David C. Steinberger</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Leah-Fruchtman-Aff3" data-author-popup="auth-Leah-Fruchtman-Aff3" data-author-search="Fruchtman, Leah">Leah Fruchtman</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Joseph_P_-Culver-Aff1-Aff4-Aff5-Aff6-Aff7" data-author-popup="auth-Joseph_P_-Culver-Aff1-Aff4-Aff5-Aff6-Aff7" data-author-search="Culver, Joseph P.">Joseph P. Culver</a><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff4">4</a>,<a href="#Aff5">5</a>,<a href="#Aff6">6</a>,<a href="#Aff7">7</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Chad_M_-Sylvester-Aff1-Aff2" data-author-popup="auth-Chad_M_-Sylvester-Aff1-Aff2" data-author-search="Sylvester, Chad M.">Chad M. Sylvester</a><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 9 authors for this article" title="Show all 9 authors for this article">…</li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Deanna_M_-Barch-Aff1-Aff2-Aff3" data-author-popup="auth-Deanna_M_-Barch-Aff1-Aff2-Aff3" data-author-search="Barch, Deanna M.">Deanna M. Barch</a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0003-1693-8506"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-1693-8506</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a>,<a href="#Aff3">3</a></sup> </li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>

                    

                    <p class="c-article-info-details" data-container-section="info">
                        
    <a data-test="journal-link" href="/neuro" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Nature Neuroscience</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 26</b>, <span class="u-visually-hidden">pages </span>1256–1266 (<span data-test="article-publication-year">2023</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>
                    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">6098 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">79 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/articles/s41593-023-01358-9/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
                    
                </header>

                
    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/development-of-the-nervous-system" data-track="click" data-track-action="view subject" data-track-label="link">Development of the nervous system</a></li><li class="c-article-subject-list__subject"><a href="/subjects/emotion" data-track="click" data-track-action="view subject" data-track-label="link">Emotion</a></li><li class="c-article-subject-list__subject"><a href="/subjects/social-neuroscience" data-track="click" data-track-action="view subject" data-track-label="link">Social neuroscience</a></li>
        </ul>
    </div>

                
    
    
        <div class="u-mb-8 c-status-message c-status-message--boxed c-status-message--info">
            
                <span class="c-status-message__icon">
                    <svg class="u-icon" width="18" height="18" aria-hidden="true" focusable="false">
                        <use xlink:href="#icon-eds-i-info-filled-medium"></use>
                    </svg>
                </span>
            
            
                
                    <p class="u-mt-0">An <a href="https://doi.org/10.1038/s41593-023-01420-6" class="relation-link" data-track="click" data-track-action="view linked article" data-track-label="link">Author Correction</a> to this article was published on 27 July 2023</p>
                
            
        </div>
    

    
        <div class="u-mb-8 c-status-message c-status-message--boxed c-status-message--info">
            
                <span class="c-status-message__icon">
                    <svg class="u-icon" width="18" height="18" aria-hidden="true" focusable="false">
                        <use xlink:href="#icon-eds-i-info-filled-medium"></use>
                    </svg>
                </span>
            
            <p class="u-mt-0">This article has been <a href="#change-history">updated</a></p>
        </div>
    
    

                
            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Humans require a shared conceptualization of others’ emotions for adaptive social functioning. A concept is a mental blueprint that gives our brains parameters for predicting what will happen next. Emotion concepts undergo refinement with development, but it is not known whether their neural representations change in parallel. Here, in a sample of 5–15-year-old children (<i>n</i> = 823), we show that the brain represents different emotion concepts distinctly throughout the cortex, cerebellum and caudate. Patterns of activation to each emotion changed little across development. Using a model-free approach, we show that activation patterns were more similar between older children than between younger children. Moreover, scenes that required inferring negative emotional states elicited higher default mode network activation similarity in older children than younger children. These results suggest that representations of emotion concepts are relatively stable by mid to late childhood and synchronize between individuals during adolescence.</p></div></div></section>

            <noscript>
                
                    
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01358-9.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                
            </noscript>

            
                <div class="js-context-bar-sticky-point-mobile">
                    
                        <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-entitled-mobile
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01358-9.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                    
                </div>
            

            
                
                    
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41593-024-01647-x/MediaObjects/41593_2024_1647_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41593-024-01647-x?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1038/s41593-024-01647-x">Cortico-cortical transfer of socially derived information gates emotion recognition
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">20 May 2024</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-020-59282-y/MediaObjects/41598_2020_59282_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41598-020-59282-y?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1038/s41598-020-59282-y">Emotions and brain function are altered up to one month after a single high dose of psilocybin
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">10 February 2020</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-48367-1/MediaObjects/41467_2024_48367_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41467-024-48367-1?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1038/s41467-024-48367-1">Hippocampal sharp-wave ripples correlate with periods of naturally occurring self-generated thoughts in humans
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">22 May 2024</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'topic',
                        model: 'visits_v2',
                        policy_id: 'speedy-BootstrappedUCB',
                        timestamp: 1717218558,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                
                
                <div class="main-content">
                    <section data-title="Main"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Main</h2><div class="c-article-section__content" id="Sec1-content"><p>A critical social skill developed in childhood is the ability to discern and interpret emotions of other people. To aid in emotion perception and inference in real time, it is theorized that our brains use established concepts—blueprints for how specific interactions typically unfold—for prediction<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Alba, J. W. &amp; Hasher, L. Is memory schematic? Psychol. Bull. 93, 203–231 (1983)." href="#ref-CR1" id="ref-link-section-d52698776e653">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Masís-Obando, R., Norman, K. A. &amp; Baldassano, C. Schema representations in distinct brain networks support narrative memory during encoding and retrieval. eLife 11, e70445 (2022)." href="#ref-CR2" id="ref-link-section-d52698776e653_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="van Kesteren, M. T. R., Ruiter, D. J., Fernández, G. &amp; Henson, R. N. How schema and novelty augment memory formation. Trends Neurosci. 35, 211–219 (2012)." href="#ref-CR3" id="ref-link-section-d52698776e653_2">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Darley, J. M. &amp; Fazio, R. H. Expectancy confirmation processes arising in the social interaction sequence. Am. Psychol. 35, 867–881 (1980)." href="/articles/s41593-023-01358-9#ref-CR4" id="ref-link-section-d52698776e656">4</a></sup>. Behavioral work has indicated that concepts for how to glean emotion-related cues from the environment begin developing shortly after birth<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ruba, A. L. &amp; Pollak, S. D. The development of emotion reasoning in infancy and early childhood. Annu. Rev. Dev. Psychol. 2, 503–531 (2020)." href="/articles/s41593-023-01358-9#ref-CR5" id="ref-link-section-d52698776e660">5</a></sup>, although it is unclear how the brain develops these concepts or to what extent such concepts are shared across individuals. Indeed, separate studies have elucidated changes in brain structure and function across childhood and adolescence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Mills, K. L. et al. Structural brain development between childhood and adulthood: convergence across four longitudinal samples. Neuroimage 141, 273–281 (2016)." href="#ref-CR6" id="ref-link-section-d52698776e664">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Giedd, J. N. &amp; Rapoport, J. L. Structural MRI of pediatric brain development: what have we learned and where are we going? Neuron 67, 728–734 (2010)." href="#ref-CR7" id="ref-link-section-d52698776e664_1">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Grayson, D. S. &amp; Fair, D. A. Development of large-scale functional networks from birth to adulthood: a guide to the neuroimaging literature. Neuroimage 160, 15–31 (2017)." href="/articles/s41593-023-01358-9#ref-CR8" id="ref-link-section-d52698776e667">8</a></sup> and changes in activation to different emotions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Malsert, J., Palama, A. &amp; Gentaz, E. Emotional facial perception development in 7, 9 and 11 year-old children: the emergence of a silent eye-tracked emotional other-race effect. PLoS ONE 15, e0233008 (2020)." href="/articles/s41593-023-01358-9#ref-CR9" id="ref-link-section-d52698776e671">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Batty, M. &amp; Taylor, M. J. The development of emotional face processing during childhood. Dev. Sci. 9, 207–220 (2006)." href="/articles/s41593-023-01358-9#ref-CR10" id="ref-link-section-d52698776e674">10</a></sup>, none of which use contextualized stimuli that would activate naturalistic emotion concepts. Thus, to understand how children develop real-world emotion concepts, we must investigate how the brain encodes emotional cues with complete narrative/social context. Several questions remain. How are emotion concepts represented in the brain? Does this representation change across childhood and adolescence as children develop and refine their emotion reasoning skills? Identifying how emotion concept development and neurodevelopment are linked would provide fundamental insight into not only how emotional development occurs in conjunction with neuroplasticity but also when we may best intervene to improve health outcomes in children at risk for common psychiatric disorders that are associated with dysfunctions in emotion perception and inference.</p><p>Emotion perception and inference includes complex processes: detecting information or pertinent cues, attending to important or salient cues and interpreting and contextualizing those cues within our current state and past experiences<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Lemerise, E. A. &amp; Arsenio, W. F. An integrated model of emotion processes and cognition in social information processing. Child Dev. 71, 107–118 (2000)." href="/articles/s41593-023-01358-9#ref-CR11" id="ref-link-section-d52698776e681">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Crick, N. R. &amp; Dodge, K. A. A review and reformulation of social information-processing mechanisms in children’s social adjustment. Psychol. Bull. 115, 74–101 (1994)." href="/articles/s41593-023-01358-9#ref-CR12" id="ref-link-section-d52698776e684">12</a></sup>. Concepts are theorized to influence these processes by providing computational shortcuts, allowing our brains to make quick assessments and accurate predictions. When one considers what cognitive processes are implicated in real-world social interactions, we would expect that nearly every cognitive network would be necessary for these computations. For instance, consider the common experience of hearing an acquaintance tell you a fond story about your mutual friend. This interaction is expected to recruit primary sensory regions for visual and auditory processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Mishkin, M., Ungerleider, L. G. &amp; Macko, K. A. Object vision and spatial vision: two cortical pathways. Trends Neurosci. 6, 414–417 (1983)." href="/articles/s41593-023-01358-9#ref-CR13" id="ref-link-section-d52698776e688">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Dehaene-Lambertz, G., Hertz-Pannier, L. &amp; Dubois, J. Nature and nurture in language acquisition: anatomical and functional brain-imaging studies in infants. Trends Neurosci. 29, 367–373 (2006)." href="/articles/s41593-023-01358-9#ref-CR14" id="ref-link-section-d52698776e691">14</a></sup>; the saliency, cingulo-opercular and attentional networks for alertness, cue detection and attention<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Corbetta, M. &amp; Shulman, G. L. Control of goal-directed and stimulus-driven attention in the brain. Nat. Rev. Neurosci. 3, 215–229 (2002)." href="#ref-CR15" id="ref-link-section-d52698776e695">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Sadaghiani, S. &amp; D’Esposito, M. Functional characterization of the cingulo-opercular network in the maintenance of tonic alertness. Cereb. Cortex 25, 2763–2773 (2015)." href="#ref-CR16" id="ref-link-section-d52698776e695_1">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Barrett, L. F. &amp; Satpute, A. B. Large-scale brain networks in affective and social neuroscience: towards an integrative functional architecture of the brain. Curr. Opin. Neurobiol. 23, 361–372 (2013)." href="/articles/s41593-023-01358-9#ref-CR17" id="ref-link-section-d52698776e698">17</a></sup>; the default mode network for memory, self and relational processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Buckner, R. L. &amp; DiNicola, L. M. The brain’s default network: updated anatomy, physiology and evolving insights. Nat. Rev. Neurosci. 20, 593–608 (2019)." href="/articles/s41593-023-01358-9#ref-CR18" id="ref-link-section-d52698776e702">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Satpute, A. B. &amp; Lindquist, K. A. The default mode network’s role in discrete emotion. Trends Cogn. Sci. 23, 851–864 (2019)." href="/articles/s41593-023-01358-9#ref-CR19" id="ref-link-section-d52698776e705">19</a></sup>; and the frontoparietal network for working memory and behavioral organization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Marek, S. &amp; Dosenbach, N. U. F. The frontoparietal network: function, electrophysiology, and importance of individual precision mapping. Dialogues Clin. Neurosci. 20, 133–140 (2018)." href="/articles/s41593-023-01358-9#ref-CR20" id="ref-link-section-d52698776e709">20</a></sup>. If this is indeed how emotional cues are processed in the real world, we would expect there to be distinct brain-wide patterns for functionally distinct emotion concepts (for example, anger versus happiness) as they require distinct behavioral responses. Rather than finding patterns of activation to external emotion cues throughout the brain in line with this theory, small sample work examining how static emotional pictures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Lindquist, K. A. &amp; Barrett, L. F. A functional architecture of the human brain: emerging insights from the science of emotion. Trends Cogn. Sci. 16, 533–540 (2012)." href="/articles/s41593-023-01358-9#ref-CR21" id="ref-link-section-d52698776e714">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Kragel, P. A., Reddan, M. C., LaBar, K. S. &amp; Wager, T. D. Emotion schemas are embedded in the human visual system. Sci. Adv. 5, eaaw4358 (2019)." href="/articles/s41593-023-01358-9#ref-CR22" id="ref-link-section-d52698776e717">22</a></sup> and positive and negative video content<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Park, A. T. et al. Early stressful experiences are associated with reduced neural responses to naturalistic emotional and social content in children. Dev. Cogn. Neurosci. 57, 101152 (2022)." href="/articles/s41593-023-01358-9#ref-CR23" id="ref-link-section-d52698776e721">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Gruskin, D. C., Rosenberg, M. D. &amp; Holmes, A. J. Relationships between depressive symptoms and brain responses during emotional movie viewing emerge in adolescence. Neuroimage 216, 116217 (2020)." href="/articles/s41593-023-01358-9#ref-CR24" id="ref-link-section-d52698776e724">24</a></sup> are encoded in the brain has largely identified discrete regions or networks. Recent work using single-emotion movie trailers or film clips have found representations of emotion-specific information in primary sensory regions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Kragel, P. A., Reddan, M. C., LaBar, K. S. &amp; Wager, T. D. Emotion schemas are embedded in the human visual system. Sci. Adv. 5, eaaw4358 (2019)." href="/articles/s41593-023-01358-9#ref-CR22" id="ref-link-section-d52698776e728">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Camacho, M. C., Karim, H. T. &amp; Perlman, S. B. Neural architecture supporting active emotion processing in children: a multivariate approach. Neuroimage 188, 171–180 (2019)." href="/articles/s41593-023-01358-9#ref-CR25" id="ref-link-section-d52698776e731">25</a></sup>. Real-world processing is more complex than single-valence clips can capture, however. Thus, a formal test of how the brain supports real-world emotion processing—using stimuli that are rich in both content and narrative, with multiple emotions presented at once across different characters—has yet to be conducted. For that, more narrative and dynamic stimuli are needed to activate real-world emotion concepts.</p><p>Behavioral work suggests that children develop emotion concepts in a broad to specific pattern rapidly across early childhood, learning to parse cues for sadness, anger, fear and happiness by around ages 5–8 years<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Widen, S. C. Children’s interpretation of facial expressions: the long path from valence-based to specific discrete categories. Emot. Rev. 5, 72–77 (2013)." href="/articles/s41593-023-01358-9#ref-CR26" id="ref-link-section-d52698776e738">26</a></sup>. Alhough at a slower rate of change, there is evidence that emotion concepts continue to change across childhood and early adolescence, with rates of change in emotion inference abilities slowing markedly at around ages 15–17 years<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nook, E. C., Sasse, S. F., Lambert, H. K., McLaughlin, K. A. &amp; Somerville, L. H. The nonlinear development of emotion differentiation: granular emotional experience is low in adolescence. Psychol. Sci. 29, 1346–1357 (2018)." href="#ref-CR27" id="ref-link-section-d52698776e742">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nook, E. C. et al. Charting the development of emotion comprehension and abstraction from childhood to adulthood using observer-rated and linguistic measures. Emotion 20, 773–792 (2020)." href="#ref-CR28" id="ref-link-section-d52698776e742_1">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Motta-Mena, N. V. &amp; Scherf, K. S. Pubertal development shapes perception of complex facial expressions. Dev. Sci. 20, e12451 (2017)." href="/articles/s41593-023-01358-9#ref-CR29" id="ref-link-section-d52698776e745">29</a></sup>. Most previous work examining developmental differences in neural activation to emotional stimuli have fewer than 30 individuals in their child group<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Thomas, K. M. et al. Amygdala response to facial expressions in children and adults. Biol. Psychiatry 49, 309–316 (2001)." href="#ref-CR30" id="ref-link-section-d52698776e749">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wiggins, J. L. et al. Developmental differences in the neural mechanisms of facial emotion labeling. Soc. Cogn. Affect. Neurosci. 11, 172–181 (2016)." href="#ref-CR31" id="ref-link-section-d52698776e749_1">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Marusak, H. A., Carré, J. M. &amp; Thomason, M. E. The stimuli drive the response: an fMRI study of youth processing adult or child emotional face stimuli. Neuroimage 83, 679–689 (2013)." href="#ref-CR32" id="ref-link-section-d52698776e749_2">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ladouceur, C. D., Schlund, M. W. &amp; Segreti, A.-M. Positive reinforcement modulates fronto-limbic systems subserving emotional interference in adolescents. Behav. Brain Res. 338, 109–117 (2018)." href="#ref-CR33" id="ref-link-section-d52698776e749_3">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hoehl, S., Brauer, J., Brasse, G., Striano, T. &amp; Friederici, A. D. Children’s processing of emotions expressed by peers and adults: an fMRI study. Soc. Neurosci. 5, 543–559 (2010)." href="#ref-CR34" id="ref-link-section-d52698776e749_4">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Haller, S. P. et al. Reliability of neural activation and connectivity during implicit face emotion processing in youth. Dev. Cogn. Neurosci. 31, 67–73 (2018)." href="#ref-CR35" id="ref-link-section-d52698776e749_5">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Lobaugh, N. J., Gibson, E. &amp; Taylor, M. J. Children recruit distinct neural systems for implicit emotional face processing. Neuroreport 17, 215–219 (2006)." href="/articles/s41593-023-01358-9#ref-CR36" id="ref-link-section-d52698776e752">36</a></sup> and/or fewer than 10 individuals on average per year of age<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Guyer, A. E. et al. A developmental examination of amygdala response to facial expressions. J. Cogn. Neurosci. 20, 1565–1582 (2008)." href="#ref-CR37" id="ref-link-section-d52698776e756">37</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pagliaccio, D. et al. Functional brain activation to emotional and nonemotional faces in healthy children: evidence for developmentally undifferentiated amygdala function during the school-age period. Cogn. Affect. Behav. Neurosci. 13, 771–789 (2013)." href="#ref-CR38" id="ref-link-section-d52698776e756_1">38</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Hare, T. A. et al. Biological substrates of emotional reactivity and regulation in adolescence during an emotional go-nogo task. Biol. Psychiatry 63, 927–934 (2008)." href="/articles/s41593-023-01358-9#ref-CR39" id="ref-link-section-d52698776e759">39</a></sup>, and many report only amygdala region-of-interest analyses (for a review, see ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Somerville, L. H., Fani, N. &amp; McClure-Tone, E. B. Behavioral and neural representation of emotional facial expressions across the lifespan. Dev. Neuropsychol. 36, 408–428 (2011)." href="/articles/s41593-023-01358-9#ref-CR40" id="ref-link-section-d52698776e763">40</a></sup>). Although adults and older adolescents readily conjure what static emotional pictures represent, recent work has demonstrated that children rely upon social context and learned labels to be able to verbalize and identify emotions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Widen, S. C. &amp; Russell, J. A. Children acquire emotion categories gradually. Cogn. Dev. 23, 291–312 (2008)." href="/articles/s41593-023-01358-9#ref-CR41" id="ref-link-section-d52698776e768">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Widen, S. C. &amp; Russell, J. A. Children’s scripts for social emotions: causes and consequences are more central than are facial expressions. Br. J. Dev. Psychol. 28, 565–581 (2010)." href="/articles/s41593-023-01358-9#ref-CR42" id="ref-link-section-d52698776e771">42</a></sup>, consistent with the broader literature demonstrating that children rely on social information for learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Wu, Y., Schulz, L. E., Frank, M. C. &amp; Gweon, H. Emotion as information in early social learning. Curr. Dir. Psychol. Sci. 30, 468–475 (2021)." href="/articles/s41593-023-01358-9#ref-CR43" id="ref-link-section-d52698776e775">43</a></sup>. Movie stimuli can, therefore, be used to provide full narrative context, enhancing ecological validity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Cantlon, J. F. The balance of rigor and reality in developmental neuroscience. Neuroimage 216, 116464 (2020)." href="#ref-CR44" id="ref-link-section-d52698776e779">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Vanderwal, T., Eilbott, J. &amp; Castellanos, F. X. Movies in the magnet: naturalistic paradigms in developmental functional neuroimaging. Dev. Cogn. Neurosci. 36, 100600 (2019)." href="#ref-CR45" id="ref-link-section-d52698776e779_1">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Eickhoff, S. B., Milham, M. &amp; Vanderwal, T. Towards clinical applications of movie fMRI. Neuroimage 217, 116860 (2020)." href="/articles/s41593-023-01358-9#ref-CR46" id="ref-link-section-d52698776e782">46</a></sup>. Although movies still differ from reality in important ways (for example, movies have scene cuts, follow a coherent narrative and can use scenery or lighting to add to the emotional cues), they offer a considerably more naturalistic index of emotion processing over single-valence clips or static images. Only two studies have examined activation to emotional stimuli using video stimuli in children, both in relation to emotional valence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Park, A. T. et al. Early stressful experiences are associated with reduced neural responses to naturalistic emotional and social content in children. Dev. Cogn. Neurosci. 57, 101152 (2022)." href="/articles/s41593-023-01358-9#ref-CR23" id="ref-link-section-d52698776e786">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Gruskin, D. C., Rosenberg, M. D. &amp; Holmes, A. J. Relationships between depressive symptoms and brain responses during emotional movie viewing emerge in adolescence. Neuroimage 216, 116217 (2020)." href="/articles/s41593-023-01358-9#ref-CR24" id="ref-link-section-d52698776e789">24</a></sup>. Both samples were small (fewer than 10 children on average per year), and neither examined activation to functionally distinct emotions, making it difficult to connect these studies to behavioral emotion development. Thus, the potential for using movies to examine emotion concept neurodevelopment has yet to be explored.</p><p>In this study, we leveraged a large cross-sectional pediatric dataset to characterize the brain network activation modulated by emotions and how they differ across development. Specifically, we aimed to characterize (1) how broad (positive and negative) and specific (angry, happy, sad, fearful and excited) emotion concepts are represented in the brain; (2) how these representations differ across development; and (3) what specific emotional contexts elicit shared and diverse activation responses with respect to developmental stage. Furthermore, we used a discovery and replication approach in all analyses to ensure generalization of our findings. We predict that emotion categories will have distinct representations in the brain, reflecting a shared functional understanding of these complex socio-emotional cues. Based on the behavioral emotion development literature, we expect changes across adolescence to reflect refinement of emotion processing, which would likely be reflected in change in higher-order cognitive networks that integrate sensory signals. It is possible that emotion processing changes in a manner that is not well captured by linear or curvilinear processes. For instance, activation may stabilize across development, resulting in older children having more similar activation patterns (convergence on a shared concept), or activation may diverge across development, reflecting individual differences in experiences or expectations. Thus, we examined the viability of converging or diverging models of development using similarity analysis in addition to examining linear and curvilinear patterns. Data were pre-processed using the Human Connectome Project minimal processing pipeline<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Glasser, M. F. et al. The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage 80, 105–124 (2013)." href="/articles/s41593-023-01358-9#ref-CR47" id="ref-link-section-d52698776e796">47</a></sup>, and post-processing and analysis were carried out in Python (code available at <a href="https://github.com/catcamacho/hbn_analysis">https://github.com/catcamacho/hbn_analysis</a>).</p></div></div></section><section data-title="Results"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Results</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Contextualized emotions are represented throughout the brain</h3><p>Videos were processed using the EmoCodes system<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Camacho, M. C. et al. EmoCodes: a standardized coding system for socio-emotional content in complex video stimuli. Affect. Sci. 3, 168–181 (2022)." href="/articles/s41593-023-01358-9#ref-CR48" id="ref-link-section-d52698776e819">48</a></sup>, a standardized coding system that we developed to characterize emotional/non-emotional and low-level/high-level information in cartoon videos, which cannot be characterized using existing algorithms designed for live-action stimuli. Using this system, we derived traces (adult ratings) of general (positive and negative) and specific (angry, happy, sad, fearful and excited) emotion content as well as potential low-level covariates (see Appendix A for full video characterization). We estimated activation to each contextualized emotion by convolving each emotion-specific timeseries (rescaled to a range of 0–1) with the hemodynamic response function and entering them into a general linear model (GLM) predicting each parcel’s blood-oxygen-level-dependent (BOLD) signal. The general emotions activation model included positive emotion, negative emotion, brightness, loudness, spoken words and written words traces. The specific emotions activation model included angry, happy, sad, fearful and excited emotions as well as brightness, loudness, spoken words and written words traces. Presence of each of these video features varied widely across the videos, from 2.3% to 75.2% of the run time (Appendix A). Mean emotion activation maps (beta weights for each model) are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig1">1</a>, and difference maps are in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig7">1</a>. Parcel-level subject-level maps (encompassing 394 regions from 12 cortical networks and eight subcortical structures) were used as features in the support vector machine analyses.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Mean activation (model coefficients or betas) to each emotion category across both videos."><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1: Mean activation (model coefficients or betas) to each emotion category across both videos.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="250"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>No statistical test was performed on these data directly. Top: General emotions. The general emotions were modeled as regressors in a GLM predicting BOLD signal alongside brightness, loudness, spoken words and written words. Bottom: Specific emotions. The specific emotions were modeled as regressors alongside brightness and loudness.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>To test if activation patterns were dissociable across emotions, we performed support vector classification analysis on the subject-level parcel activation maps (one per emotion per video) predicting emotion data labels. We trained each model on parcel-wise activation maps from the Discovery sample (<i>n</i> = 424 participants) with 10-fold cross-validation and tested on the Replication sample (<i>n</i> = 399 participants). Across both the general and specific emotion whole-brain models, activation to emotions was classified with high accuracy (general: 88% (chance = 50%) and specific: 73% (chance = 20%)), indicating that activation patterns to specific emotions are highly dissociable. Models performed significantly above chance even when non-emotion feature activation maps (brightness, loudness, spoken words and written words) were included alongside emotion activation maps for model fitting and testing (general: 76% accuracy (chance = 20%) and specific: 66% accuracy (chance = 11%)). Full model statistics are reported in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41593-023-01358-9#Tab1">1</a>. Follow-up permutation-based testing of the emotion-only models revealed that several networks were informative to the model, with brain regions spanning primary sensory and higher-level cortex. Specifically, for the model predicting general emotion activation labels (positive and negative), permuting the Visual, Default Mode, Somatomotor-Hand and Cingulo-Opercular networks each affected the model classification accuracy more than random parcels of the same size of the other networks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig2">2a</a>), suggesting that these networks have unique emotion information relative to other brain regions/networks. Interestingly, for the specific emotion model (angry, excited, fearful, happy and sad), the Visual network had the most unique emotion-specific information above and beyond random regions, followed by the Dorsal Attention, Ventral Attention, Cingulo-Opercular, Somatomotor-Hand, Cerebellum, Fronto-Parietal, Temporo-ventromedial prefrontal cortex (VMPFC), Auditory and Default Mode networks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig2">2b</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Emotion activation classification</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/articles/s41593-023-01358-9/tables/1" aria-label="Full size table 1"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Activation classification analysis results."><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2: Activation classification analysis results.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="381"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Activation classification analysis results for general emotions (Negative and Positive, <b>a</b>) and for specific emotions (Angry, Happy, Sad, Fearful and Excited, <b>b</b>). Top left: confusion matrix indicating the proportion of correct data labels and incorrect data labels for each emotion category in the testing (unseen) dataset. Top right: Parcel-level mean importance scores derived from permutation-based analysis. Higher values indicate that permuting that data label resulted in a decrease in model accuracy. Note that no one parcel permutation causes the model accuracy to drop below chance. Bottom left: network/region-level permutation-based importance testing results. Null distributions were generated by permutation random parcels of the same <i>n</i> as the target network/region. Boxen plots include the median as the center of the distribution with each successive level outwards including 50% of the remaining data in the distribution. Bottom right: mean classification accuracy when train/test data are limited to a single region/network. Limiting data did not improve model performance for either General or Specific emotion classification. The 95% CIs were estimated using a pseudo-bootstrap method in which a subset of the testing data was used to estimate accuracy. This procedure was repeated 10,000 times, and the resulting distribution was used to estimate the CI. SM, Somatomotor.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Systematically permuting data from each of these regions/networks did not lower accuracy to chance levels, however, suggesting that there was still sufficient information throughout the rest of the brain to make accurate classifications. To directly examine which networks/regions contained sufficient information to classify emotion activation maps at above chance levels, we systematically limited Training and Testing data to single networks/regions at a time and computed confidence intervals (CIs) using a bootstrapping approach. When the activation data were systematically limited to each region/network, we found that most of the brain had some emotion information represented, with parcel activation from 15 of 21 regions/networks able to predict general emotion activation labels significantly above chance: Visual, Temporo-VMPFC, Fronto-Parietal, Auditory, Ventral Attention, Default, Dorsal Attention, Cingulo-Opercular, Cerebellum, Somatomotor-Hand, Medial Parietal, Caudate, Somatomotor-Mouth, Parietal-Occipital and the Thalamus. For the model predicting specific emotion activation labels, activation from each of 14 of 21 networks/regions alone was able to classify emotion activation labels better than chance: Visual, Temporo-VMPFC, Cingulo-Opercular, Ventral Attention, Dorsal Attention, Auditory, Default Mode, Somatomotor-Hand, Fronto-Parietal, Cerebellum, Medial Parietal, Somatomotor-Mouth and Salience. Together, these results indicate that emotional content is processed throughout the brain in children, with the most unique information represented in primary sensory visual and auditory regions as well as higher-order associative networks that span the frontal, temporal and parietal lobes. As a follow-up, we repeated these analyses using data for each video separately. Those results are reported in Appendix B and largely replicate the main analyses.</p><h3 class="c-article__sub-heading" id="Sec4">Activation patterns are relatively stable across development</h3><p>To test if activation to general or specific emotions differed across maturity, we performed support vector regression predicting chronological age and puberty scores from each emotion activation map. Across all models, test sample accuracy was poor as indicated by high mean squared error (MSE; 9.27–12.16 for age and 22.21–27.60 for puberty) and modest correlations between actual and predicted labels (0.07–0.19), suggesting a weak linear association between maturity indices and activation, with no meaningful differences between age and puberty model performance. Interestingly, we found similar model fits for activation to non-emotional content (brightness, loudness, spoken words and written words), suggesting modest differences in global processing across development that is not specific to a single cue modality. Full results are reported in Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM1">SC2</a>. Using limited data or a curvilinear kernel did not meaningfully improve model performance (see Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM1">SC2</a> for full results).</p><p>Post hoc examination of parcel-wise correlations with maturity confirmed a modest linear association between maturity and activation (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig8">2</a>), with maturity explaining, at most, 3.8% of the variance in parcel-level activation (Age-Activation <i>r</i><sup>2</sup> range: Negative 0–0.017, Positive 0–0.030, Angry 0–0.020, Happy 0–0.026, Sad 0–0.023, Excited 0–0.028, Fearful 0–0.023; Puberty-Activation <i>r</i><sup>2</sup> range: Negative 0–0.029, Positive 0–0.030, Angry 0–0.024, Happy 0–0.038, Sad 0–0.014, Excited 0–0.030, Fearful 0–0.021).</p><h3 class="c-article__sub-heading" id="Sec5">Synchronization of emotion activation across development</h3><p>It is possible that linear/curvilinear models are not able to fully capture the changes associated with processing complex stimuli (for example, if activation variability changes across development rather than magnitude of signal); thus, we also tested if we could identify emotion processing-related differences across maturity using a model-free approach. Specifically, we sought to test if (1) there were differences across maturity not captured by linear/curvilinear models and (2) if specific scenes elicited lesser or greater similarity within older/younger children. To accomplish this, we used inter-subject representational similarity and tested if similarity in activation across the video (inter-subject correlation (ISC)) corresponded to one of three different models of cognitive affective development: nearest neighbor (are children proximal in maturity processing the video similarly?), convergence (do children process the video more similarly as they mature?) and divergence (do children process the video less similarly as they mature?). We found the most evidence for convergence in parcel-wise activation across both age and puberty, with age fitting the data better than puberty as indicated by more significant parcels and higher similarity coefficients. Specifically, for each <i>Despicable Me</i> and <i>The Present</i>, the convergence models (that is, that activations among older children exhibit more similarity than activations among younger children) had the most significant parcels and higher similarity coefficients after false discovery rate (FDR) correction (<i>Despicable Me</i> Age: 176 parcels, 0.01–0.16 rho; <i>The Present</i> Age: 150 parcels, 0.01–0.13 rho; <i>Despicable Me</i> Puberty: 38 parcels, 0.02–0.08 rho; <i>The Present</i> Puberty: 84 parcels, 0.02–0.10 rho). Regions highest in similarity were primarily found in occipital, lateral parietal and temporal cortex, similarly to the group-level ISC maps for each video (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig9">3a</a>). Similarity statistical maps are shown in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig9">3b–d</a>. We compared model fits for each parcel using a bootstrapping approach. The results of this analysis are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig3">3</a>. Parcels that replicated across samples for each movie are shown with parcels that replicated across movies outlined. Across significant parcels, the convergence model was predominantly the best fit, indicating that older children had more similar activation patterns to each other when viewing the videos than did younger children.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Similarity analysis results."><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3: Similarity analysis results.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="437"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p><b>a</b>, Similarity models of development examined for each movie and for each maturity metric. <b>b</b>, Best fit models for each parcel using chronological age as the metric of maturity. Parcels were considered significant if the similarity coefficient (Spearman <i>r</i>, one-sided) was FDR-corrected <i>P</i> &lt; 0.05 in both the Discovery and Replication samples. Model fits were determined by generating a distribution of similarity coefficients (through bootstrapping) and comparing distributions for each model, pairwise (two-sided <i>t</i>-test with a permutation-based <i>P</i> value). For each parcel, the model with the highest ranking of similarity coefficients was determined to be the best fit. Outlined parcels are those that were the same model designation across both movies.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec6">Emotional intensity is associated with increased synchrony</h3><p>We next sought to examine what specific scenes or content elicited increased synchrony in the older age group. Because the convergence model of age best fit most parcels in the brain, we then followed up this analysis by examining dynamic synchrony (inter-subject phase synchrony (ISPS)) both across the full sample and within the oldest children (top 20% by age). Significant parcels were grouped into networks that were found to encode emotion-specific information (visual, auditory, dorsal attention, ventral attention, cingulo-opercular, default mode and fronto-parietal) and averaged across participants for group-level peak analysis. We identified scenes at least 5 s long of high network-level synchrony across the sample (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig4">4a</a> and Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig10">4</a>). Differences in high and low synchrony scenes were examined, revealing that scenes higher in emotional intensity elicited increased synchrony across the sample and across movies in the visual, ventral attention and default mode networks.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Group-level dynamic synchrony results for Despicable Me."><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4: Group-level dynamic synchrony results for <i>Despicable Me</i>.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="616"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p> <b>a</b>, Dynamic synchrony across the full sample with replicating peaks in synchrony shaded purple. Replicating peaks were defined as peaks at least 5 s wide and with a prominence higher than the 95th percentile value for that network (permutation-based <i>P</i> &lt; 0.001) appearing in both Discovery and Replication samples. Parcels were limited to those that were significantly correlated across the sample at the group level after FDR correction. <b>b</b>, Video feature means within the peaks were compared to features outside of the peaks using a two-sided <i>t</i>-test to test if specific video features elicited increased synchronization. Plotted features were those that were significantly different (two-sided <i>t</i>-test, FDR-corrected <i>P</i> &lt; 0.05). Bar plots indicate mean values with overlaid dots of individual data points. <b>c</b>, Violin plots of mean synchrony values by network. White dots indicate median value; box indicates the 50% interquartile range; and whiskers indicate each the upper and lower 25%. The dashed horizontal line indicates the value at permuted <i>P</i> &lt; 0.05 after FDR correction. NumChars, number of characters; SaliencyFract, fraction of frame containing highly salient content.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec7">Default mode network synchronization across development</h3><p>A side-by-side comparison of full group dynamic analysis results and the results from repeating the analysis in the oldest children only for each video indicated yielded three observations. First, although the overall range of synchrony was higher for the oldest children, the specific scenes that elicited higher synchrony largely overlapped with scenes derived from the full sample dynamic synchrony traces for the visual, auditory, dorsal attention, ventral attention and cingulo-opercular networks. The same number of scenes elicited high synchrony in the oldest children as in the full sample for both videos in the fronto-parietal network, although the specific scenes did not overlap. Because few parcels from the frontoparietal network were significant from the inter-subject representational similarity analysis (IS-RSA), we did not interpret results from this network further. Finally, the default mode network demonstrated the largest relative difference in synchrony between the oldest children and the full sample. Specifically, three additional scenes evoked higher synchrony in the oldest children that did not appear in the full sample analysis. Results from this analysis are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig5">5</a> and Extended Data Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig11">5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig12">6</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Dynamic synchrony analysis results in the oldest children for Despicable Me."><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5: Dynamic synchrony analysis results in the oldest children for <i>Despicable Me</i>.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="420"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p><b>a</b>, Network dynamic activation similarity (synchrony) for each of the oldest and youngest children in the sample. Included parcels are shown to the left of each trace. Shaded areas denote significant increases in synchrony in the oldest children (1.5 s.d. above the mean). <b>b</b>, Bar plots of the results from the video feature analysis comparing portions of the video within peaks of inter-subject synchrony to outside the peaks using a two-sided <i>t</i>-test. Bar plots are of the mean with individual datapoints overlaid. Only features that significantly differed (two-sided FDR-corrected <i>P</i> &lt; 0.05) are plotted.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Quantitative analysis of the video features under the peaks and outside of the peaks indicated that increased synchrony across the whole sample and the oldest children was induced during scenes depicting negative emotion. Qualitative analysis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig6">6</a>) of the scenes indicated that the three <i>Despicable Me</i> scenes that induced increased synchrony in the older children, and were not identified in the full sample, were those with multiple layers of implicit emotional information. For example, the scene with the highest peak synchrony of the three is during the rocket launch, when the minions place a ticket to a ballet recital in Gru’s pocket, and Gru quickly gets upset with the minions in response. To understand why Gru reacts so strongly in this scene, the viewer would have to have understood the inner turmoil that Gru experienced in the previous scenes. Thus, it is possible that this analysis is capturing development in the interpretation and integration stages of emotion processing (from the social information processing model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Lemerise, E. A. &amp; Arsenio, W. F. An integrated model of emotion processes and cognition in social information processing. Child Dev. 71, 107–118 (2000)." href="/articles/s41593-023-01358-9#ref-CR11" id="ref-link-section-d52698776e1403">11</a></sup>) represented in converging activation of the default mode network.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Qualitative examination of the scenes eliciting higher synchrony in the oldest children for each video."><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6: Qualitative examination of the scenes eliciting higher synchrony in the oldest children for each video.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig6_HTML.png?as=webp"><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="362"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p><b>a</b>, Dynamic synchrony in the oldest and youngest children for each video. Shaded areas are peaks of increased synchrony in the oldest children. <b>b</b>, Bar plots of the results from the video feature analysis comparing portions of the video within peaks of inter-subject synchrony to outside the peaks using a two-sided <i>t</i>-test. Bar plots are of the mean with individual datapoints overlaid. Only features that significantly differed (two-sided FDR-corrected <i>P</i> &lt; 0.05) are plotted. <b>c</b>, Descriptions of each scene, all of which included negative emotional inference.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-023-01358-9/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Discussion</h2><div class="c-article-section__content" id="Sec8-content"><p>In this study, we provide, to our knowledge, the first empirical evidence that contextualized emotion processing—which activates emotion concepts—is represented throughout the brain in middle childhood through mid-adolescence. We demonstrate that activation to each general emotion (positive and negative) and specific emotion (angry, happy, sad, fearful and excited) cues are highly dissociable, indicating well-established concept representation of these emotions in this age range. We demonstrate through multiple approaches that concept representation shifts modestly across age and puberty, with some evidence that older children show more similar patterns of activation to video stimuli to each other than do younger children. Dynamic analysis revealed that this effect was most profound for the default mode network, with scenes depicting negative emotions evoking increasingly similar activation patterns with age. Taken together, our work suggests that, across late childhood and early adolescence, there is fine-tuning of pre-existing emotion concepts. These findings have important implications for not only how our brain processes real-world emotions but also how to conceptualize risk for emotion dysregulation and the design of psychosocial interventions.</p><p>We found evidence that contextualized emotion-specific cues are encoded throughout the brain. We were able to classify specific and broad emotion concepts with high levels of precision. This suggests that there are both shared and distinct activation features of each broad and specific emotions, likely capturing shared and distinct functions. Notably, every cortical network had some level of emotion-specific activation, supporting the social information processing model of emotion processing presented in the introduction<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Lemerise, E. A. &amp; Arsenio, W. F. An integrated model of emotion processes and cognition in social information processing. Child Dev. 71, 107–118 (2000)." href="/articles/s41593-023-01358-9#ref-CR11" id="ref-link-section-d52698776e1453">11</a></sup> as well as extensions of this framework<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Sander, D., Grandjean, D. &amp; Scherer, K. R. An appraisal-driven componential approach to the emotional brain. Emot. Rev. 10, 219–231 (2018)." href="/articles/s41593-023-01358-9#ref-CR49" id="ref-link-section-d52698776e1457">49</a></sup>. These frameworks use cognitive models of emotion processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Pessoa, L. Understanding emotion with brain networks. Curr. Opin. Behav. Sci. 19, 19–25 (2018)." href="/articles/s41593-023-01358-9#ref-CR50" id="ref-link-section-d52698776e1461">50</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Lindquist, K. A. &amp; MacCormack, J. K. Comment: Constructionism is a multilevel framework for affective science. Emot. Rev. 6, 134–135 (2014)." href="/articles/s41593-023-01358-9#ref-CR51" id="ref-link-section-d52698776e1464">51</a></sup>, which propose that emotions are interpreted as context-dependent concepts (for example, anger may be indicated by any number of signals) and are, therefore, not localized to any one network or region. We observed more widespread activation pattern differences than previous work conducted using single-emotion/valence narrative stimuli<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Kragel, P. A., Reddan, M. C., LaBar, K. S. &amp; Wager, T. D. Emotion schemas are embedded in the human visual system. Sci. Adv. 5, eaaw4358 (2019)." href="/articles/s41593-023-01358-9#ref-CR22" id="ref-link-section-d52698776e1468">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Camacho, M. C., Karim, H. T. &amp; Perlman, S. B. Neural architecture supporting active emotion processing in children: a multivariate approach. Neuroimage 188, 171–180 (2019)." href="/articles/s41593-023-01358-9#ref-CR25" id="ref-link-section-d52698776e1471">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Skerry, A. E. &amp; Saxe, R. Neural representations of emotion are organized around abstract event features. Curr. Biol. 25, 1945–1954 (2015)." href="/articles/s41593-023-01358-9#ref-CR52" id="ref-link-section-d52698776e1474">52</a></sup>, replicating and extending their findings. This is likely due to the additional cognitive processes involved in understanding contextualized emotions, resulting in a broader range of moment-to-moment possibilities that the brain may predict. Thus, it is possible that we are capturing this complex cognition necessary for real-world identification of specific emotions in others in our results. Our results can also be interpreted to support discrete or basic emotion theories, which posit that there is a finite number of core emotions each with unique attributes (see ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Tracy, J. L. &amp; Randles, D. Four models of basic emotions: a review of Ekman and Cordaro, Izard, Levenson, and Panksepp and Watt. Emot. Rev. 3, 397–405 (2011)." href="/articles/s41593-023-01358-9#ref-CR53" id="ref-link-section-d52698776e1478">53</a></sup> for a synthesis). One way of interpreting these theories in the context of neural computation would be that each core emotion has a unique expression and, therefore, would evoke a unique activation pattern regardless of the context. Although we did find unique activation patterns to each emotion that we examined, we did not define our emotion regressors assuming that emotions have unique behavioral cues. Instead, we used functional emotion definitions that emphasize context in combination with behavioral cues. More systematic examination of real-world emotion perception is needed to fully disentangle context-dependent versus context-independent activation to emotion cues. Another interpretation is that discrete or ‘basic’ emotions exist at a level in brain circuitry that is shared across species<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Panksepp, J. &amp; Watt, D. What is basic about basic emotions? Lasting lessons from affective neuroscience. Emot. Rev. 3, 387–396 (2011)." href="/articles/s41593-023-01358-9#ref-CR54" id="ref-link-section-d52698776e1483">54</a></sup>. By this logic, expression of these emotions would have distinct presentations, resulting in distinct activation of sensory cortex. Although we did observe strong emotion-specific signals in sensory cortex in our results, we also observed differences across the rest of the brain, suggesting that emotion context is not only a difference of sensory input. Emotion-specific information is also modulating activation of higher-order cognitive networks, in line with the notion of constructed emotion concepts rather than of discrete emotion configuration.</p><p>Another interpretation of our results is that it is possible that activation to naturalistic emotion cues is innate to some degree. Specifically, the fact that these activation patterns were so consistent across the sample with relatively subtle changes across age suggests that the basic architecture for processing social emotional cues may be present very young, refining quickly across childhood. More research is needed to tease apart if our results represent more constructionist notions of emotion, discrete emotion theory or something in between. Specifically, future work could compare annotations of several longer video clips based on functional definitions (as used here) and discrete expressions and systematically test which coding schemes capture more variance in the activation data across ages. Matching these data to individual differences in emotion reasoning would also help clarify how emotion cues are represented in the brain across development.</p><p>We found evidence for only a modest change in activation across age, suggesting that the core functional architecture of how contextualized emotion cues are interpreted is already present by the time children enter school. Furthermore, similarity analysis revealed that the modest differences across age observed were best characterized as converging similarity, with older children having more similar activation patterns than did younger children. This pattern is consistent with behavioral work, which has shown that, although children continue to develop a shared understanding of what isolated faces represent well into late childhood and early adolescence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Somerville, L. H., Fani, N. &amp; McClure-Tone, E. B. Behavioral and neural representation of emotional facial expressions across the lifespan. Dev. Neuropsychol. 36, 408–428 (2011)." href="/articles/s41593-023-01358-9#ref-CR40" id="ref-link-section-d52698776e1493">40</a></sup>, young children are able to readily distinguish happy, angry, sad and scared emotional states in others when provided narrative context<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Widen, S. C. &amp; Russell, J. A. Children acquire emotion categories gradually. Cogn. Dev. 23, 291–312 (2008)." href="/articles/s41593-023-01358-9#ref-CR41" id="ref-link-section-d52698776e1497">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ogren, M. &amp; Johnson, S. P. Factors facilitating early emotion understanding development: contributions to individual differences. Hum. Dev. 64, 108–118 (2020)." href="#ref-CR55" id="ref-link-section-d52698776e1500">55</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ogren, M. &amp; Sandhofer, C. M. Emotion words link faces to emotional scenarios in early childhood. Emotion 22, 167–178 (2022)." href="#ref-CR56" id="ref-link-section-d52698776e1500_1">56</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Camras, L. A. &amp; Allison, K. Children’s understanding of emotional facial expressions and verbal labels. J. Nonverbal Behav. 9, 84–94 (1985)." href="/articles/s41593-023-01358-9#ref-CR57" id="ref-link-section-d52698776e1503">57</a></sup>. It is, therefore, likely then that the convergence pattern in the current study indicates that existing shared emotion concepts are refined with increasing age. Notably, the fact that we observed this convergence across regions of nearly every cortical network suggests that refinement may not be limited to single facets of emotion stimuli but, rather, the overall gestalt. Interestingly, puberty was not as strongly associated with the neural activation data. Previous work found an association between pubertal status and emotion processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Motta-Mena, N. V. &amp; Scherf, K. S. Pubertal development shapes perception of complex facial expressions. Dev. Sci. 20, e12451 (2017)." href="/articles/s41593-023-01358-9#ref-CR29" id="ref-link-section-d52698776e1507">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Lawrence, K., Campbell, R. &amp; Skuse, D. Age, gender, and puberty influence the development of facial emotion recognition. Front. Psychol. 6, 761 (2015)." href="/articles/s41593-023-01358-9#ref-CR58" id="ref-link-section-d52698776e1510">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Keulers, E. H. H., Evers, E. A. T., Stiers, P. &amp; Jolles, J. Age, sex, and pubertal phase influence mentalizing about emotions and actions in adolescents. Dev. Neuropsychol. 35, 555–569 (2010)." href="/articles/s41593-023-01358-9#ref-CR59" id="ref-link-section-d52698776e1513">59</a></sup>; however, this research did not find a consistent association between puberty and activation to specific emotions or an association that holds above and beyond associations with age<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Dai, J. &amp; Scherf, K. S. Puberty and functional brain development in humans: convergence in findings? Dev. Cogn. Neurosci. 39, 100690 (2019)." href="/articles/s41593-023-01358-9#ref-CR60" id="ref-link-section-d52698776e1517">60</a></sup>. Thus, our data suggest that activation to emotional content in videos is likely capturing the effects of accumulation of life experience on emotion processing development rather than pubertal influences. Our findings provide support for the constructionist theory of emotional development, which posits that children develop shared concepts of socially embedded emotions through socialization, language learning and improved integration of multi-sensory information in the brain<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Hoemann, K., Xu, F. &amp; Barrett, L. F. Emotion words, emotion concepts, and emotional development in children: a constructionist hypothesis. Dev. Psychol. 55, 1830–1849 (2019)." href="/articles/s41593-023-01358-9#ref-CR61" id="ref-link-section-d52698776e1521">61</a></sup>, which enhances children’s abilities to anticipate the consequences of emotions in the real world. Because we used functional definitions when identifying emotional content and found consistent patterns of activation of early-learned emotion concepts across age (and less change in relation to puberty than age), our results are in line with this theory.</p><p>We used adult ratings of functional emotion cues for these data. This was done following standard procedures for examining emotion reasoning development, where studies typically use the adult judgments as the benchmark against which to judge child judgments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Ruba, A. L. &amp; Pollak, S. D. The development of emotion reasoning in infancy and early childhood. Annu. Rev. Dev. Psychol. 2, 503–531 (2020)." href="/articles/s41593-023-01358-9#ref-CR5" id="ref-link-section-d52698776e1529">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Widen, S. C. Children’s interpretation of facial expressions: the long path from valence-based to specific discrete categories. Emot. Rev. 5, 72–77 (2013)." href="/articles/s41593-023-01358-9#ref-CR26" id="ref-link-section-d52698776e1532">26</a></sup>. Although this methodological decision enables us to use this literature to interpret our findings, we would expect individual differences in emotion cue perception to color how these cues are encoded. For instance, there is evidence to suggest that school-age children (who make up the younger end of our age range) tend to accurately label emotion cues consistently, assigning a single emotion label to a given situation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Widen, S. C. Children’s interpretation of facial expressions: the long path from valence-based to specific discrete categories. Emot. Rev. 5, 72–77 (2013)." href="/articles/s41593-023-01358-9#ref-CR26" id="ref-link-section-d52698776e1536">26</a></sup>. Across late childhood and adolescence, however, children are increasingly likely to assign multiple possible emotion labels to the same set of cues, reflecting a better understanding of individual differences in experience<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Nook, E. C., Sasse, S. F., Lambert, H. K., McLaughlin, K. A. &amp; Somerville, L. H. The nonlinear development of emotion differentiation: granular emotional experience is low in adolescence. Psychol. Sci. 29, 1346–1357 (2018)." href="/articles/s41593-023-01358-9#ref-CR27" id="ref-link-section-d52698776e1540">27</a></sup>. Although work in adolescents could be conducted to tease apart these differences in youth versus adult judgments, it would be difficult to examine age-related differences in emotion cue labeling in young children who may not be as adept at talking about emotions at an abstracted level (what emotion is communicated versus what the child feels, for example). Further work is needed to develop ways to acquire these data in young children to test differences in activation related to adult-defined labels versus individual-defined labels.</p><p>Similarity analysis from the longer video clip, <i>Despicable Me</i>, revealed a striking difference in which scenes elicited greater similarity in default mode activation between the full sample and in the oldest children. Intriguingly, scenes that required negative emotional inference along a longer timescale (that is, remembering previously inferred emotional states) induced more similar activation in older children. Our findings provide a theoretical extension of previous work examining mentalizing network—a subnetwork of the default mode network<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Buckner, R. L. &amp; DiNicola, L. M. The brain’s default network: updated anatomy, physiology and evolving insights. Nat. Rev. Neurosci. 20, 593–608 (2019)." href="/articles/s41593-023-01358-9#ref-CR18" id="ref-link-section-d52698776e1550">18</a></sup>—development in 3–12-year-old children. Specifically, previous work has shown that default mode network activation to movie scenes that require inference about a character’s emotional state increases with age and with mentalizing skills<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Richardson, H., Lisandrelli, G., Riobueno-Naylor, A. &amp; Saxe, R. Development of the social brain from age three to twelve years. Nat. Commun. 9, 1027 (2018)." href="/articles/s41593-023-01358-9#ref-CR62" id="ref-link-section-d52698776e1554">62</a></sup>. Across childhood and adolescence, the default mode network becomes increasingly modular, with increasing intra-network connectivity and decreasing connectivity with other networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Richardson, H., Lisandrelli, G., Riobueno-Naylor, A. &amp; Saxe, R. Development of the social brain from age three to twelve years. Nat. Commun. 9, 1027 (2018)." href="/articles/s41593-023-01358-9#ref-CR62" id="ref-link-section-d52698776e1558">62</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Fan, F. et al. Development of the default-mode network during childhood and adolescence: a longitudinal resting-state fMRI study. Neuroimage 226, 117581 (2021)." href="/articles/s41593-023-01358-9#ref-CR63" id="ref-link-section-d52698776e1561">63</a></sup>. Previous work examining a large sample of 5–12-year-old children found that this change in network connectivity was driven by increasing connectivity between the parietal/temporal network nodes and the prefrontal nodes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Fan, F. et al. Development of the default-mode network during childhood and adolescence: a longitudinal resting-state fMRI study. Neuroimage 226, 117581 (2021)." href="/articles/s41593-023-01358-9#ref-CR63" id="ref-link-section-d52698776e1565">63</a></sup>, major hubs of not only the default mode network but also of the brain more broadly<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Gordon, E. M. et al. Three distinct sets of connector hubs integrate human brain function. Cell Rep. 24, 1687–1695 (2018)." href="/articles/s41593-023-01358-9#ref-CR64" id="ref-link-section-d52698776e1570">64</a></sup>. Within the context of work conducted in adults<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gordon, E. M. et al. Three distinct sets of connector hubs integrate human brain function. Cell Rep. 24, 1687–1695 (2018)." href="#ref-CR64" id="ref-link-section-d52698776e1574">64</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Brandman, T., Malach, R. &amp; Simony, E. The surprising role of the default mode network in naturalistic perception. Commun. Biol. 4, 79 (2021)." href="#ref-CR65" id="ref-link-section-d52698776e1574_1">65</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="da Silva, P. H. R., Rondinoni, C. &amp; Leoni, R. F. Non-classical behavior of the default mode network regions during an information processing task. Brain Struct. Funct. 225, 2553–2562 (2020)." href="#ref-CR66" id="ref-link-section-d52698776e1574_2">66</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Gordon, E. M. et al. Default-mode network streams for coupling to language and control systems. Proc. Natl Acad. Sci. USA 117, 17308–17319 (2020)." href="/articles/s41593-023-01358-9#ref-CR67" id="ref-link-section-d52698776e1577">67</a></sup>, it is likely that this change in default mode network modularity across development is indicative of improved integration of multi-modal information across the brain. Taken together, it is likely that differences across age in activation to contextualized emotional stimuli that we observed in the default mode network reflect a combination of increased efficiency in multi-modal inferences across development and cognitive improvements in emotion inference skills. Considering how commonly default mode network dysfunction is observed across psychiatric disorders<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kaiser, R. H., Andrews-Hanna, J. R., Wager, T. D. &amp; Pizzagalli, D. A. Large-scale network dysfunction in major depressive disorder. JAMA Psychiatry 72, 603 (2015)." href="#ref-CR68" id="ref-link-section-d52698776e1581">68</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Vargas, C., López-Jaramillo, C. &amp; Vieta, E. A systematic literature review of resting state network—functional MRI in bipolar disorder. J. Affect. Disord. 150, 727–735 (2013)." href="#ref-CR69" id="ref-link-section-d52698776e1581_1">69</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Broyd, S. J. et al. Default-mode brain dysfunction in mental disorders: a systematic review. Neurosci. Biobehav. Rev. 33, 279–296 (2009)." href="#ref-CR70" id="ref-link-section-d52698776e1581_2">70</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Williams, L. M. Defining biotypes for depression and anxiety based on large-scale circuit dysfunction: a theoretical review of the evidence and future directions for clinical translation. Depress. Anxiety 34, 9–24 (2017)." href="/articles/s41593-023-01358-9#ref-CR71" id="ref-link-section-d52698776e1584">71</a></sup> and the default mode network’s proposed role in emotion experience<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Satpute, A. B. &amp; Lindquist, K. A. The default mode network’s role in discrete emotion. Trends Cogn. Sci. 23, 851–864 (2019)." href="/articles/s41593-023-01358-9#ref-CR19" id="ref-link-section-d52698776e1588">19</a></sup>, fully mapping how changes in default mode network activation and connectivity shift with emotional development would provide valuable insight into how emotion dysregulation emerges. Although we did not have measures of subjective experience in this study, previous work in adults has found that activation patterns in default mode network encodes subjective ratings of valence and arousal<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Satpute, A. B. &amp; Lindquist, K. A. The default mode network’s role in discrete emotion. Trends Cogn. Sci. 23, 851–864 (2019)." href="/articles/s41593-023-01358-9#ref-CR19" id="ref-link-section-d52698776e1592">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Lettieri, G. et al. Emotionotopy in the human right temporo-parietal cortex. Nat. Commun. 10, 5568 (2019)." href="/articles/s41593-023-01358-9#ref-CR72" id="ref-link-section-d52698776e1595">72</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Lettieri, G. et al. Default and control network connectivity dynamics track the stream of affect at multiple timescales. Soc. Cogn. Affect. Neurosci. 17, 461–469 (2022)." href="/articles/s41593-023-01358-9#ref-CR73" id="ref-link-section-d52698776e1598">73</a></sup>, whereas studies examining discrete emotion states have found brain-wide differences in activation during discrete states<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kragel, P. A. &amp; LaBar, K. S. Multivariate neural biomarkers of emotional states are categorically distinct. Soc. Cogn. Affect. Neurosci. 10, 1437–1448 (2015)." href="#ref-CR74" id="ref-link-section-d52698776e1602">74</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kragel, P. A., Knodt, A. R., Hariri, A. R. &amp; Labar, K. S. Decoding spontaneous emotional states in the human brain. PLoS Biol. 14, e2000106 (2016)." href="#ref-CR75" id="ref-link-section-d52698776e1602_1">75</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Kragel, P. A. &amp; Labar, K. S. Decoding the nature of emotion in the brain. Trends Cogn. Sci. 20, 444–455 (2016)." href="/articles/s41593-023-01358-9#ref-CR76" id="ref-link-section-d52698776e1605">76</a></sup>. This suggests that some of the variance in brain-wide activation that we observed is likely individual differences in how these emotion cues made the children feel. Using facial expressions rather than relying solely on individual ratings, a recent study identified the VMPFC as a region capturing individual experiences during movie watching<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Chang, L. J. et al. Endogenous variation in ventromedial prefrontal cortex state dynamics during naturalistic viewing reflects affective experience. Sci. Adv. 7, eabf7129 (2021)." href="/articles/s41593-023-01358-9#ref-CR77" id="ref-link-section-d52698776e1610">77</a></sup>. Interestingly, this region is also one of the few regions that we observed to become less synchronized in activation in older children compared to younger children. Thus, it is possible that the decrease in synchronization across age reflects a greater influence of individual differences in affective experience during the videos in older children. Further work is needed to tease apart how individual differences in subjective experience during movie watching relate to how emotion cues are encoded in the brain across development.</p><p>This study has several major strengths, including (1) a large sample size, which is critical for assessing general trends across development; (2) use of discovery and replication cohorts in addition to FDR correction to maximize replicability and generalization; (3) use of multiple video stimuli; and (4) careful video analysis and reporting, including of both high-level (emotions, objects, etc.) and low-level (brightness, loudness, etc.) video features. A critical limitation of this study is that it is not longitudinal; thus, these findings must be interpreted as associations with maturity, not development. Although we speculate that associations with maturity are reflecting developmental processes, emotional development is likely highly specific to the individual because our previous experiences shape how we interact with others in the future. Thus, these trends should be confirmed in large, longitudinal and richly characterized samples to better map how humans experience affective neural activation to real-world emotions. Additionally, although cartoons are more naturalistic stimuli than single-valence clips or pictures, they are imperfect proxies for real-life emotion perception. Specifically, cartoon videos often exaggerate emotion states in characters or manipulate lighting and color palettes to evoke a particular emotion, neither of which is consistent with reality. Nonetheless, this work provides a substantial step forward to understanding complex and contextualized emotion perception. Another important note is that, although there were no associations between non-emotional video features and emotional content, we did not systematically vary audio-visual features and emotional content. We, therefore, cannot completely disentangle non-emotional sensory processing from emotion perception. Finally, we do not have measures of the children’s subjective experiences or behaviors during the videos. Future work should seek to include measures of subjective experience to better understand how each child’s experiences color their perception of external emotion cues.</p><p>We present empirical evidence that specific contextualized emotion cues are encoded throughout the cortex, cerebellum and caudate. We showed that activation to contextualized emotion stimuli was fairly stable across development, with modest changes suggesting improved integration of multi-modal information as children refine their shared conceptual understanding of emotion cues. This work has important implications for models of affective neurodevelopment. Further work is needed to longitudinally confirm these trends and to map how they may go awry in individuals with socio-emotional dysregulation.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Methods</h2><div class="c-article-section__content" id="Sec9-content"><p>Study procedures were approved by the Chesapeake Institutional Review Board. For participants under the age of 18 years, written assent was obtained from the participant, and written consent was obtained from the participant’s parent or legal guardian. Monetary compensation was provided for time and expenses incurred as a result of participating (for example, travel).</p><h3 class="c-article__sub-heading" id="Sec10">Study description</h3><p>Analyses included data from 823 participants from the first nine releases of the Healthy Brain Network (HBN). The HBN study is a large, multi-site study of children and young adults ages 5–21 years all collected in the New York area. Recruitment, consent and study procedures are described in detail at https://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/index.html and in the data publication<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 78" title="Alexander, L. M. et al. An open resource for transdiagnostic research in pediatric mental health and learning disorders. Sci. Data 4, 170181 (2017)." href="/articles/s41593-023-01358-9#ref-CR78" id="ref-link-section-d52698776e1635">78</a></sup>. Of the available datasets, 88% were from two of the four sites; thus, the data from the other two sites were excluded. Additionally, because 92% of the data were from children under the age of 16 years, and because previous work suggests that rates of change of emotion inference and reasoning skills sharply decrease at around age 15–18 years<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nook, E. C., Sasse, S. F., Lambert, H. K., McLaughlin, K. A. &amp; Somerville, L. H. The nonlinear development of emotion differentiation: granular emotional experience is low in adolescence. Psychol. Sci. 29, 1346–1357 (2018)." href="#ref-CR79" id="ref-link-section-d52698776e1639">79</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nook, E. C. et al. Charting the development of emotion comprehension and abstraction from childhood to adulthood using observer-rated and linguistic measures. Emotion 20, 773–792 (2020)." href="#ref-CR80" id="ref-link-section-d52698776e1639_1">80</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Motta-Mena, N. V. &amp; Scherf, K. S. Pubertal development shapes perception of complex facial expressions. Dev. Sci. 20, e12451 (2017)." href="#ref-CR81" id="ref-link-section-d52698776e1639_2">81</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Thomas, L. A., De Bellis, M. D., Graham, R. &amp; LaBar, K. S. Development of emotional facial recognition in late childhood and adolescence. Dev. Sci. 10, 547–558 (2007)." href="#ref-CR82" id="ref-link-section-d52698776e1639_3">82</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Durand, K., Gallay, M., Seigneuric, A., Robichon, F. &amp; Baudouin, J.-Y. The development of facial emotion recognition: the role of configural information. J. Exp. Child Psychol. 97, 14–27 (2007)." href="#ref-CR83" id="ref-link-section-d52698776e1639_4">83</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Wu, M. et al. Age-related changes in amygdala-frontal connectivity during emotional face processing from childhood into young adulthood. Hum. Brain Mapp. 37, 1684–1695 (2016)." href="/articles/s41593-023-01358-9#ref-CR84" id="ref-link-section-d52698776e1642">84</a></sup>, we limited our analyses to the 5–15 year olds. Thus, datasets from 2,053 participants were considered for analysis. Of those data, two were removed for intracranial anomalies; 522 were removed for incomplete data; and 679 were removed for high motion based on strict data processing standards (see the ‘MRI data processing’ subsection). Final sample characteristics are included in Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM1">SC1</a>, and the distribution plots for age and puberty scores are in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM1">SC2</a>. No power analysis was conducted to determine our final sample size, but our sample exceeds those used in previous similar analyses.</p><h3 class="c-article__sub-heading" id="Sec11">Magnetic resonance imaging data (MRI)</h3><p>MRI data from two sites, the CitiGroup Cornell Brain Imaging Center (CBIC) and the Rutgers University Brain Imaging Center (RUBIC), were considered for analysis. CBIC data were collected on a Siemens 3T Prisma scanner, and RUBIC data were collected on a Siemens 3T Trio scanner. Both sites used a 64-channel head coil. The MRI data included in this analysis were T1-weighted and T2-weighted anatomy scans as well as BOLD functional MRI (fMRI). fMRI data were simultaneous multi-band echo planar imaging sequences with an 800-ms repetition time collected while children watched two video clips during fMRI scanning, a 10-min clip from the movie <i>Despicable Me</i> and a 3-min, 20-s short called <i>The Present</i>. For datasets with multiple T1-weighted or T2-weighted images, the image with the least artifact/motion was used for processing. Sequence parameters are listed in detail on the HBN project website.</p><h3 class="c-article__sub-heading" id="Sec12">MRI data processing</h3><p>Data were pre-processed using the Human Connectome Project minimal processing pipeline<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Glasser, M. F. et al. The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage 80, 105–124 (2013)." href="/articles/s41593-023-01358-9#ref-CR47" id="ref-link-section-d52698776e1674">47</a></sup>. Additional processing steps were carried out using custom-written Python (version 3.8) scripts using the numpy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 85" title="Harris, C. R. et al. Array programming with NumPy. Nature 585, 357–362 (2020)." href="/articles/s41593-023-01358-9#ref-CR85" id="ref-link-section-d52698776e1678">85</a></sup> version 1.21.6, scipy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="Virtanen, P. et al. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nat. Methods 17, 261–272 (2020)." href="/articles/s41593-023-01358-9#ref-CR86" id="ref-link-section-d52698776e1682">86</a></sup> version 1.7.3, nibabel version 3.2.1 and pandas version 1.1.2 libraries. These scripts are publicly available at <a href="https://github.com/catcamacho/hbn_analysis">https://github.com/catcamacho/hbn_analysis</a>.</p><p>In brief, structural T1-weighted and T2-weighted data were aligned and corrected for both gradient and bias field<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Andersson, J. L. R. &amp; Sotiropoulos, S. N. An integrated approach to correction for off-resonance effects and subject movement in diffusion MR imaging. Neuroimage 125, 1063–1078 (2016)." href="/articles/s41593-023-01358-9#ref-CR87" id="ref-link-section-d52698776e1696">87</a></sup> distortions before processing through FreeSurfer’s recon-all pipeline<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 88" title="Fischl, B. FreeSurfer. Neuroimage 62, 774–781 (2012)." href="/articles/s41593-023-01358-9#ref-CR88" id="ref-link-section-d52698776e1700">88</a></sup> version 7. FreeSurfer-generated surfaces were visually inspected for accuracy, and datasets with surface errors were removed from analysis (<i>n</i> = 336). Most of the rejected surfaces had motion in either the T1 or T2 image and would require substantive manual editing to correct, potentially introducing barriers to reproducibility. We, therefore, opted to remove these data rather than invest time and resources in manual editing. Surfaces were then aligned to template space using multi-modal surface matching<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 89" title="Robinson, E. C. et al. Multimodal surface matching with higher-order smoothness constraints. Neuroimage 167, 453–465 (2018)." href="/articles/s41593-023-01358-9#ref-CR89" id="ref-link-section-d52698776e1707">89</a></sup> and resampled to 32,000 vertices.</p><p>Functional data were corrected for both gradient and bias field<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Andersson, J. L. R. &amp; Sotiropoulos, S. N. An integrated approach to correction for off-resonance effects and subject movement in diffusion MR imaging. Neuroimage 125, 1063–1078 (2016)." href="/articles/s41593-023-01358-9#ref-CR87" id="ref-link-section-d52698776e1714">87</a></sup> distortions, slice time corrected, rigidly realigned, normalized to a global mean and masked in volume space. Volumes were next converted into surface space by aligning the cortical ribbon produced from the anatomical data processing. In surface space, functional data were rescaled to standard units before applying nuisance regression and bandpass filtering (0.008–0.1 Hz). Nuisance regressors included global signal, six motion parameters (translation and rotation in each <i>x</i>, <i>y</i> and <i>z</i> directions), framewise displacement and temporal censoring of volumes with a framewise displacement of 0.9 mm or more, a motion cutoff that is found to be optimal for task-based analyses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 90" title="Siegel, J. S. et al. Statistical improvements in functional magnetic resonance imaging analyses produced by censoring high-motion data points. Hum. Brain Mapp. 35, 1981–1996 (2014)." href="/articles/s41593-023-01358-9#ref-CR90" id="ref-link-section-d52698776e1727">90</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Siegel, J. S. et al. Data quality influences observed links between functional connectivity and behavior. Cereb. Cortex 27, 4492–4502 (2017)." href="/articles/s41593-023-01358-9#ref-CR91" id="ref-link-section-d52698776e1730">91</a></sup>. Although movie stimuli have been shown to enhance task compliance in children<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Greene, D. J. et al. Behavioral interventions for reducing head motion during MRI scans in children. Neuroimage 171, 234–245 (2018)." href="/articles/s41593-023-01358-9#ref-CR92" id="ref-link-section-d52698776e1735">92</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 93" title="Vanderwal, T., Kelly, C., Eilbott, J., Mayes, L. C. &amp; Castellanos, F. X. Inscapes: a movie paradigm to improve compliance in functional magnetic resonance imaging. Neuroimage 122, 222–232 (2015)." href="/articles/s41593-023-01358-9#ref-CR93" id="ref-link-section-d52698776e1738">93</a></sup>, we employed these strict motion correction procedures to avoid motion contamination of inter-subject synchrony signals. For each functional run, data were considered usable if 80% of the sequence had a framewise displacement of less than 0.9 mm. Finally, the Gordon cortical<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 94" title="Gordon, E. M. et al. Generation and evaluation of a cortical area parcellation from resting-state correlations. Cereb. Cortex 26, 288–303 (2016)." href="/articles/s41593-023-01358-9#ref-CR94" id="ref-link-section-d52698776e1742">94</a></sup> and Seitzman subcortical and cerebellum<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 95" title="Seitzman, B. A. et al. A set of functionally-defined brain regions with improved representation of the subcortex and cerebellum. Neuroimage 206, 116290 (2020)." href="/articles/s41593-023-01358-9#ref-CR95" id="ref-link-section-d52698776e1746">95</a></sup> atlases were applied to the residuals, resulting in a denoised timeseries for 333 cortical regions representing 12 cognitive networks, 34 subcortical regions and 27 cerebellar regions, for a total of 394 parcels for analysis. Data were split by site into Discovery/Training (RUBIC, <i>n</i> = 424) and Replication/Testing (CBIC, <i>n</i> = 389) samples for analysis.</p><h3 class="c-article__sub-heading" id="Sec13">Video stimuli analysis</h3><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec14">Emotional content characterization</h4><p>Videos were coded and processed using the EmoCodes system version 1.0 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 96" title="Camacho, M. C. et al. EmoCodes: a standardized coding system for socio-emotional content in complex video stimuli. Affect. Sci. 3, 168–181 (2022)." href="/articles/s41593-023-01358-9#ref-CR96" id="ref-link-section-d52698776e1769">96</a></sup>). The EmoCodes system is a tool for characterizing low-level and affective content in movies that is both reproducible and externally validated. Two independent raters coded each video for faces, body parts and written words as well as expressions of negative and positive emotionality of each character (Sørensen–Dice similarity = 0.65–1.00), which were averaged across raters before applying to further analysis. We refer to the ‘negative’ and ‘positive’ emotion codes together as ‘global’ or ‘general’ emotions rather than valence because valence typically implies that a given cue is either positive or negative, whereas our coding allows a given cue to be both. For most of the video frames, however, the term valence could still be used accurately to describe the coding because it was rare for a given cue to be both negative and positive. These global negative and positive codes for each character were then combined with character affective intensity codes to create a summary measure of overall, frame-by-frame negative and positive emotional intensity per externally validated procedures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Sadaghiani, S. &amp; D’Esposito, M. Functional characterization of the cingulo-opercular network in the maintenance of tonic alertness. Cereb. Cortex 25, 2763–2773 (2015)." href="/articles/s41593-023-01358-9#ref-CR16" id="ref-link-section-d52698776e1773">16</a></sup>. Specific emotions that are cognitively basic and learned across childhood were also coded for each character: anger, fear, excitement, sadness and happiness. Specifically, for each frame and each character the facial, physical and verbal expression of each of these emotions were coded as present or not (1 or 0, respectively) and then summed for each frame. Just as with general emotions, this sum was then multiplied by the character’s intensity rating and summed across characters to create a frame-by-frame global measure of the intensity of each specific emotion across each video. These ratings are shown for each video in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM1">SA1</a>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec15">Low-level video feature analysis</h4><p>Low-level video features were extracted using pliers library version 0.4.1 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 97" title="McNamara, Q., De La Vega, A. &amp; Yarkoni, T. Developing a comprehensive framework for multimodal feature extraction. In Proc. 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Matwin, S. et al.) 1567–1574 (Association for Computing Machinery, 2017)." href="/articles/s41593-023-01358-9#ref-CR97" id="ref-link-section-d52698776e1788">97</a></sup>) via EmoCodes Python library version 1.0.10 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 96" title="Camacho, M. C. et al. EmoCodes: a standardized coding system for socio-emotional content in complex video stimuli. Affect. Sci. 3, 168–181 (2022)." href="/articles/s41593-023-01358-9#ref-CR96" id="ref-link-section-d52698776e1792">96</a></sup>) and included framewise brightness, visual sharpness, optical flow, fraction of frame containing highly salient content, visual vibrance, loudness and a rolling measure of tempo. We tested if the coded emotion content timeseries were collinear with each of these features using the SummarizeFeatures class in the EmoCodes library. Key figures from this report are included in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM1">SA2</a>. Except for brightness in <i>The Present</i> (positive <i>r</i> = 0.39 and negative <i>r</i> = −0.66), none of these features was notably correlated with emotion metrics (<i>rs</i> &lt; |0.30|). Notably, none of the variance inflation factors exceeded 3 across both videos, suggesting that, even with the higher correlation between brightness and positive/negative content, the variables are not so correlated to destabilize a multiple regression model. Thus, activation models included the affective measure (positive and negative content), brightness and loudness (included for thoroughness).</p><p>A detailed description of each feature coded in the videos and their covariance is included in Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM1">SA1</a>. An analysis of video features in relation to motion metrics is included in Appendix <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM1">A2</a> and Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM1">SA3</a>.</p><h3 class="c-article__sub-heading" id="Sec16">Estimating activation to emotional content</h3><p>The positive, negative, brightness and loudness signals were next convolved with a double gamma hemodynamic response function (5-s peak and 12-s undershoot) for within-subject activation analysis. The transformed regressors were next entered into a GLM, predicting the BOLD timeseries for each voxel. Parameter estimate maps from this analysis were used in group-level analysis. Sample average activation maps are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig1">1</a>, and difference maps are shown in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig7">1</a>. General and specific activated maps were extracted separately: general emotion regressors included negative, positive, brightness, loudness, written words and spoken words, whereas specific emotion regressors included anger, happiness, fear, excitement, sadness, brightness, loudness, written words and spoken words. As expected given the lack of collinearity with the emotion regressors, including the ‘words’ and ‘speaking’ regressors in the activation models did not change our main emotion analysis findings. The statsmodels version 0.13.2, scipy version 1.7.3, numpy version 1.21.6, nibabel version 3.2.1 and pandas version 1.1.2 libraries were used for these analyses.</p><h3 class="c-article__sub-heading" id="Sec17">Emotion activation classification</h3><p>Support vector machine classification on the activation (beta) maps was used to characterize the regions for which activation differentiates emotions across the sample. For each analysis, data were split into a training and testing dataset by collection site. Support vector models were fitted to the training data using 10-fold cross-validation. Specifically, the training data were split into 10 partitions, and training was conducted on nine partitions before being tested on the tenth. This process resulted in 10 models, which were then combined for final performance assessment. Final performance was determined using the left-out, unseen testing data. Accuracy point estimates were obtained based on the correct number of label assignments. A pseudo-bootstrapping procedure was used to compute 95% CIs for each the training cross-validated accuracy and the final test accuracy to provide statistical confidence of these point estimates. This procedure involved taking random subsamples from the dataset (of random size <i>n</i>, which is between 50% and 75% of the full dataset size) and either training or testing the models as appropriate. CIs were estimated from the resulting distribution of scores. Finally, a <i>P</i> value was assigned to the full model performance by permuting the feature set 1,000 times to build a null distribution to compare against the accuracy point estimates. This procedure was done separately for the general emotion labels (positive and negative) and the specific emotion labels (angry, happy, sad, fearful and excited). Results were practically identical regardless of which site was used for training or testing. The scikit-learn version 0.24.2, numpy version 1.21.6, nibabel version 3.2.1, pandas version 1.1.2 and scipy version 1.7.3 libraries were used for these analyses, and seaborn version 0.11.1 and matplotlib version 3.4.2 were used for visualization.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec18">Characterize regions that are critical for differentiating affective content</h4><p>Specific parcels, regions or networks that are critical for the models to identify emotion labels were systematically permuted, and the change in accuracy (original minus new) was recorded. Specifically, activation for either each parcel (for example, L_visual_1 and L_putamen) or each network or subcortical region label (for example, visual and putamen) was permuted by shuffling the values associated with that label or labels among datasets, and the model was retrained 1,000 times to create a distribution of accuracy change scores, with positive numbers indicating that the permuted model did a poorer job of identifying the emotion labels. As a control for network/region-level approach, we also repeated this procedure permuting random labels of the same number of parcels as were included in the previously described analysis, because each cognitive network and subcortical region set are not of equal sizes (for example, visual network is composed of 39 parcels, whereas the pallidum has only two parcels, right and left). In other words, we sought to differentiate changes in accuracy due to missing specific information (the network or region) versus missing that amount of information (the <i>n</i> parcels permuted). Both distributions were plotted, and regions/networks with a mean accuracy change score that did not overlap with the null distribution was considered ‘important’ for model accuracy.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec19">Characterize which networks are sufficient for differentiating affective content</h4><p>Another important question is whether a specific network or region is sufficient for predicting emotion labels or if the multivariate information is critical for classification. To test this, we also completed the model fitting procedures described earlier on datasets limited to each network or region.</p><h3 class="c-article__sub-heading" id="Sec20">Identify inter-subject similarities in activation to the videos</h3><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec21">ISC analysis</h4><p>We computed inter-subject activation similarity across each video by computing the ISC following standard procedures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 98" title="Nastase, S. A., Gazzola, V., Hasson, U. &amp; Keysers, C. Measuring shared responses across subjects using intersubject correlation. Soc. Cogn. Affect. Neurosci. 14, 669–687 (2019)." href="/articles/s41593-023-01358-9#ref-CR98" id="ref-link-section-d52698776e1884">98</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 99" title="Hasson, U., Nir, Y., Levy, I., Fuhrmann, G. &amp; Malach, R. Intersubject synchronization of cortical activity during natural vision. Science 303, 1634–1640 (2004)." href="/articles/s41593-023-01358-9#ref-CR99" id="ref-link-section-d52698776e1887">99</a></sup>. Raw <i>P</i> values were assigned from a null distribution of 10,000 ISC values derived from shuffled raw data. Bonferroni FDR correction was applied, and parcels significant across both the Discovery and Replication samples were considered regions of similarity in activation across the sample. Significant parcels from this analysis are reported in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-023-01358-9#Fig9">3</a>. This analysis was performed to compare against the maturity inter-subject representational analysis results.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec22">Dynamic similarity analysis</h4><p>Dynamic inter-subject similarity was computed using ISPS analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 100" title="Aviyente, S., Bernat, E. M., Evans, W. S. &amp; Sponheim, S. R. A phase synchrony measure for quantifying dynamic functional integration in the brain. Hum. Brain Mapp. 32, 80–93 (2011)." href="/articles/s41593-023-01358-9#ref-CR100" id="ref-link-section-d52698776e1905">100</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 101" title="Glerean, E., Salmi, J., Lahnakoski, J. M., Jääskeläinen, I. P. &amp; Sams, M. Functional magnetic resonance imaging phase synchronization as a measure of dynamic functional connectivity. Brain Connect. 2, 91–101 (2012)." href="/articles/s41593-023-01358-9#ref-CR101" id="ref-link-section-d52698776e1908">101</a></sup> to identify which scenes induced similar changes in activation across the sample (that is, increased inter-subject synchrony). ISPS was computed using previously established procedures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 100" title="Aviyente, S., Bernat, E. M., Evans, W. S. &amp; Sponheim, S. R. A phase synchrony measure for quantifying dynamic functional integration in the brain. Hum. Brain Mapp. 32, 80–93 (2011)." href="/articles/s41593-023-01358-9#ref-CR100" id="ref-link-section-d52698776e1912">100</a></sup>. For each parcel and for each participant, activation timeseries were <i>z</i>-scored, and phase angle was computed using a Hilbert transformation. This approach emphasizes the change in signal over the absolute magnitude of signal at each timepoint, allowing for comparisons in dynamic processing without the confound of differences in arbitrary signal units. Next, pairwise phase synchrony was computed as 1 minus the sine of the difference in phase angle between the two participants, resulting in a measure between 0 and 1 for each pair at each timepoint, with 1 indicating perfect synchrony. To identify specific timepoints when children were more in sync in activation, pairwise ISPS was averaged across networks and then across participants. Raw <i>P</i> values were assigned to each synchrony value by comparing the value to a distribution of null synchrony values created by permuting the original pre-processed data. To be considered a significant peak, the peak had to be at least 5 s wide with a prominence of Bonferroni FDR-corrected <i>P</i> &lt; 0.05 detected using the scipy find_peaks function and had to appear in both the Discovery and Replication samples. To ensure that the peaks being detected were indeed the highest synchrony, if the synchrony threshold at FDR-corrected <i>P</i> &lt; 0.05 was less than the value at the 95th percentile, the latter was used to define the peak prominence threshold instead.</p><h3 class="c-article__sub-heading" id="Sec23">Characterize linear and curvilinear activation differences across development</h3><p>Support vector regression analysis was used to test if activation to specific emotion categories differs across development (either chronological age or pubertal stage). Pubertal stage scores were derived from the Peterson Puberty Scale<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 102" title="Petersen, A. C., Crockett, L., Richards, M. &amp; Boxer, A. A self-report measure of pubertal status: reliability, validity, and initial norms. J. Youth Adolesc. 17, 117–133 (1988)." href="/articles/s41593-023-01358-9#ref-CR102" id="ref-link-section-d52698776e1938">102</a></sup> using standard scoring procedures, resulting in scores ranging from 5 to 20. As expected, age and puberty scores were highly correlated (<i>r</i> = 0.76, <i>P</i> &lt; 0.001). Just as with the support vector classification analyses, data were split into a training and a testing dataset by collection site; models were fitted to the training data using 10-fold cross-validation; and final performance was determined using the left-out, unseen testing data. We computed parametric and non-parametric correlations as well as the MSE between actual and predicted data labels as model performance metrics. CIs were assigned using the same procedures as used in the classification models. MSE <i>P</i> values were assigned using the permutation-based approach described in the previous section. <i>P</i> values for the correlation statistics were computed per standard procedures. Analyses were completed using a linear support vector kernel and a curvilinear kernel (radial basis function kernel) to test for each association between activation and maturity.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec24">Test if maturity can be predicted from activation</h4><p>Activation maps from each emotion (positive, negative, angry, happy, sad, fearful and excited) were used as separate feature sets to predict each maturity measure (age and puberty) for a total of 14 emotion models. Like the classification analyses, first whole-brain activation was used to predict each chronological age and puberty scores, followed by post hoc permutation-based importance testing. Model-fitting procedures were then repeated, limiting the dataset to a specific region/network to test if information from each is sufficient for predicting maturity.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec25">Specificity analyses and control analyses</h4><p>Activation maps from each low-level (brightness and loudness) and non-emotion higher-level (spoken words and written words) feature included in the first-level activation models were used as separate features sets to predict each maturity metric using the same procedures as in the emotion analyses. Additionally, we tested if we could predict motion (mean framewise displacement) from each emotion (negative, positive, angry, fearful, excited, sad and happy) activation map.</p><h3 class="c-article__sub-heading" id="Sec26">Inter-subject similarity across development</h3><p>We also tested if maturity was best captured using a nonlinear, model-free approach through similarity analysis. This approach complements the activation classification analysis; in the classification analysis, we used video features to extract activation, whereas, in this similarity analysis, we used activation patterns to identify what video content induces maturity-related differences in activation. To do this, we examined the association between the inter-subject similarity in activation across each movie (ISC) and the below maturity models using IS-RSA<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 103" title="Finn, E. S. et al. Idiosynchrony: from shared responses to individual differences during naturalistic neuroimaging. Neuroimage 215, 116828 (2020)." href="/articles/s41593-023-01358-9#ref-CR103" id="ref-link-section-d52698776e1978">103</a></sup>. Maturational similarity was computed in three ways, each testing a different hypothesis of how cognitive affective processing changes with age or puberty:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">(1)</span>
                    
                      <p>Nearest neighbor: children of similar maturity will process complex socio-emotional stimuli similarly (metric: sample maximum minus pair absolute difference)</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">(2)</span>
                    
                      <p>Divergence: younger children process complex socio-emotional stimuli more similarly, with divergence across development representing effects of individual differences in experience (metric: sample maximum minus pair average)</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">(3)</span>
                    
                      <p>Convergence: older children process complex socio-emotional stimuli more similarly, representing convergence on a shared social concept (metric: pair minimum)</p>
                    
                  </li>
                </ol><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec27">IS-RSA of ISC data</h4><p>We computed ISC across each video following standard procedures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 98" title="Nastase, S. A., Gazzola, V., Hasson, U. &amp; Keysers, C. Measuring shared responses across subjects using intersubject correlation. Soc. Cogn. Affect. Neurosci. 14, 669–687 (2019)." href="/articles/s41593-023-01358-9#ref-CR98" id="ref-link-section-d52698776e2024">98</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 99" title="Hasson, U., Nir, Y., Levy, I., Fuhrmann, G. &amp; Malach, R. Intersubject synchronization of cortical activity during natural vision. Science 303, 1634–1640 (2004)." href="/articles/s41593-023-01358-9#ref-CR99" id="ref-link-section-d52698776e2027">99</a></sup>. First, we computed the pairwise ISC in activation timeseries on a parcel-wise basis. Next, the maturity similarity ratings were each correlated with the parcel-wise ISCs. <i>P</i> values were assigned using a Mantel test<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 104" title="Mantel, N. The detection of disease clustering and a generalized regression approach. Cancer Res. 27, 209–220 (1967)." href="/articles/s41593-023-01358-9#ref-CR104" id="ref-link-section-d52698776e2034">104</a></sup> with FDR correction. Specifically, the maturity ratings were systematically shuffled before analysis 10,000 to create a null distribution. <i>P</i> values were estimated from this permuted distribution, and the significance threshold was determined using a Bonferroni FDR for two independent samples. The exact same procedures were conducted for each video, with only parcels that were significant for both the Discovery and Replication samples reported in the results.</p><p>We then compared model fits for each parcel using a bootstrapping approach. Specifically, we ranked the model point estimates for each parcel. If the parcel was significant for more than one model, we used a bootstrapping method to generate a distribution of similarity coefficients for each of the two models and a <i>t</i>-test to compare estimates. If one model’s point estimates were significantly higher than the other, that model was assigned for that parcel. We also compared the winning models for each parcel between videos and additionally report those that were consistent.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec28">IS-RSA of ISPS data</h4><p>ISPS was computed using previously established procedures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 100" title="Aviyente, S., Bernat, E. M., Evans, W. S. &amp; Sponheim, S. R. A phase synchrony measure for quantifying dynamic functional integration in the brain. Hum. Brain Mapp. 32, 80–93 (2011)." href="/articles/s41593-023-01358-9#ref-CR100" id="ref-link-section-d52698776e2055">100</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 101" title="Glerean, E., Salmi, J., Lahnakoski, J. M., Jääskeläinen, I. P. &amp; Sams, M. Functional magnetic resonance imaging phase synchronization as a measure of dynamic functional connectivity. Brain Connect. 2, 91–101 (2012)." href="/articles/s41593-023-01358-9#ref-CR101" id="ref-link-section-d52698776e2058">101</a></sup>. In brief, activation timeseries were <i>z</i>-scored, and phase angle was computed using a Hilbert transformation for each parcel and each participant. This approach emphasizes the change in signal over the absolute magnitude of signal at each timepoint, allowing for comparisons in dynamic processing without the confound of differences in arbitrary signal units. Next, pairwise phase synchrony was computed as 1 minus the sine of the difference in phase angle between the two participants, resulting in a measure between 0 and 1 for each pair at each timepoint, with 1 indicating perfect synchrony.</p><p>Dynamic IS-RSA was guided by the ISC IS-RSA results. We limited the parcels to those that were significant for the prevailing developmental model. Because there were numerous cortical parcels spanning cognitive networks and no significant subcortical or cerebellar regions, we then grouped the significant parcels based on cognitive network and averaged for further analysis. Furthermore, because age captured variance in the data better than puberty based on the total number of parcels and magnitude of IS-RSA similarity, we did not examine puberty further. Next, we grouped the samples based on the prevailing developmental model to capture developmental differences in activation similarity across the sample. Because the Convergence model was the best fit across the majority of significant parcels, we, therefore, examined dynamic changes in cognitive network synchrony in the oldest children (top 20% by age) to compare to the full sample results for each sample and each video (<i>Despicable Me</i> Discovery = 12.63–15.90 years; <i>Despicable Me</i> Replication = 13.85–15.97 years; <i>The Present</i> Discovery = 12.95–15.99 years; <i>The Present</i> Replication = 13.46–15.97 years). ISPS data were averaged across the oldest children, and the same peak detection procedures used in the full sample were applied (minimum peak width of 5 s, prominence of FDR-corrected <i>P</i> &lt; 0.05). Nearly the full timeseries was above the peak threshold in the oldest children, however, so we adjusted the minimum to 1.5 s.d. above the mean for each network to identify scenes of relatively high synchrony. As with the full sample, peaks must be present in both the Discovery and Replication samples to be considered significant.</p><h3 class="c-article__sub-heading" id="Sec29">Scene analysis</h3><p>To determine if specific video features elicited greater similarity in children of similar ages, we conducted <i>t</i>-tests comparing coded video features from video segments identified in the previous section compared to other sections. Features were time-shifted six repetition times (4.8 s) to account for the delay in the hemodynamic response before conducting <i>t</i>-tests comparing the ratings within the identified peaks to ratings outside of the peaks. Only features that were significantly different (FDR-corrected<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Benjamini, Y. &amp; Hochberg, Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing. J. R. Stat. Soc. B 57, 289–300 (1995)." href="/articles/s41593-023-01358-9#ref-CR105" id="ref-link-section-d52698776e2099">105</a></sup> <i>P</i> &lt; 0.05) are reported and plotted.</p><h3 class="c-article__sub-heading" id="Sec30">Reporting summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-023-01358-9#MOESM2">Nature Portfolio Reporting Summary</a> linked to this article.</p></div></div></section>
                </div>
            

            <div>
                <section data-title="Data availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data availability</h2><div class="c-article-section__content" id="data-availability-content">
              
              <p>MRI data: The first nine releases of the Healthy Brain Network Biobank, an open dataset, were used in these analyses. These data are available at http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/sharing_neuro.html.</p>
              <p>Video codes: The codes for the videos obtained using the EmoCodes system are available at <a href="https://emocodes.org/datasets/">https://emocodes.org/datasets/</a>.</p>
            </div></div></section><section data-title="Code availability"><div class="c-article-section" id="code-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="code-availability">Code availability</h2><div class="c-article-section__content" id="code-availability-content">
              
              <p>Pre-processing was carried out using the Human Connectome Project minimal processing pipeline (available at <a href="https://github.com/Washington-University/HCPpipelines">https://github.com/Washington-University/HCPpipelines</a>). Additional processing, analyses and plotting were carried out using custom scripts written in Python 3.7.2 (available at <a href="https://github.com/catcamacho/hbn_analysis">https://github.com/catcamacho/hbn_analysis</a>) using the numpy version 1.21.6, scipy version 1.7.3, nibabel version 3.2.1 and pandas version 1.1.2 libraries. Analyses were carried out using the pliers version 0.4.1, statsmodels version 0.13.2 and scikit-learn version 0.24.2 libraries. Plotting was carried out using the seaborn version 0.11.1 and matplotlib version 3.4.2 libraries.</p>
            </div></div></section><section data-title="Change history"><div class="c-article-section" id="change-history-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="change-history">Change history</h2><div class="c-article-section__content" id="change-history-content"><ul class="c-article-change-list"><li class="c-article-change-list__item u-mb-24" id="chg1"><ins datetime="2023-07-27"><h3 class="c-article-change-list__heading u-h3 u-pr-8 u-display-inline">27 July 2023</h3><div class="c-article-change-list__details"><p>A Correction to this paper has been published: <a href="https://doi.org/10.1038/s41593-023-01420-6">https://doi.org/10.1038/s41593-023-01420-6</a></p></div></ins></li></ul></div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Alba, J. W. &amp; Hasher, L. Is memory schematic? <i>Psychol. Bull.</i> <b>93</b>, 203–231 (1983).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Is%20memory%20schematic%3F&amp;journal=Psychol.%20Bull.&amp;volume=93&amp;pages=203-231&amp;publication_year=1983&amp;author=Alba%2CJW&amp;author=Hasher%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Masís-Obando, R., Norman, K. A. &amp; Baldassano, C. Schema representations in distinct brain networks support narrative memory during encoding and retrieval. <i>eLife</i> <b>11</b>, e70445 (2022).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=35393941" aria-label="PubMed reference 2">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8993217" aria-label="PubMed Central reference 2">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Schema%20representations%20in%20distinct%20brain%20networks%20support%20narrative%20memory%20during%20encoding%20and%20retrieval&amp;journal=eLife&amp;volume=11&amp;publication_year=2022&amp;author=Mas%C3%ADs-Obando%2CR&amp;author=Norman%2CKA&amp;author=Baldassano%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">van Kesteren, M. T. R., Ruiter, D. J., Fernández, G. &amp; Henson, R. N. How schema and novelty augment memory formation. <i>Trends Neurosci.</i> <b>35</b>, 211–219 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22398180" aria-label="PubMed reference 3">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20schema%20and%20novelty%20augment%20memory%20formation&amp;journal=Trends%20Neurosci.&amp;volume=35&amp;pages=211-219&amp;publication_year=2012&amp;author=Kesteren%2CMTR&amp;author=Ruiter%2CDJ&amp;author=Fern%C3%A1ndez%2CG&amp;author=Henson%2CRN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Darley, J. M. &amp; Fazio, R. H. Expectancy confirmation processes arising in the social interaction sequence. <i>Am. Psychol.</i> <b>35</b>, 867–881 (1980).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Expectancy%20confirmation%20processes%20arising%20in%20the%20social%20interaction%20sequence&amp;journal=Am.%20Psychol.&amp;volume=35&amp;pages=867-881&amp;publication_year=1980&amp;author=Darley%2CJM&amp;author=Fazio%2CRH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Ruba, A. L. &amp; Pollak, S. D. The development of emotion reasoning in infancy and early childhood. <i>Annu. Rev. Dev. Psychol.</i> <b>2</b>, 503–531 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20development%20of%20emotion%20reasoning%20in%20infancy%20and%20early%20childhood&amp;journal=Annu.%20Rev.%20Dev.%20Psychol.&amp;volume=2&amp;pages=503-531&amp;publication_year=2020&amp;author=Ruba%2CAL&amp;author=Pollak%2CSD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Mills, K. L. et al. Structural brain development between childhood and adulthood: convergence across four longitudinal samples. <i>Neuroimage</i> <b>141</b>, 273–281 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27453157" aria-label="PubMed reference 6">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Structural%20brain%20development%20between%20childhood%20and%20adulthood%3A%20convergence%20across%20four%20longitudinal%20samples&amp;journal=Neuroimage&amp;volume=141&amp;pages=273-281&amp;publication_year=2016&amp;author=Mills%2CKL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Giedd, J. N. &amp; Rapoport, J. L. Structural MRI of pediatric brain development: what have we learned and where are we going? <i>Neuron</i> <b>67</b>, 728–734 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3cXhtFamtb%2FM" aria-label="CAS reference 7">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20826305" aria-label="PubMed reference 7">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3285464" aria-label="PubMed Central reference 7">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Structural%20MRI%20of%20pediatric%20brain%20development%3A%20what%20have%20we%20learned%20and%20where%20are%20we%20going%3F&amp;journal=Neuron&amp;volume=67&amp;pages=728-734&amp;publication_year=2010&amp;author=Giedd%2CJN&amp;author=Rapoport%2CJL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Grayson, D. S. &amp; Fair, D. A. Development of large-scale functional networks from birth to adulthood: a guide to the neuroimaging literature. <i>Neuroimage</i> <b>160</b>, 15–31 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28161313" aria-label="PubMed reference 8">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20large-scale%20functional%20networks%20from%20birth%20to%20adulthood%3A%20a%20guide%20to%20the%20neuroimaging%20literature&amp;journal=Neuroimage&amp;volume=160&amp;pages=15-31&amp;publication_year=2017&amp;author=Grayson%2CDS&amp;author=Fair%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Malsert, J., Palama, A. &amp; Gentaz, E. Emotional facial perception development in 7, 9 and 11 year-old children: the emergence of a silent eye-tracked emotional other-race effect. <i>PLoS ONE</i> <b>15</b>, e0233008 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXhtVGjurfE" aria-label="CAS reference 9">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32392271" aria-label="PubMed reference 9">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7213684" aria-label="PubMed Central reference 9">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotional%20facial%20perception%20development%20in%207%2C%209%20and%2011%20year-old%20children%3A%20the%20emergence%20of%20a%20silent%20eye-tracked%20emotional%20other-race%20effect&amp;journal=PLoS%20ONE&amp;volume=15&amp;publication_year=2020&amp;author=Malsert%2CJ&amp;author=Palama%2CA&amp;author=Gentaz%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Batty, M. &amp; Taylor, M. J. The development of emotional face processing during childhood. <i>Dev. Sci.</i> <b>9</b>, 207–220 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16472321" aria-label="PubMed reference 10">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20development%20of%20emotional%20face%20processing%20during%20childhood&amp;journal=Dev.%20Sci.&amp;volume=9&amp;pages=207-220&amp;publication_year=2006&amp;author=Batty%2CM&amp;author=Taylor%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Lemerise, E. A. &amp; Arsenio, W. F. An integrated model of emotion processes and cognition in social information processing. <i>Child Dev.</i> <b>71</b>, 107–118 (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD3cvlslektQ%3D%3D" aria-label="CAS reference 11">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10836564" aria-label="PubMed reference 11">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20integrated%20model%20of%20emotion%20processes%20and%20cognition%20in%20social%20information%20processing&amp;journal=Child%20Dev.&amp;volume=71&amp;pages=107-118&amp;publication_year=2000&amp;author=Lemerise%2CEA&amp;author=Arsenio%2CWF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Crick, N. R. &amp; Dodge, K. A. A review and reformulation of social information-processing mechanisms in children’s social adjustment. <i>Psychol. Bull.</i> <b>115</b>, 74–101 (1994).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20review%20and%20reformulation%20of%20social%20information-processing%20mechanisms%20in%20children%E2%80%99s%20social%20adjustment&amp;journal=Psychol.%20Bull.&amp;volume=115&amp;pages=74-101&amp;publication_year=1994&amp;author=Crick%2CNR&amp;author=Dodge%2CKA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Mishkin, M., Ungerleider, L. G. &amp; Macko, K. A. Object vision and spatial vision: two cortical pathways. <i>Trends Neurosci.</i> <b>6</b>, 414–417 (1983).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Object%20vision%20and%20spatial%20vision%3A%20two%20cortical%20pathways&amp;journal=Trends%20Neurosci.&amp;volume=6&amp;pages=414-417&amp;publication_year=1983&amp;author=Mishkin%2CM&amp;author=Ungerleider%2CLG&amp;author=Macko%2CKA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Dehaene-Lambertz, G., Hertz-Pannier, L. &amp; Dubois, J. Nature and nurture in language acquisition: anatomical and functional brain-imaging studies in infants. <i>Trends Neurosci.</i> <b>29</b>, 367–373 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28Xns1Sltbo%3D" aria-label="CAS reference 14">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16815562" aria-label="PubMed reference 14">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Nature%20and%20nurture%20in%20language%20acquisition%3A%20anatomical%20and%20functional%20brain-imaging%20studies%20in%20infants&amp;journal=Trends%20Neurosci.&amp;volume=29&amp;pages=367-373&amp;publication_year=2006&amp;author=Dehaene-Lambertz%2CG&amp;author=Hertz-Pannier%2CL&amp;author=Dubois%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Corbetta, M. &amp; Shulman, G. L. Control of goal-directed and stimulus-driven attention in the brain. <i>Nat. Rev. Neurosci.</i> <b>3</b>, 215–229 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Control%20of%20goal-directed%20and%20stimulus-driven%20attention%20in%20the%20brain&amp;journal=Nat.%20Rev.%20Neurosci.&amp;volume=3&amp;pages=215-229&amp;publication_year=2002&amp;author=Corbetta%2CM&amp;author=Shulman%2CGL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Sadaghiani, S. &amp; D’Esposito, M. Functional characterization of the cingulo-opercular network in the maintenance of tonic alertness. <i>Cereb. Cortex</i> <b>25</b>, 2763–2773 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24770711" aria-label="PubMed reference 16">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Functional%20characterization%20of%20the%20cingulo-opercular%20network%20in%20the%20maintenance%20of%20tonic%20alertness&amp;journal=Cereb.%20Cortex&amp;volume=25&amp;pages=2763-2773&amp;publication_year=2015&amp;author=Sadaghiani%2CS&amp;author=D%E2%80%99Esposito%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Barrett, L. F. &amp; Satpute, A. B. Large-scale brain networks in affective and social neuroscience: towards an integrative functional architecture of the brain. <i>Curr. Opin. Neurobiol.</i> <b>23</b>, 361–372 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXht1Oqurg%3D" aria-label="CAS reference 17">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23352202" aria-label="PubMed reference 17">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4119963" aria-label="PubMed Central reference 17">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Large-scale%20brain%20networks%20in%20affective%20and%20social%20neuroscience%3A%20towards%20an%20integrative%20functional%20architecture%20of%20the%20brain&amp;journal=Curr.%20Opin.%20Neurobiol.&amp;volume=23&amp;pages=361-372&amp;publication_year=2013&amp;author=Barrett%2CLF&amp;author=Satpute%2CAB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Buckner, R. L. &amp; DiNicola, L. M. The brain’s default network: updated anatomy, physiology and evolving insights. <i>Nat. Rev. Neurosci.</i> <b>20</b>, 593–608 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1MXhslensbrJ" aria-label="CAS reference 18">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31492945" aria-label="PubMed reference 18">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20brain%E2%80%99s%20default%20network%3A%20updated%20anatomy%2C%20physiology%20and%20evolving%20insights&amp;journal=Nat.%20Rev.%20Neurosci.&amp;volume=20&amp;pages=593-608&amp;publication_year=2019&amp;author=Buckner%2CRL&amp;author=DiNicola%2CLM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Satpute, A. B. &amp; Lindquist, K. A. The default mode network’s role in discrete emotion. <i>Trends Cogn. Sci.</i> <b>23</b>, 851–864 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31427147" aria-label="PubMed reference 19">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7281778" aria-label="PubMed Central reference 19">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20default%20mode%20network%E2%80%99s%20role%20in%20discrete%20emotion&amp;journal=Trends%20Cogn.%20Sci.&amp;volume=23&amp;pages=851-864&amp;publication_year=2019&amp;author=Satpute%2CAB&amp;author=Lindquist%2CKA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Marek, S. &amp; Dosenbach, N. U. F. The frontoparietal network: function, electrophysiology, and importance of individual precision mapping. <i>Dialogues Clin. Neurosci.</i> <b>20</b>, 133–140 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30250390" aria-label="PubMed reference 20">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6136121" aria-label="PubMed Central reference 20">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20frontoparietal%20network%3A%20function%2C%20electrophysiology%2C%20and%20importance%20of%20individual%20precision%20mapping&amp;journal=Dialogues%20Clin.%20Neurosci.&amp;volume=20&amp;pages=133-140&amp;publication_year=2018&amp;author=Marek%2CS&amp;author=Dosenbach%2CNUF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Lindquist, K. A. &amp; Barrett, L. F. A functional architecture of the human brain: emerging insights from the science of emotion. <i>Trends Cogn. Sci.</i> <b>16</b>, 533–540 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23036719" aria-label="PubMed reference 21">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3482298" aria-label="PubMed Central reference 21">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20functional%20architecture%20of%20the%20human%20brain%3A%20emerging%20insights%20from%20the%20science%20of%20emotion&amp;journal=Trends%20Cogn.%20Sci.&amp;volume=16&amp;pages=533-540&amp;publication_year=2012&amp;author=Lindquist%2CKA&amp;author=Barrett%2CLF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Kragel, P. A., Reddan, M. C., LaBar, K. S. &amp; Wager, T. D. Emotion schemas are embedded in the human visual system. <i>Sci. Adv.</i> <b>5</b>, eaaw4358 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31355334" aria-label="PubMed reference 22">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6656543" aria-label="PubMed Central reference 22">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20schemas%20are%20embedded%20in%20the%20human%20visual%20system&amp;journal=Sci.%20Adv.&amp;volume=5&amp;publication_year=2019&amp;author=Kragel%2CPA&amp;author=Reddan%2CMC&amp;author=LaBar%2CKS&amp;author=Wager%2CTD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Park, A. T. et al. Early stressful experiences are associated with reduced neural responses to naturalistic emotional and social content in children. <i>Dev. Cogn. Neurosci.</i> <b>57</b>, 101152 (2022).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36137356" aria-label="PubMed reference 23">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9493069" aria-label="PubMed Central reference 23">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Early%20stressful%20experiences%20are%20associated%20with%20reduced%20neural%20responses%20to%20naturalistic%20emotional%20and%20social%20content%20in%20children&amp;journal=Dev.%20Cogn.%20Neurosci.&amp;volume=57&amp;publication_year=2022&amp;author=Park%2CAT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Gruskin, D. C., Rosenberg, M. D. &amp; Holmes, A. J. Relationships between depressive symptoms and brain responses during emotional movie viewing emerge in adolescence. <i>Neuroimage</i> <b>216</b>, 116217 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31628982" aria-label="PubMed reference 24">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Relationships%20between%20depressive%20symptoms%20and%20brain%20responses%20during%20emotional%20movie%20viewing%20emerge%20in%20adolescence&amp;journal=Neuroimage&amp;volume=216&amp;publication_year=2020&amp;author=Gruskin%2CDC&amp;author=Rosenberg%2CMD&amp;author=Holmes%2CAJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Camacho, M. C., Karim, H. T. &amp; Perlman, S. B. Neural architecture supporting active emotion processing in children: a multivariate approach. <i>Neuroimage</i> <b>188</b>, 171–180 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30537564" aria-label="PubMed reference 25">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20architecture%20supporting%20active%20emotion%20processing%20in%20children%3A%20a%20multivariate%20approach&amp;journal=Neuroimage&amp;volume=188&amp;pages=171-180&amp;publication_year=2019&amp;author=Camacho%2CMC&amp;author=Karim%2CHT&amp;author=Perlman%2CSB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Widen, S. C. Children’s interpretation of facial expressions: the long path from valence-based to specific discrete categories. <i>Emot. Rev.</i> <b>5</b>, 72–77 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Children%E2%80%99s%20interpretation%20of%20facial%20expressions%3A%20the%20long%20path%20from%20valence-based%20to%20specific%20discrete%20categories&amp;journal=Emot.%20Rev.&amp;volume=5&amp;pages=72-77&amp;publication_year=2013&amp;author=Widen%2CSC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Nook, E. C., Sasse, S. F., Lambert, H. K., McLaughlin, K. A. &amp; Somerville, L. H. The nonlinear development of emotion differentiation: granular emotional experience is low in adolescence. <i>Psychol. Sci.</i> <b>29</b>, 1346–1357 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29878880" aria-label="PubMed reference 27">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6088506" aria-label="PubMed Central reference 27">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20nonlinear%20development%20of%20emotion%20differentiation%3A%20granular%20emotional%20experience%20is%20low%20in%20adolescence&amp;journal=Psychol.%20Sci.&amp;volume=29&amp;pages=1346-1357&amp;publication_year=2018&amp;author=Nook%2CEC&amp;author=Sasse%2CSF&amp;author=Lambert%2CHK&amp;author=McLaughlin%2CKA&amp;author=Somerville%2CLH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Nook, E. C. et al. Charting the development of emotion comprehension and abstraction from childhood to adulthood using observer-rated and linguistic measures. <i>Emotion</i> <b>20</b>, 773–792 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31192665" aria-label="PubMed reference 28">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Charting%20the%20development%20of%20emotion%20comprehension%20and%20abstraction%20from%20childhood%20to%20adulthood%20using%20observer-rated%20and%20linguistic%20measures&amp;journal=Emotion&amp;volume=20&amp;pages=773-792&amp;publication_year=2020&amp;author=Nook%2CEC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Motta-Mena, N. V. &amp; Scherf, K. S. Pubertal development shapes perception of complex facial expressions. <i>Dev. Sci.</i> <b>20</b>, e12451 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Pubertal%20development%20shapes%20perception%20of%20complex%20facial%20expressions&amp;journal=Dev.%20Sci.&amp;volume=20&amp;publication_year=2017&amp;author=Motta-Mena%2CNV&amp;author=Scherf%2CKS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Thomas, K. M. et al. Amygdala response to facial expressions in children and adults. <i>Biol. Psychiatry</i> <b>49</b>, 309–316 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD3Mzmsl2msA%3D%3D" aria-label="CAS reference 30">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11239901" aria-label="PubMed reference 30">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Amygdala%20response%20to%20facial%20expressions%20in%20children%20and%20adults&amp;journal=Biol.%20Psychiatry&amp;volume=49&amp;pages=309-316&amp;publication_year=2001&amp;author=Thomas%2CKM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Wiggins, J. L. et al. Developmental differences in the neural mechanisms of facial emotion labeling. <i>Soc. Cogn. Affect. Neurosci.</i> <b>11</b>, 172–181 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26245836" aria-label="PubMed reference 31">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Developmental%20differences%20in%20the%20neural%20mechanisms%20of%20facial%20emotion%20labeling&amp;journal=Soc.%20Cogn.%20Affect.%20Neurosci.&amp;volume=11&amp;pages=172-181&amp;publication_year=2016&amp;author=Wiggins%2CJL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Marusak, H. A., Carré, J. M. &amp; Thomason, M. E. The stimuli drive the response: an fMRI study of youth processing adult or child emotional face stimuli. <i>Neuroimage</i> <b>83</b>, 679–689 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23851324" aria-label="PubMed reference 32">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20stimuli%20drive%20the%20response%3A%20an%20fMRI%20study%20of%20youth%20processing%20adult%20or%20child%20emotional%20face%20stimuli&amp;journal=Neuroimage&amp;volume=83&amp;pages=679-689&amp;publication_year=2013&amp;author=Marusak%2CHA&amp;author=Carr%C3%A9%2CJM&amp;author=Thomason%2CME">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Ladouceur, C. D., Schlund, M. W. &amp; Segreti, A.-M. Positive reinforcement modulates fronto-limbic systems subserving emotional interference in adolescents. <i>Behav. Brain Res.</i> <b>338</b>, 109–117 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29079512" aria-label="PubMed reference 33">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Positive%20reinforcement%20modulates%20fronto-limbic%20systems%20subserving%20emotional%20interference%20in%20adolescents&amp;journal=Behav.%20Brain%20Res.&amp;volume=338&amp;pages=109-117&amp;publication_year=2018&amp;author=Ladouceur%2CCD&amp;author=Schlund%2CMW&amp;author=Segreti%2CA-M">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Hoehl, S., Brauer, J., Brasse, G., Striano, T. &amp; Friederici, A. D. Children’s processing of emotions expressed by peers and adults: an fMRI study. <i>Soc. Neurosci.</i> <b>5</b>, 543–559 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20486013" aria-label="PubMed reference 34">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Children%E2%80%99s%20processing%20of%20emotions%20expressed%20by%20peers%20and%20adults%3A%20an%20fMRI%20study&amp;journal=Soc.%20Neurosci.&amp;volume=5&amp;pages=543-559&amp;publication_year=2010&amp;author=Hoehl%2CS&amp;author=Brauer%2CJ&amp;author=Brasse%2CG&amp;author=Striano%2CT&amp;author=Friederici%2CAD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Haller, S. P. et al. Reliability of neural activation and connectivity during implicit face emotion processing in youth. <i>Dev. Cogn. Neurosci.</i> <b>31</b>, 67–73 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29753993" aria-label="PubMed reference 35">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6054466" aria-label="PubMed Central reference 35">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Reliability%20of%20neural%20activation%20and%20connectivity%20during%20implicit%20face%20emotion%20processing%20in%20youth&amp;journal=Dev.%20Cogn.%20Neurosci.&amp;volume=31&amp;pages=67-73&amp;publication_year=2018&amp;author=Haller%2CSP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Lobaugh, N. J., Gibson, E. &amp; Taylor, M. J. Children recruit distinct neural systems for implicit emotional face processing. <i>Neuroreport</i> <b>17</b>, 215–219 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16407774" aria-label="PubMed reference 36">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Children%20recruit%20distinct%20neural%20systems%20for%20implicit%20emotional%20face%20processing&amp;journal=Neuroreport&amp;volume=17&amp;pages=215-219&amp;publication_year=2006&amp;author=Lobaugh%2CNJ&amp;author=Gibson%2CE&amp;author=Taylor%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Guyer, A. E. et al. A developmental examination of amygdala response to facial expressions. <i>J. Cogn. Neurosci.</i> <b>20</b>, 1565–1582 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18345988" aria-label="PubMed reference 37">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2902865" aria-label="PubMed Central reference 37">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20developmental%20examination%20of%20amygdala%20response%20to%20facial%20expressions&amp;journal=J.%20Cogn.%20Neurosci.&amp;volume=20&amp;pages=1565-1582&amp;publication_year=2008&amp;author=Guyer%2CAE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Pagliaccio, D. et al. Functional brain activation to emotional and nonemotional faces in healthy children: evidence for developmentally undifferentiated amygdala function during the school-age period. <i>Cogn. Affect. Behav. Neurosci.</i> <b>13</b>, 771–789 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23636982" aria-label="PubMed reference 38">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Functional%20brain%20activation%20to%20emotional%20and%20nonemotional%20faces%20in%20healthy%20children%3A%20evidence%20for%20developmentally%20undifferentiated%20amygdala%20function%20during%20the%20school-age%20period&amp;journal=Cogn.%20Affect.%20Behav.%20Neurosci.&amp;volume=13&amp;pages=771-789&amp;publication_year=2013&amp;author=Pagliaccio%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Hare, T. A. et al. Biological substrates of emotional reactivity and regulation in adolescence during an emotional go-nogo task. <i>Biol. Psychiatry</i> <b>63</b>, 927–934 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18452757" aria-label="PubMed reference 39">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2664095" aria-label="PubMed Central reference 39">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Biological%20substrates%20of%20emotional%20reactivity%20and%20regulation%20in%20adolescence%20during%20an%20emotional%20go-nogo%20task&amp;journal=Biol.%20Psychiatry&amp;volume=63&amp;pages=927-934&amp;publication_year=2008&amp;author=Hare%2CTA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Somerville, L. H., Fani, N. &amp; McClure-Tone, E. B. Behavioral and neural representation of emotional facial expressions across the lifespan. <i>Dev. Neuropsychol.</i> <b>36</b>, 408–428 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21516541" aria-label="PubMed reference 40">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3084535" aria-label="PubMed Central reference 40">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Behavioral%20and%20neural%20representation%20of%20emotional%20facial%20expressions%20across%20the%20lifespan&amp;journal=Dev.%20Neuropsychol.&amp;volume=36&amp;pages=408-428&amp;publication_year=2011&amp;author=Somerville%2CLH&amp;author=Fani%2CN&amp;author=McClure-Tone%2CEB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Widen, S. C. &amp; Russell, J. A. Children acquire emotion categories gradually. <i>Cogn. Dev.</i> <b>23</b>, 291–312 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Children%20acquire%20emotion%20categories%20gradually&amp;journal=Cogn.%20Dev.&amp;volume=23&amp;pages=291-312&amp;publication_year=2008&amp;author=Widen%2CSC&amp;author=Russell%2CJA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Widen, S. C. &amp; Russell, J. A. Children’s scripts for social emotions: causes and consequences are more central than are facial expressions. <i>Br. J. Dev. Psychol.</i> <b>28</b>, 565–581 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20849034" aria-label="PubMed reference 42">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Children%E2%80%99s%20scripts%20for%20social%20emotions%3A%20causes%20and%20consequences%20are%20more%20central%20than%20are%20facial%20expressions&amp;journal=Br.%20J.%20Dev.%20Psychol.&amp;volume=28&amp;pages=565-581&amp;publication_year=2010&amp;author=Widen%2CSC&amp;author=Russell%2CJA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Wu, Y., Schulz, L. E., Frank, M. C. &amp; Gweon, H. Emotion as information in early social learning. <i>Curr. Dir. Psychol. Sci.</i> <b>30</b>, 468–475 (2021).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20as%20information%20in%20early%20social%20learning&amp;journal=Curr.%20Dir.%20Psychol.%20Sci.&amp;volume=30&amp;pages=468-475&amp;publication_year=2021&amp;author=Wu%2CY&amp;author=Schulz%2CLE&amp;author=Frank%2CMC&amp;author=Gweon%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Cantlon, J. F. The balance of rigor and reality in developmental neuroscience. <i>Neuroimage</i> <b>216</b>, 116464 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31874256" aria-label="PubMed reference 44">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20balance%20of%20rigor%20and%20reality%20in%20developmental%20neuroscience&amp;journal=Neuroimage&amp;volume=216&amp;publication_year=2020&amp;author=Cantlon%2CJF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Vanderwal, T., Eilbott, J. &amp; Castellanos, F. X. Movies in the magnet: naturalistic paradigms in developmental functional neuroimaging. <i>Dev. Cogn. Neurosci.</i> <b>36</b>, 100600 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30551970" aria-label="PubMed reference 45">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Movies%20in%20the%20magnet%3A%20naturalistic%20paradigms%20in%20developmental%20functional%20neuroimaging&amp;journal=Dev.%20Cogn.%20Neurosci.&amp;volume=36&amp;publication_year=2019&amp;author=Vanderwal%2CT&amp;author=Eilbott%2CJ&amp;author=Castellanos%2CFX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Eickhoff, S. B., Milham, M. &amp; Vanderwal, T. Towards clinical applications of movie fMRI. <i>Neuroimage</i> <b>217</b>, 116860 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32376301" aria-label="PubMed reference 46">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20clinical%20applications%20of%20movie%20fMRI&amp;journal=Neuroimage&amp;volume=217&amp;publication_year=2020&amp;author=Eickhoff%2CSB&amp;author=Milham%2CM&amp;author=Vanderwal%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Glasser, M. F. et al. The minimal preprocessing pipelines for the Human Connectome Project. <i>Neuroimage</i> <b>80</b>, 105–124 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23668970" aria-label="PubMed reference 47">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20minimal%20preprocessing%20pipelines%20for%20the%20Human%20Connectome%20Project&amp;journal=Neuroimage&amp;volume=80&amp;pages=105-124&amp;publication_year=2013&amp;author=Glasser%2CMF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">Camacho, M. C. et al. EmoCodes: a standardized coding system for socio-emotional content in complex video stimuli. <i>Affect. Sci.</i> <b>3</b>, 168–181 (2022).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36046099" aria-label="PubMed reference 48">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9383008" aria-label="PubMed Central reference 48">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=EmoCodes%3A%20a%20standardized%20coding%20system%20for%20socio-emotional%20content%20in%20complex%20video%20stimuli&amp;journal=Affect.%20Sci.&amp;volume=3&amp;pages=168-181&amp;publication_year=2022&amp;author=Camacho%2CMC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">Sander, D., Grandjean, D. &amp; Scherer, K. R. An appraisal-driven componential approach to the emotional brain. <i>Emot. Rev.</i> <b>10</b>, 219–231 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20appraisal-driven%20componential%20approach%20to%20the%20emotional%20brain&amp;journal=Emot.%20Rev.&amp;volume=10&amp;pages=219-231&amp;publication_year=2018&amp;author=Sander%2CD&amp;author=Grandjean%2CD&amp;author=Scherer%2CKR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">Pessoa, L. Understanding emotion with brain networks. <i>Curr. Opin. Behav. Sci.</i> <b>19</b>, 19–25 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29915794" aria-label="PubMed reference 50">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20emotion%20with%20brain%20networks&amp;journal=Curr.%20Opin.%20Behav.%20Sci.&amp;volume=19&amp;pages=19-25&amp;publication_year=2018&amp;author=Pessoa%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">Lindquist, K. A. &amp; MacCormack, J. K. Comment: Constructionism is a multilevel framework for affective science. <i>Emot. Rev.</i> <b>6</b>, 134–135 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=Comment%3A%20Constructionism%20is%20a%20multilevel%20framework%20for%20affective%20science&amp;journal=Emot.%20Rev.&amp;volume=6&amp;pages=134-135&amp;publication_year=2014&amp;author=Lindquist%2CKA&amp;author=MacCormack%2CJK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">Skerry, A. E. &amp; Saxe, R. Neural representations of emotion are organized around abstract event features. <i>Curr. Biol.</i> <b>25</b>, 1945–1954 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXht1Cku7rI" aria-label="CAS reference 52">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26212878" aria-label="PubMed reference 52">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4824044" aria-label="PubMed Central reference 52">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20representations%20of%20emotion%20are%20organized%20around%20abstract%20event%20features&amp;journal=Curr.%20Biol.&amp;volume=25&amp;pages=1945-1954&amp;publication_year=2015&amp;author=Skerry%2CAE&amp;author=Saxe%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">Tracy, J. L. &amp; Randles, D. Four models of basic emotions: a review of Ekman and Cordaro, Izard, Levenson, and Panksepp and Watt. <i>Emot. Rev.</i> <b>3</b>, 397–405 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Four%20models%20of%20basic%20emotions%3A%20a%20review%20of%20Ekman%20and%20Cordaro%2C%20Izard%2C%20Levenson%2C%20and%20Panksepp%20and%20Watt&amp;journal=Emot.%20Rev.&amp;volume=3&amp;pages=397-405&amp;publication_year=2011&amp;author=Tracy%2CJL&amp;author=Randles%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR54">Panksepp, J. &amp; Watt, D. What is basic about basic emotions? Lasting lessons from affective neuroscience. <i>Emot. Rev.</i> <b>3</b>, 387–396 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20is%20basic%20about%20basic%20emotions%3F%20Lasting%20lessons%20from%20affective%20neuroscience&amp;journal=Emot.%20Rev.&amp;volume=3&amp;pages=387-396&amp;publication_year=2011&amp;author=Panksepp%2CJ&amp;author=Watt%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55."><p class="c-article-references__text" id="ref-CR55">Ogren, M. &amp; Johnson, S. P. Factors facilitating early emotion understanding development: contributions to individual differences. <i>Hum. Dev.</i> <b>64</b>, 108–118 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34305161" aria-label="PubMed reference 55">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Factors%20facilitating%20early%20emotion%20understanding%20development%3A%20contributions%20to%20individual%20differences&amp;journal=Hum.%20Dev.&amp;volume=64&amp;pages=108-118&amp;publication_year=2020&amp;author=Ogren%2CM&amp;author=Johnson%2CSP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56."><p class="c-article-references__text" id="ref-CR56">Ogren, M. &amp; Sandhofer, C. M. Emotion words link faces to emotional scenarios in early childhood. <i>Emotion</i> <b>22</b>, 167–178 (2022).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=35084908" aria-label="PubMed reference 56">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8924982" aria-label="PubMed Central reference 56">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20words%20link%20faces%20to%20emotional%20scenarios%20in%20early%20childhood&amp;journal=Emotion&amp;volume=22&amp;pages=167-178&amp;publication_year=2022&amp;author=Ogren%2CM&amp;author=Sandhofer%2CCM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57."><p class="c-article-references__text" id="ref-CR57">Camras, L. A. &amp; Allison, K. Children’s understanding of emotional facial expressions and verbal labels. <i>J. Nonverbal Behav.</i> <b>9</b>, 84–94 (1985).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Children%E2%80%99s%20understanding%20of%20emotional%20facial%20expressions%20and%20verbal%20labels&amp;journal=J.%20Nonverbal%20Behav.&amp;volume=9&amp;pages=84-94&amp;publication_year=1985&amp;author=Camras%2CLA&amp;author=Allison%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58."><p class="c-article-references__text" id="ref-CR58">Lawrence, K., Campbell, R. &amp; Skuse, D. Age, gender, and puberty influence the development of facial emotion recognition. <i>Front. Psychol.</i> <b>6</b>, 761 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26136697" aria-label="PubMed reference 58">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4468868" aria-label="PubMed Central reference 58">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Age%2C%20gender%2C%20and%20puberty%20influence%20the%20development%20of%20facial%20emotion%20recognition&amp;journal=Front.%20Psychol.&amp;volume=6&amp;publication_year=2015&amp;author=Lawrence%2CK&amp;author=Campbell%2CR&amp;author=Skuse%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59."><p class="c-article-references__text" id="ref-CR59">Keulers, E. H. H., Evers, E. A. T., Stiers, P. &amp; Jolles, J. Age, sex, and pubertal phase influence mentalizing about emotions and actions in adolescents. <i>Dev. Neuropsychol.</i> <b>35</b>, 555–569 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20721775" aria-label="PubMed reference 59">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Age%2C%20sex%2C%20and%20pubertal%20phase%20influence%20mentalizing%20about%20emotions%20and%20actions%20in%20adolescents&amp;journal=Dev.%20Neuropsychol.&amp;volume=35&amp;pages=555-569&amp;publication_year=2010&amp;author=Keulers%2CEHH&amp;author=Evers%2CEAT&amp;author=Stiers%2CP&amp;author=Jolles%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60."><p class="c-article-references__text" id="ref-CR60">Dai, J. &amp; Scherf, K. S. Puberty and functional brain development in humans: convergence in findings? <i>Dev. Cogn. Neurosci.</i> <b>39</b>, 100690 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31450015" aria-label="PubMed reference 60">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6969369" aria-label="PubMed Central reference 60">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Puberty%20and%20functional%20brain%20development%20in%20humans%3A%20convergence%20in%20findings%3F&amp;journal=Dev.%20Cogn.%20Neurosci.&amp;volume=39&amp;publication_year=2019&amp;author=Dai%2CJ&amp;author=Scherf%2CKS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61."><p class="c-article-references__text" id="ref-CR61">Hoemann, K., Xu, F. &amp; Barrett, L. F. Emotion words, emotion concepts, and emotional development in children: a constructionist hypothesis. <i>Dev. Psychol.</i> <b>55</b>, 1830–1849 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31464489" aria-label="PubMed reference 61">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6716622" aria-label="PubMed Central reference 61">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20words%2C%20emotion%20concepts%2C%20and%20emotional%20development%20in%20children%3A%20a%20constructionist%20hypothesis&amp;journal=Dev.%20Psychol.&amp;volume=55&amp;pages=1830-1849&amp;publication_year=2019&amp;author=Hoemann%2CK&amp;author=Xu%2CF&amp;author=Barrett%2CLF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62."><p class="c-article-references__text" id="ref-CR62">Richardson, H., Lisandrelli, G., Riobueno-Naylor, A. &amp; Saxe, R. Development of the social brain from age three to twelve years. <i>Nat. Commun.</i> <b>9</b>, 1027 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29531321" aria-label="PubMed reference 62">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5847587" aria-label="PubMed Central reference 62">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20the%20social%20brain%20from%20age%20three%20to%20twelve%20years&amp;journal=Nat.%20Commun.&amp;volume=9&amp;publication_year=2018&amp;author=Richardson%2CH&amp;author=Lisandrelli%2CG&amp;author=Riobueno-Naylor%2CA&amp;author=Saxe%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63."><p class="c-article-references__text" id="ref-CR63">Fan, F. et al. Development of the default-mode network during childhood and adolescence: a longitudinal resting-state fMRI study. <i>Neuroimage</i> <b>226</b>, 117581 (2021).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33221440" aria-label="PubMed reference 63">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 63" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20the%20default-mode%20network%20during%20childhood%20and%20adolescence%3A%20a%20longitudinal%20resting-state%20fMRI%20study&amp;journal=Neuroimage&amp;volume=226&amp;publication_year=2021&amp;author=Fan%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64."><p class="c-article-references__text" id="ref-CR64">Gordon, E. M. et al. Three distinct sets of connector hubs integrate human brain function. <i>Cell Rep.</i> <b>24</b>, 1687–1695 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1cXhsFahtrnM" aria-label="CAS reference 64">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30110625" aria-label="PubMed reference 64">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6886580" aria-label="PubMed Central reference 64">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=Three%20distinct%20sets%20of%20connector%20hubs%20integrate%20human%20brain%20function&amp;journal=Cell%20Rep.&amp;volume=24&amp;pages=1687-1695&amp;publication_year=2018&amp;author=Gordon%2CEM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65."><p class="c-article-references__text" id="ref-CR65">Brandman, T., Malach, R. &amp; Simony, E. The surprising role of the default mode network in naturalistic perception. <i>Commun. Biol.</i> <b>4</b>, 79 (2021).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33469113" aria-label="PubMed reference 65">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7815915" aria-label="PubMed Central reference 65">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20surprising%20role%20of%20the%20default%20mode%20network%20in%20naturalistic%20perception&amp;journal=Commun.%20Biol.&amp;volume=4&amp;publication_year=2021&amp;author=Brandman%2CT&amp;author=Malach%2CR&amp;author=Simony%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66."><p class="c-article-references__text" id="ref-CR66">da Silva, P. H. R., Rondinoni, C. &amp; Leoni, R. F. Non-classical behavior of the default mode network regions during an information processing task. <i>Brain Struct. Funct.</i> <b>225</b>, 2553–2562 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32939584" aria-label="PubMed reference 66">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 66" href="http://scholar.google.com/scholar_lookup?&amp;title=Non-classical%20behavior%20of%20the%20default%20mode%20network%20regions%20during%20an%20information%20processing%20task&amp;journal=Brain%20Struct.%20Funct.&amp;volume=225&amp;pages=2553-2562&amp;publication_year=2020&amp;author=Silva%2CPHR&amp;author=Rondinoni%2CC&amp;author=Leoni%2CRF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67."><p class="c-article-references__text" id="ref-CR67">Gordon, E. M. et al. Default-mode network streams for coupling to language and control systems. <i>Proc. Natl Acad. Sci. USA</i> <b>117</b>, 17308–17319 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXisVWkur%2FN" aria-label="CAS reference 67">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32632019" aria-label="PubMed reference 67">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7382234" aria-label="PubMed Central reference 67">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 67" href="http://scholar.google.com/scholar_lookup?&amp;title=Default-mode%20network%20streams%20for%20coupling%20to%20language%20and%20control%20systems&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;volume=117&amp;pages=17308-17319&amp;publication_year=2020&amp;author=Gordon%2CEM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68."><p class="c-article-references__text" id="ref-CR68">Kaiser, R. H., Andrews-Hanna, J. R., Wager, T. D. &amp; Pizzagalli, D. A. Large-scale network dysfunction in major depressive disorder. <i>JAMA Psychiatry</i> <b>72</b>, 603 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25785575" aria-label="PubMed reference 68">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4456260" aria-label="PubMed Central reference 68">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 68" href="http://scholar.google.com/scholar_lookup?&amp;title=Large-scale%20network%20dysfunction%20in%20major%20depressive%20disorder&amp;journal=JAMA%20Psychiatry&amp;volume=72&amp;publication_year=2015&amp;author=Kaiser%2CRH&amp;author=Andrews-Hanna%2CJR&amp;author=Wager%2CTD&amp;author=Pizzagalli%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69."><p class="c-article-references__text" id="ref-CR69">Vargas, C., López-Jaramillo, C. &amp; Vieta, E. A systematic literature review of resting state network—functional MRI in bipolar disorder. <i>J. Affect. Disord.</i> <b>150</b>, 727–735 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23830141" aria-label="PubMed reference 69">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 69" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20systematic%20literature%20review%20of%20resting%20state%20network%E2%80%94functional%20MRI%20in%20bipolar%20disorder&amp;journal=J.%20Affect.%20Disord.&amp;volume=150&amp;pages=727-735&amp;publication_year=2013&amp;author=Vargas%2CC&amp;author=L%C3%B3pez-Jaramillo%2CC&amp;author=Vieta%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70."><p class="c-article-references__text" id="ref-CR70">Broyd, S. J. et al. Default-mode brain dysfunction in mental disorders: a systematic review. <i>Neurosci. Biobehav. Rev.</i> <b>33</b>, 279–296 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18824195" aria-label="PubMed reference 70">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Default-mode%20brain%20dysfunction%20in%20mental%20disorders%3A%20a%20systematic%20review&amp;journal=Neurosci.%20Biobehav.%20Rev.&amp;volume=33&amp;pages=279-296&amp;publication_year=2009&amp;author=Broyd%2CSJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="71."><p class="c-article-references__text" id="ref-CR71">Williams, L. M. Defining biotypes for depression and anxiety based on large-scale circuit dysfunction: a theoretical review of the evidence and future directions for clinical translation. <i>Depress. Anxiety</i> <b>34</b>, 9–24 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27653321" aria-label="PubMed reference 71">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 71" href="http://scholar.google.com/scholar_lookup?&amp;title=Defining%20biotypes%20for%20depression%20and%20anxiety%20based%20on%20large-scale%20circuit%20dysfunction%3A%20a%20theoretical%20review%20of%20the%20evidence%20and%20future%20directions%20for%20clinical%20translation&amp;journal=Depress.%20Anxiety&amp;volume=34&amp;pages=9-24&amp;publication_year=2017&amp;author=Williams%2CLM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="72."><p class="c-article-references__text" id="ref-CR72">Lettieri, G. et al. Emotionotopy in the human right temporo-parietal cortex. <i>Nat. Commun.</i> <b>10</b>, 5568 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1MXitlGiu7bK" aria-label="CAS reference 72">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31804504" aria-label="PubMed reference 72">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6895053" aria-label="PubMed Central reference 72">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 72" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotionotopy%20in%20the%20human%20right%20temporo-parietal%20cortex&amp;journal=Nat.%20Commun.&amp;volume=10&amp;publication_year=2019&amp;author=Lettieri%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="73."><p class="c-article-references__text" id="ref-CR73">Lettieri, G. et al. Default and control network connectivity dynamics track the stream of affect at multiple timescales. <i>Soc. Cogn. Affect. Neurosci.</i> <b>17</b>, 461–469 (2022).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34673987" aria-label="PubMed reference 73">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=Default%20and%20control%20network%20connectivity%20dynamics%20track%20the%20stream%20of%20affect%20at%20multiple%20timescales&amp;journal=Soc.%20Cogn.%20Affect.%20Neurosci.&amp;volume=17&amp;pages=461-469&amp;publication_year=2022&amp;author=Lettieri%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="74."><p class="c-article-references__text" id="ref-CR74">Kragel, P. A. &amp; LaBar, K. S. Multivariate neural biomarkers of emotional states are categorically distinct. <i>Soc. Cogn. Affect. Neurosci.</i> <b>10</b>, 1437–1448 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25813790" aria-label="PubMed reference 74">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4631142" aria-label="PubMed Central reference 74">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 74" href="http://scholar.google.com/scholar_lookup?&amp;title=Multivariate%20neural%20biomarkers%20of%20emotional%20states%20are%20categorically%20distinct&amp;journal=Soc.%20Cogn.%20Affect.%20Neurosci.&amp;volume=10&amp;pages=1437-1448&amp;publication_year=2015&amp;author=Kragel%2CPA&amp;author=LaBar%2CKS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="75."><p class="c-article-references__text" id="ref-CR75">Kragel, P. A., Knodt, A. R., Hariri, A. R. &amp; Labar, K. S. Decoding spontaneous emotional states in the human brain. <i>PLoS Biol.</i> <b>14</b>, e2000106 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27627738" aria-label="PubMed reference 75">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5023171" aria-label="PubMed Central reference 75">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 75" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20spontaneous%20emotional%20states%20in%20the%20human%20brain&amp;journal=PLoS%20Biol.&amp;volume=14&amp;publication_year=2016&amp;author=Kragel%2CPA&amp;author=Knodt%2CAR&amp;author=Hariri%2CAR&amp;author=Labar%2CKS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="76."><p class="c-article-references__text" id="ref-CR76">Kragel, P. A. &amp; Labar, K. S. Decoding the nature of emotion in the brain. <i>Trends Cogn. Sci.</i> <b>20</b>, 444–455 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27133227" aria-label="PubMed reference 76">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4875847" aria-label="PubMed Central reference 76">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 76" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20the%20nature%20of%20emotion%20in%20the%20brain&amp;journal=Trends%20Cogn.%20Sci.&amp;volume=20&amp;pages=444-455&amp;publication_year=2016&amp;author=Kragel%2CPA&amp;author=Labar%2CKS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="77."><p class="c-article-references__text" id="ref-CR77">Chang, L. J. et al. Endogenous variation in ventromedial prefrontal cortex state dynamics during naturalistic viewing reflects affective experience. <i>Sci. Adv.</i> <b>7</b>, eabf7129 (2021).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="78."><p class="c-article-references__text" id="ref-CR78">Alexander, L. M. et al. An open resource for transdiagnostic research in pediatric mental health and learning disorders. <i>Sci. Data</i> <b>4</b>, 170181 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29257126" aria-label="PubMed reference 78">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5735921" aria-label="PubMed Central reference 78">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 78" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20open%20resource%20for%20transdiagnostic%20research%20in%20pediatric%20mental%20health%20and%20learning%20disorders&amp;journal=Sci.%20Data&amp;volume=4&amp;publication_year=2017&amp;author=Alexander%2CLM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="79."><p class="c-article-references__text" id="ref-CR79">Nook, E. C., Sasse, S. F., Lambert, H. K., McLaughlin, K. A. &amp; Somerville, L. H. The nonlinear development of emotion differentiation: granular emotional experience is low in adolescence. <i>Psychol. Sci.</i> <b>29</b>, 1346–1357 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29878880" aria-label="PubMed reference 79">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6088506" aria-label="PubMed Central reference 79">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 79" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20nonlinear%20development%20of%20emotion%20differentiation%3A%20granular%20emotional%20experience%20is%20low%20in%20adolescence&amp;journal=Psychol.%20Sci.&amp;volume=29&amp;pages=1346-1357&amp;publication_year=2018&amp;author=Nook%2CEC&amp;author=Sasse%2CSF&amp;author=Lambert%2CHK&amp;author=McLaughlin%2CKA&amp;author=Somerville%2CLH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="80."><p class="c-article-references__text" id="ref-CR80">Nook, E. C. et al. Charting the development of emotion comprehension and abstraction from childhood to adulthood using observer-rated and linguistic measures. <i>Emotion</i> <b>20</b>, 773–792 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31192665" aria-label="PubMed reference 80">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 80" href="http://scholar.google.com/scholar_lookup?&amp;title=Charting%20the%20development%20of%20emotion%20comprehension%20and%20abstraction%20from%20childhood%20to%20adulthood%20using%20observer-rated%20and%20linguistic%20measures&amp;journal=Emotion&amp;volume=20&amp;pages=773-792&amp;publication_year=2020&amp;author=Nook%2CEC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="81."><p class="c-article-references__text" id="ref-CR81">Motta-Mena, N. V. &amp; Scherf, K. S. Pubertal development shapes perception of complex facial expressions. <i>Dev. Sci.</i> <b>20</b>, e12451 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 81" href="http://scholar.google.com/scholar_lookup?&amp;title=Pubertal%20development%20shapes%20perception%20of%20complex%20facial%20expressions&amp;journal=Dev.%20Sci.&amp;volume=20&amp;publication_year=2017&amp;author=Motta-Mena%2CNV&amp;author=Scherf%2CKS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="82."><p class="c-article-references__text" id="ref-CR82">Thomas, L. A., De Bellis, M. D., Graham, R. &amp; LaBar, K. S. Development of emotional facial recognition in late childhood and adolescence. <i>Dev. Sci.</i> <b>10</b>, 547–558 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17683341" aria-label="PubMed reference 82">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 82" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20emotional%20facial%20recognition%20in%20late%20childhood%20and%20adolescence&amp;journal=Dev.%20Sci.&amp;volume=10&amp;pages=547-558&amp;publication_year=2007&amp;author=Thomas%2CLA&amp;author=Bellis%2CMD&amp;author=Graham%2CR&amp;author=LaBar%2CKS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="83."><p class="c-article-references__text" id="ref-CR83">Durand, K., Gallay, M., Seigneuric, A., Robichon, F. &amp; Baudouin, J.-Y. The development of facial emotion recognition: the role of configural information. <i>J. Exp. Child Psychol.</i> <b>97</b>, 14–27 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17291524" aria-label="PubMed reference 83">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 83" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20development%20of%20facial%20emotion%20recognition%3A%20the%20role%20of%20configural%20information&amp;journal=J.%20Exp.%20Child%20Psychol.&amp;volume=97&amp;pages=14-27&amp;publication_year=2007&amp;author=Durand%2CK&amp;author=Gallay%2CM&amp;author=Seigneuric%2CA&amp;author=Robichon%2CF&amp;author=Baudouin%2CJ-Y">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="84."><p class="c-article-references__text" id="ref-CR84">Wu, M. et al. Age-related changes in amygdala-frontal connectivity during emotional face processing from childhood into young adulthood. <i>Hum. Brain Mapp.</i> <b>37</b>, 1684–1695 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26931629" aria-label="PubMed reference 84">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4837047" aria-label="PubMed Central reference 84">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 84" href="http://scholar.google.com/scholar_lookup?&amp;title=Age-related%20changes%20in%20amygdala-frontal%20connectivity%20during%20emotional%20face%20processing%20from%20childhood%20into%20young%20adulthood&amp;journal=Hum.%20Brain%20Mapp.&amp;volume=37&amp;pages=1684-1695&amp;publication_year=2016&amp;author=Wu%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="85."><p class="c-article-references__text" id="ref-CR85">Harris, C. R. et al. Array programming with NumPy. <i>Nature</i> <b>585</b>, 357–362 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXitlWmsbbN" aria-label="CAS reference 85">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32939066" aria-label="PubMed reference 85">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7759461" aria-label="PubMed Central reference 85">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 85" href="http://scholar.google.com/scholar_lookup?&amp;title=Array%20programming%20with%20NumPy&amp;journal=Nature&amp;volume=585&amp;pages=357-362&amp;publication_year=2020&amp;author=Harris%2CCR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="86."><p class="c-article-references__text" id="ref-CR86">Virtanen, P. et al. SciPy 1.0: fundamental algorithms for scientific computing in Python. <i>Nat. Methods</i> <b>17</b>, 261–272 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXislCjuro%3D" aria-label="CAS reference 86">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32015543" aria-label="PubMed reference 86">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7056644" aria-label="PubMed Central reference 86">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 86" href="http://scholar.google.com/scholar_lookup?&amp;title=SciPy%201.0%3A%20fundamental%20algorithms%20for%20scientific%20computing%20in%20Python&amp;journal=Nat.%20Methods&amp;volume=17&amp;pages=261-272&amp;publication_year=2020&amp;author=Virtanen%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="87."><p class="c-article-references__text" id="ref-CR87">Andersson, J. L. R. &amp; Sotiropoulos, S. N. An integrated approach to correction for off-resonance effects and subject movement in diffusion MR imaging. <i>Neuroimage</i> <b>125</b>, 1063–1078 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26481672" aria-label="PubMed reference 87">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 87" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20integrated%20approach%20to%20correction%20for%20off-resonance%20effects%20and%20subject%20movement%20in%20diffusion%20MR%20imaging&amp;journal=Neuroimage&amp;volume=125&amp;pages=1063-1078&amp;publication_year=2016&amp;author=Andersson%2CJLR&amp;author=Sotiropoulos%2CSN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="88."><p class="c-article-references__text" id="ref-CR88">Fischl, B. FreeSurfer. <i>Neuroimage</i> <b>62</b>, 774–781 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22248573" aria-label="PubMed reference 88">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 88" href="http://scholar.google.com/scholar_lookup?&amp;title=FreeSurfer&amp;journal=Neuroimage&amp;volume=62&amp;pages=774-781&amp;publication_year=2012&amp;author=Fischl%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="89."><p class="c-article-references__text" id="ref-CR89">Robinson, E. C. et al. Multimodal surface matching with higher-order smoothness constraints. <i>Neuroimage</i> <b>167</b>, 453–465 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29100940" aria-label="PubMed reference 89">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 89" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20surface%20matching%20with%20higher-order%20smoothness%20constraints&amp;journal=Neuroimage&amp;volume=167&amp;pages=453-465&amp;publication_year=2018&amp;author=Robinson%2CEC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="90."><p class="c-article-references__text" id="ref-CR90">Siegel, J. S. et al. Statistical improvements in functional magnetic resonance imaging analyses produced by censoring high-motion data points. <i>Hum. Brain Mapp.</i> <b>35</b>, 1981–1996 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23861343" aria-label="PubMed reference 90">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 90" href="http://scholar.google.com/scholar_lookup?&amp;title=Statistical%20improvements%20in%20functional%20magnetic%20resonance%20imaging%20analyses%20produced%20by%20censoring%20high-motion%20data%20points&amp;journal=Hum.%20Brain%20Mapp.&amp;volume=35&amp;pages=1981-1996&amp;publication_year=2014&amp;author=Siegel%2CJS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="91."><p class="c-article-references__text" id="ref-CR91">Siegel, J. S. et al. Data quality influences observed links between functional connectivity and behavior. <i>Cereb. Cortex</i> <b>27</b>, 4492–4502 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27550863" aria-label="PubMed reference 91">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 91" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20quality%20influences%20observed%20links%20between%20functional%20connectivity%20and%20behavior&amp;journal=Cereb.%20Cortex&amp;volume=27&amp;pages=4492-4502&amp;publication_year=2017&amp;author=Siegel%2CJS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="92."><p class="c-article-references__text" id="ref-CR92">Greene, D. J. et al. Behavioral interventions for reducing head motion during MRI scans in children. <i>Neuroimage</i> <b>171</b>, 234–245 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29337280" aria-label="PubMed reference 92">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 92" href="http://scholar.google.com/scholar_lookup?&amp;title=Behavioral%20interventions%20for%20reducing%20head%20motion%20during%20MRI%20scans%20in%20children&amp;journal=Neuroimage&amp;volume=171&amp;pages=234-245&amp;publication_year=2018&amp;author=Greene%2CDJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="93."><p class="c-article-references__text" id="ref-CR93">Vanderwal, T., Kelly, C., Eilbott, J., Mayes, L. C. &amp; Castellanos, F. X. Inscapes: a movie paradigm to improve compliance in functional magnetic resonance imaging. <i>Neuroimage</i> <b>122</b>, 222–232 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26241683" aria-label="PubMed reference 93">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 93" href="http://scholar.google.com/scholar_lookup?&amp;title=Inscapes%3A%20a%20movie%20paradigm%20to%20improve%20compliance%20in%20functional%20magnetic%20resonance%20imaging&amp;journal=Neuroimage&amp;volume=122&amp;pages=222-232&amp;publication_year=2015&amp;author=Vanderwal%2CT&amp;author=Kelly%2CC&amp;author=Eilbott%2CJ&amp;author=Mayes%2CLC&amp;author=Castellanos%2CFX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="94."><p class="c-article-references__text" id="ref-CR94">Gordon, E. M. et al. Generation and evaluation of a cortical area parcellation from resting-state correlations. <i>Cereb. Cortex</i> <b>26</b>, 288–303 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25316338" aria-label="PubMed reference 94">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 94" href="http://scholar.google.com/scholar_lookup?&amp;title=Generation%20and%20evaluation%20of%20a%20cortical%20area%20parcellation%20from%20resting-state%20correlations&amp;journal=Cereb.%20Cortex&amp;volume=26&amp;pages=288-303&amp;publication_year=2016&amp;author=Gordon%2CEM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="95."><p class="c-article-references__text" id="ref-CR95">Seitzman, B. A. et al. A set of functionally-defined brain regions with improved representation of the subcortex and cerebellum. <i>Neuroimage</i> <b>206</b>, 116290 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31634545" aria-label="PubMed reference 95">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 95" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20set%20of%20functionally-defined%20brain%20regions%20with%20improved%20representation%20of%20the%20subcortex%20and%20cerebellum&amp;journal=Neuroimage&amp;volume=206&amp;publication_year=2020&amp;author=Seitzman%2CBA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="96."><p class="c-article-references__text" id="ref-CR96">Camacho, M. C. et al. EmoCodes: a standardized coding system for socio-emotional content in complex video stimuli. <i>Affect. Sci.</i> <b>3</b>, 168–181 (2022).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36046099" aria-label="PubMed reference 96">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9383008" aria-label="PubMed Central reference 96">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 96" href="http://scholar.google.com/scholar_lookup?&amp;title=EmoCodes%3A%20a%20standardized%20coding%20system%20for%20socio-emotional%20content%20in%20complex%20video%20stimuli&amp;journal=Affect.%20Sci.&amp;volume=3&amp;pages=168-181&amp;publication_year=2022&amp;author=Camacho%2CMC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="97."><p class="c-article-references__text" id="ref-CR97">McNamara, Q., De La Vega, A. &amp; Yarkoni, T. Developing a comprehensive framework for multimodal feature extraction. In <i>Proc. 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (eds Matwin, S. et al.) 1567–1574 (Association for Computing Machinery, 2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="98."><p class="c-article-references__text" id="ref-CR98">Nastase, S. A., Gazzola, V., Hasson, U. &amp; Keysers, C. Measuring shared responses across subjects using intersubject correlation. <i>Soc. Cogn. Affect. Neurosci.</i> <b>14</b>, 669–687 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 98" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20shared%20responses%20across%20subjects%20using%20intersubject%20correlation&amp;journal=Soc.%20Cogn.%20Affect.%20Neurosci.&amp;volume=14&amp;pages=669-687&amp;publication_year=2019&amp;author=Nastase%2CSA&amp;author=Gazzola%2CV&amp;author=Hasson%2CU&amp;author=Keysers%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="99."><p class="c-article-references__text" id="ref-CR99">Hasson, U., Nir, Y., Levy, I., Fuhrmann, G. &amp; Malach, R. Intersubject synchronization of cortical activity during natural vision. <i>Science</i> <b>303</b>, 1634–1640 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2cXhvFCnsbw%3D" aria-label="CAS reference 99">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15016991" aria-label="PubMed reference 99">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 99" href="http://scholar.google.com/scholar_lookup?&amp;title=Intersubject%20synchronization%20of%20cortical%20activity%20during%20natural%20vision&amp;journal=Science&amp;volume=303&amp;pages=1634-1640&amp;publication_year=2004&amp;author=Hasson%2CU&amp;author=Nir%2CY&amp;author=Levy%2CI&amp;author=Fuhrmann%2CG&amp;author=Malach%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="100."><p class="c-article-references__text" id="ref-CR100">Aviyente, S., Bernat, E. M., Evans, W. S. &amp; Sponheim, S. R. A phase synchrony measure for quantifying dynamic functional integration in the brain. <i>Hum. Brain Mapp.</i> <b>32</b>, 80–93 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20336687" aria-label="PubMed reference 100">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 100" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20phase%20synchrony%20measure%20for%20quantifying%20dynamic%20functional%20integration%20in%20the%20brain&amp;journal=Hum.%20Brain%20Mapp.&amp;volume=32&amp;pages=80-93&amp;publication_year=2011&amp;author=Aviyente%2CS&amp;author=Bernat%2CEM&amp;author=Evans%2CWS&amp;author=Sponheim%2CSR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="101."><p class="c-article-references__text" id="ref-CR101">Glerean, E., Salmi, J., Lahnakoski, J. M., Jääskeläinen, I. P. &amp; Sams, M. Functional magnetic resonance imaging phase synchronization as a measure of dynamic functional connectivity. <i>Brain Connect.</i> <b>2</b>, 91–101 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22559794" aria-label="PubMed reference 101">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3624768" aria-label="PubMed Central reference 101">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 101" href="http://scholar.google.com/scholar_lookup?&amp;title=Functional%20magnetic%20resonance%20imaging%20phase%20synchronization%20as%20a%20measure%20of%20dynamic%20functional%20connectivity&amp;journal=Brain%20Connect.&amp;volume=2&amp;pages=91-101&amp;publication_year=2012&amp;author=Glerean%2CE&amp;author=Salmi%2CJ&amp;author=Lahnakoski%2CJM&amp;author=J%C3%A4%C3%A4skel%C3%A4inen%2CIP&amp;author=Sams%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="102."><p class="c-article-references__text" id="ref-CR102">Petersen, A. C., Crockett, L., Richards, M. &amp; Boxer, A. A self-report measure of pubertal status: reliability, validity, and initial norms. <i>J. Youth Adolesc.</i> <b>17</b>, 117–133 (1988).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BC2c3gvVWisQ%3D%3D" aria-label="CAS reference 102">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24277579" aria-label="PubMed reference 102">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 102" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20self-report%20measure%20of%20pubertal%20status%3A%20reliability%2C%20validity%2C%20and%20initial%20norms&amp;journal=J.%20Youth%20Adolesc.&amp;volume=17&amp;pages=117-133&amp;publication_year=1988&amp;author=Petersen%2CAC&amp;author=Crockett%2CL&amp;author=Richards%2CM&amp;author=Boxer%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="103."><p class="c-article-references__text" id="ref-CR103">Finn, E. S. et al. Idiosynchrony: from shared responses to individual differences during naturalistic neuroimaging. <i>Neuroimage</i> <b>215</b>, 116828 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32276065" aria-label="PubMed reference 103">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 103" href="http://scholar.google.com/scholar_lookup?&amp;title=Idiosynchrony%3A%20from%20shared%20responses%20to%20individual%20differences%20during%20naturalistic%20neuroimaging&amp;journal=Neuroimage&amp;volume=215&amp;publication_year=2020&amp;author=Finn%2CES">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="104."><p class="c-article-references__text" id="ref-CR104">Mantel, N. The detection of disease clustering and a generalized regression approach. <i>Cancer Res.</i> <b>27</b>, 209–220 (1967).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaF2s%2FptlSnsA%3D%3D" aria-label="CAS reference 104">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=6018555" aria-label="PubMed reference 104">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 104" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20detection%20of%20disease%20clustering%20and%20a%20generalized%20regression%20approach&amp;journal=Cancer%20Res.&amp;volume=27&amp;pages=209-220&amp;publication_year=1967&amp;author=Mantel%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="105."><p class="c-article-references__text" id="ref-CR105">Benjamini, Y. &amp; Hochberg, Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing. <i>J. R. Stat. Soc. B</i> <b>57</b>, 289–300 (1995).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 105" href="http://scholar.google.com/scholar_lookup?&amp;title=Controlling%20the%20false%20discovery%20rate%3A%20a%20practical%20and%20powerful%20approach%20to%20multiple%20testing&amp;journal=J.%20R.%20Stat.%20Soc.%20B&amp;volume=57&amp;pages=289-300&amp;publication_year=1995&amp;author=Benjamini%2CY&amp;author=Hochberg%2CY">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-023-01358-9?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank A. Witherspoon for assistance with video coding, the families who participated in the Healthy Brain Network study and the Child Mind Institute for sharing the data publicly. This work was funded by the National Science Foundation (DGE-1745038 to M.C.C.) and the National Institutes of Health (HD102156 to M.C.C. and MH109589 to D.M.B.).</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Division of Biology and Biomedical Sciences, Washington University School of Medicine, St. Louis, MO, USA</p><p class="c-article-author-affiliation__authors-list">M. Catalina Camacho, Joseph P. Culver, Chad M. Sylvester &amp; Deanna M. Barch</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Psychiatry, Washington University School of Medicine, St. Louis, MO, USA</p><p class="c-article-author-affiliation__authors-list">Ashley N. Nielsen, Emily Furtado, Chad M. Sylvester &amp; Deanna M. Barch</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Psychological and Brain Sciences, Washington University in St. Louis, St. Louis, MO, USA</p><p class="c-article-author-affiliation__authors-list">Dori Balser, David C. Steinberger, Leah Fruchtman &amp; Deanna M. Barch</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Department of Radiology, Washington University School of Medicine, St. Louis, MO, USA</p><p class="c-article-author-affiliation__authors-list">Joseph P. Culver</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">Department of Physics, Washington University in St. Louis, St. Louis, MO, USA</p><p class="c-article-author-affiliation__authors-list">Joseph P. Culver</p></li><li id="Aff6"><p class="c-article-author-affiliation__address">Department of Biomedical Engineering, Washington University in St. Louis, St. Louis, MO, USA</p><p class="c-article-author-affiliation__authors-list">Joseph P. Culver</p></li><li id="Aff7"><p class="c-article-author-affiliation__address">Department of Electrical and Systems Engineering, Washington University in St. Louis, St. Louis, MO, USA</p><p class="c-article-author-affiliation__authors-list">Joseph P. Culver</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-M__Catalina-Camacho-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">M. Catalina Camacho</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=M.%20Catalina%20Camacho" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=M.%20Catalina%20Camacho" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22M.%20Catalina%20Camacho%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Ashley_N_-Nielsen-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Ashley N. Nielsen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Ashley%20N.%20Nielsen" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ashley%20N.%20Nielsen" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ashley%20N.%20Nielsen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Dori-Balser-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Dori Balser</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Dori%20Balser" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dori%20Balser" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dori%20Balser%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Emily-Furtado-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Emily Furtado</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Emily%20Furtado" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Emily%20Furtado" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Emily%20Furtado%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-David_C_-Steinberger-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">David C. Steinberger</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=David%20C.%20Steinberger" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=David%20C.%20Steinberger" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22David%20C.%20Steinberger%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Leah-Fruchtman-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Leah Fruchtman</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Leah%20Fruchtman" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Leah%20Fruchtman" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Leah%20Fruchtman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Joseph_P_-Culver-Aff1-Aff4-Aff5-Aff6-Aff7"><span class="c-article-authors-search__title u-h3 js-search-name">Joseph P. Culver</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Joseph%20P.%20Culver" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Joseph%20P.%20Culver" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Joseph%20P.%20Culver%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Chad_M_-Sylvester-Aff1-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Chad M. Sylvester</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Chad%20M.%20Sylvester" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chad%20M.%20Sylvester" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chad%20M.%20Sylvester%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Deanna_M_-Barch-Aff1-Aff2-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Deanna M. Barch</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Deanna%20M.%20Barch" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Deanna%20M.%20Barch" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Deanna%20M.%20Barch%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>M.C.C.: conceptualization, methodology, software, validation, formal analysis, data curation, writing—original draft, writing—review and editing, visualization and project administration. A.N.N.: conceptualization, methodology, supervision and writing—review and editing. D.B.: conceptualization, investigation and writing—review and editing. E.F.: conceptualization, writing—original draft and writing—review and editing. D.C.S.: conceptualization, investigation and writing—review and editing. L.F.: conceptualization, investigation and writing—review and editing. J.P.C.: conceptualization, supervision and writing—review and editing. C.M.S.: conceptualization, supervision and writing—review and editing. D.M.B.: conceptualization, methodology, resources, supervision, writing—review and editing and funding acquisition.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:camachoc@wustl.edu">M. Catalina Camacho</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading" id="FPar4">Competing interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div></section><section data-title="Peer review"><div class="c-article-section" id="peer-review-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="peer-review">Peer review</h2><div class="c-article-section__content" id="peer-review-content">
              
              
                <h3 class="c-article__sub-heading" id="FPar3">Peer review information</h3>
                <p><i>Nature Neuroscience</i> thanks Erik Nook, Heini Saarimäki and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Extended data"><div class="c-article-section" id="Sec32-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec32">Extended data</h2><div class="c-article-section__content" id="Sec32-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig7"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 1 emotion-specific activation d" href="/articles/s41593-023-01358-9/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig7_ESM.jpg">Extended Data Fig. 1 Emotion-specific activation differences maps.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>(a)</b> Difference maps for general emotions (column minus row). Mean activation maps are shown on the diagonal. <b>(b)</b> Difference maps for specific emotions (column minus row). Mean activation maps are shown on the diagonal.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig8"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 2 parcel-wise pearson correlati" href="/articles/s41593-023-01358-9/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig8_ESM.jpg">Extended Data Fig. 2 Parcel-wise Pearson Correlations between chronological age and activation to each emotion.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Age explained at most 3.8% of the variance in parcel-level activation (Age-Activation r2 range: Negative 0-0.017, Positive 0-0.030, Anger 0-0.020, Happy 0-0.026, Sad 0-0.023, Excited 0-0.028, Fearful 0-0.023; Puberty-Activation r2 range: Negative 0-0.029, Positive 0-0.030, Anger 0-0.024, Happy 0-0.038, Sad 0-0.014, Excited 0-0.030, Fearful 0-0.021).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig9"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 3 significant parcels and simil" href="/articles/s41593-023-01358-9/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig9_ESM.jpg">Extended Data Fig. 3 Significant parcels and similarity coefficients for each model of maturation after FDR correction.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>(a)</b> Average inter-subject correlations for each parcel for each movie. Significant parcels (one-sided Pearson’s r; permutation based and FDR-corrected p &lt; 0.05) are outlined in black. <b>(b)</b> Significant parcels and their coefficient magnitudes for the Convergence similarity model of maturation. <b>(c)</b> Significant parcels for the Nearest Neighbor and <b>(d)</b> Divergence models of maturation across chronological age and puberty.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig10"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 4 full sample dynamic analysis " href="/articles/s41593-023-01358-9/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig10_ESM.jpg">Extended Data Fig. 4 Full sample dynamic analysis results for The Present.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) Dynamic synchrony across the full sample with replicating peaks in synchrony shaded purple. Peaks at least 5 seconds wide and with a prominence higher than the 95<sup>th</sup> percentile value for that network (permutation-based p &lt; 0.001). Parcels were limited to those that were significantly correlated across the sample at the group level after FDR correction. (<b>b</b>) Video feature means within the peaks were compared to features outside of the peaks using a two-sided t-test to test if specific video features elicited increased synchronization. Plotted features were those that were significantly different (FDR-corrected p &lt; 0.05). Bars are plotted at the mean with dots indicating each measure used in computing the mean. (<b>C</b>) Violin plots of mean synchrony values by network. White dots indicate median value, the box the 50% interquartile range, and the whiskers each the upper and lower 25%. The dashed horizontal line indicates the value at permuted p &lt; 0.05 after FDR correction.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig11"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 5 dynamic synchrony analysis re" href="/articles/s41593-023-01358-9/figures/11" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig11_ESM.jpg">Extended Data Fig. 5 Dynamic Synchrony analysis results in the oldest children for <i>The Present</i>.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) Network dynamic activation similarity (synchrony) for each the oldest and youngest children in the sample. Included parcels are shown to the left of each trace. Shaded areas denote significant increases in synchrony (1.5 standard deviations above the mean) in the oldest children. (<b>b</b>) Bar plots of the mean value show the results from the video feature analysis comparing portions of the video within peaks of inter-subject synchrony to outside the peaks. Dots overlaid on the bar plots indicate each measure used in computing the mean Only features that significantly differed after FDR-correction are plotted.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig12"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 6 network-wise dynamic similari" href="/articles/s41593-023-01358-9/figures/12" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_Fig12_ESM.jpg">Extended Data Fig. 6 Network-wise dynamic similarity plots for 3 age groups: oldest, middle, and youngest, for each movie. Included parcels are shown to the left of each trace.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Shaded areas denote significant increases in synchrony (1.5 standard deviations above the mean) in the oldest children.</p></div></div></div></div></div></section><section data-title="Supplementary information"><div class="c-article-section" id="Sec33-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec33">Supplementary information</h2><div class="c-article-section__content" id="Sec33-content"><div data-test="supplementary-info"><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary information" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Information</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Supplementary Methods and Analyses (Appendix A and Appendix B) and Figures and Tables (Appendix C).</p></div></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="reporting summary" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-023-01358-9/MediaObjects/41593_2023_1358_MOESM2_ESM.pdf" data-supp-info-image="">Reporting Summary</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p>Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.</p><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Large-scale%20encoding%20of%20emotion%20concepts%20becomes%20increasingly%20similar%20between%20individuals%20from%20childhood%20to%20adolescence&amp;author=M.%20Catalina%20Camacho%20et%20al&amp;contentID=10.1038%2Fs41593-023-01358-9&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer%20Nature%20America%2C%20Inc.&amp;publication=1097-6256&amp;publicationDate=2023-06-08&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/s41593-023-01358-9" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41593-023-01358-9" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Camacho, M.C., Nielsen, A.N., Balser, D. <i>et al.</i> Large-scale encoding of emotion concepts becomes increasingly similar between individuals from childhood to adolescence.
                    <i>Nat Neurosci</i> <b>26</b>, 1256–1266 (2023). https://doi.org/10.1038/s41593-023-01358-9</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-023-01358-9?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2022-06-12">12 June 2022</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-05-12">12 May 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-06-08">08 June 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-07">July 2023</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/s41593-023-01358-9</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>

            
        <section>
            <div class="c-article-section js-article-section" id="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Dynamic Organization of Large-scale Functional Brain Networks Supports Interactions Between Emotion and Executive Control" href="https://doi.org/10.1007/s12264-023-01168-w">
                                        Dynamic Organization of Large-scale Functional Brain Networks Supports Interactions Between Emotion and Executive Control
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Haiyang Geng</li><li>Pengfei Xu</li><li>Yue-Jia Luo</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Neuroscience Bulletin</i> (2024)</p>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </section>
    

            
        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="js-context-bar-sticky-point-desktop">
        

        
            <noscript>
                
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01358-9.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </noscript>
            <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
                <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-right-column
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-023-01358-9.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </div>
        
    </div>

    
        
    

    
    

    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>
                            
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=s41593-023-01358-9;doi=10.1038/s41593-023-01358-9;techmeta=36,59;subjmeta=1457,2571,2645,378,631;kwrd=Development+of+the+nervous+system,Emotion,Social+neuroscience">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=795205588&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-023-01358-9%26doi%3D10.1038/s41593-023-01358-9%26techmeta%3D36,59%26subjmeta%3D1457,2571,2645,378,631%26kwrd%3DDevelopment+of+the+nervous+system,Emotion,Social+neuroscience">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=795205588&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-023-01358-9%26doi%3D10.1038/s41593-023-01358-9%26techmeta%3D36,59%26subjmeta%3D1457,2571,2645,378,631%26kwrd%3DDevelopment+of+the+nervous+system,Emotion,Social+neuroscience"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>


    
        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-and-analysis"
                                   data-track="click"
                                   data-track-action="reviews &amp; analysis"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Reviews &amp; Analysis
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/video"
                                   data-track="click"
                                   data-track-action="videos"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Videos
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/current-issue"
                                   data-track="click"
                                   data-track-action="current issue"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Current issue
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>
                        
                    
                </ul>
                <ul class="c-header__list c-header__list--js-stack">
                    
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/natureneuro"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on Twitter
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;6"
                               rel="nofollow"
                               data-track="click"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/neuro.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </nav>
    
    
        
            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/aims"
                                   data-track="click"
                                   data-track-action="aims &amp; scope"
                                   data-track-label="link">
                                    Aims &amp; Scope
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-information"
                                   data-track="click"
                                   data-track-action="journal information"
                                   data-track-label="link">
                                    Journal Information
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-impact"
                                   data-track="click"
                                   data-track-action="journal metrics"
                                   data-track-label="link">
                                    Journal Metrics
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editors"
                                   data-track="click"
                                   data-track-action="about the editors"
                                   data-track-label="link">
                                    About the Editors
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="research cross-journal editorial team"
                                   data-track-label="link">
                                    Research Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="reviews cross-journal editorial team"
                                   data-track-label="link">
                                    Reviews Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/our-publishing-models"
                                   data-track="click"
                                   data-track-action="our publishing models"
                                   data-track-label="link">
                                    Our publishing models
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-values-statement"
                                   data-track="click"
                                   data-track-action="editorial values statement"
                                   data-track-label="link">
                                    Editorial Values Statement
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-policies"
                                   data-track="click"
                                   data-track-action="editorial policies"
                                   data-track-label="link">
                                    Editorial Policies
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/content"
                                   data-track="click"
                                   data-track-action="content types"
                                   data-track-label="link">
                                    Content Types
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/web-feeds"
                                   data-track="click"
                                   data-track-action="web feeds"
                                   data-track-label="link">
                                    Web Feeds
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/posters"
                                   data-track="click"
                                   data-track-action="posters"
                                   data-track-label="link">
                                    Posters
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        

        
            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/submission-guidelines"
                                   data-track="click"
                                   data-track-action="submission guidelines"
                                   data-track-label="link">
                                    Submission Guidelines
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/for-reviewers"
                                   data-track="click"
                                   data-track-action="for reviewers"
                                   data-track-label="link">
                                    For Reviewers
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="click||nav_language_services"
                                   data-track-context="header publish with us dropdown menu"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://mts-nn.nature.com/cgi-bin/main.plex?form_type&#x3D;home&amp;from_idp&#x3D;1"
                                   data-track="click"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external>Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        
    


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">
                            
                                
                                    <option value="" selected>All journals</option>
                                    <option value="neuro">This journal</option>
                                
                            
                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        

        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
            

            <p class="c-meta u-ma-0 u-flex-shrink">
                <span class="c-meta__item">
                    Nature Neuroscience (<i>Nat Neurosci</i>)
                </span>
                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="onlineIssn">1546-1726</span> (online)
    </span>
    


                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="printIssn">1097-6256</span> (print)
    </span>
    

            </p>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.protocols.io/"
                                                  data-track="click" data-track-action="protocols.io"
                                                  data-track-label="link">protocols.io</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natitaly"
                                                  data-track="click" data-track-action="nature Italy"
                                                  data-track-label="link">Nature Italy</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>
            
        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-ea63208b81.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2024 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path id="shape" fill-rule="evenodd" clip-rule="evenodd" d="M1 3.78571C1 2.75867 1.85698 2 2.8209 2H6.1791C7.14302 2 8 2.75867 8 3.78571V4H11.1668C11.885 4 12.5585 4.42017 12.8494 5.07033C12.9893 4.98169 13.1425 4.91101 13.3056 4.86206L16.5222 3.89704C17.4454 3.62005 18.4843 4.10046 18.7794 5.08419L22.9256 18.9042C23.2207 19.8878 22.618 20.8608 21.6947 21.1378L18.4781 22.1029C17.5548 22.3799 16.516 21.8993 16.2209 20.9157L13.0001 10.1804V20.2143C13.0001 21.255 12.1231 22 11.1668 22H7.83346C7.54206 22 7.25803 21.9308 7.00392 21.8052C6.75263 21.9305 6.47077 22 6.1791 22H2.8209C1.85693 22 1 21.2412 1 20.2143V3.78571ZM3 4V15H6V4H3ZM3 20V17H6V20H3ZM18.0749 20.1358L17.2129 17.2623L20.0863 16.4002L20.9484 19.2737L18.0749 20.1358ZM19.5116 14.4846L16.6381 15.3466L14.0519 6.72624L16.9254 5.86416L19.5116 14.4846ZM8.00012 20L8.00012 6H11.0001L11.0001 20H8.00012Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</footer>




    

    

<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">

    
    <div class="c-site-messages__banner-large">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing" src="/static/images/logos/nature-briefing-logo-n150-white-d81c9da3ec.svg" width="250" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing</em> newsletter — what matters in science, free to your inbox daily.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/briefing" method="post" data-location="banner" data-track="submit||nature_briefing_sign_up" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="MainBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick">
                        <input type="hidden" value="false" name="marketing" id="marketing_input">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick">
                        <input type="hidden" value="MainBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">
                            
                            <input type="hidden" value="true" name="N:nature_briefing_daily" id="defaultNewsletter">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>

    
    <div class="c-site-messages__banner-small">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner">Sign up for Nature Briefing
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/s41593-023-01358-9&amp;format=js&amp;last_modified=2023-07-27" async></script>
<img src="/70j2tkdl/article/s41593-023-01358-9" width="1" height="1" alt="" class="u-visually-hidden">
</body>
</html>