<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>Elucidating the underlying components of food valuation in the human orbitofrontal cortex | Nature Neuroscience</title>
    
        
<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/neuro.rss"/>


    
        

        <script id="save-data-connection-testing">
            function hasConnection() {
                return navigator.connection || navigator.mozConnection || navigator.webkitConnection || navigator.msConnection;
            }

            function createLink(src) {
                var preloadLink = document.createElement("link");
                preloadLink.rel = "preload";
                preloadLink.href = src;
                preloadLink.as = "font";
                preloadLink.type = "font/woff2";
                preloadLink.crossOrigin = "";
                document.head.insertBefore(preloadLink, document.head.firstChild);
            }

            var connectionDetail = {
                saveDataEnabled: false,
                slowConnection: false
            };

            var connection = hasConnection();
            if (connection) {
                connectionDetail.saveDataEnabled = connection.saveData;
                if (/\slow-2g|2g/.test(connection.effectiveType)) {
                    connectionDetail.slowConnection = true;
                }
            }

            if (!(connectionDetail.saveDataEnabled || connectionDetail.slowConnection)) {
                createLink("/static/fonts/HardingText-Regular-Web-cecd90984f.woff2");
            } else {
                document.documentElement.classList.add('save-data');
            }
        </script>
    

<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"decision;reward","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Nature Neuroscience","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article"}},"article":{"doi":"10.1038/s41593-017-0008-x"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["Shinsuke Suzuki","Logan Cross","John P. O’Doherty"],"publishedAt":1508716800,"publishedAtString":"2017-10-23","title":"Elucidating the underlying components of food valuation in the human orbitofrontal cortex","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"neuro","title":"nature neuroscience","volume":"20","issue":"12"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"nature-onwards-journey","active":false},{"name":"getftr-entitled","active":false},{"name":"paywall_recommendations","active":true}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"US","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://collect.nature.com',
        imprint: 'nature'
    });
</script>

<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
 



     
    
    
        
    
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}button:focus{outline:3px solid #fece3e;will-change:transform}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:8px}.c-recommendations-list__item .c-card{flex-basis:100%}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 0 0 16px;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #cedbe0;height:auto;min-height:0;position:static}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0 0 8px;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:inherit}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}.c-card__title-recommendation .MathJax_Display{display:inline!important}.c-card__link:not(.c-card__link--no-block-link):before{z-index:1}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a,.c-article-recommendations-card__link{color:inherit}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-article-recommendations-card__meta-type,.c-meta .c-meta__item:first-child{font-weight:700}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:539px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px 13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.c-pdf-download__link{padding:13px 24px} } </style>




    
        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    
    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-122346e276.css" media="print">
    



<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">


<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h.indexOf('preview-www.nature.com') > -1) return;

            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('nature.com') > -1) {
                if (h.indexOf('test-www.nature.com') > -1) {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-26e142e9c6.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    
                        {src: '/static/js/global-article-es6-bundle-782fd09f66.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-d66d49033d.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-aca08c055a.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-4fba787158.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-1fe07484e5.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;
                        
                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-8fc1a30809.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-e0c7186f28.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-2399be388c.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">





    
    <script type="application/ld+json">{"mainEntity":{"headline":"Elucidating the underlying components of food valuation in the human orbitofrontal cortex","description":"The valuation of food is a fundamental component of our decision-making. Yet little is known about how value signals for food and other rewards are constructed by the brain. Using a food-based decision task in human participants, we found that subjective values can be predicted from beliefs about constituent nutritive attributes of food: protein, fat, carbohydrates and vitamin content. Multivariate analyses of functional MRI data demonstrated that, while food value is represented in patterns of neural activity in both medial and lateral parts of the orbitofrontal cortex (OFC), only the lateral OFC represents the elemental nutritive attributes. Effective connectivity analyses further indicate that information about the nutritive attributes represented in the lateral OFC is integrated within the medial OFC to compute an overall value. These findings provide a mechanistic account for the construction of food value from its constituent nutrients. Suzuki et al. found that food valuation is related to beliefs about nutritive attributes. Functional MRI revealed these attribute codes in lateral orbitofrontal cortex, suggesting a mechanism by which value signals are constructed from constituent attributes.","datePublished":"2017-10-23T00:00:00Z","dateModified":"2017-10-23T00:00:00Z","pageStart":"1780","pageEnd":"1786","sameAs":"https://doi.org/10.1038/s41593-017-0008-x","keywords":["Decision","Reward","Biomedicine","general","Neurosciences","Behavioral Sciences","Biological Techniques","Neurobiology","Animal Genetics and Genomics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig1_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig2_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig3_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig4_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig5_HTML.jpg"],"isPartOf":{"name":"Nature Neuroscience","issn":["1546-1726","1097-6256"],"volumeNumber":"20","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Shinsuke Suzuki","url":"http://orcid.org/0000-0002-9816-9423","affiliation":[{"name":"California Institute of Technology","address":{"name":"Division of the Humanities and Social Sciences, California Institute of Technology, Pasadena, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Tohoku University","address":{"name":"Frontier Research Institute for Interdisciplinary Sciences, Tohoku University, Sendai, Japan","@type":"PostalAddress"},"@type":"Organization"},{"name":"Tohoku University","address":{"name":"Institute of Development, Aging and Cancer, Tohoku University, Sendai, Japan","@type":"PostalAddress"},"@type":"Organization"}],"email":"shinsuke.szk@gmail.com","@type":"Person"},{"name":"Logan Cross","affiliation":[{"name":"California Institute of Technology","address":{"name":"Computation and Neural Systems, California Institute of Technology, Pasadena, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"John P. O’Doherty","affiliation":[{"name":"California Institute of Technology","address":{"name":"Division of the Humanities and Social Sciences, California Institute of Technology, Pasadena, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"California Institute of Technology","address":{"name":"Computation and Neural Systems, California Institute of Technology, Pasadena, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>




    
    
    


    
    <link rel="canonical" href="https://www.nature.com/articles/s41593-017-0008-x">
    
    
    <meta name="journal_id" content="41593"/>
    <meta name="dc.title" content="Elucidating the underlying components of food valuation in the human orbitofrontal cortex"/>
    <meta name="dc.source" content="Nature Neuroscience 2017 20:12"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2017-10-23"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2017 The Author(s)"/>
    <meta name="dc.rights" content="2017 The Author(s)"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="The valuation of food is a fundamental component of our decision-making. Yet little is known about how value signals for food and other rewards are constructed by the brain. Using a food-based decision task in human participants, we found that subjective values can be predicted from beliefs about constituent nutritive attributes of food: protein, fat, carbohydrates and vitamin content. Multivariate analyses of functional MRI data demonstrated that, while food value is represented in patterns of neural activity in both medial and lateral parts of the orbitofrontal cortex (OFC), only the lateral OFC represents the elemental nutritive attributes. Effective connectivity analyses further indicate that information about the nutritive attributes represented in the lateral OFC is integrated within the medial OFC to compute an overall value. These findings provide a mechanistic account for the construction of food value from its constituent nutrients. Suzuki et al. found that food valuation is related to beliefs about nutritive attributes. Functional MRI revealed these attribute codes in lateral orbitofrontal cortex, suggesting a mechanism by which value signals are constructed from constituent attributes."/>
    <meta name="prism.issn" content="1546-1726"/>
    <meta name="prism.publicationName" content="Nature Neuroscience"/>
    <meta name="prism.publicationDate" content="2017-10-23"/>
    <meta name="prism.volume" content="20"/>
    <meta name="prism.number" content="12"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="1780"/>
    <meta name="prism.endingPage" content="1786"/>
    <meta name="prism.copyright" content="2017 The Author(s)"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/s41593-017-0008-x"/>
    <meta name="prism.doi" content="doi:10.1038/s41593-017-0008-x"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/s41593-017-0008-x.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/s41593-017-0008-x"/>
    <meta name="citation_journal_title" content="Nature Neuroscience"/>
    <meta name="citation_journal_abbrev" content="Nat Neurosci"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="1546-1726"/>
    <meta name="citation_title" content="Elucidating the underlying components of food valuation in the human orbitofrontal cortex"/>
    <meta name="citation_volume" content="20"/>
    <meta name="citation_issue" content="12"/>
    <meta name="citation_publication_date" content="2017/12"/>
    <meta name="citation_online_date" content="2017/10/23"/>
    <meta name="citation_firstpage" content="1780"/>
    <meta name="citation_lastpage" content="1786"/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/s41593-017-0008-x"/>
    <meta name="DOI" content="10.1038/s41593-017-0008-x"/>
    <meta name="size" content="240507"/>
    <meta name="citation_doi" content="10.1038/s41593-017-0008-x"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/s41593-017-0008-x&amp;api_key="/>
    <meta name="description" content="The valuation of food is a fundamental component of our decision-making. Yet little is known about how value signals for food and other rewards are constructed by the brain. Using a food-based decision task in human participants, we found that subjective values can be predicted from beliefs about constituent nutritive attributes of food: protein, fat, carbohydrates and vitamin content. Multivariate analyses of functional MRI data demonstrated that, while food value is represented in patterns of neural activity in both medial and lateral parts of the orbitofrontal cortex (OFC), only the lateral OFC represents the elemental nutritive attributes. Effective connectivity analyses further indicate that information about the nutritive attributes represented in the lateral OFC is integrated within the medial OFC to compute an overall value. These findings provide a mechanistic account for the construction of food value from its constituent nutrients. Suzuki et al. found that food valuation is related to beliefs about nutritive attributes. Functional MRI revealed these attribute codes in lateral orbitofrontal cortex, suggesting a mechanism by which value signals are constructed from constituent attributes."/>
    <meta name="dc.creator" content="Suzuki, Shinsuke"/>
    <meta name="dc.creator" content="Cross, Logan"/>
    <meta name="dc.creator" content="O&#8217;Doherty, John P."/>
    <meta name="dc.subject" content="Decision"/>
    <meta name="dc.subject" content="Reward"/>
    <meta name="citation_reference" content="citation_journal_title=Soc. Cogn. Affect. Neurosci.; citation_title=Informatic parcellation of the network involved in the computation of subjective value; citation_author=JA Clithero, A Rangel; citation_volume=9; citation_publication_date=2014; citation_pages=1289-1302; citation_doi=10.1093/scan/nst106; citation_id=CR1"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Neurons in the orbitofrontal cortex encode economic value; citation_author=C Padoa-Schioppa, JA Assad; citation_volume=441; citation_publication_date=2006; citation_pages=223-226; citation_doi=10.1038/nature04676; citation_id=CR2"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Decoding subjective decisions from orbitofrontal cortex; citation_author=EL Rich, JD Wallis; citation_volume=19; citation_publication_date=2016; citation_pages=973-980; citation_doi=10.1038/nn.4320; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=The orbitofrontal oracle: cortical mechanisms for the prediction and evaluation of specific behavioral outcomes; citation_author=PH Rudebeck, EA Murray; citation_volume=84; citation_publication_date=2014; citation_pages=1143-1156; citation_doi=10.1016/j.neuron.2014.10.049; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Value, pleasure and choice in the ventral prefrontal cortex; citation_author=F Grabenhorst, ET Rolls; citation_volume=15; citation_publication_date=2011; citation_pages=56-67; citation_doi=10.1016/j.tics.2010.12.004; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex; citation_author=D McNamee, A Rangel, JP O&#8217;Doherty; citation_volume=16; citation_publication_date=2013; citation_pages=479-485; citation_doi=10.1038/nn.3337; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Population coding of affect across stimuli, modalities and individuals; citation_author=J Chikazoe, DH Lee, N Kriegeskorte, AK Anderson; citation_volume=17; citation_publication_date=2014; citation_pages=1114-1122; citation_doi=10.1038/nn.3749; citation_id=CR7"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Identity-specific coding of future rewards in the human orbitofrontal cortex; citation_author=JD Howard, JA Gottfried, PN Tobler, T Kahnt; citation_volume=112; citation_publication_date=2015; citation_pages=5195-5200; citation_doi=10.1073/pnas.1503550112; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=An automatic valuation system in the human brain: evidence from functional neuroimaging; citation_author=M Lebreton, S Jorge, V Michel, B Thirion, M Pessiglione; citation_volume=64; citation_publication_date=2009; citation_pages=431-439; citation_doi=10.1016/j.neuron.2009.09.040; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Dissociation of neural representation of intensity and affective valuation in human gustation; citation_author=DM Small; citation_volume=39; citation_publication_date=2003; citation_pages=701-711; citation_doi=10.1016/S0896-6273(03)00467-7; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=The neural correlates of subjective value during intertemporal choice; citation_author=JW Kable, PW Glimcher; citation_volume=10; citation_publication_date=2007; citation_pages=1625-1633; citation_doi=10.1038/nn2007; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Commun; citation_title=Orbitofrontal neurons infer the value and identity of predicted outcomes; citation_author=TA Stalnaker; citation_volume=5; citation_publication_date=2014; citation_doi=10.1038/ncomms4926; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Value signals in the prefrontal cortex predict individual preferences across reward categories; citation_author=J Gross; citation_volume=34; citation_publication_date=2014; citation_pages=7580-7586; citation_doi=10.1523/JNEUROSCI.5082-13.2014; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Evidence for a common representation of decision values for dissimilar goods in human ventromedial prefrontal cortex; citation_author=VS Chib, A Rangel, S Shimojo, JP O&#8217;Doherty; citation_volume=29; citation_publication_date=2009; citation_pages=12315-12320; citation_doi=10.1523/JNEUROSCI.2575-09.2009; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Comparing apples and oranges: using reward-specific and reward-general subjective value representation in the brain; citation_author=DJ Levy, PW Glimcher; citation_volume=31; citation_publication_date=2011; citation_pages=14693-14707; citation_doi=10.1523/JNEUROSCI.2218-11.2011; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Learning to simulate others&#8217; decisions; citation_author=S Suzuki; citation_volume=74; citation_publication_date=2012; citation_pages=1125-1137; citation_doi=10.1016/j.neuron.2012.04.030; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Neural mechanisms underlying human consensus decision-making; citation_author=S Suzuki, R Adachi, S Dunne, P Bossaerts, JP O&#8217;Doherty; citation_volume=86; citation_publication_date=2015; citation_pages=591-602; citation_doi=10.1016/j.neuron.2015.03.019; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Neural mechanisms supporting maladaptive food choices in anorexia nervosa; citation_author=K Foerde, JE Steinglass, D Shohamy, BT Walsh; citation_volume=18; citation_publication_date=2015; citation_pages=1571-1573; citation_doi=10.1038/nn.4136; citation_id=CR18"/>
    <meta name="citation_reference" content="citation_journal_title=Obes. Rev.; citation_title=Neuroimaging and obesity: current knowledge and future directions; citation_author=S Carnell, C Gibson, L Benson, CN Ochner, A Geliebter; citation_volume=13; citation_publication_date=2012; citation_pages=43-56; citation_doi=10.1111/j.1467-789X.2011.00927.x; citation_id=CR19"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Online evaluation of novel choices by simultaneous representation of multiple memories; citation_author=HC Barron, RJ Dolan, TEJ Behrens; citation_volume=16; citation_publication_date=2013; citation_pages=1492-1498; citation_doi=10.1038/nn.3515; citation_id=CR20"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Segregated encoding of reward-identity and stimulus-reward associations in human orbitofrontal cortex; citation_author=MC Klein-Fl&#252;gge, HC Barron, KH Brodersen, RJ Dolan, TEJ Behrens; citation_volume=33; citation_publication_date=2013; citation_pages=3202-3211; citation_doi=10.1523/JNEUROSCI.2532-12.2013; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans; citation_author=D Ong&#252;r, JL Price; citation_volume=10; citation_publication_date=2000; citation_pages=206-219; citation_doi=10.1093/cercor/10.3.206; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Sci.; citation_title=Behavioral and neural valuation of foods is driven by implicit knowledge of caloric content; citation_author=DW Tang, LK Fellows, A Dagher; citation_volume=25; citation_publication_date=2014; citation_pages=2168-2176; citation_doi=10.1177/0956797614552081; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=Cell; citation_title=Food for the brain; citation_author=CS Zuker; citation_volume=161; citation_publication_date=2015; citation_pages=9-11; citation_doi=10.1016/j.cell.2015.03.016; citation_id=CR24"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Food reward in the absence of taste receptor signaling; citation_author=IE Araujo; citation_volume=57; citation_publication_date=2008; citation_pages=930-941; citation_doi=10.1016/j.neuron.2008.01.032; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Separate circuitries encode the hedonic and nutritional values of sugar; citation_author=LA Tellez; citation_volume=19; citation_publication_date=2016; citation_pages=465-470; citation_doi=10.1038/nn.4224; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=A primer on pattern-based approaches to fMRI: principles, pitfalls, and perspectives; citation_author=J-D Haynes; citation_volume=87; citation_publication_date=2015; citation_pages=257-270; citation_doi=10.1016/j.neuron.2015.05.025; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Valid conjunction inference with the minimum statistic; citation_author=T Nichols, M Brett, J Andersson, T Wager, J-B Poline; citation_volume=25; citation_publication_date=2005; citation_pages=653-660; citation_doi=10.1016/j.neuroimage.2004.12.005; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Representational geometry: integrating cognition, computation, and the brain; citation_author=N Kriegeskorte, RA Kievit; citation_volume=17; citation_publication_date=2013; citation_pages=401-412; citation_doi=10.1016/j.tics.2013.06.007; citation_id=CR29"/>
    <meta name="citation_reference" content="Efron, B. &amp; Tibshirani, R. J. An Introduction to the Bootstrap (CRC Press, 1993)."/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Ubiquity and specificity of reinforcement signals throughout the human brain; citation_author=TJ Vickery, MM Chun, D Lee; citation_volume=72; citation_publication_date=2011; citation_pages=166-177; citation_doi=10.1016/j.neuron.2011.08.011; citation_id=CR31"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Disentangling neural representations of value and salience in the human brain; citation_author=T Kahnt, SQ Park, J-D Haynes, PN Tobler; citation_volume=111; citation_publication_date=2014; citation_pages=5000-5005; citation_doi=10.1073/pnas.1320189111; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Encoding predictive reward value in human amygdala and orbitofrontal cortex; citation_author=JA Gottfried, J O&#8217;Doherty, RJ Dolan; citation_volume=301; citation_publication_date=2003; citation_pages=1104-1107; citation_doi=10.1126/science.1087919; citation_id=CR33"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Neurosci.; citation_title=Object vision and spatial vision: two cortical pathways; citation_author=M Mishkin, LG Ungerleider, KA Macko; citation_volume=6; citation_publication_date=1983; citation_pages=414-417; citation_doi=10.1016/0166-2236(83)90190-X; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Identity-specific reward representations in orbitofrontal cortex are modulated by selective devaluation; citation_author=JD Howard, T Kahnt; citation_volume=37; citation_publication_date=2017; citation_pages=2627-2638; citation_doi=10.1523/JNEUROSCI.3473-16.2017; citation_id=CR35"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex; citation_author=MP Noonan; citation_volume=107; citation_publication_date=2010; citation_pages=20547-20552; citation_doi=10.1073/pnas.1012246107; citation_id=CR36"/>
    <meta name="citation_reference" content="citation_journal_title=Annu. Rev. Nutr.; citation_title=Food likes and dislikes; citation_author=P Rozin, TA Vollmecke; citation_volume=6; citation_publication_date=1986; citation_pages=433-456; citation_doi=10.1146/annurev.nu.06.070186.002245; citation_id=CR37"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Focusing attention on the health aspects of foods changes value signals in vmPFC and improves dietary choice; citation_author=TA Hare, J Malmaud, A Rangel; citation_volume=31; citation_publication_date=2011; citation_pages=11077-11087; citation_doi=10.1523/JNEUROSCI.6383-10.2011; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=Behav Sci; citation_title=Measuring utility by a single-response sequential method; citation_author=GM Becker, MH DeGroot, J Marschak; citation_volume=9; citation_publication_date=1964; citation_pages=226-232; citation_doi=10.1002/bs.3830090304; citation_id=CR39"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Optimized EPI for fMRI studies of the orbitofrontal cortex; citation_author=R Deichmann, JA Gottfried, C Hutton, R Turner; citation_volume=19; citation_publication_date=2003; citation_pages=430-441; citation_doi=10.1016/S1053-8119(03)00073-9; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Neuroinform.; citation_title=The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data; citation_author=MN Hebart, K G&#246;rgen, J-D Haynes; citation_volume=8; citation_publication_date=2015; citation_doi=10.3389/fninf.2014.00088; citation_id=CR41"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain; citation_author=N Tzourio-Mazoyer; citation_volume=15; citation_publication_date=2002; citation_pages=273-289; citation_doi=10.1006/nimg.2001.0978; citation_id=CR42"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Valid population inference for information-based imaging: From the second-level t-test to prevalence inference; citation_author=C Allefeld, K G&#246;rgen, J-D Haynes; citation_volume=141; citation_publication_date=2016; citation_pages=378-392; citation_doi=10.1016/j.neuroimage.2016.07.040; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Information-based functional brain mapping; citation_author=N Kriegeskorte, R Goebel, P Bandettini; citation_volume=103; citation_publication_date=2006; citation_pages=3863-3868; citation_doi=10.1073/pnas.0600244103; citation_id=CR44"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Characterizing the associative content of brain structures involved in habitual and goal-directed actions in humans: a multivariate fMRI study; citation_author=D McNamee, M Liljeholm, O Zika, JP O&#8217;Doherty; citation_volume=35; citation_publication_date=2015; citation_pages=3764-3771; citation_doi=10.1523/JNEUROSCI.4677-14.2015; citation_id=CR45"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Syst. Neurosci.; citation_title=Representational similarity analysis - connecting the branches of systems neuroscience; citation_author=N Kriegeskorte, M Mur, P Bandettini; citation_volume=2; citation_publication_date=2008; citation_doi=10.3389/neuro.01.016.2008; citation_id=CR46"/>
    <meta name="citation_reference" content="Friston, K.J., Ashburner, J.T., Kiebel, S.J., Nichols, T.E. &amp; Penny, W.D. Statistical Parametric Mapping: the Analysis of Functional Brain Images (Academic Press, 2006)."/>
    <meta name="citation_author" content="Suzuki, Shinsuke"/>
    <meta name="citation_author_institution" content="Division of the Humanities and Social Sciences, California Institute of Technology, Pasadena, USA"/>
    <meta name="citation_author_institution" content="Frontier Research Institute for Interdisciplinary Sciences, Tohoku University, Sendai, Japan"/>
    <meta name="citation_author_institution" content="Institute of Development, Aging and Cancer, Tohoku University, Sendai, Japan"/>
    <meta name="citation_author" content="Cross, Logan"/>
    <meta name="citation_author_institution" content="Computation and Neural Systems, California Institute of Technology, Pasadena, USA"/>
    <meta name="citation_author" content="O&#8217;Doherty, John P."/>
    <meta name="citation_author_institution" content="Division of the Humanities and Social Sciences, California Institute of Technology, Pasadena, USA"/>
    <meta name="citation_author_institution" content="Computation and Neural Systems, California Institute of Technology, Pasadena, USA"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@natureneuro"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Elucidating the underlying components of food valuation in the human orbitofrontal cortex"/>
    <meta name="twitter:description" content="Nature Neuroscience - Suzuki et al. found that food valuation is related to beliefs about nutritive attributes. Functional MRI revealed these attribute codes in lateral orbitofrontal cortex,..."/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig1_HTML.jpg"/>
    

    
    
    <meta property="og:url" content="https://www.nature.com/articles/s41593-017-0008-x"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="Elucidating the underlying components of food valuation in the human orbitofrontal cortex - Nature Neuroscience"/>
    <meta property="og:description" content="Suzuki et al. found that food valuation is related to beliefs about nutritive attributes. Functional MRI revealed these attribute codes in lateral orbitofrontal cortex, suggesting a mechanism by which value signals are constructed from constituent attributes."/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig1_HTML.jpg"/>
    

    <script>
        window.eligibleForRa21 = 'true'; 
    </script>
</head>
<body class="article-page">

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>



<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
        
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>

    

    <div class="u-lazy-ad-wrapper u-mbs-0">
            <div class="deferred-placeholder" data-replace="true"
                 data-placeholder="/placeholder/v1/institutionalBanner?bpids=[bpids] #institutional-banner-container"></div>
            <aside class="c-ad c-ad--728x90">
                <div class="c-ad__inner" data-container-type="banner-advert">
                    <p class="c-ad__label">Advertisement</p>
                    
        
            
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=s41593-017-0008-x;doi=10.1038/s41593-017-0008-x;subjmeta=1409,1788,2649,378,631;kwrd=Decision,Reward">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-1276301892&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-017-0008-x%26doi%3D10.1038/s41593-017-0008-x%26subjmeta%3D1409,1788,2649,378,631%26kwrd%3DDecision,Reward">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-1276301892&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-017-0008-x%26doi%3D10.1038/s41593-017-0008-x%26subjmeta%3D1409,1788,2649,378,631%26kwrd%3DDecision,Reward"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

        
    
                </div>
            </aside>
        </div>
    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#00928c">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">
                    
                    
                    <div class="c-header__logo-container">
                        
                        <a href="/neuro"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-0ccc487532906d646419e51f647ce79a.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-880e5942f43b9213989c58a04ab5c8e6.svg" height="32" alt="Nature Neuroscience">
                            </picture>
                        </a>
                    
                    </div>
                    
                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">
                            
                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41593-017-0008-x'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
                            
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        
            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">
                            
                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span><span class="c-header__show-text">Explore</span> content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                                
                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>
                                
                            
                            
                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">
                            
                                <li class="c-header__item">
                                    <a class="c-header__link"
                                       href="https://idp.nature.com/auth/personal/springernature?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Fmy-account%2Falerts%2Fsubscribe-journal%3Flist-id%3D6"
                                       rel="nofollow"
                                       data-track="click"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/neuro.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        
    </header>


    
    
        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature neuroscience"><span itemprop="name">nature neuroscience</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro/articles?type&#x3D;article" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="name">articles</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>
    



    

</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
        
            <div class="c-context-bar u-hide"
                 data-test="context-bar"
                 data-context-bar
                 aria-hidden="true">
                <div class="c-context-bar__container u-container">
                    <div class="c-context-bar__title">
                        Elucidating the underlying components of food valuation in the human orbitofrontal cortex
                    </div>
                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-017-0008-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                </div>
            </div>
        
        <article lang="en">
            
            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Article</li>
    
    

                        <li class="c-article-identifiers__item">Published: <time datetime="2017-10-23">23 October 2017</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Elucidating the underlying components of food valuation in the human orbitofrontal cortex</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Shinsuke-Suzuki-Aff1-Aff2-Aff3" data-author-popup="auth-Shinsuke-Suzuki-Aff1-Aff2-Aff3" data-author-search="Suzuki, Shinsuke" data-corresp-id="c1">Shinsuke Suzuki<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0002-9816-9423"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-9816-9423</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a>,<a href="#Aff3">3</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Logan-Cross-Aff4" data-author-popup="auth-Logan-Cross-Aff4" data-author-search="Cross, Logan">Logan Cross</a><sup class="u-js-hide"><a href="#Aff4">4</a></sup> &amp; </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-John_P_-O_Doherty-Aff1-Aff4" data-author-popup="auth-John_P_-O_Doherty-Aff1-Aff4" data-author-search="O’Doherty, John P.">John P. O’Doherty</a><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff4">4</a></sup> </li></ul>

                    

                    <p class="c-article-info-details" data-container-section="info">
                        
    <a data-test="journal-link" href="/neuro" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Nature Neuroscience</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 20</b>, <span class="u-visually-hidden">pages </span>1780–1786 (<span data-test="article-publication-year">2017</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>
                    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">7473 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">115 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">59 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/articles/s41593-017-0008-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
                    
                </header>

                
    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/decision" data-track="click" data-track-action="view subject" data-track-label="link">Decision</a></li><li class="c-article-subject-list__subject"><a href="/subjects/reward" data-track="click" data-track-action="view subject" data-track-label="link">Reward</a></li>
        </ul>
    </div>

                
    
    

    
    

                
            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>The valuation of food is a fundamental component of our decision-making. Yet little is known about how value signals for food and other rewards are constructed by the brain. Using a food-based decision task in human participants, we found that subjective values can be predicted from beliefs about constituent nutritive attributes of food: protein, fat, carbohydrates and vitamin content. Multivariate analyses of functional MRI data demonstrated that, while food value is represented in patterns of neural activity in both medial and lateral parts of the orbitofrontal cortex (OFC), only the lateral OFC represents the elemental nutritive attributes. Effective connectivity analyses further indicate that information about the nutritive attributes represented in the lateral OFC is integrated within the medial OFC to compute an overall value. These findings provide a mechanistic account for the construction of food value from its constituent nutrients.</p></div></div></section>

            <noscript>
                
                    
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-017-0008-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                
            </noscript>

            
                <div class="js-context-bar-sticky-point-mobile">
                    
                        <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-entitled-mobile
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-017-0008-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                    
                </div>
            

            
                
                    
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-022-32199-y/MediaObjects/41467_2022_32199_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41467-022-32199-y?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1038/s41467-022-32199-y">Orbitofrontal cortex contributes to the comparison of values underlying economic choices
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">29 July 2022</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Sébastien Ballesta, Weikang Shi &amp; Camillo Padoa-Schioppa</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-022-31273-9/MediaObjects/41467_2022_31273_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41467-022-31273-9?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1038/s41467-022-31273-9">A structural and functional subdivision in central orbitofrontal cortex
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">24 June 2022</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Maya Zhe Wang, Benjamin Y. Hayden &amp; Sarah R. Heilbronner</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41586-020-2880-x/MediaObjects/41586_2020_2880_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41586-020-2880-x?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1038/s41586-020-2880-x">Values encoded in orbitofrontal cortex are causally related to economic choices
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">02 November 2020</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Sébastien Ballesta, Weikang Shi, … Camillo Padoa-Schioppa</p>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'speedy-BootstrappedUCB',
                        timestamp: 1711582552,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                
                
                <div class="main-content">
                    <section data-title="Main"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Main</h2><div class="c-article-section__content" id="Sec1-content"><p>There is accumulating evidence, from an array of studies using diverse methods in multiple species, of a key role for the OFC and adjacent medial prefrontal cortex (PFC) in representing the expected value or utility of options at the time of decision-making<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Clithero, J. A. &amp; Rangel, A. Informatic parcellation of the network involved in the computation of subjective value. Soc. Cogn. Affect. Neurosci. 9, 1289–1302 (2014)." href="#ref-CR1" id="ref-link-section-d153902708e377">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Padoa-Schioppa, C. &amp; Assad, J. A. Neurons in the orbitofrontal cortex encode economic value. Nature 441, 223–226 (2006)." href="#ref-CR2" id="ref-link-section-d153902708e377_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Rich, E. L. &amp; Wallis, J. D. Decoding subjective decisions from orbitofrontal cortex. Nat. Neurosci. 19, 973–980 (2016)." href="#ref-CR3" id="ref-link-section-d153902708e377_2">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Rudebeck, P. H. &amp; Murray, E. A. The orbitofrontal oracle: cortical mechanisms for the prediction and evaluation of specific behavioral outcomes. Neuron 84, 1143–1156 (2014)." href="#ref-CR4" id="ref-link-section-d153902708e377_3">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Grabenhorst, F. &amp; Rolls, E. T. Value, pleasure and choice in the ventral prefrontal cortex. Trends Cogn. Sci. 15, 56–67 (2011)." href="/articles/s41593-017-0008-x#ref-CR5" id="ref-link-section-d153902708e380">5</a></sup>. It has been suggested that such value signals can serve as inputs into the decision process, thereby enabling individuals to choose actions yielding outcomes that maximize expected gains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Clithero, J. A. &amp; Rangel, A. Informatic parcellation of the network involved in the computation of subjective value. Soc. Cogn. Affect. Neurosci. 9, 1289–1302 (2014)." href="/articles/s41593-017-0008-x#ref-CR1" id="ref-link-section-d153902708e384">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Padoa-Schioppa, C. &amp; Assad, J. A. Neurons in the orbitofrontal cortex encode economic value. Nature 441, 223–226 (2006)." href="/articles/s41593-017-0008-x#ref-CR2" id="ref-link-section-d153902708e387">2</a></sup>. Value signals have been found in this region in response to cues or actions associated with many different types of potential outcomes, including food rewards, monetary rewards, consumer goods and even more abstract goals such as pursuing imaginary leisure activities<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Clithero, J. A. &amp; Rangel, A. Informatic parcellation of the network involved in the computation of subjective value. Soc. Cogn. Affect. Neurosci. 9, 1289–1302 (2014)." href="/articles/s41593-017-0008-x#ref-CR1" id="ref-link-section-d153902708e391">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479–485 (2013)." href="#ref-CR6" id="ref-link-section-d153902708e394">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chikazoe, J., Lee, D. H., Kriegeskorte, N. &amp; Anderson, A. K. Population coding of affect across stimuli, modalities and individuals. Nat. Neurosci. 17, 1114–1122 (2014)." href="#ref-CR7" id="ref-link-section-d153902708e394_1">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Howard, J. D., Gottfried, J. A., Tobler, P. N. &amp; Kahnt, T. Identity-specific coding of future rewards in the human orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 112, 5195–5200 (2015)." href="#ref-CR8" id="ref-link-section-d153902708e394_2">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lebreton, M., Jorge, S., Michel, V., Thirion, B. &amp; Pessiglione, M. An automatic valuation system in the human brain: evidence from functional neuroimaging. Neuron 64, 431–439 (2009)." href="#ref-CR9" id="ref-link-section-d153902708e394_3">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Small, D. M. et al. Dissociation of neural representation of intensity and affective valuation in human gustation. Neuron 39, 701–711 (2003)." href="#ref-CR10" id="ref-link-section-d153902708e394_4">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kable, J. W. &amp; Glimcher, P. W. The neural correlates of subjective value during intertemporal choice. Nat. Neurosci. 10, 1625–1633 (2007)." href="#ref-CR11" id="ref-link-section-d153902708e394_5">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Stalnaker, T. A. et al. Orbitofrontal neurons infer the value and identity of predicted outcomes. Nat. Commun 5, 3926 (2014)." href="#ref-CR12" id="ref-link-section-d153902708e394_6">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gross, J. et al. Value signals in the prefrontal cortex predict individual preferences across reward categories. J. Neurosci. 34, 7580–7586 (2014)." href="#ref-CR13" id="ref-link-section-d153902708e394_7">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chib, V. S., Rangel, A., Shimojo, S. &amp; O’Doherty, J. P. Evidence for a common representation of decision values for dissimilar goods in human ventromedial prefrontal cortex. J. Neurosci. 29, 12315–12320 (2009)." href="#ref-CR14" id="ref-link-section-d153902708e394_8">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Levy, D. J. &amp; Glimcher, P. W. Comparing apples and oranges: using reward-specific and reward-general subjective value representation in the brain. J. Neurosci. 31, 14693–14707 (2011)." href="#ref-CR15" id="ref-link-section-d153902708e394_9">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Suzuki, S. et al. Learning to simulate others’ decisions. Neuron 74, 1125–1137 (2012)." href="#ref-CR16" id="ref-link-section-d153902708e394_10">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Suzuki, S., Adachi, R., Dunne, S., Bossaerts, P. &amp; O’Doherty, J. P. Neural mechanisms underlying human consensus decision-making. Neuron 86, 591–602 (2015)." href="/articles/s41593-017-0008-x#ref-CR17" id="ref-link-section-d153902708e397">17</a></sup>. However, while value signals in OFC have been well characterized, much less is known about how it is that value signals are constructed in the first place.</p><p>In the present study, we focus on valuation for food rewards. The valuation of food is a fundamental component of the decision-making process that all humans complete on a daily basis. A dysfunctional food valuation process may result in the development of obesity and eating disorders<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Foerde, K., Steinglass, J. E., Shohamy, D. &amp; Walsh, B. T. Neural mechanisms supporting maladaptive food choices in anorexia nervosa. Nat. Neurosci. 18, 1571–1573 (2015)." href="/articles/s41593-017-0008-x#ref-CR18" id="ref-link-section-d153902708e404">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Carnell, S., Gibson, C., Benson, L., Ochner, C. N. &amp; Geliebter, A. Neuroimaging and obesity: current knowledge and future directions. Obes. Rev. 13, 43–56 (2012)." href="/articles/s41593-017-0008-x#ref-CR19" id="ref-link-section-d153902708e407">19</a></sup>. Recent human neuroimaging studies have begun to elaborate functional contributions of OFC in food value computations. Medial OFC encodes value signals independent of the identity of food rewards<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Howard, J. D., Gottfried, J. A., Tobler, P. N. &amp; Kahnt, T. Identity-specific coding of future rewards in the human orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 112, 5195–5200 (2015)." href="/articles/s41593-017-0008-x#ref-CR8" id="ref-link-section-d153902708e411">8</a></sup>, irrespective of whether the value information is acquired through direct experience or through imagining the consequences of a new experience<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Barron, H. C., Dolan, R. J. &amp; Behrens, T. E. J. Online evaluation of novel choices by simultaneous representation of multiple memories. Nat. Neurosci. 16, 1492–1498 (2013)." href="/articles/s41593-017-0008-x#ref-CR20" id="ref-link-section-d153902708e415">20</a></sup>. On the other hand, lateral OFC encodes value in an identity-specific manner<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Howard, J. D., Gottfried, J. A., Tobler, P. N. &amp; Kahnt, T. Identity-specific coding of future rewards in the human orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 112, 5195–5200 (2015)." href="/articles/s41593-017-0008-x#ref-CR8" id="ref-link-section-d153902708e419">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Klein-Flügge, M. C., Barron, H. C., Brodersen, K. H., Dolan, R. J. &amp; Behrens, T. E. J. Segregated encoding of reward-identity and stimulus-reward associations in human orbitofrontal cortex. J. Neurosci. 33, 3202–3211 (2013)." href="/articles/s41593-017-0008-x#ref-CR21" id="ref-link-section-d153902708e422">21</a></sup>. However, the constituent attributes that underlie the construction of food value and how these constituent attributes are represented and integrated in the OFC remain elusive.</p><p>We hypothesized that the value of a food reward is at least in part computed by taking into account beliefs about the properties of the constituent nutritive attributes of a food item. We focused on beliefs about the amount of protein, carbohydrates and fat, and we also included beliefs about the specifically sweet carbohydrates (sugar), sodium and vitamin content contained in a food item. We further hypothesized that the OFC would play a role in representing these elemental attributes, which could thereby constitute precursor representations used to generate an integrated value signal.</p><p>In the human brain, value signals for food rewards have been reported throughout the orbital surface, most prominently in the medial OFC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Clithero, J. A. &amp; Rangel, A. Informatic parcellation of the network involved in the computation of subjective value. Soc. Cogn. Affect. Neurosci. 9, 1289–1302 (2014)." href="/articles/s41593-017-0008-x#ref-CR1" id="ref-link-section-d153902708e432">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Grabenhorst, F. &amp; Rolls, E. T. Value, pleasure and choice in the ventral prefrontal cortex. Trends Cogn. Sci. 15, 56–67 (2011)." href="/articles/s41593-017-0008-x#ref-CR5" id="ref-link-section-d153902708e435">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479–485 (2013)." href="/articles/s41593-017-0008-x#ref-CR6" id="ref-link-section-d153902708e438">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chib, V. S., Rangel, A., Shimojo, S. &amp; O’Doherty, J. P. Evidence for a common representation of decision values for dissimilar goods in human ventromedial prefrontal cortex. J. Neurosci. 29, 12315–12320 (2009)." href="/articles/s41593-017-0008-x#ref-CR14" id="ref-link-section-d153902708e441">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Levy, D. J. &amp; Glimcher, P. W. Comparing apples and oranges: using reward-specific and reward-general subjective value representation in the brain. J. Neurosci. 31, 14693–14707 (2011)." href="/articles/s41593-017-0008-x#ref-CR15" id="ref-link-section-d153902708e444">15</a></sup>. However, sensory inputs from the visual, auditory, gustatory, olfactory and somatosensory systems arrive into the OFC primarily in the lateral portions of the orbital surface<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Ongür, D. &amp; Price, J. L. The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans. Cereb. Cortex 10, 206–219 (2000)." href="/articles/s41593-017-0008-x#ref-CR22" id="ref-link-section-d153902708e448">22</a></sup>. Thus, we hypothesized that more lateral parts of the OFC would be especially involved in encoding elemental attributes about a food outcome, in contrast to the medial OFC, which we hypothesized would be especially involved in encoding an overall subjective goal-value signal for the foods, as found in many previous reports<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Clithero, J. A. &amp; Rangel, A. Informatic parcellation of the network involved in the computation of subjective value. Soc. Cogn. Affect. Neurosci. 9, 1289–1302 (2014)." href="/articles/s41593-017-0008-x#ref-CR1" id="ref-link-section-d153902708e452">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479–485 (2013)." href="/articles/s41593-017-0008-x#ref-CR6" id="ref-link-section-d153902708e455">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Gross, J. et al. Value signals in the prefrontal cortex predict individual preferences across reward categories. J. Neurosci. 34, 7580–7586 (2014)." href="/articles/s41593-017-0008-x#ref-CR13" id="ref-link-section-d153902708e458">13</a></sup>.</p></div></div></section><section data-title="Results"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Results</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Experimental task and behavior</h3><p>To test these hypotheses, we scanned 23 human participants using functional MRI (fMRI) while they reported their ‘willingness to pay’ (WTP; i.e., subjective value) for 56 food items (WTP task; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1a</a>)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479–485 (2013)." href="/articles/s41593-017-0008-x#ref-CR6" id="ref-link-section-d153902708e477">6</a></sup>. After the MRI scan, the participants provided subjective ratings about the constituent nutrient attributes for the same set of items. Specifically, we asked participants to rate the quantities of fat, sodium, carbohydrates, sugar, protein and vitamins contained in the foods, as well as to provide an estimate of the overall caloric content<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Tang, D. W., Fellows, L. K. &amp; Dagher, A. Behavioral and neural valuation of foods is driven by implicit knowledge of caloric content. Psychol. Sci. 25, 2168–2176 (2014)." href="/articles/s41593-017-0008-x#ref-CR23" id="ref-link-section-d153902708e481">23</a></sup> (attribute-rating task; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1b</a>). In this task, subjective ratings about the nutrient factors were found to be significantly correlated with the objective factors (<i>P</i> &lt; 0.01 for all factors; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1c</a>). Moreover, while performing the WTP task in the scanner, the participants were not aware that they would be subsequently required to rate the nutrient attributes of the items, and thus they were not biased by experimenter-demand effects to artificially reflect on information about nutrient attributes during the food valuation phase.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Experimental task and behavior."><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1: Experimental task and behavior.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig1_HTML.jpg?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig1_HTML.jpg" alt="figure 1" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>
                                    <b>a</b>, Timeline of one trial in the WTP task. On each trial, participants reported their willingness to pay (i.e., subjective value) for one food item. Note that in the bid phase, mappings between keys and dollar amounts were randomized across trials. <b>b</b>, Timeline of one trial in the attribute-rating task. On each trial, participants answered one question (for example, ‘How high is the item in fat?’) for one item on a continuous scale from ‘not at all’ to ‘very much’ by moving a red pointer, with no time constraint. <b>c</b>, Correlations between the subjective and the objective nutrient factors (<i>n</i> = 23 participants). In each box and whisker plot, the central line denotes the median, and the bottom and top edges of the box indicate the 25th and 75th percentiles (<i>q</i>
                                    <sub>25</sub> and <i>q</i>
                                    <sub>75</sub> respectively). The ends of the whiskers represent the maximum and minimum data points not considered outliers. Data points are considered outliers (open circles) if they are greater than <i>q</i>
                                    <sub>75</sub> + 1.5 × (<i>q</i>
                                    <sub>75</sub> – <i>q</i>
                                    <sub>25</sub>) or less than <i>q</i>
                                    <sub>25</sub> – 1.5 × (<i>q</i>
                                    <sub>75</sub> – <i>q</i>
                                    <sub>25</sub>). **<i>P</i> &lt; 0.01, <i>t</i> test (fat: <i>t</i>
                                    <sub>22</sub> = 17.73, <i>P</i> &lt; 0.001; sodium: <i>t</i>
                                    <sub>22</sub> = 18.38, <i>P</i> &lt; 0.001; carbohydrates (carb.): <i>t</i>
                                    <sub>22</sub> = 7.71, <i>P</i> &lt; 0.001; sugar: <i>t</i>
                                    <sub>22</sub> = 26.34, <i>P</i> &lt; 0.001; protein: <i>t</i>
                                    <sub>22</sub> = 18.64, <i>P</i> &lt; 0.001; vitamins: <i>t</i>
                                    <sub>22</sub> = 23.70, <i>P</i> &lt; 0.001). <b>d</b>, Prediction performance of the subjective value in each regression model (<i>n</i> = 23 participants). Performance was assessed by the cross-validated correlation between the predicted and actual values. Box and whisker plots are as in <b>c</b>. **<i>P</i> &lt; 0.01, <i>t</i> test (subjective factors: <i>t</i>
                                    <sub>22</sub> = 12.36, <i>P</i> &lt; 0.001; objective factors: <i>t</i>
                                    <sub>22</sub> = 8.34, <i>P</i> &lt; 0.001; subjective calories: <i>t</i>
                                    <sub>22</sub> = –0.26, <i>P</i> = 0.607; objective calories: <i>t</i>
                                    <sub>22</sub> = –1.18, <i>P</i> = 0.875). <b>e</b>, Prediction performance of the subjective value in each logistic regression model (<i>n</i> = 23 participants). Performance was assessed by cross-validated accuracy. Box and whisker plots are as in <b>c</b>. **<i>P</i> &lt; 0.01 and *<i>P</i> &lt; 0.05, <i>t</i> test vs. 50% (subjective factors: <i>t</i>
                                    <sub>22</sub> = 13.61, <i>P</i> &lt; 0.001; objective factors: <i>t</i>
                                    <sub>22</sub> = 9.98, <i>P</i> &lt; 0.001; subjective calories: <i>t</i>
                                    <sub>22</sub> = 2.05, <i>P</i> = 0.026; objective calories: <i>t</i>
                                    <sub>22</sub> = 2.43, <i>P</i> = 0.012).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div>
                <p>We first conducted behavioral analyses to test our hypothesis that participants’ ratings of the elemental nutritive attributes of a food would predict the subjective valuation of the food items. As some nutritive attribute ratings were tightly coupled with others (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig6">1</a>), including all the attributes in the predictive model did not necessarily provide the best prediction of value. To specify which combinations of subjective nutrient factors provided the best prediction about subjective value, we performed a series of linear regression analyses (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-017-0008-x#Sec11">Methods</a>). In the regression analyses, performance of the prediction was assessed by leave-one-item-out cross-validation. Comparing every possible combination of the six nutrient factors (i.e., 2<sup>6</sup> = 64 models), we found that subjective value was best predicted by a model including the following four subjective nutrient factors: fat, carbohydrates, protein and vitamin (Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-017-0008-x#MOESM1">1</a>). Consistent with this result, among the best 10 models, protein and vitamin appeared in all 10 models; fat and carbohydrate appeared in 8 and 6 models, respectively; and sodium and sugar were present only in 5 and 4 models, respectively (Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-017-0008-x#MOESM1">1</a>).</p><p>Here we note that sugar content did not make a significant contribution to the food valuation, despite previous findings showing a role for sugar content in food intake behaviors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zuker, C. S. Food for the brain. Cell 161, 9–11 (2015)." href="#ref-CR24" id="ref-link-section-d153902708e751">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="de Araujo, I. E. et al. Food reward in the absence of taste receptor signaling. Neuron 57, 930–941 (2008)." href="#ref-CR25" id="ref-link-section-d153902708e751_1">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Tellez, L. A. et al. Separate circuitries encode the hedonic and nutritional values of sugar. Nat. Neurosci. 19, 465–470 (2016)." href="/articles/s41593-017-0008-x#ref-CR26" id="ref-link-section-d153902708e754">26</a></sup>. Given that sugar is a subcomponent of carbohydrates and that subjective ratings about the two factors were indeed highly correlated (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig6">1b</a>), a reasonable interpretation of this result is that the effects of sugar content are subsumed under the more general carbohydrate category. This interpretation was further supported by an additional analysis demonstrating that including sugar instead of carbohydrate to the regression model significantly reduced the accuracy of the model for predicting subjective value (<i>P</i> &lt; 0.05).</p><p>The prediction performance of the best fitting model was better than chance-level (at <i>P</i> &lt; 0.01; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1d</a>). Even when implementing Bonferroni corrections for every possible combination of variables we ran (<i>n</i> = 64), the prediction performance of the best fitting model was nevertheless still significant at <i>P</i> &lt; 0.01. In addition to testing for the role of subjective beliefs about the nutritive content of the foods, we also extracted objective information about the nutritive content of the foods and used that information in a regression analysis similar to that performed using the subjective ratings. We found that the best fitting model with subjective nutrient factors outperformed the best fitting model with objective factors (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1d</a>). Furthermore, the regression model that included subjective beliefs about all four nutritive factors also performed better than subjective or objective estimates of overall caloric content (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1d</a>).</p><p>We further validated these results by using logistic regression analyses with categorical binary predicted variables (constructed by splitting subjective value into low and high categories based on a median split). That is, the model providing the best prediction in the original linear regression analyses outperformed the other models also in the logistic regression analyses (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1e</a> and Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-017-0008-x#MOESM1">1</a>). Collectively, these behavioral analyses support the notion that food value is computed through integrating information about the subjective beliefs about the nutrient factors of fat, carbohydrates, protein and vitamin content.</p><h3 class="c-article__sub-heading" id="Sec4">Representation of subjective value in the OFC</h3><p>Having established that the subjective value of food items can be predicted in part from subjective beliefs about nutritive content, we set out to replicate previous findings of a role for OFC in encoding the subjective value of the food items, using multivoxel pattern analyses (MVPA)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Haynes, J.-D. A primer on pattern-based approaches to fMRI: principles, pitfalls, and perspectives. Neuron 87, 257–270 (2015)." href="/articles/s41593-017-0008-x#ref-CR27" id="ref-link-section-d153902708e804">27</a></sup> with leave-one-run-out cross-validation. In this analysis, a linear classifier was trained on patterns of fMRI response to categorize food items as being either high or low in subjective value based on each participant’s ratings (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-017-0008-x#Sec11">Methods</a>).</p><p>Consistent with our hypothesis, value representations could be decoded from medial parts of the OFC at the time of valuation. In addition, subjective value codes were also found in parts of lateral OFC, consistent with other previous reports<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chikazoe, J., Lee, D. H., Kriegeskorte, N. &amp; Anderson, A. K. Population coding of affect across stimuli, modalities and individuals. Nat. Neurosci. 17, 1114–1122 (2014)." href="/articles/s41593-017-0008-x#ref-CR7" id="ref-link-section-d153902708e814">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Howard, J. D., Gottfried, J. A., Tobler, P. N. &amp; Kahnt, T. Identity-specific coding of future rewards in the human orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 112, 5195–5200 (2015)." href="/articles/s41593-017-0008-x#ref-CR8" id="ref-link-section-d153902708e817">8</a></sup>. Specifically, subjective value could be decoded above chance from patterns of fMRI response within anatomically defined medial as well as lateral OFC regions of interest (ROIs; <i>P</i> &lt; 0.01 for both ROIs, <i>t</i> test and permutation test; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig2">2a</a>; and see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig7">2a</a> for information about the ROIs). Value information could also be decoded both at the time of bidding and at the time of feedback (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig7">2b</a>). A searchlight analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Haynes, J.-D. A primer on pattern-based approaches to fMRI: principles, pitfalls, and perspectives. Neuron 87, 257–270 (2015)." href="/articles/s41593-017-0008-x#ref-CR27" id="ref-link-section-d153902708e837">27</a></sup> also identified significant codes of subjective value in both medial and lateral OFC (<i>P</i> &lt; 0.05, family-wise error rate small-volume corrected (FWE SVC); Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig2">2b</a>). Furthermore, we found that for each classifier, the classification weights of the voxels were broadly distributed across the range of negative to positive values (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig7">2c,d</a>), suggesting that the subjective value codes in the OFC are multivariate in nature.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Neural representation of subjective value."><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2: Neural representation of subjective value.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig2_HTML.jpg?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig2_HTML.jpg" alt="figure 2" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>
                                    <b>a</b>, Subjective value signals can be decoded in both lateral and medial OFC (lOFC and mOFC, respectively). Decoding accuracy is plotted for the lOFC and the mOFC ROIs (<i>n</i> = 23 participants). Left: box and whisker plots are as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1c</a>. **<i>P</i> &lt; 0.01, <i>t</i> test vs. 50% (lOFC: <i>t</i>
                                    <sub>22</sub> = 4.75, <i>P</i> &lt; 0.001; mOFC: <i>t</i>
                                    <sub>22</sub> = 3.57, <i>P</i> &lt; 0.001). Right: each point denotes the mean accuracy across participants. Gray horizontal lines indicate the 95th percentiles of the null distributions obtained from the permutation test procedure (lOFC: <i>P</i> &lt; 0.001; mOFC: <i>P</i> &lt; 0.001). <b>b</b>, Subregions of the OFC encoding subjective value. The decoding accuracy map obtained from the searchlight analysis is thresholded at <i>P</i> &lt; 0.005 (uncorrected) for display purposes (<i>n</i> = 23 participants). Peak voxels: Montreal Neurological Institute coordinates (MNI): <i>x</i>, <i>y</i>, <i>z</i> = –36, 26, –11 and 12, 53, –8 (<i>P</i> &lt; 0.05, small-volume corrected).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec5">Representation of nutrient factors in the OFC</h3><p>We then tested whether, while evaluating a food item for decision-making (i.e., during the WTP task), the OFC represented information about the four subjective nutrient factors identified as predictors of the value. To this end, we applied the same MVPA procedure used for value coding (see above) to each of the four subjective nutrient factor ratings. Consistent with our initial hypothesis, information about the subjective nutrient factors could be significantly decoded at the time of valuation in the lateral OFC ROI (<i>P</i> &lt; 0.05, conjunction test against the conjunction null<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Nichols, T., Brett, M., Andersson, J., Wager, T. &amp; Poline, J.-B. Valid conjunction inference with the minimum statistic. Neuroimage 25, 653–660 (2005)." href="/articles/s41593-017-0008-x#ref-CR28" id="ref-link-section-d153902708e943">28</a></sup> based on <i>t</i> and permutation tests; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig3">3a</a>; see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-017-0008-x#Sec11">Methods</a> for detailed information about the conjunction test; classification scores are plotted as functions of subjective nutrient factors in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig8">3</a>) but not in the medial OFC ROI (<i>P</i> &gt; 0.05, conjunction test; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig3">3b</a>). On the other hand, at the time of bidding or feedback, we found no significant decoding of the subjective nutrient factors either in lateral or medial OFC (<i>P</i> &gt; 0.05, conjunction test; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4a,b</a>), suggesting that the lateral OFC represents information about the nutrient factors only at the timing of valuation. Searchlight analyses confirmed encoding of information for each of the subjective nutrient factors at the time of valuation in various loci within the lateral OFC (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig3">3c</a>), with clusters encoding fat, protein and carbohydrate content all significant at <i>P</i> &lt; 0.05 under voxel-level multiple-comparison correction within the anatomically defined lateral OFC ROI (i.e., FWE SVC), while the cluster encoding vitamin content bordered on significance (<i>P</i> = 0.080 FWE SVC; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig3">3c</a>). Moreover, the distributions of the classification weights across the voxels were not highly biased toward negative or positive values (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4c,d</a>), consistent with the notion that the representations of subjective nutrient factors are multivariate. In sum, these results suggest that, during food valuation, information about subjective nutrient factors is encoded in the lateral OFC but not in the medial OFC.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Neural representation of subjective nutrient factors."><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3: Neural representation of subjective nutrient factors.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig3_HTML.jpg?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig3_HTML.jpg" alt="figure 3" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>
                                    <b>a</b>, Subjective nutrient factors can be significantly decoded from lOFC. Decoding accuracies are plotted for the lOFC ROI (<i>n</i> = 23 participants). Significant encoding was found for each of the nutrient factors, thereby indicating a significant conjunction effect<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Foerde, K., Steinglass, J. E., Shohamy, D. &amp; Walsh, B. T. Neural mechanisms supporting maladaptive food choices in anorexia nervosa. Nat. Neurosci. 18, 1571–1573 (2015)." href="/articles/s41593-017-0008-x#ref-CR18" id="ref-link-section-d153902708e1006">18</a></sup> at <i>P</i> &lt; 0.05. Left: box and whisker plots are as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1c.</a> *<i>P</i> &lt; 0.05 and **<i>P</i> &lt; 0.01 for each factor, <i>t</i> test vs. 50% (fat: <i>t</i>
                                    <sub>22</sub> = 2.40, <i>P</i> = 0.013; carb.: <i>t</i>
                                    <sub>22</sub> = 2.77, <i>P</i> = 0.006; protein: <i>t</i>
                                    <sub>22</sub> = 2.31, <i>P</i> = 0.015; vitamins: <i>t</i>
                                    <sub>22</sub> = 2.32, <i>P</i> = 0.015). Right: as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig2">2a</a> (right). Permutation test (fat: <i>P</i> = 0.004; carb.: <i>P</i> &lt; 0.001; protein: <i>P</i> = 0.013; vitamins: <i>P</i> = 0.001). <b>b</b>, As in <b>a</b> but for mOFC. Subjective nutrient factors were not significantly decodable above chance levels in mOFC (<i>n</i> = 23 participants). Left: <i>t</i> test vs. 50% (fat: <i>t</i>
                                    <sub>22</sub> = 0.68, <i>P</i> = 0.250; carb.: <i>t</i>
                                    <sub>22</sub> = –1.74, <i>P</i> = 0.952; protein: <i>t</i>
                                    <sub>22</sub> = 0.75, <i>P</i> = 0.230; vitamins: <i>t</i>
                                    <sub>22</sub> = –0.02, <i>P</i> = 0.508). Right: permutation test (fat: <i>P</i> = 0.238; carb.: <i>P</i> = 0.923; protein: <i>P</i> = 0.159; vitamins: <i>P</i> = 0.519). <b>c</b>, Subregions of lOFC encoding each of the subjective nutrient factors (<i>n</i> = 23 participants). Decoding accuracy maps obtained from the searchlight analyses, thresholded at <i>P</i> &lt; 0.005 (uncorrected) for display purpose. Peak voxels: MNI <i>x</i>, <i>y</i>, <i>z</i> = –21, 56, –8 for fat (<i>P</i> &lt; 0.05, small-volume corrected); –15, 14, –17 for carbohydrates (<i>P</i> &lt; 0.05, small-volume corrected); 33, 38, –14 for protein (<i>P</i> &lt; 0.05, small-volume corrected); and 18, 17, –20 for vitamins (<i>P</i> = 0.080, small-volume corrected). <b>d</b>, Decoding of subjective nutrient factors in lOFC after regressing out the effect of value (<i>n</i> = 23 participants). Box and whisker plots are as in <b>a</b>. <sup>+</sup>
                                    <i>P</i> &lt; 0.10, *<i>P</i> &lt; 0.05 and **<i>P</i> &lt; 0.01 for each factor, <i>t</i> test vs. 50% (fat: <i>t</i>
                                    <sub>22</sub> = 1.53, <i>P</i> = 0.070; carb.: <i>t</i>
                                    <sub>22</sub> = 2.20, <i>P</i> = 0.020; protein: <i>t</i>
                                    <sub>22</sub> = 4.06, <i>P</i> &lt; 0.001; vitamins: <i>t</i>
                                    <sub>22</sub> = 2.90, <i>P</i> = 0.004).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div>
                <p>We also examined whether linear classifiers can decode the subjective nutrient factors of novel food items. Given that, in our experiment, half of the items were presented in run 1 and 3 while the other half were presented in runs 2 and 4, we trained the classifiers on the data from runs 1 and 3 and tested them on runs 2 and 4 (and vice versa; accuracy scores were averaged; c.f. the leave-one-run-out cross-validation used in the above main analyses). The analysis revealed that, in the lateral OFC, decoding accuracies were significantly greater than chance for fat, carbohydrates and vitamins (<i>P</i> &lt; 0.05; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4e</a>), while the accuracy for protein was at the trend level (<i>P</i> &lt; 0.10; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4e</a>).</p><p>Here we note that the differential decoding performance between the lateral and medial OFC cannot be attributed to any differences in the number of voxels contained in the ROIs (although the lateral and the medial OFC ROIs contained 2,325 and 533 voxels respectively). To demonstrate this, we randomly resampled the same number of adjacent voxels from the lateral OFC as found in the medial OFC ROI (i.e., forming a continuous cluster consisting of 553 voxels), and we then tested whether information about the subjective nutrient factors could still be decoded within this reduced ROI (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4f</a>). The analysis demonstrated that, even with the smaller number of voxels, we could still decode information about the subjective nutrient factors from patterns of fMRI activity in the lateral OFC (<i>P</i> &lt; 0.05, conjunction test; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4f</a>).</p><p>We next confirmed that, at the time of valuation, the lateral OFC represented subjective nutrient factors and value using distinct codes, by showing the following: first, in the lateral OFC ROI, classifiers trained to predict information about each of the nutrient factors could not significantly decode value information (<i>P</i> &gt; 0.05 for all); second, even after regressing out the effects of value from both the ratings about the nutrient factors and the fMRI responses to food items (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-017-0008-x#Sec11">Methods</a>), subjective nutrient factors could still be decoded in the lateral OFC ROI (for carbohydrates, fat and vitamins at <i>P</i> &lt; 0.05 and at the trend level for protein at <i>P</i> &lt; 0.10; conjunction test at <i>P</i> &lt; 0.10; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig3">3d</a>).</p><p>Furthermore, we examined whether distinct patterns of voxel activity in the lateral OFC represent each of the four subjective nutrient factors. Since the classification weights of each voxel were correlated across some combinations of the nutrient factors (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4g</a>), the alternative possibility would be that the same patterns of voxel activity represent two or more nutrient factors. To exclude the alternative possibility, we performed a cross-decoding MVPA procedure across the nutrient factors. In this analysis, for each pair of the four nutrient factors, we trained a classifier on one factor and tested it on the other factor (and the reverse; decoding accuracy was assessed by the average across both directions). Here we reasoned that if the subjective nutrient factors were coded in different patterns, the cross-decoding analysis would not provide any significant results. The analysis indeed revealed that the cross-category decoding accuracy was not significantly different from chance (<i>P</i> &gt; 0.05; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4h</a>), except for only one pair: fat and vitamins. In the fat–vitamins pair, decoding accuracy was significantly below chance (<i>P</i> &lt; 0.01; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4h</a>). There are at least two possible interpretations for the negative accuracy. One is that, in the lateral OFC, a highly similar pattern of fMRI response codes for fat and vitamins in the opposite directions of a multivariate decision boundary. The other possibility is that distinct patterns code for the two factors, but these are not dissociable in our dataset, given the highly negative correlation between subjective fat and vitamins in the behavioral ratings (<i>r</i> = –0.44 ± 0.15, mean ± s.d. across participants; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">1b</a>) and the neural classifiers’ weights (<i>r</i> = –0.41 ± 0.18, mean ± s.d.; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4g</a>). To tease apart these two possibilities, we conducted the following additional analysis: (i) 42 food items were randomly resampled from the original set of 56 items to ensure that the fat and vitamins were less correlated (mean <i>r</i> &gt; –0.3); (ii) MVPA was then performed on the resampled data; and (iii) the above procedure was repeated ten times (accuracies were averaged). On the resampled data, we found that, consistent with results from the original dataset (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig3">3a</a>), a classifier trained on fat (or vitamins) could decode information about fat (or vitamins; <i>P</i> &lt; 0.05; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4i</a>). On the other hand, in the cross-decoding (i.e., a classifier was trained on fat and tested on vitamins, and vice versa), the accuracy was not significantly different from chance (<i>P</i> &gt; 0.05; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4i</a>). These findings together suggest that, in the lateral OFC, different patterns of voxel activity represent information about different subjective nutrient factors.</p><p>While we have so far focused on the four subjective nutrient factors identified as value predictors in our behavioral analyses, we also nevertheless tested for evidence of representations of the other factors we included in our experiment (but which were found to not be significantly associated with value): subjective sodium and sugar content. Information about subjective sugar content could be significantly decoded in the lateral OFC (<i>P</i> &lt; 0.01; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4j</a>) but not in the medial OFC (<i>P</i> = 0.100; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4k</a>). On the other hand, neither the lateral nor the medial OFC showed significant decoding of sodium content (<i>P</i> = 0.474 and <i>P</i> = 0.557, respectively; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4j,k</a>).</p><p>We also investigated the extent to which objective (as opposed to subjective) nutrient content could be decoded from the OFC by training the MVPA classifiers on labels extracted from the objective nutrient content as opposed to the subjective content. This analysis identified a weaker overall effect of objective nutrient factors in the lateral and medial OFC (i.e., no significant conjunction effect at <i>P</i> &gt; 0.05; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig9">4l</a>), although a subset of the individual objective factors could be significantly decoded in the lateral OFC. These results suggest that subjective nutrient factors are more robustly represented in the OFC than objective factors.</p><h3 class="c-article__sub-heading" id="Sec6">Representation of the relative content of the subjective nutrient factors in the OFC</h3><p>To further explore how nutritive information is represented in the OFC, we implemented a representational similarity analysis (RSA)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Kriegeskorte, N. &amp; Kievit, R. A. Representational geometry: integrating cognition, computation, and the brain. Trends Cogn. Sci. 17, 401–412 (2013)." href="/articles/s41593-017-0008-x#ref-CR29" id="ref-link-section-d153902708e1376">29</a></sup> to examine the extent to which the pattern of subjective ratings of the nutritive factors was related to encoding of these factors in the orbitofrontal cortex. In the RSA, we compared the voxel-wise similarity structure obtained from the fMRI data with the similarity structure of the subjective nutritive components for each item (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig10">5</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-017-0008-x#Sec11">Methods</a>). In this analysis, the voxel-wise similarity is defined as the correlation across voxel activity for each pair of items (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig10">5a</a>), while the nutritive similarity is defined as the correlation in bundles of the four subjective nutrient factors (fat, carbohydrates, protein and vitamins) for each item pair (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig10">5b</a>). In other words, because correlation distance is employed to measure similarity, the nutritive similarity between two items is defined in terms of the relative content of the nutrient factors. The RSA revealed that the similarity of fMRI responses significantly reflected the similarity of the relative content of the nutrient factors in the lateral OFC ROI (<i>P</i> &lt; 0.01; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig10">5c</a>) but not in the medial OFC. We also conducted a searchlight RSA and found a significant association between the voxel-wise fMRI and the subjective nutritive similarity across diffuse regions of the lateral OFC (<i>P</i> &lt; 0.05 FWE SVC; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig10">5d</a>). These results suggest that there is a representation in lateral OFC of the relative content of each nutritive attribute.</p><h3 class="c-article__sub-heading" id="Sec7">Representation of low-level visual features in the OFC</h3><p>Here we aimed to rule out the possibility that the lateral OFC contains information about low-level visual features such as luminance and contrast, which could potentially be detected by the classifier if there were inadvertent correlations between such low-level sensory features and value and/or subjective nutrient factors. For this purpose, we extracted eight low-level visual features (luminance, contrast, red intensity, green intensity, blue intensity, hue, saturation and brightness) from the food images presented to participants, and then examined whether the visual features could be decoded in the OFC. Moreover, as a positive control, we also tested for the primary visual cortex (V1, Brodmann area 17). These analyses showed that neither the lateral nor the medial OFC contains significant information about low-level visual features (<i>P</i> &gt; 0.05 for all features; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig4">4a</a>), while, as would be expected, primary visual cortex did indeed contain significant information about low-level visual features (<i>P</i> &lt; 0.05, conjunction test; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig4">4b</a>). These results suggest that the lateral OFC encodes information about value and subjective nutrient factors but not low-level visual information about the food images.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Neural representation of low-level visual features."><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4: Neural representation of low-level visual features.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig4_HTML.jpg?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig4_HTML.jpg" alt="figure 4" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>
                                    <b>a</b>, Low-level visual features could not be significantly decoded from lOFC or mOFC above chance levels. Decoding accuracies are plotted for lOFC and mOFC ROI (<i>n</i> = 23 participants). Box and whisker plots are as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1c</a>. Left: <i>t</i> test vs. 50% (luminance: <i>t</i>
                                    <sub>22</sub> = 0.83, <i>P</i> = 0.208; contrast: <i>t</i>
                                    <sub>22</sub> = 1.64, <i>P</i> = 0.058; red: <i>t</i>
                                    <sub>22</sub> = 0.88, <i>P</i> = 0.195; green: <i>t</i>
                                    <sub>22</sub> = 1.64, <i>P</i> = 0.057; blue: <i>t</i>
                                    <sub>22</sub> = 0.29, <i>P</i> = 0.387; hue: <i>t</i>
                                    <sub>22</sub> = –1.30, <i>P</i> = 0.896; saturation: <i>t</i>
                                    <sub>22</sub> = 0.00, <i>P</i> = 0.520; brightness: <i>t</i>
                                    <sub>22</sub> = –0.86, <i>P</i> = 0.800). Right: <i>t</i> test vs. 50% (luminance: <i>t</i>
                                    <sub>22</sub> = –0.26, <i>P</i> = 0.603; contrast: <i>t</i>
                                    <sub>22</sub> = 0.22, <i>P</i> = 0.414; red: <i>t</i>
                                    <sub>22</sub> = 0.81, <i>P</i> = 0.212; green: <i>t</i>
                                    <sub>22</sub> = 0.87, <i>P</i> = 0.200; blue: <i>t</i>
                                    <sub>22</sub> = –0.15, <i>P</i> = 0.440; hue: <i>t</i>
                                    <sub>22</sub> = –3.12, <i>P</i> = 0.998; saturation: <i>t</i>
                                    <sub>22</sub> = 0.50, <i>P</i> = 0.313; brightness: <i>t</i>
                                    <sub>22</sub> = 1.05, <i>P</i> = 0.153). <b>b</b>, Low-level visual features could be robustly decoded from V1 (<i>n</i> = 23 participants). Box and whisker plots are as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1c</a>. *<i>P</i> &lt; 0.05 and **<i>P</i> &lt; 0.01 for each factor, <i>t</i> test vs. 50% (luminance: <i>t</i>
                                    <sub>22</sub> = 5.07, <i>P</i> &lt; 0.001; contrast: <i>t</i>
                                    <sub>22</sub> = 4.60, <i>P</i> &lt; 0.001; red: <i>t</i>
                                    <sub>22</sub> = 6.30, <i>P</i> &lt; 0.001; green: <i>t</i>
                                    <sub>22</sub> = 5.03, <i>P</i> &lt; 0.001; blue: <i>t</i>
                                    <sub>22</sub> = 4.60, <i>P</i> &lt; 0.001; hue: <i>t</i>
                                    <sub>22</sub> = 2.20, <i>P</i> = 0.019; saturation: <i>t</i>
                                    <sub>22</sub> = 8.32, <i>P</i> &lt; 0.001; brightness: <i>t</i>
                                    <sub>22</sub> = 5.38, <i>P</i> &lt; 0.001).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec8">Effective connectivity between OFC subregions at the time of valuation</h3><p>By leveraging MVPA on fMRI data, we were able to demonstrate that during food valuation, lateral OFC contains information about the elemental nutritive attributes of food. However, to compute an overall subjective value, the individual nutritive representations need to be integrated. We hypothesized that this integration of the individual nutritive representations would occur in regions found to encode subjective value in either the medial or lateral subregions of OFC. To test which of the value-encoding OFC subregions is primarily involved in the integration process, we performed an effective connectivity analysis: a psychophysiological interaction. The connectivity analysis is based on the reasoning that if a region is implicated in the integration process, the region must (i) contain information about the overall subjective value and (ii) have enhanced effective connectivity at the time of valuation with regions encoding each of the constitutive nutritive attributes of a food. The psychophysiological interaction analysis tested whether the value-related OFC subregions, identified in the searchlight MVPA (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig2">2b</a>), had increased task-related connectivity at the time of valuation with the lateral OFC subregions encoding each of the four subjective nutrient attributes (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig3">3c</a>). We found evidence for a significant increase in effective connectivity at the time of valuation between the value-related medial OFC subregion and the lateral OFC subregions representing the nutrient attributes (<i>P</i> &lt; 0.05, conjunction test; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig5">5a</a>). This result was further validated by a nonparametric bootstrap test<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Efron, B. &amp; Tibshirani, R. J. An Introduction to the Bootstrap (CRC Press, 1993)." href="/articles/s41593-017-0008-x#ref-CR30" id="ref-link-section-d153902708e1705">30</a></sup> (<i>P</i> &lt; 0.05; <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-017-0008-x#Sec11">Methods</a>), which is known to be relatively robust against potential outliers. On the other hand, we found no significant increase in the effective connectivity at the time of bidding or feedback (<i>P</i> &gt; 0.05, conjunction test:; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig11">6a,b</a>), although a subset of the individual attributes did show a connectivity effect. Also, we did not find robust evidence for a significant integration of nutrient attribute signals in the value-related lateral OFC region (<i>P</i> &gt; 0.05, conjunction test; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig5">5b</a>). These results indicate that the medial OFC satisfies both of the above two criteria for a brain region implicated in information integration, consistent with the notion that representations about elemental nutritive attributes of food in the lateral OFC are primarily integrated at the time of valuation in the medial OFC to compute subjective values.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Effective connectivity between OFC subregions at the time of valuation."><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5: Effective connectivity between OFC subregions at the time of valuation.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig5_HTML.jpg?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig5_HTML.jpg" alt="figure 5" loading="lazy"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>
                                    <b>a</b>, Results of an effective connectivity analysis between the value-encoding mOFC subregion and the lOFC subregions encoding each of the four nutrient factors. A significant connectivity effect was found for each of the nutrient factors, thereby indicating a significant conjunction effect<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Foerde, K., Steinglass, J. E., Shohamy, D. &amp; Walsh, B. T. Neural mechanisms supporting maladaptive food choices in anorexia nervosa. Nat. Neurosci. 18, 1571–1573 (2015)." href="/articles/s41593-017-0008-x#ref-CR18" id="ref-link-section-d153902708e1744">18</a></sup> at <i>P</i> &lt; 0.05. Effect sizes of the psychophysiological interaction (PPI) regressors are plotted (<i>n</i> = 23 participants). Box and whisker plots are as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1c</a>. **<i>P</i> &lt; 0.01 and *<i>P</i> &lt; 0.05 for each factor, <i>t</i> test (fat: <i>t</i>
                                    <sub>22</sub> = 1.74, <i>P</i> = 0.048; carb.: <i>t</i>
                                    <sub>22</sub> = 2.85, <i>P</i> = 0.005; protein: <i>t</i>
                                    <sub>22</sub> = 5.05, <i>P</i> &lt; 0.001; vitamins: <i>t</i>
                                    <sub>22</sub> = 3.25, <i>P</i> = 0.002). <b>b</b>, Results of an effective connectivity analysis between the value-encoding lOFC subregion and other lOFC subregions encoding each of the four nutrient factors. Box and whisker plots are as in <b>a</b>; <i>t</i> test (fat: <i>t</i>
                                    <sub>22</sub> = 0.62, <i>P</i> = 0.272; carb.: <i>t</i>
                                    <sub>22</sub> = 1.78, <i>P</i> = 0.045; protein: <i>t</i>
                                    <sub>22</sub> = 1.12, <i>P</i> = 0.137; vitamins: <i>t</i>
                                    <sub>22</sub> = 1.78, <i>P</i> = 0.045). While a significant connectivity effect was found for two of the factors (carb. and vitamins), the other two factors did not reach significance, and thus an overall significant conjunction effect was not found in lateral OFC. A.u., arbitrary units.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-017-0008-x/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec9">Representation of value and nutrient factors in other brain regions</h3><p>Previous studies demonstrated that value or reward signals are ubiquitously encoded not only in the OFC but also in other cortical regions, as well as in amygdala<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Vickery, T. J., Chun, M. M. &amp; Lee, D. Ubiquity and specificity of reinforcement signals throughout the human brain. Neuron 72, 166–177 (2011)." href="#ref-CR31" id="ref-link-section-d153902708e1861">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kahnt, T., Park, S. Q., Haynes, J.-D. &amp; Tobler, P. N. Disentangling neural representations of value and salience in the human brain. Proc. Natl. Acad. Sci. USA 111, 5000–5005 (2014)." href="#ref-CR32" id="ref-link-section-d153902708e1861_1">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Gottfried, J. A., O’Doherty, J. &amp; Dolan, R. J. Encoding predictive reward value in human amygdala and orbitofrontal cortex. Science 301, 1104–1107 (2003)." href="/articles/s41593-017-0008-x#ref-CR33" id="ref-link-section-d153902708e1864">33</a></sup>. As post hoc investigations beyond our original hypotheses, we tested for encoding of value and subjective nutrient factors in the following six ROIs: dorsomedial PFC (including anterior cingulate cortex), dorsolateral PFC, ventrolateral PFC, posterior parietal cortex (PPC), insula and amygdala (see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig12">7a</a> for information about the ROIs). Consistent with previous findings, all these ROIs were found to significantly encode information about subjective value (<i>P</i> &lt; 0.05 for all; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig12">7b</a>). On the other hand, information about the four subjective nutrient factors identified as value predictors could be significantly decoded only in the PPC (<i>P</i> &lt; 0.05, conjunction test; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig12">7c</a>), while ventrolateral PFC and dorsolateral PFC represented only one and two factors, respectively (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig12">7c</a>). However, when we applied a correction for multiple comparisons across these post hoc ROIs, the PPC ceased to be significant (<i>P</i> &gt; 0.05, conjunction test with Bonferroni correction). We also implemented a whole-brain searchlight analysis, which revealed that V1 also contained information about the four subjective nutrient factors (<i>P</i> &lt; 0.05 cluster-level FWE correction with the cluster-forming threshold <i>P</i> = 0.001, conjunction test; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig13">8</a>). These results are consistent with the possibility that not only lateral OFC but also V1 and potentially PPC represent information about the nutrient factors.</p><p>To further characterize the functional roles of the three regions, we performed the following additional analyses. First, we assessed whether those regions contain information about low-level visual features of the food images. The analyses revealed that information about low-level visual features could be decoded from both PPC and V1 (<i>P</i> &lt; 0.01; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig12">7d</a>), consistent with the previous findings that PPC and V1 are major parts of a visual pathway<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Mishkin, M., Ungerleider, L. G. &amp; Macko, K. A. Object vision and spatial vision: two cortical pathways. Trends Neurosci. 6, 414–417 (1983)." href="/articles/s41593-017-0008-x#ref-CR34" id="ref-link-section-d153902708e1908">34</a></sup> (i.e., the ‘dorsal stream’). However, we note that this is not the case in the lateral OFC, where basic visual features were not represented (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig12">7d</a>; see also Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig4">4a</a>). Second, we compared the decoding accuracies of the low-level visual features with those of the subjective nutrient factors. Accuracy for the visual features was found to be significantly higher only in V1 (<i>P</i> &lt; 0.01; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig12">7d</a>), while we found the opposite pattern in PPC and the lateral OFC (<i>P</i> &lt; 0.05; Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig12">7d</a>). These results together may suggest a functional gradient from V1 to PPC and lateral OFC: V1 predominantly represents the visual information, lateral OFC predominantly represent the nutritive information, and PPC is the intermediate locus. However, because the PPC result did not survive correction for multiple comparisons across ROIs, this result should be treated with caution until it can be independently replicated.</p></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Discussion</h2><div class="c-article-section__content" id="Sec10-content"><p>This study elucidates the constituent nutritive attributes underlying valuation of food rewards. Behaviorally, we demonstrated that the subjective value of a food was best predicted by beliefs about the content of fat, carbohydrates, protein and vitamins. This result suggests that food value is computed at least in part through integrating information about elemental nutritive attributes.</p><p>We then uncovered how information about the constituent attributes is represented and integrated in the brain. MVPA of fMRI data revealed that, while both lateral and medial parts of the OFC represented value signals, only lateral OFC represented information about the subjective nutrient factors. Furthermore, we found evidence for effective connectivity between the value-related medial OFC subregion and the lateral OFC subregions representing each of the individual nutrient attributes. Recent human neuroimaging studies have demonstrated that medial OFC and adjacent regions of medial PFC encode value information independently of the category of goods as a ‘common currency’<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479–485 (2013)." href="/articles/s41593-017-0008-x#ref-CR6" id="ref-link-section-d153902708e1943">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Howard, J. D., Gottfried, J. A., Tobler, P. N. &amp; Kahnt, T. Identity-specific coding of future rewards in the human orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 112, 5195–5200 (2015)." href="/articles/s41593-017-0008-x#ref-CR8" id="ref-link-section-d153902708e1946">8</a></sup>, while lateral OFC encodes value in an identity-specific manner<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Howard, J. D., Gottfried, J. A., Tobler, P. N. &amp; Kahnt, T. Identity-specific coding of future rewards in the human orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 112, 5195–5200 (2015)." href="/articles/s41593-017-0008-x#ref-CR8" id="ref-link-section-d153902708e1950">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Klein-Flügge, M. C., Barron, H. C., Brodersen, K. H., Dolan, R. J. &amp; Behrens, T. E. J. Segregated encoding of reward-identity and stimulus-reward associations in human orbitofrontal cortex. J. Neurosci. 33, 3202–3211 (2013)." href="/articles/s41593-017-0008-x#ref-CR21" id="ref-link-section-d153902708e1953">21</a></sup>, and that identity-specific value representations are modulated by selective devaluation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Howard, J. D. &amp; Kahnt, T. Identity-specific reward representations in orbitofrontal cortex are modulated by selective devaluation. J. Neurosci. 37, 2627–2638 (2017)." href="/articles/s41593-017-0008-x#ref-CR35" id="ref-link-section-d153902708e1957">35</a></sup>. Our findings go beyond these previous studies, in that we elucidate which constituent attributes underlie the construction of food value and how the constituent attributes are represented in the OFC.</p><p>There has been substantial debate in the literature about the distinct roles of the lateral and medial OFC in value-based decision-making<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Noonan, M. P. et al. Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 107, 20547–20552 (2010)." href="/articles/s41593-017-0008-x#ref-CR36" id="ref-link-section-d153902708e1964">36</a></sup>. Based on cytoarchitectonic structures and patterns of connectivity, neuroanatomical studies have identified a broad distinction between the medial part of the OFC, including adjacent vmPFC and the lateral part of the OFC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Ongür, D. &amp; Price, J. L. The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans. Cereb. Cortex 10, 206–219 (2000)." href="/articles/s41593-017-0008-x#ref-CR22" id="ref-link-section-d153902708e1968">22</a></sup>. It has also been suggested that lateral OFC is involved in the initial assignment or representation of value<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Padoa-Schioppa, C. &amp; Assad, J. A. Neurons in the orbitofrontal cortex encode economic value. Nature 441, 223–226 (2006)." href="/articles/s41593-017-0008-x#ref-CR2" id="ref-link-section-d153902708e1972">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Noonan, M. P. et al. Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 107, 20547–20552 (2010)." href="/articles/s41593-017-0008-x#ref-CR36" id="ref-link-section-d153902708e1975">36</a></sup>, while medial OFC is more involved in a value comparison necessary for decision-making<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Noonan, M. P. et al. Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex. Proc. Natl. Acad. Sci. USA 107, 20547–20552 (2010)." href="/articles/s41593-017-0008-x#ref-CR36" id="ref-link-section-d153902708e1979">36</a></sup>. The present study, together with these previous findings, could lead to the conjecture that information about the elemental nutritive attributes of food is first represented in the lateral OFC and then subsequently integrated in the medial OFC to guide behavior. Our finding that the initial integration of food attributes needed to compute subjective value occurs in the medial OFC, alongside a lack of evidence that such integration occurs in the lateral OFC, raises the question of how the subjective value signal located in the lateral OFC is generated. One possibility is that this signal is a secondary representation elicited via reciprocal inputs from value signals in the medial OFC. However, further work will be necessary to investigate the nature of the local circuits within OFC in more detail in order to test this possibility.</p><p>In our experiment, participants were asked to report subjective values of food items (WTP task), and then rate the quantities of six nutrient factors contained in the same foods, as well as to provide an estimate of the overall caloric content (attribute-rating task). Due to the experimental design, one might argue that ratings about nutrient factors could be biased, in that the participants justified their subjective value ratings a posteriori. We believe this was not the case in our experiment. It is unlikely that participants were able to solve the complex multidimensional inverse problem: that is, to remember and artificially manipulate their ratings about the six nutrient factors to ensure consistency with the prior ratings of subjective value. Furthermore, an easier way to ensure the consistency would be to manipulate ratings about overall caloric content, but we found that the subjective caloric content was a poor predictor of the subjective value. Taken together, we conclude that the participants were unlikely to manipulate their ratings about nutrient factors to justify the subjective value ratings post hoc.</p><p>It is important to note that, while we have shown here that the value of a food reward can in part be predicted from beliefs about its subjective nutrient qualities, the overall value of a stimulus such as food is unlikely to depend exclusively on beliefs about nutritive composition. Instead, an individual’s history of past experience with that food, including the amount of past exposure to the food and the past pairing of that food with other positive and negative experiences, are also likely to play critical roles in determining overall value. Moreover, we have left open the possibility that the overall value of a food is driven by some nonlinear combinations of the constitutive nutritive attributes corresponding to hidden superordinate properties of the food. It is also worth noting that the overall value of a food can be affected by cultural factors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Rozin, P. &amp; Vollmecke, T. A. Food likes and dislikes. Annu. Rev. Nutr. 6, 433–456 (1986)." href="/articles/s41593-017-0008-x#ref-CR37" id="ref-link-section-d153902708e1990">37</a></sup>. While we recruited participants from the general population in the greater Los Angeles area (California, USA), people living in other regions such as Asia and Africa might potentially have different preferences for food. Yet while such overall preferences might vary based on culture, and this might lead to differences in the weightings given to different nutritive attributes in computing subjective value across cultures, there is no reason to expect that the fundamental aspects of the neuronal organization of the computation of food value from its elemental attribute representations would differ across cultures. This notwithstanding, a fruitful research agenda will involve quantifying all of the additional elemental and cultural factors that influence valuation, determining the neural representation of those variables and establishing how those various signals get integrated in order to compute an overall value.</p><p>To conclude, in this study, we provide substantial insights into how a value signal for a food reward can be constructed from its constituent nutritive attributes in the brain. Given that dysfunctional food-valuation processes may play a large role in the development of obesity and anorexia<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Foerde, K., Steinglass, J. E., Shohamy, D. &amp; Walsh, B. T. Neural mechanisms supporting maladaptive food choices in anorexia nervosa. Nat. Neurosci. 18, 1571–1573 (2015)." href="/articles/s41593-017-0008-x#ref-CR18" id="ref-link-section-d153902708e1997">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Carnell, S., Gibson, C., Benson, L., Ochner, C. N. &amp; Geliebter, A. Neuroimaging and obesity: current knowledge and future directions. Obes. Rev. 13, 43–56 (2012)." href="/articles/s41593-017-0008-x#ref-CR19" id="ref-link-section-d153902708e2000">19</a></sup>, our findings have implications for understanding neural and psychological mechanisms underlying eating disorders, which is an important step toward the goal of developing novel treatments for such disorders.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Methods</h2><div class="c-article-section__content" id="Sec11-content"><h3 class="c-article__sub-heading" id="Sec12">Participants</h3><p>We recruited 24 healthy participants from the general population, as part of the recruitment pool for the NIMH Caltech Conte center for social decision-making. Data from one participant were excluded due to technical problems with the fMRI scan. We therefore used the data from the remaining 23 participants (8 females; age, 30.7 ± 4.12 years, mean ± s.d.; and BMI, 23.51 ± 4.00, mean ± s.d.). All the participants were preassessed to exclude those with any previous history of neurological/psychiatric illness. We also confirmed that the participants were not on a diet or seeking to lose weight for any reason. They gave their informed written consent and received monetary and food rewards depending on their performance in the WTP task (see below) in addition to the participation fee of $50. No statistical methods were used to predetermine the sample size, but our sample size was motivated by those used in previous studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479–485 (2013)." href="/articles/s41593-017-0008-x#ref-CR6" id="ref-link-section-d153902708e2016">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chib, V. S., Rangel, A., Shimojo, S. &amp; O’Doherty, J. P. Evidence for a common representation of decision values for dissimilar goods in human ventromedial prefrontal cortex. J. Neurosci. 29, 12315–12320 (2009)." href="/articles/s41593-017-0008-x#ref-CR14" id="ref-link-section-d153902708e2019">14</a></sup>. The study protocol was approved by the Institutional Review Board of the California Institute of Technology.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec13">Stimuli</h3><p>In our experiment, we used 56 food items (for example, snacks, fruits, salads, etc.; some were selected from the previous study<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Hare, T. A., Malmaud, J. &amp; Rangel, A. Focusing attention on the health aspects of foods changes value signals in vmPFC and improves dietary choice. J. Neurosci. 31, 11077–11087 (2011)." href="/articles/s41593-017-0008-x#ref-CR38" id="ref-link-section-d153902708e2031">38</a></sup>; Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-017-0008-x#MOESM1">2</a>). These items were highly familiar and available at local stores. Indeed, during the attribute-rating task (see below), on average only 1.43 ± 2.84 food items (mean ± s.d.) of the 56 were rated as ‘not familiar at all’. Information about the objective nutrient factors of the items was obtained from the package label or from an online calorie counter.</p><p>All the items were presented to participants as high-resolution color images. Information about the low-level visual features of the images (luminance, contrast, red intensity, green intensity, blue intensity, hue, saturation and brightness) was extracted using the Image Processing Toolbox included with Matlab. For each of the images, red, green and blue intensities in each pixel were extracted using the toolbox, and then luminance was computed as the weighted sum of the intensities (0.2126 × red + 0.7152 × green + 0.0722 × blue). We also computed hue, saturation and brightness in each pixel using Matlab’s rgb2hsv function. For each whole image, each low-level visual feature was defined as the averaged values across all of the pixels. Finally, the (local) contrast of each image was defined as the s.d. of the pixel luminance values<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chikazoe, J., Lee, D. H., Kriegeskorte, N. &amp; Anderson, A. K. Population coding of affect across stimuli, modalities and individuals. Nat. Neurosci. 17, 1114–1122 (2014)." href="/articles/s41593-017-0008-x#ref-CR7" id="ref-link-section-d153902708e2041">7</a></sup>.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec14">Experimental tasks</h3><p>Participants performed the WTP task inside the MRI scanner, and then subsequently performed the attribute-rating task outside the scanner. To enhance participants’ motivation for the foods, we asked them to refrain from eating or drinking any liquids, besides water, for 3 h before the experiment. Compliance was confirmed by self-reports, and the participants’ hunger rating was on average 4.13 ± 0.92 (mean ± s.d.; scaled from 1, ‘not at all hungry’, to 6, ‘very hungry’). Furthermore, participants were asked to stay at the laboratory for 30 min after the experiment, during which time the only thing they were able to eat was the food obtained in the experiment.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec15">WTP task (inside the MRI scanner)</h4><p>Following the procedure used in previous studies from our laboratory<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479–485 (2013)." href="/articles/s41593-017-0008-x#ref-CR6" id="ref-link-section-d153902708e2060">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chib, V. S., Rangel, A., Shimojo, S. &amp; O’Doherty, J. P. Evidence for a common representation of decision values for dissimilar goods in human ventromedial prefrontal cortex. J. Neurosci. 29, 12315–12320 (2009)." href="/articles/s41593-017-0008-x#ref-CR14" id="ref-link-section-d153902708e2063">14</a></sup>, we employed a modified version of the BDM auction task<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Becker, G. M., DeGroot, M. H. &amp; Marschak, J. Measuring utility by a single-response sequential method. Behav. Sci. 9, 226–232 (1964)." href="/articles/s41593-017-0008-x#ref-CR39" id="ref-link-section-d153902708e2067">39</a></sup> to measure participants’ willingness to pay (i.e., subjective value) for food items (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1a</a>). In each trial of this task, a participant was endowed with $3 and made a bid ($0, $1, $2 or $3) for one of the 56 items. At the end of the experiment, the computer randomly selected one of the trials to be implemented. For the selected trial, a random counter-bid was drawn from {$0, $1, $2, $3} with equal probability. If the participant’s bid was equal to or greater than the counter-bid, he or she paid the counter-bid and received the food item. Otherwise, the participant kept the initial endowment $3 and received no food. The auction mechanism is incentive-compatible in the sense that the optimal strategy for the participants is to always bid the number closest to their true willingness to pay for obtaining that item<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Becker, G. M., DeGroot, M. H. &amp; Marschak, J. Measuring utility by a single-response sequential method. Behav. Sci. 9, 226–232 (1964)." href="/articles/s41593-017-0008-x#ref-CR39" id="ref-link-section-d153902708e2074">39</a></sup>. Participants were explicitly instructed in the optimal strategy, and using a questionnaire, we confirmed that they correctly understood the experimental mechanism. Furthermore, to control for effects of retail price, we instructed the participants that the amount of each food item was determined so that the retail price is around $4.</p><p>This task consisted of four fMRI runs of the 56 trials. In each of the runs 1 and 3, a randomly selected 28 of the 56 items were presented twice in random order (i.e., 56 trials per run). The other 28 items were presented twice in each of the runs 2 and 4. In total, participants made a bid four times for each food item. We refer to the averaged amount of bid (i.e., willingness to pay) over the four trials as the ‘subjective value’ of the item.</p><p>At the beginning of each trial, a participant was shown one food item (valuation phase, 3 s; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1a</a>). In the next phase, the participant made a bid for that item by pressing the key on a numeric keypad that corresponded to the bid dollar amount (bid phase, within 2.5 s). Here to dissociate the bid amount from the spatial information, mappings between keys and bid amounts were randomized across trials. The bid the participant made was immediately presented (feedback phase, 0.5 s), followed by a jittered intertrial interval (ITI phase, 2–12 s). During this task, participants failed to make a response only in 1.55 ± 2.94% of trials (mean ± s.d.), and the missed trials were modeled as a nuisance regressor in the fMRI analysis (see below).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec16">Attribute-rating task (outside the MRI scanner)</h4><p>Participants rated subjective nutrient factors of the 56 food items (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1b</a>). Notably, the instructions for the attribute-rating task was given after completing the WTP task, and thus the participants were not aware during the WTP task that they would be subsequently required to rate nutrient factors of the items. This helped us exclude an explanation for the behavioral and fMRI results in terms of somehow biasing or artificially inducing participants to focus on such food attributes at the time of valuation.</p><p>This task consisted of eight sessions. In each session, participants were asked to answer one of the following eight questions for the 56 items about the six nutrient factors as well as the overall calorie content and the familiarity:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">(1)</span>
                        
                          <p>how high is the item in fat?</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(2)</span>
                        
                          <p>how high is the item in carbohydrates?</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(3)</span>
                        
                          <p>how high is the item in protein?</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(4)</span>
                        
                          <p>how high is the item in vitamins?</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(5)</span>
                        
                          <p>how high is the item in sugar?</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(6)</span>
                        
                          <p>how high is the item in sodium (salt)?</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(7)</span>
                        
                          <p>how high is the item in calories?</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(8)</span>
                        
                          <p>how familiar is the item?</p>
                        
                      </li>
                    </ol>
                    <p>The order of the eight questions was randomized across participants. Notably, in this task the participants were asked to rate the ‘density’ of the nutrient factors in each food item. In the instruction sheets, we explicitly told participants, “please indicate your guess about the density of the nutrient, that is, the amount of the nutrient contained per unit of weight (for example, 10 oz. of the item).”</p><p>On each trial, the participant answered a question for one item on a continuous scale from ‘not at all’ to ‘very much’ by moving a red pointer, with no time constraint (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1b</a>). The initial position of the pointer was randomized on each trial, and the pointer moved toward the right (or left) by pressing the key [1] (or [2]) on a numeric keypad. The answer was finally registered by pressing the key [3].</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec17">Behavioral analyses</h3><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec18">Regression analysis with subjective nutrient factors</h4><p>To examine which combination of the six subjective nutrient factors provided the best prediction of the subjective value, we ran the following linear regression analysis. For each participant, we regressed the value of each item against the subjective nutrient factors. Comparing all the possible 2<sup>6</sup> = 64 models including none, some or all of the six factors, we found that a combination of the four factors fat, carbohydrates, protein and vitamins provided the best prediction performance (Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-017-0008-x#MOESM1">1</a>).</p><p>Here the prediction performance of each model (i.e., combination of the nutrient factors) was assessed by a leave-one-item-out cross-validation. That is, for each participant and each model, (i) we ran the regression analysis, leaving out one of the 56 items; (ii) computed the predicted value of the left-out item on the basis of the obtained regression coefficients; (iii) repeated the above procedure for each of the 56 items; and (iv) computed the correlation between the predicted and the actual values. The overall performance of the model was obtained by averaging the correlation across participants.</p><p>As a robustness check, we also conducted a logistic regression with the categorical predicted values (low and high subjective values split for each participant by the median value). The procedure was the same as for the linear regression, except that we used ‘accuracy’ as a measure of performance instead of the correlation between the predicted and actual values. The analysis demonstrated that, consistent with the linear regression results, the combination of the four factors fat, carbohydrates, protein and vitamins provided the best prediction (Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-017-0008-x#MOESM1">1</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec19">Regression analysis with objective nutrient factors</h4><p>We ran the same linear and logistic regression analyses, using the objective nutrient factors as explanatory variables (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1d,e</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec20">Regression analysis with the overall calorie content</h4><p>We also ran the same analyses using subjective or objective estimates of total calorie content (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig1">1d,e</a>).</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec21">fMRI data acquisition</h3><p>We collected the fMRI images using a 3 T Siemens (Erlangen) Trio scanner located at the Caltech Brain Imaging Center (Pasadena, CA) with a 32-channel radio frequency coil. The BOLD signal was measured using a one-shot T2*-weighted echo planar imaging sequence (Volume TR = 2,780 ms, TE = 30 ms, FA = 80°). We acquired 44 oblique slices (thickness = 3.0 mm, gap = 0 mm, FOV = 192 × 192 mm, matrix = 64 × 64) per volume. The slices were aligned 30° to the AC–PC plane to reduce signal dropout in the orbitofrontal area<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Deichmann, R., Gottfried, J. A., Hutton, C. &amp; Turner, R. Optimized EPI for fMRI studies of the orbitofrontal cortex. Neuroimage 19, 430–441 (2003)." href="/articles/s41593-017-0008-x#ref-CR40" id="ref-link-section-d153902708e2261">40</a></sup>. After the four functional runs, high-resolution (1 mm<sup>3</sup>) anatomical images were acquired using a standard MPRAGE pulse sequence (TR = 1,500 ms, TE = 2.63 ms, FA = 10°). The fMRI data were analyzed using SPM8 in Matlab R2013b on a MacBook Pro (Retina, 15-inch, mid-2015; Mac OS X 10.11.6). Data collection and analysis were not performed blind to the conditions of the experiments.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec22">fMRI data preprocessing</h3><p>fMRI images for each participant were preprocessed using the standard procedure in SPM8: after slice-timing correction, the images were realigned to the first volume to correct for participants’ motion, spatially normalized and temporally filtered (using a high-pass filter width of 128 s). Spatial smoothing with an 8-mm FWHM Gaussian kernel was applied to the fMRI images only for psychophysiological interaction analysis (see below) but not for MVPA or representational similarity analysis (RSA). For searchlight MVPA and RSA, smoothing was applied to the accuracy and the correlation maps, respectively, but not to the fMRI images (see below).</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec23">Multivoxel pattern analysis (MVPA)</h3><p>To examine whether information about subjective value can be decoded from patterns of fMRI response, we conducted a classification analysis, multivoxel pattern analysis (see below). Also, the same procedure was applied to the classification analyses for nutrient factors and low-level visual features.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec24">Classification samples</h4><p>We extracted voxel-wise fMRI responses to each food item as classification samples. For each participant and each run, we designed a general linear model (GLM). The GLM contained 28 regressors indicating the valuation phases (duration = 3 s) of the 28 different food items, as well as four regressors indicating the bid phases (duration = reaction time), feedback phases (duration = 0.5 s), timing of the key press (duration = 0 s) and missed trials (valuation phase, duration = 3 s). All the regressors were convolved with a canonical hemodynamic response function. In addition, six motion-correction parameters and the linear trend were included as regressors of no interest to account for motion-related artifacts. For each voxel, the parameter estimates of the first 28 regressors corresponded to the fMRI responses to each of the 28 food items in each run. The fMRI responses to each food item were then entered into the classification analysis as classification samples.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec25">Classification label</h4><p>In some participants, depending on their rating behaviors, distributions of subjective value or nutrient factors were highly skewed. To avoid the imbalance issue, for each participant, we median-split the set of the samples into ‘high’ and ‘low’ value labels. Note that the median split was done on a cross-run basis for each participant, which does not ensure that the labels were perfectly balanced in each run (see the “Classification algorithm” section, below).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec26">Classification algorithm</h4><p>We employed a linear support vector machine with a cost parameter <i>C</i> = 1 as a classifier. We performed the classification analysis using The Decoding Toolbox (TDT)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Hebart, M. N., Görgen, K. &amp; Haynes, J.-D. The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data. Front. Neuroinform. 8, 88 (2015)." href="/articles/s41593-017-0008-x#ref-CR41" id="ref-link-section-d153902708e2309">41</a></sup>. Classification accuracy was estimated using a leave-one-run-out cross-validation: for each of the four runs, a classifier was trained on the other three runs and tested on the remaining focal run; and the procedure was repeated for the four runs (accuracy scores were averaged).</p><p>More specifically, to avoid label imbalance bias in each run (see the “Classification label” section, above), we performed a bootstrap sampling procedure repeated 1,000 times<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Hebart, M. N., Görgen, K. &amp; Haynes, J.-D. The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data. Front. Neuroinform. 8, 88 (2015)." href="/articles/s41593-017-0008-x#ref-CR41" id="ref-link-section-d153902708e2316">41</a></sup>. That is, we randomly removed some samples (without replacement) to ensure that the number of samples in each label was equalized for each run; the above classification analysis was then performed on the balanced data; and the procedure was repeated 1,000 times resulting in an average classification accuracy.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec27">ROI analysis</h4><p>We anatomically defined regions of interest (ROIs), lateral OFC, medial OFC and other areas based on the AAL database<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Tzourio-Mazoyer, N. et al. Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. Neuroimage 15, 273–289 (2002)." href="/articles/s41593-017-0008-x#ref-CR42" id="ref-link-section-d153902708e2329">42</a></sup>. See Supplementary Figures <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-017-0008-x#MOESM1">2</a>a and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-017-0008-x#MOESM1">7a</a> for details. fMRI responses in each of the ROIs were entered into the above classification analysis. We then examined, for each ROI, whether the mean accuracy across participants was greater than 50% (chance, given the binary label) using one-sampled <i>t</i> tests (one-tailed). A two-tailed test was employed only for the cross-decoding analysis (see main text) to examine whether the mean accuracy was greater or less than 50%. We also employed a permutation test (permuting the classification labels within each participant 1,000 times; one-tailed) to check whether the mean accuracy was significantly greater than chance. See Allefeld, Görgon &amp; Haynes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Allefeld, C., Görgen, K. &amp; Haynes, J.-D. Valid population inference for information-based imaging: From the second-level t-test to prevalence inference. Neuroimage 141, 378–392 (2016)." href="/articles/s41593-017-0008-x#ref-CR43" id="ref-link-section-d153902708e2342">43</a></sup> for advanced issues pertaining to population-level inferences in MVPA studies.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec28">Searchlight analysis</h4><p>We also conducted a searchlight decoding analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Kriegeskorte, N., Goebel, R. &amp; Bandettini, P. Information-based functional brain mapping. Proc. Natl. Acad. Sci. USA 103, 3863–3868 (2006)." href="/articles/s41593-017-0008-x#ref-CR44" id="ref-link-section-d153902708e2354">44</a></sup> with a radius of 3 voxels (i.e., 9 mm), as in our previous studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479–485 (2013)." href="/articles/s41593-017-0008-x#ref-CR6" id="ref-link-section-d153902708e2358">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="McNamee, D., Liljeholm, M., Zika, O. &amp; O’Doherty, J. P. Characterizing the associative content of brain structures involved in habitual and goal-directed actions in humans: a multivariate fMRI study. J. Neurosci. 35, 3764–3771 (2015)." href="/articles/s41593-017-0008-x#ref-CR45" id="ref-link-section-d153902708e2361">45</a></sup>, within the entire OFC ROI (i.e., summation of the lateral and medial OFC). In this analysis, each participant’s accuracy map was spatially smoothed with an 8-mm FWHM Gaussian kernel and entered into the second-level analysis performed by SPM8. The statistical significance was assessed by <i>t</i> test vs. 50% with a voxel-level FWE small-volume correction within the lateral and medial anatomical OFC ROIs. For the whole-brain analysis, we employed a cluster-level FWE correction for multiple comparisons (cluster-forming threshold, <i>P</i> = 0.001).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec29">Conjunction test</h4><p>In the conjunction test<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Nichols, T., Brett, M., Andersson, J., Wager, T. &amp; Poline, J.-B. Valid conjunction inference with the minimum statistic. Neuroimage 25, 653–660 (2005)." href="/articles/s41593-017-0008-x#ref-CR28" id="ref-link-section-d153902708e2379">28</a></sup>, if all of the individual factors are significantly decoded (<i>P</i> &lt; 0.05), we reject the null hypothesis that at least one of the factors was not represented; such result thus supports the alternative hypothesis that all of the factors were represented. In this study, we mainly employed conjunction analyses using <i>t</i> tests, while for some key results we also performed conjunction tests based on a permutation test (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig3">3a</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec30">Additional analysis (regressing out the effects of value)</h4><p>In this analysis, we regressed out the effect of value from both the ratings about nutrient factors (i.e., classification labels) and the fMRI responses to each food item (i.e., classification samples), and then tested whether each of the nutrient factors could be still decoded. For the ratings data, in each participant we regressed values of food items against the ratings about each of the nutrient factors and took the residuals. We also regressed values against the fMRI responses to food items and then obtained the residuals (note: this procedure was performed for each participant and each run).</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec31">Representational similarity analysis (RSA)</h3><p>To further examine the manner in which subjective nutritive information is represented in the OFC, we performed a representational similarity analysis (RSA)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Kriegeskorte, N. &amp; Kievit, R. A. Representational geometry: integrating cognition, computation, and the brain. Trends Cogn. Sci. 17, 401–412 (2013)." href="/articles/s41593-017-0008-x#ref-CR29" id="ref-link-section-d153902708e2409">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/s41593-017-0008-x#ref-CR46" id="ref-link-section-d153902708e2412">46</a></sup>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec32">Voxel-wise representational dissimilar matrix (RDM)</h4><p>As in the case of MVPA (see the “Classification samples” section, above), we extracted voxel-wise fMRI responses to each food item for each participant and each run. Averaging the fMRI responses over the runs, we estimated each voxel’s response to each item for each participant (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig10">5a</a>). We then created an RDM based on the correlation distance (i.e., 1 – Pearson’s correlation coefficient across voxels) for each pair of the 56 items (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig10">5a</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec33">Behavioral RDM</h4><p>A behavioral RDM was created based on the correlation distance for each item pair in bundles of the four subjective nutrient factors (fat, carbohydrates, protein and vitamins; and for each nutrient factor, rating values were <i>z</i>-normalized across the items). Note that the correlation distance in bundles reflects the dissimilarity between two items in terms of the relative contents of the four nutrient factors (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig10">5b</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec34">Comparison of voxel-wise and behavioral RDMs</h4><p>We computed the Spearman’s rank correlation between upper triangular portions of the voxel-wise and the behavioral RDMs. The Fisher <i>z</i>-transformed correlation coefficient for each participant was then entered into the population-level inference.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec35">ROI analysis</h4><p>For the lateral and medial OFC ROIs (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig7">2a</a>), we performed the above analysis and then examined whether the mean correlation coefficient was greater than 0 using one-sampled <i>t</i> tests (one-tailed).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec36">Searchlight analysis</h4><p>We also conducted a searchlight analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Kriegeskorte, N., Goebel, R. &amp; Bandettini, P. Information-based functional brain mapping. Proc. Natl. Acad. Sci. USA 103, 3863–3868 (2006)." href="/articles/s41593-017-0008-x#ref-CR44" id="ref-link-section-d153902708e2477">44</a></sup> with a radius of 3 voxels (i.e., 9 mm), as in the MVPA, within the entire OFC ROI (i.e., summation of the lateral and medial OFC ROIs). In this analysis, each participant’s correlation map was spatially smoothed with an 8-mm FWHM Gaussian kernel and entered into the second-level random-effect analysis performed by SPM8. The statistical significance was assessed by performing a <i>t</i> test vs. 0 with a voxel-level FWE small-volume correction within the lateral and medial anatomical OFC ROIs.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec37">Psychophysiological interaction (PPI) analysis</h3><p>Following the standard procedure in SPM8, we performed a PPI analysis on the spatially smoothed fMRI images, as follows.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec38">Extraction of BOLD signals</h4><p>We first constructed a GLM for the extraction of BOLD signals. The GLM contained regressors indicating the valuation phase (duration = 3 s), bid phase (duration = reaction time), feedback phase (duration = 0.5 s), timing of the key press (duration = 0 s), missed trials (valuation phase, duration = 3 s), six motion-correction parameters and the linear trend, as well as parametric modulators of the valuation phase regressor depicting the subjective value and the four subjective nutrient factors (<i>z</i>-normalized across items). Based on the GLM, we extracted BOLD signals (eigenvariates adjusted for the valuation phase) from the lateral and medial OFC ROIs identified as encoding value information by the searchlight MVPA (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig2">2b</a>; spheres with a radius of 3 voxels centered at the respective peak voxels).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec39">PPI model specification and estimation</h4><p>We then constructed another GLM for the PPI analysis including the following regressors: (i) a physiological factor, the BOLD signal from lOFC; (ii) a physiological factor, the BOLD signal from mOFC; (iii) a psychological factor, the boxcar regressor indicating the valuation phase (duration = 3 s; we call this regressor VAL); (iv) a psychophysiological interaction (PPI) factor, an interaction of the deconvolved lOFC BOLD signal and the psychological factor (VAL); and (v) a PPI factor, an interaction of the deconvolved mOFC BOLD signal and the psychological factor (VAL):</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$Y\,=\,{\beta }_{1}\,{\rm{l}}{\rm{O}}{\rm{F}}{\rm{C}}\,+\,{\beta }_{2}\,{\rm{m}}{\rm{O}}{\rm{F}}{\rm{C}}\,+\,{\beta }_{3}\,{\rm{V}}{\rm{A}}{\rm{L}}\,+\,{\beta }_{4}\,{\rm{l}}{\rm{O}}{\rm{F}}{\rm{C}}\,\times {\rm{V}}{\rm{A}}{\rm{L}}\,+\,{\beta }_{5}\,{\rm{m}}{\rm{O}}{\rm{F}}{\rm{C}}\,\times {\rm{V}}{\rm{A}}{\rm{L}}\,\phantom{\rule{0ex}{0ex}}+\,{\boldsymbol{X}}\beta \,+\,\varepsilon $$</span></div></div>
                    <p>where <i>Y</i> denotes a BOLD signal in the target ROI, <b><i>X</i></b> is a set of the other regressors (see below), <i>β</i> values indicate regression coefficients, and <i>ε</i> represents the residual. Note that in the PPI analysis, the mOFC BOLD signal, the lOFC BOLD signal and the corresponding PPI factors were included in the same GLM. To control for nuisance effects, we included four regressors indicating bid phases (duration = reaction time), feedback phases (duration = 0.5 s), timing of the key press (duration = 0 s) and missed trials (valuation phase, duration = 3 s), as well as parametric modulators of the valuation-phase regressor representing the subjective value and the four subjective nutrient factors of the presented food item. All of the regressors except for the physiological factors were convolved with a canonical HRF. In addition, six motion-correction parameters were included as regressors of no interest to account for motion-related artifacts. For each participant, regression coefficients of the PPI factors were estimated at the lateral OFC ROIs identified in the searchlight MVPA as representing each of the four subjective nutrient factors (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-017-0008-x#Fig3">3c</a>; spheres with a radius of 3 voxels centered at the respective peak voxels).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec40">Statistical test of the PPI effect</h4><p>We then examined for each of the four ROIs to determine whether the mean regression coefficient across participants was greater than 0 using one-sampled <i>t</i> tests (one-tailed). To further support the examination, we also employed a bootstrap test<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Efron, B. &amp; Tibshirani, R. J. An Introduction to the Bootstrap (CRC Press, 1993)." href="/articles/s41593-017-0008-x#ref-CR30" id="ref-link-section-d153902708e2826">30</a></sup>, which is known to be relatively robust against potential outliers. In the bootstrap test, we obtained 100,000 bootstrap datasets of the same size as the original sample size by resampling from the original data with replacement; then obtained the distribution of their mean values; finally we tested whether the 5% quintile of the distribution was greater than 0.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec41">Overview of the statistical tests used in the present study</h3><p>Parametric tests were used with the assumption of normality (the normality of the data was not formally tested). This approach is typical in the analysis approaches used for neuroimaging<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. Nat. Neurosci. 16, 479–485 (2013)." href="/articles/s41593-017-0008-x#ref-CR6" id="ref-link-section-d153902708e2839">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="McNamee, D., Liljeholm, M., Zika, O. &amp; O’Doherty, J. P. Characterizing the associative content of brain structures involved in habitual and goal-directed actions in humans: a multivariate fMRI study. J. Neurosci. 35, 3764–3771 (2015)." href="/articles/s41593-017-0008-x#ref-CR45" id="ref-link-section-d153902708e2842">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Friston, K.J., Ashburner, J.T., Kiebel, S.J., Nichols, T.E. &amp; Penny, W.D. Statistical Parametric Mapping: the Analysis of Functional Brain Images (Academic Press, 2006)." href="/articles/s41593-017-0008-x#ref-CR47" id="ref-link-section-d153902708e2845">47</a></sup>. It is worth noting that, for some key results, we also conducted permutation tests and bootstrap tests, which do not require normality assumptions about the data. We employed one-tailed tests unless otherwise noted, as the tests examined whether the decoding accuracy is greater than chance. A two-tailed test was employed for the cross-decoding analysis (see the main text) to examine whether the mean accuracy was greater or less than 50%. In searchlight analyses, the statistical significance was assessed with a voxel-level FWE small-volume correction for the ROI analyses and a cluster-level FWE correction (cluster-forming threshold, <i>P</i> = 0.001) for the whole-brain analysis. A Life Sciences Reporting Summary is available.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec42">Data and code availability</h3><p>The data and code that support the findings of this study are available from the corresponding author upon reasonable request. The MRI data will also be posted to the NDARS data repository at <a href="https://ndar.nih.gov/edit_collection.html?id=2417">https://ndar.nih.gov/edit_collection.html?id=2417</a> after publication.</p></div></div></section>
                </div>
            

            <div>
                <div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Clithero, J. A. &amp; Rangel, A. Informatic parcellation of the network involved in the computation of subjective value. <i>Soc. Cogn. Affect. Neurosci.</i> <b>9</b>, 1289–1302 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/scan/nst106" data-track-action="article reference" href="https://doi.org/10.1093%2Fscan%2Fnst106" aria-label="Article reference 1" data-doi="10.1093/scan/nst106">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23887811" aria-label="PubMed reference 1">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Informatic%20parcellation%20of%20the%20network%20involved%20in%20the%20computation%20of%20subjective%20value&amp;journal=Soc.%20Cogn.%20Affect.%20Neurosci.&amp;doi=10.1093%2Fscan%2Fnst106&amp;volume=9&amp;pages=1289-1302&amp;publication_year=2014&amp;author=Clithero%2CJA&amp;author=Rangel%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Padoa-Schioppa, C. &amp; Assad, J. A. Neurons in the orbitofrontal cortex encode economic value. <i>Nature</i> <b>441</b>, 223–226 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature04676" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature04676" aria-label="Article reference 2" data-doi="10.1038/nature04676">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28XksVGnsrk%3D" aria-label="CAS reference 2">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16633341" aria-label="PubMed reference 2">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2630027" aria-label="PubMed Central reference 2">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Neurons%20in%20the%20orbitofrontal%20cortex%20encode%20economic%20value&amp;journal=Nature&amp;doi=10.1038%2Fnature04676&amp;volume=441&amp;pages=223-226&amp;publication_year=2006&amp;author=Padoa-Schioppa%2CC&amp;author=Assad%2CJA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Rich, E. L. &amp; Wallis, J. D. Decoding subjective decisions from orbitofrontal cortex. <i>Nat. Neurosci.</i> <b>19</b>, 973–980 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.4320" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.4320" aria-label="Article reference 3" data-doi="10.1038/nn.4320">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC28XpsVCrsr0%3D" aria-label="CAS reference 3">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27273768" aria-label="PubMed reference 3">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4925198" aria-label="PubMed Central reference 3">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20subjective%20decisions%20from%20orbitofrontal%20cortex&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.4320&amp;volume=19&amp;pages=973-980&amp;publication_year=2016&amp;author=Rich%2CEL&amp;author=Wallis%2CJD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Rudebeck, P. H. &amp; Murray, E. A. The orbitofrontal oracle: cortical mechanisms for the prediction and evaluation of specific behavioral outcomes. <i>Neuron</i> <b>84</b>, 1143–1156 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2014.10.049" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2014.10.049" aria-label="Article reference 4" data-doi="10.1016/j.neuron.2014.10.049">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXntV2ntg%3D%3D" aria-label="CAS reference 4">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25521376" aria-label="PubMed reference 4">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4271193" aria-label="PubMed Central reference 4">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20orbitofrontal%20oracle%3A%20cortical%20mechanisms%20for%20the%20prediction%20and%20evaluation%20of%20specific%20behavioral%20outcomes&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2014.10.049&amp;volume=84&amp;pages=1143-1156&amp;publication_year=2014&amp;author=Rudebeck%2CPH&amp;author=Murray%2CEA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Grabenhorst, F. &amp; Rolls, E. T. Value, pleasure and choice in the ventral prefrontal cortex. <i>Trends Cogn. Sci.</i> <b>15</b>, 56–67 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2010.12.004" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2010.12.004" aria-label="Article reference 5" data-doi="10.1016/j.tics.2010.12.004">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21216655" aria-label="PubMed reference 5">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Value%2C%20pleasure%20and%20choice%20in%20the%20ventral%20prefrontal%20cortex&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2010.12.004&amp;volume=15&amp;pages=56-67&amp;publication_year=2011&amp;author=Grabenhorst%2CF&amp;author=Rolls%2CET">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">McNamee, D., Rangel, A. &amp; O’Doherty, J. P. Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex. <i>Nat. Neurosci.</i> <b>16</b>, 479–485 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.3337" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.3337" aria-label="Article reference 6" data-doi="10.1038/nn.3337">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXis1ahtLg%3D" aria-label="CAS reference 6">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23416449" aria-label="PubMed reference 6">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3665508" aria-label="PubMed Central reference 6">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Category-dependent%20and%20category-independent%20goal-value%20codes%20in%20human%20ventromedial%20prefrontal%20cortex&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.3337&amp;volume=16&amp;pages=479-485&amp;publication_year=2013&amp;author=McNamee%2CD&amp;author=Rangel%2CA&amp;author=O%E2%80%99Doherty%2CJP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Chikazoe, J., Lee, D. H., Kriegeskorte, N. &amp; Anderson, A. K. Population coding of affect across stimuli, modalities and individuals. <i>Nat. Neurosci.</i> <b>17</b>, 1114–1122 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.3749" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.3749" aria-label="Article reference 7" data-doi="10.1038/nn.3749">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXhtVais7jM" aria-label="CAS reference 7">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24952643" aria-label="PubMed reference 7">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4317366" aria-label="PubMed Central reference 7">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Population%20coding%20of%20affect%20across%20stimuli%2C%20modalities%20and%20individuals&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.3749&amp;volume=17&amp;pages=1114-1122&amp;publication_year=2014&amp;author=Chikazoe%2CJ&amp;author=Lee%2CDH&amp;author=Kriegeskorte%2CN&amp;author=Anderson%2CAK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Howard, J. D., Gottfried, J. A., Tobler, P. N. &amp; Kahnt, T. Identity-specific coding of future rewards in the human orbitofrontal cortex. <i>Proc. Natl. Acad. Sci. USA</i> <b>112</b>, 5195–5200 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1503550112" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1503550112" aria-label="Article reference 8" data-doi="10.1073/pnas.1503550112">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXlvVGrsLs%3D" aria-label="CAS reference 8">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25848032" aria-label="PubMed reference 8">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4413264" aria-label="PubMed Central reference 8">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Identity-specific%20coding%20of%20future%20rewards%20in%20the%20human%20orbitofrontal%20cortex&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1503550112&amp;volume=112&amp;pages=5195-5200&amp;publication_year=2015&amp;author=Howard%2CJD&amp;author=Gottfried%2CJA&amp;author=Tobler%2CPN&amp;author=Kahnt%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Lebreton, M., Jorge, S., Michel, V., Thirion, B. &amp; Pessiglione, M. An automatic valuation system in the human brain: evidence from functional neuroimaging. <i>Neuron</i> <b>64</b>, 431–439 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2009.09.040" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2009.09.040" aria-label="Article reference 9" data-doi="10.1016/j.neuron.2009.09.040">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXhsFylsbvM" aria-label="CAS reference 9">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19914190" aria-label="PubMed reference 9">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20automatic%20valuation%20system%20in%20the%20human%20brain%3A%20evidence%20from%20functional%20neuroimaging&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2009.09.040&amp;volume=64&amp;pages=431-439&amp;publication_year=2009&amp;author=Lebreton%2CM&amp;author=Jorge%2CS&amp;author=Michel%2CV&amp;author=Thirion%2CB&amp;author=Pessiglione%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Small, D. M. et al. Dissociation of neural representation of intensity and affective valuation in human gustation. <i>Neuron</i> <b>39</b>, 701–711 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0896-6273(03)00467-7" data-track-action="article reference" href="https://doi.org/10.1016%2FS0896-6273%2803%2900467-7" aria-label="Article reference 10" data-doi="10.1016/S0896-6273(03)00467-7">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3sXmslOmu7g%3D" aria-label="CAS reference 10">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12925283" aria-label="PubMed reference 10">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissociation%20of%20neural%20representation%20of%20intensity%20and%20affective%20valuation%20in%20human%20gustation&amp;journal=Neuron&amp;doi=10.1016%2FS0896-6273%2803%2900467-7&amp;volume=39&amp;pages=701-711&amp;publication_year=2003&amp;author=Small%2CDM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Kable, J. W. &amp; Glimcher, P. W. The neural correlates of subjective value during intertemporal choice. <i>Nat. Neurosci.</i> <b>10</b>, 1625–1633 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn2007" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn2007" aria-label="Article reference 11" data-doi="10.1038/nn2007">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2sXhtlKltLfO" aria-label="CAS reference 11">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17982449" aria-label="PubMed reference 11">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2845395" aria-label="PubMed Central reference 11">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20neural%20correlates%20of%20subjective%20value%20during%20intertemporal%20choice&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn2007&amp;volume=10&amp;pages=1625-1633&amp;publication_year=2007&amp;author=Kable%2CJW&amp;author=Glimcher%2CPW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Stalnaker, T. A. et al. Orbitofrontal neurons infer the value and identity of predicted outcomes. <i>Nat. Commun</i> <b>5</b>, 3926 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/ncomms4926" data-track-action="article reference" href="https://doi.org/10.1038%2Fncomms4926" aria-label="Article reference 12" data-doi="10.1038/ncomms4926">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXksVeqtb4%3D" aria-label="CAS reference 12">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24894805" aria-label="PubMed reference 12">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4056018" aria-label="PubMed Central reference 12">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Orbitofrontal%20neurons%20infer%20the%20value%20and%20identity%20of%20predicted%20outcomes&amp;journal=Nat.%20Commun&amp;doi=10.1038%2Fncomms4926&amp;volume=5&amp;publication_year=2014&amp;author=Stalnaker%2CTA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Gross, J. et al. Value signals in the prefrontal cortex predict individual preferences across reward categories. <i>J. Neurosci.</i> <b>34</b>, 7580–7586 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.5082-13.2014" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.5082-13.2014" aria-label="Article reference 13" data-doi="10.1523/JNEUROSCI.5082-13.2014">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXpt12rtLo%3D" aria-label="CAS reference 13">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24872562" aria-label="PubMed reference 13">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Value%20signals%20in%20the%20prefrontal%20cortex%20predict%20individual%20preferences%20across%20reward%20categories&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.5082-13.2014&amp;volume=34&amp;pages=7580-7586&amp;publication_year=2014&amp;author=Gross%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Chib, V. S., Rangel, A., Shimojo, S. &amp; O’Doherty, J. P. Evidence for a common representation of decision values for dissimilar goods in human ventromedial prefrontal cortex. <i>J. Neurosci.</i> <b>29</b>, 12315–12320 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.2575-09.2009" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.2575-09.2009" aria-label="Article reference 14" data-doi="10.1523/JNEUROSCI.2575-09.2009">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXht12qu7rI" aria-label="CAS reference 14">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19793990" aria-label="PubMed reference 14">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Evidence%20for%20a%20common%20representation%20of%20decision%20values%20for%20dissimilar%20goods%20in%20human%20ventromedial%20prefrontal%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.2575-09.2009&amp;volume=29&amp;pages=12315-12320&amp;publication_year=2009&amp;author=Chib%2CVS&amp;author=Rangel%2CA&amp;author=Shimojo%2CS&amp;author=O%E2%80%99Doherty%2CJP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Levy, D. J. &amp; Glimcher, P. W. Comparing apples and oranges: using reward-specific and reward-general subjective value representation in the brain. <i>J. Neurosci.</i> <b>31</b>, 14693–14707 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.2218-11.2011" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.2218-11.2011" aria-label="Article reference 15" data-doi="10.1523/JNEUROSCI.2218-11.2011">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXhtlCgtrrI" aria-label="CAS reference 15">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21994386" aria-label="PubMed reference 15">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3763520" aria-label="PubMed Central reference 15">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparing%20apples%20and%20oranges%3A%20using%20reward-specific%20and%20reward-general%20subjective%20value%20representation%20in%20the%20brain&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.2218-11.2011&amp;volume=31&amp;pages=14693-14707&amp;publication_year=2011&amp;author=Levy%2CDJ&amp;author=Glimcher%2CPW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Suzuki, S. et al. Learning to simulate others’ decisions. <i>Neuron</i> <b>74</b>, 1125–1137 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2012.04.030" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2012.04.030" aria-label="Article reference 16" data-doi="10.1016/j.neuron.2012.04.030">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XptVKrtbk%3D" aria-label="CAS reference 16">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22726841" aria-label="PubMed reference 16">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20to%20simulate%20others%E2%80%99%20decisions&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2012.04.030&amp;volume=74&amp;pages=1125-1137&amp;publication_year=2012&amp;author=Suzuki%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Suzuki, S., Adachi, R., Dunne, S., Bossaerts, P. &amp; O’Doherty, J. P. Neural mechanisms underlying human consensus decision-making. <i>Neuron</i> <b>86</b>, 591–602 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2015.03.019" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2015.03.019" aria-label="Article reference 17" data-doi="10.1016/j.neuron.2015.03.019">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXmsVeqtL4%3D" aria-label="CAS reference 17">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25864634" aria-label="PubMed reference 17">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4409560" aria-label="PubMed Central reference 17">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20mechanisms%20underlying%20human%20consensus%20decision-making&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2015.03.019&amp;volume=86&amp;pages=591-602&amp;publication_year=2015&amp;author=Suzuki%2CS&amp;author=Adachi%2CR&amp;author=Dunne%2CS&amp;author=Bossaerts%2CP&amp;author=O%E2%80%99Doherty%2CJP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Foerde, K., Steinglass, J. E., Shohamy, D. &amp; Walsh, B. T. Neural mechanisms supporting maladaptive food choices in anorexia nervosa. <i>Nat. Neurosci.</i> <b>18</b>, 1571–1573 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.4136" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.4136" aria-label="Article reference 18" data-doi="10.1038/nn.4136">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXhs1ChurfM" aria-label="CAS reference 18">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26457555" aria-label="PubMed reference 18">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4624561" aria-label="PubMed Central reference 18">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20mechanisms%20supporting%20maladaptive%20food%20choices%20in%20anorexia%20nervosa&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.4136&amp;volume=18&amp;pages=1571-1573&amp;publication_year=2015&amp;author=Foerde%2CK&amp;author=Steinglass%2CJE&amp;author=Shohamy%2CD&amp;author=Walsh%2CBT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Carnell, S., Gibson, C., Benson, L., Ochner, C. N. &amp; Geliebter, A. Neuroimaging and obesity: current knowledge and future directions. <i>Obes. Rev.</i> <b>13</b>, 43–56 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1111/j.1467-789X.2011.00927.x" data-track-action="article reference" href="https://doi.org/10.1111%2Fj.1467-789X.2011.00927.x" aria-label="Article reference 19" data-doi="10.1111/j.1467-789X.2011.00927.x">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38Xis1CnsLs%3D" aria-label="CAS reference 19">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21902800" aria-label="PubMed reference 19">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Neuroimaging%20and%20obesity%3A%20current%20knowledge%20and%20future%20directions&amp;journal=Obes.%20Rev.&amp;doi=10.1111%2Fj.1467-789X.2011.00927.x&amp;volume=13&amp;pages=43-56&amp;publication_year=2012&amp;author=Carnell%2CS&amp;author=Gibson%2CC&amp;author=Benson%2CL&amp;author=Ochner%2CCN&amp;author=Geliebter%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Barron, H. C., Dolan, R. J. &amp; Behrens, T. E. J. Online evaluation of novel choices by simultaneous representation of multiple memories. <i>Nat. Neurosci.</i> <b>16</b>, 1492–1498 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.3515" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.3515" aria-label="Article reference 20" data-doi="10.1038/nn.3515">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhtl2kt7vI" aria-label="CAS reference 20">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24013592" aria-label="PubMed reference 20">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4001211" aria-label="PubMed Central reference 20">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Online%20evaluation%20of%20novel%20choices%20by%20simultaneous%20representation%20of%20multiple%20memories&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.3515&amp;volume=16&amp;pages=1492-1498&amp;publication_year=2013&amp;author=Barron%2CHC&amp;author=Dolan%2CRJ&amp;author=Behrens%2CTEJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Klein-Flügge, M. C., Barron, H. C., Brodersen, K. H., Dolan, R. J. &amp; Behrens, T. E. J. Segregated encoding of reward-identity and stimulus-reward associations in human orbitofrontal cortex. <i>J. Neurosci.</i> <b>33</b>, 3202–3211 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.2532-12.2013" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.2532-12.2013" aria-label="Article reference 21" data-doi="10.1523/JNEUROSCI.2532-12.2013">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23407973" aria-label="PubMed reference 21">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3586675" aria-label="PubMed Central reference 21">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Segregated%20encoding%20of%20reward-identity%20and%20stimulus-reward%20associations%20in%20human%20orbitofrontal%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.2532-12.2013&amp;volume=33&amp;pages=3202-3211&amp;publication_year=2013&amp;author=Klein-Fl%C3%BCgge%2CMC&amp;author=Barron%2CHC&amp;author=Brodersen%2CKH&amp;author=Dolan%2CRJ&amp;author=Behrens%2CTEJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Ongür, D. &amp; Price, J. L. The organization of networks within the orbital and medial prefrontal cortex of rats, monkeys and humans. <i>Cereb. Cortex</i> <b>10</b>, 206–219 (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/10.3.206" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2F10.3.206" aria-label="Article reference 22" data-doi="10.1093/cercor/10.3.206">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10731217" aria-label="PubMed reference 22">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20organization%20of%20networks%20within%20the%20orbital%20and%20medial%20prefrontal%20cortex%20of%20rats%2C%20monkeys%20and%20humans&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2F10.3.206&amp;volume=10&amp;pages=206-219&amp;publication_year=2000&amp;author=Ong%C3%BCr%2CD&amp;author=Price%2CJL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Tang, D. W., Fellows, L. K. &amp; Dagher, A. Behavioral and neural valuation of foods is driven by implicit knowledge of caloric content. <i>Psychol. Sci.</i> <b>25</b>, 2168–2176 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1177/0956797614552081" data-track-action="article reference" href="https://doi.org/10.1177%2F0956797614552081" aria-label="Article reference 23" data-doi="10.1177/0956797614552081">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25304885" aria-label="PubMed reference 23">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Behavioral%20and%20neural%20valuation%20of%20foods%20is%20driven%20by%20implicit%20knowledge%20of%20caloric%20content&amp;journal=Psychol.%20Sci.&amp;doi=10.1177%2F0956797614552081&amp;volume=25&amp;pages=2168-2176&amp;publication_year=2014&amp;author=Tang%2CDW&amp;author=Fellows%2CLK&amp;author=Dagher%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Zuker, C. S. Food for the brain. <i>Cell</i> <b>161</b>, 9–11 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.cell.2015.03.016" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cell.2015.03.016" aria-label="Article reference 24" data-doi="10.1016/j.cell.2015.03.016">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXls1ags7c%3D" aria-label="CAS reference 24">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25815979" aria-label="PubMed reference 24">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Food%20for%20the%20brain&amp;journal=Cell&amp;doi=10.1016%2Fj.cell.2015.03.016&amp;volume=161&amp;pages=9-11&amp;publication_year=2015&amp;author=Zuker%2CCS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">de Araujo, I. E. et al. Food reward in the absence of taste receptor signaling. <i>Neuron</i> <b>57</b>, 930–941 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2008.01.032" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2008.01.032" aria-label="Article reference 25" data-doi="10.1016/j.neuron.2008.01.032">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18367093" aria-label="PubMed reference 25">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Food%20reward%20in%20the%20absence%20of%20taste%20receptor%20signaling&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2008.01.032&amp;volume=57&amp;pages=930-941&amp;publication_year=2008&amp;author=Araujo%2CIE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Tellez, L. A. et al. Separate circuitries encode the hedonic and nutritional values of sugar. <i>Nat. Neurosci.</i> <b>19</b>, 465–470 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.4224" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.4224" aria-label="Article reference 26" data-doi="10.1038/nn.4224">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC28XhsFyit78%3D" aria-label="CAS reference 26">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26807950" aria-label="PubMed reference 26">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4767614" aria-label="PubMed Central reference 26">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Separate%20circuitries%20encode%20the%20hedonic%20and%20nutritional%20values%20of%20sugar&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.4224&amp;volume=19&amp;pages=465-470&amp;publication_year=2016&amp;author=Tellez%2CLA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Haynes, J.-D. A primer on pattern-based approaches to fMRI: principles, pitfalls, and perspectives. <i>Neuron</i> <b>87</b>, 257–270 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2015.05.025" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2015.05.025" aria-label="Article reference 27" data-doi="10.1016/j.neuron.2015.05.025">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXht1GrtLzK" aria-label="CAS reference 27">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26182413" aria-label="PubMed reference 27">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20primer%20on%20pattern-based%20approaches%20to%20fMRI%3A%20principles%2C%20pitfalls%2C%20and%20perspectives&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2015.05.025&amp;volume=87&amp;pages=257-270&amp;publication_year=2015&amp;author=Haynes%2CJ-D">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Nichols, T., Brett, M., Andersson, J., Wager, T. &amp; Poline, J.-B. Valid conjunction inference with the minimum statistic. <i>Neuroimage</i> <b>25</b>, 653–660 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2004.12.005" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2004.12.005" aria-label="Article reference 28" data-doi="10.1016/j.neuroimage.2004.12.005">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15808966" aria-label="PubMed reference 28">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Valid%20conjunction%20inference%20with%20the%20minimum%20statistic&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2004.12.005&amp;volume=25&amp;pages=653-660&amp;publication_year=2005&amp;author=Nichols%2CT&amp;author=Brett%2CM&amp;author=Andersson%2CJ&amp;author=Wager%2CT&amp;author=Poline%2CJ-B">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Kriegeskorte, N. &amp; Kievit, R. A. Representational geometry: integrating cognition, computation, and the brain. <i>Trends Cogn. Sci.</i> <b>17</b>, 401–412 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2013.06.007" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2013.06.007" aria-label="Article reference 29" data-doi="10.1016/j.tics.2013.06.007">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23876494" aria-label="PubMed reference 29">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3730178" aria-label="PubMed Central reference 29">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Representational%20geometry%3A%20integrating%20cognition%2C%20computation%2C%20and%20the%20brain&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2013.06.007&amp;volume=17&amp;pages=401-412&amp;publication_year=2013&amp;author=Kriegeskorte%2CN&amp;author=Kievit%2CRA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Efron, B. &amp; Tibshirani, R. J. <i>An Introduction to the Bootstrap</i> (CRC Press, 1993).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Vickery, T. J., Chun, M. M. &amp; Lee, D. Ubiquity and specificity of reinforcement signals throughout the human brain. <i>Neuron</i> <b>72</b>, 166–177 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2011.08.011" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2011.08.011" aria-label="Article reference 31" data-doi="10.1016/j.neuron.2011.08.011">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXht12kt7nL" aria-label="CAS reference 31">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21982377" aria-label="PubMed reference 31">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Ubiquity%20and%20specificity%20of%20reinforcement%20signals%20throughout%20the%20human%20brain&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2011.08.011&amp;volume=72&amp;pages=166-177&amp;publication_year=2011&amp;author=Vickery%2CTJ&amp;author=Chun%2CMM&amp;author=Lee%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Kahnt, T., Park, S. Q., Haynes, J.-D. &amp; Tobler, P. N. Disentangling neural representations of value and salience in the human brain. <i>Proc. Natl. Acad. Sci. USA</i> <b>111</b>, 5000–5005 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1320189111" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1320189111" aria-label="Article reference 32" data-doi="10.1073/pnas.1320189111">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXksVSmsLw%3D" aria-label="CAS reference 32">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24639493" aria-label="PubMed reference 32">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3977254" aria-label="PubMed Central reference 32">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Disentangling%20neural%20representations%20of%20value%20and%20salience%20in%20the%20human%20brain&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1320189111&amp;volume=111&amp;pages=5000-5005&amp;publication_year=2014&amp;author=Kahnt%2CT&amp;author=Park%2CSQ&amp;author=Haynes%2CJ-D&amp;author=Tobler%2CPN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Gottfried, J. A., O’Doherty, J. &amp; Dolan, R. J. Encoding predictive reward value in human amygdala and orbitofrontal cortex. <i>Science</i> <b>301</b>, 1104–1107 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1087919" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1087919" aria-label="Article reference 33" data-doi="10.1126/science.1087919">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3sXmsFWisbk%3D" aria-label="CAS reference 33">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12934011" aria-label="PubMed reference 33">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Encoding%20predictive%20reward%20value%20in%20human%20amygdala%20and%20orbitofrontal%20cortex&amp;journal=Science&amp;doi=10.1126%2Fscience.1087919&amp;volume=301&amp;pages=1104-1107&amp;publication_year=2003&amp;author=Gottfried%2CJA&amp;author=O%E2%80%99Doherty%2CJ&amp;author=Dolan%2CRJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Mishkin, M., Ungerleider, L. G. &amp; Macko, K. A. Object vision and spatial vision: two cortical pathways. <i>Trends Neurosci.</i> <b>6</b>, 414–417 (1983).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/0166-2236(83)90190-X" data-track-action="article reference" href="https://doi.org/10.1016%2F0166-2236%2883%2990190-X" aria-label="Article reference 34" data-doi="10.1016/0166-2236(83)90190-X">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Object%20vision%20and%20spatial%20vision%3A%20two%20cortical%20pathways&amp;journal=Trends%20Neurosci.&amp;doi=10.1016%2F0166-2236%2883%2990190-X&amp;volume=6&amp;pages=414-417&amp;publication_year=1983&amp;author=Mishkin%2CM&amp;author=Ungerleider%2CLG&amp;author=Macko%2CKA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Howard, J. D. &amp; Kahnt, T. Identity-specific reward representations in orbitofrontal cortex are modulated by selective devaluation. <i>J. Neurosci.</i> <b>37</b>, 2627–2638 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.3473-16.2017" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.3473-16.2017" aria-label="Article reference 35" data-doi="10.1523/JNEUROSCI.3473-16.2017">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2sXhvFSisLjI" aria-label="CAS reference 35">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28159906" aria-label="PubMed reference 35">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5354319" aria-label="PubMed Central reference 35">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Identity-specific%20reward%20representations%20in%20orbitofrontal%20cortex%20are%20modulated%20by%20selective%20devaluation&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.3473-16.2017&amp;volume=37&amp;pages=2627-2638&amp;publication_year=2017&amp;author=Howard%2CJD&amp;author=Kahnt%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Noonan, M. P. et al. Separate value comparison and learning mechanisms in macaque medial and lateral orbitofrontal cortex. <i>Proc. Natl. Acad. Sci. USA</i> <b>107</b>, 20547–20552 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1012246107" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1012246107" aria-label="Article reference 36" data-doi="10.1073/pnas.1012246107">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3cXhsFaisr%2FL" aria-label="CAS reference 36">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21059901" aria-label="PubMed reference 36">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996698" aria-label="PubMed Central reference 36">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Separate%20value%20comparison%20and%20learning%20mechanisms%20in%20macaque%20medial%20and%20lateral%20orbitofrontal%20cortex&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1012246107&amp;volume=107&amp;pages=20547-20552&amp;publication_year=2010&amp;author=Noonan%2CMP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Rozin, P. &amp; Vollmecke, T. A. Food likes and dislikes. <i>Annu. Rev. Nutr.</i> <b>6</b>, 433–456 (1986).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1146/annurev.nu.06.070186.002245" data-track-action="article reference" href="https://doi.org/10.1146%2Fannurev.nu.06.070186.002245" aria-label="Article reference 37" data-doi="10.1146/annurev.nu.06.070186.002245">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaL283mvFGhtA%3D%3D" aria-label="CAS reference 37">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=3524623" aria-label="PubMed reference 37">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Food%20likes%20and%20dislikes&amp;journal=Annu.%20Rev.%20Nutr.&amp;doi=10.1146%2Fannurev.nu.06.070186.002245&amp;volume=6&amp;pages=433-456&amp;publication_year=1986&amp;author=Rozin%2CP&amp;author=Vollmecke%2CTA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Hare, T. A., Malmaud, J. &amp; Rangel, A. Focusing attention on the health aspects of foods changes value signals in vmPFC and improves dietary choice. <i>J. Neurosci.</i> <b>31</b>, 11077–11087 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.6383-10.2011" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.6383-10.2011" aria-label="Article reference 38" data-doi="10.1523/JNEUROSCI.6383-10.2011">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXhtVSit77L" aria-label="CAS reference 38">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21795556" aria-label="PubMed reference 38">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Focusing%20attention%20on%20the%20health%20aspects%20of%20foods%20changes%20value%20signals%20in%20vmPFC%20and%20improves%20dietary%20choice&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.6383-10.2011&amp;volume=31&amp;pages=11077-11087&amp;publication_year=2011&amp;author=Hare%2CTA&amp;author=Malmaud%2CJ&amp;author=Rangel%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Becker, G. M., DeGroot, M. H. &amp; Marschak, J. Measuring utility by a single-response sequential method. <i>Behav. Sci.</i> <b>9</b>, 226–232 (1964).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1002/bs.3830090304" data-track-action="article reference" href="https://doi.org/10.1002%2Fbs.3830090304" aria-label="Article reference 39" data-doi="10.1002/bs.3830090304">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaF1c7lvVemsA%3D%3D" aria-label="CAS reference 39">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=5888778" aria-label="PubMed reference 39">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20utility%20by%20a%20single-response%20sequential%20method&amp;journal=Behav%20Sci&amp;doi=10.1002%2Fbs.3830090304&amp;volume=9&amp;pages=226-232&amp;publication_year=1964&amp;author=Becker%2CGM&amp;author=DeGroot%2CMH&amp;author=Marschak%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Deichmann, R., Gottfried, J. A., Hutton, C. &amp; Turner, R. Optimized EPI for fMRI studies of the orbitofrontal cortex. <i>Neuroimage</i> <b>19</b>, 430–441 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S1053-8119(03)00073-9" data-track-action="article reference" href="https://doi.org/10.1016%2FS1053-8119%2803%2900073-9" aria-label="Article reference 40" data-doi="10.1016/S1053-8119(03)00073-9">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD3s3ovVWmtQ%3D%3D" aria-label="CAS reference 40">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12814592" aria-label="PubMed reference 40">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimized%20EPI%20for%20fMRI%20studies%20of%20the%20orbitofrontal%20cortex&amp;journal=Neuroimage&amp;doi=10.1016%2FS1053-8119%2803%2900073-9&amp;volume=19&amp;pages=430-441&amp;publication_year=2003&amp;author=Deichmann%2CR&amp;author=Gottfried%2CJA&amp;author=Hutton%2CC&amp;author=Turner%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Hebart, M. N., Görgen, K. &amp; Haynes, J.-D. The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data. <i>Front. Neuroinform.</i> <b>8</b>, 88 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3389/fninf.2014.00088" data-track-action="article reference" href="https://doi.org/10.3389%2Ffninf.2014.00088" aria-label="Article reference 41" data-doi="10.3389/fninf.2014.00088">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25610393" aria-label="PubMed reference 41">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4285115" aria-label="PubMed Central reference 41">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Decoding%20Toolbox%20%28TDT%29%3A%20a%20versatile%20software%20package%20for%20multivariate%20analyses%20of%20functional%20imaging%20data&amp;journal=Front.%20Neuroinform.&amp;doi=10.3389%2Ffninf.2014.00088&amp;volume=8&amp;publication_year=2015&amp;author=Hebart%2CMN&amp;author=G%C3%B6rgen%2CK&amp;author=Haynes%2CJ-D">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Tzourio-Mazoyer, N. et al. Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. <i>Neuroimage</i> <b>15</b>, 273–289 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1006/nimg.2001.0978" data-track-action="article reference" href="https://doi.org/10.1006%2Fnimg.2001.0978" aria-label="Article reference 42" data-doi="10.1006/nimg.2001.0978">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD38%2FltFCntw%3D%3D" aria-label="CAS reference 42">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11771995" aria-label="PubMed reference 42">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20anatomical%20labeling%20of%20activations%20in%20SPM%20using%20a%20macroscopic%20anatomical%20parcellation%20of%20the%20MNI%20MRI%20single-subject%20brain&amp;journal=Neuroimage&amp;doi=10.1006%2Fnimg.2001.0978&amp;volume=15&amp;pages=273-289&amp;publication_year=2002&amp;author=Tzourio-Mazoyer%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Allefeld, C., Görgen, K. &amp; Haynes, J.-D. Valid population inference for information-based imaging: From the second-level <i>t</i>-test to prevalence inference. <i>Neuroimage</i> <b>141</b>, 378–392 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2016.07.040" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2016.07.040" aria-label="Article reference 43" data-doi="10.1016/j.neuroimage.2016.07.040">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27450073" aria-label="PubMed reference 43">PubMed</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Valid%20population%20inference%20for%20information-based%20imaging%3A%20From%20the%20second-level%20t-test%20to%20prevalence%20inference&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2016.07.040&amp;volume=141&amp;pages=378-392&amp;publication_year=2016&amp;author=Allefeld%2CC&amp;author=G%C3%B6rgen%2CK&amp;author=Haynes%2CJ-D">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Kriegeskorte, N., Goebel, R. &amp; Bandettini, P. Information-based functional brain mapping. <i>Proc. Natl. Acad. Sci. USA</i> <b>103</b>, 3863–3868 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.0600244103" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.0600244103" aria-label="Article reference 44" data-doi="10.1073/pnas.0600244103">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28XivFWisbw%3D" aria-label="CAS reference 44">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16537458" aria-label="PubMed reference 44">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1383651" aria-label="PubMed Central reference 44">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Information-based%20functional%20brain%20mapping&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.0600244103&amp;volume=103&amp;pages=3863-3868&amp;publication_year=2006&amp;author=Kriegeskorte%2CN&amp;author=Goebel%2CR&amp;author=Bandettini%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">McNamee, D., Liljeholm, M., Zika, O. &amp; O’Doherty, J. P. Characterizing the associative content of brain structures involved in habitual and goal-directed actions in humans: a multivariate fMRI study. <i>J. Neurosci.</i> <b>35</b>, 3764–3771 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.4677-14.2015" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.4677-14.2015" aria-label="Article reference 45" data-doi="10.1523/JNEUROSCI.4677-14.2015">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXhtVGhsLjJ" aria-label="CAS reference 45">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25740507" aria-label="PubMed reference 45">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4348182" aria-label="PubMed Central reference 45">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Characterizing%20the%20associative%20content%20of%20brain%20structures%20involved%20in%20habitual%20and%20goal-directed%20actions%20in%20humans%3A%20a%20multivariate%20fMRI%20study&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.4677-14.2015&amp;volume=35&amp;pages=3764-3771&amp;publication_year=2015&amp;author=McNamee%2CD&amp;author=Liljeholm%2CM&amp;author=Zika%2CO&amp;author=O%E2%80%99Doherty%2CJP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. <i>Front. Syst. Neurosci.</i> <b>2</b>, 4 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3389/neuro.01.016.2008" data-track-action="article reference" href="https://doi.org/10.3389%2Fneuro.01.016.2008" aria-label="Article reference 46" data-doi="10.3389/neuro.01.016.2008">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19104670" aria-label="PubMed reference 46">PubMed</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405" aria-label="PubMed Central reference 46">PubMed Central</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Representational%20similarity%20analysis%20-%20connecting%20the%20branches%20of%20systems%20neuroscience&amp;journal=Front.%20Syst.%20Neurosci.&amp;doi=10.3389%2Fneuro.01.016.2008&amp;volume=2&amp;publication_year=2008&amp;author=Kriegeskorte%2CN&amp;author=Mur%2CM&amp;author=Bandettini%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Friston, K.J., Ashburner, J.T., Kiebel, S.J., Nichols, T.E. &amp; Penny, W.D. <i>Statistical Parametric Mapping: the Analysis of Functional Brain Images</i> (Academic Press, 2006).</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-017-0008-x?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported by the JSPS Postdoctoral Fellowship for Research Abroad (S.S.), JSPS KAKENHI Grants JP17H05933 and JP17H06022 (S.S.) and the NIMH Caltech Conte Center for the Neurobiology of Social Decision Making (J.P.O.).</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Division of the Humanities and Social Sciences, California Institute of Technology, Pasadena, CA, USA</p><p class="c-article-author-affiliation__authors-list">Shinsuke Suzuki &amp; John P. O’Doherty</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Frontier Research Institute for Interdisciplinary Sciences, Tohoku University, Sendai, Japan</p><p class="c-article-author-affiliation__authors-list">Shinsuke Suzuki</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Institute of Development, Aging and Cancer, Tohoku University, Sendai, Japan</p><p class="c-article-author-affiliation__authors-list">Shinsuke Suzuki</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Computation and Neural Systems, California Institute of Technology, Pasadena, CA, USA</p><p class="c-article-author-affiliation__authors-list">Logan Cross &amp; John P. O’Doherty</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Shinsuke-Suzuki-Aff1-Aff2-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Shinsuke Suzuki</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Shinsuke%20Suzuki" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shinsuke%20Suzuki" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shinsuke%20Suzuki%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Logan-Cross-Aff4"><span class="c-article-authors-search__title u-h3 js-search-name">Logan Cross</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Logan%20Cross" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Logan%20Cross" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Logan%20Cross%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-John_P_-O_Doherty-Aff1-Aff4"><span class="c-article-authors-search__title u-h3 js-search-name">John P. O’Doherty</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=John%20P.%20O%E2%80%99Doherty" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=John%20P.%20O%E2%80%99Doherty" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22John%20P.%20O%E2%80%99Doherty%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>S.S., L.C. and J.P.O. designed the research; S.S. and L.C. carried out the experiment; S.S. and L.C. analyzed the data; and S.S., L.C. and J.P.O. wrote the paper.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:shinsuke.szk@gmail.com">Shinsuke Suzuki</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading" id="FPar1">Competing interests</h3>
                <p>The authors declare no competing financial interest.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>
                <b>Publisher’s note:</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Integrated supplementary information"><div class="c-article-section" id="Sec44-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec44">Integrated supplementary information</h2><div class="c-article-section__content" id="Sec44-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig6"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 1 ratings of nutrient factor." href="/articles/s41593-017-0008-x/figures/6" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig6_ESM.jpg">Supplementary Figure 1 Ratings of nutrient factor.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(a) Subjective ratings about the 56 food items. For each item, we plot the participants’ ratings about the six nutrient factors (<i>cyan</i>: fat; <i>magenta</i>: sodium; <i>black</i>: carbohydrate; <i>red</i>: sugar; <i>green</i>: protein; and <i>blue</i>: vitamin). See Table S2 for the item list. The rating data were z-normalized across the food items, within each participant and each nutrient factor. (b) Pair-wise correlations among subjective ratings of the nutrient factors (MEAN across participants; <i>n</i> = 23).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig7"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 2 supplementary results of th" href="/articles/s41593-017-0008-x/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig7_ESM.jpg">Supplementary Figure 2 Supplementary results of the neural representation of subjective value.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(a) Anatomical OFC ROIs used in this study. The ROIs are defined based on the AAL database<sup>42</sup> as follows: lOFC, bilateral MNI_Frontal_Mid_Orb + MNI_Frontal_Inf_Orb + MNI_Frontal_Sup_Orb; and mOFC, bilateral MNI_Frontal_Med_Orb. lOFC, lateral orbitofrontal cortex; and mOFC, medial orbitofrontal cortex. (b) Evidence for significant decoding of subjective value at the Bid phase (<i>left</i>) and at the Feedback phase (<i>right</i>) (<i>n</i> = 23 participants). The format is the same as in Fig. 2a (left). **<i>P</i> &lt; 0.01 and *<i>P</i> &lt; 0.05, <i>t</i>-test against 50% (Bid phase, lOFC: <i>t</i>
                                    <sub>22</sub> = 2.78, <i>P</i> = 0.005; mOFC: <i>t</i>
                                    <sub>22</sub> = 1.96, <i>P</i> = 0.031; and Feedback phase, lOFC: <i>t</i>
                                    <sub>22</sub> = 2.23, <i>P</i> = 0.018; mOFC: <i>t</i>
                                    <sub>22</sub> = 3.40, <i>P</i> = 0.001). (c) Weights of voxels in the value classifiers obtained from the ROI analyses (see Fig. 2a). We plot the weights of the voxels for each participant within the lOFC (<i>left</i>) and the mOFC (<i>right</i>) ROIs separately. Format of the box and whisker plots is the same as in Fig. 1c. (d) Weights of voxels in the value classifiers obtained from the searchlight analyses (see Fig. 2b). We plot the weights of the voxels within a radius of 3 voxels (i.e., 9 mm) around the peak voxels in lOFC (<i>left</i>) and mOFC (<i>right</i>). See Fig. 2b for information about the peak voxels. Format of the box and whisker plots is the same as in Fig. 1c.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig8"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 3 classification scores in th" href="/articles/s41593-017-0008-x/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig8_ESM.jpg">Supplementary Figure 3 Classification scores in the decoding analysis for subjective nutrient factors.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>We plot the classification scores in the lOFC ROI obtained by the classifier trained on fat, carb., protein and vitamin respectively as functions of the subjective nutrititive ratings (MEAN ± SEM across participants; <i>n</i> = 23; see Fig. 3a). Note that ratings for each nutrient factors were binned based on the rank order; that each classifier is trained to discriminate high vs. low ratings (i.e., 1 &amp; 2 vs. 3 &amp; 4); and that the classification weights of each voxel were estimated on a subset of the data and the classification scores were computed on the other subset of the data (i.e., leave-one-run-out cross-validation; see Methods for details). lOFC, lateral orbitofrontal cortex; and Carb., carbohydrade.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig9"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 4 supplementary results of th" href="/articles/s41593-017-0008-x/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig9_ESM.jpg">Supplementary Figure 4 Supplementary results of the neural representation of subjective nutrient factors.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(a) Decoding accuracies of subjective nutrient factors at the time of bidding revealing a lack of significant decoding at this time-point (<i>n</i> = 23 participants). The format is the same as in Fig. 3a (<i>left</i>). <i>Left</i>. <i>t</i>-test against 50% (fat: <i>t</i>
                                    <sub>22</sub> = 1.38, <i>P</i> = 0.091; carb.: <i>t</i>
                                    <sub>22</sub> = -2.47, <i>P</i> = 0.989; protein: <i>t</i>
                                    <sub>22</sub> = 1.11, <i>P</i> = 0.139; and vitamin: <i>t</i>
                                    <sub>22</sub> = 0.48, <i>P</i> = 0.320). <i>Right</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = -0.42, <i>P</i> = 0.660; carb.: <i>t</i>
                                    <sub>22</sub> = 0.14, <i>P</i> = 0.444; protein: <i>t</i>
                                    <sub>22</sub> = 1.26, <i>P</i> = 0.110; and vitamin: <i>t</i>
                                    <sub>22</sub> = 0.42, <i>P</i> = 0.339). lOFC, lateral orbitofrontal cortex; mOFC, medial orbitofrontal cortex; and Carb., carbohydrate. (b) Decoding accuracies of subjective nutrient factors at the time of feedback revealing little evidence for significant decoding at this time-point (<i>n</i> = 23 participants). The format is the same as in Fig. 3a (<i>left</i>). <i>Left</i>. <i>t</i>-test against 50% (fat: <i>t</i>
                                    <sub>22</sub> = 0.72, <i>P</i> = 0.239; carb.: <i>t</i>
                                    <sub>22</sub> = -0.43, <i>P</i> = 0.664; protein: <i>t</i>
                                    <sub>22</sub> = 1.38, <i>P</i> = 0.090; and vitamin: <i>t</i>
                                    <sub>22</sub> = 0.18, <i>P</i> = 0.431). <i>Right</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = 0.90, <i>P</i> = 0.190; carb.: <i>t</i>
                                    <sub>22</sub> = -0.80, <i>P</i> = 0.783; protein: <i>t</i>
                                    <sub>22</sub> = 1.07, <i>P</i> = 0.149; and vitamin: <i>t</i>
                                    <sub>22</sub> = 1.01, <i>P</i> = 0.162). (c) Weights of voxels in the classifiers for each of the subjective nutrient factors obtained from the ROI analyses (see Fig. 3a). We plot the weights of the voxels for each participant within the lOFC ROI. Format of the box and whisker plots is the same as in Fig. 1c. (d) Weights of voxels in the classifiers for each of the subjective nutrient factors obtained from the search analyses (see Fig. 3c). We plot the weights of the voxels within a radius of 3 voxels (i.e., 9 mm) around the peak voxels in lOFC. See Fig. 3c for information about the peak voxels. Format of the box and whisker plots is the same as in Fig. 1c. (e) Decoding accuracies of the subjective nutrient factors for novel food items (<i>n</i> = 23 participants). The format is the same as in Fig. 3a (<i>left</i>). <sup><i>+</i></sup>
                                    <i>P</i> &lt; 0.10, *<i>P</i> &lt; 0.05 and **<i>P</i> &lt; 0.01 for each factor, <i>t</i>-test against 50% (fat: <i>t</i>
                                    <sub>22</sub> = 2.42, <i>P</i> = 0.012; carb.: <i>t</i>
                                    <sub>22</sub> = 1.90, <i>P</i> = 0.035; protein: <i>t</i>
                                    <sub>22</sub> = 1.41, <i>P</i> = 0.087; and vitamin: <i>t</i>
                                    <sub>22</sub> = 1.84, <i>P</i> = 0.039). (f) Decoding accuracies of the subjective nutrient factors in the reduced lOFC ROIs (<i>n</i> = 23 participants). In this analysis, (i) we randomly re-sampled adjacent 533 voxels from the lOFC ROI (i.e., forming a continuous cluster consisting of the 553 voxels); then (ii) we tested if information about the subjective nutrient factors could be decoded from the re-sampled voxels; and (iii) the above procedure was repeated 100 times (the decoding accuracies were averaged). The format is the same as in Fig. 3a (<i>left</i>). *<i>P</i> &lt; 0.05 and **<i>P</i> &lt; 0.01 for each factor, <i>t</i>-test against 50% (fat: <i>t</i>
                                    <sub>22</sub> = 2.45, <i>P</i> = 0.011; carb.: <i>t</i>
                                    <sub>22</sub> = 1.81, <i>P</i> = 0.042; protein: <i>t</i>
                                    <sub>22</sub> = 2.59, <i>P</i> = 0.008; and vitamin: <i>t</i>
                                    <sub>22</sub> = 2.46, <i>P</i> = 0.011). (g) Pair-wise correlations among the classifiers’ weights for the four nutrient factors (MEAN across participants; <i>n</i> = 23). For each pair of the nutrient factors, we obtained the correlation coefficient in the classification weights of the voxels within the lOFC ROI. (h) Decoding accuracies in the cross-decoding analyses (<i>n</i> = 23 participants). Format of the box and whisker plots is the same as in Fig. 1c. Two nutrient factors in each parenthesis denote the pair used for the cross-decoding. **<i>P</i> &lt; 0.01, two-tailed <i>t</i>-test against 50% ([fat, carb.]: <i>t</i>
                                    <sub>22</sub> = 1.59, <i>P</i> = 0.127; [fat, protein]: <i>t</i>
                                    <sub>22</sub> = -0.35, <i>P</i> = 0.729; [fat, vitamin]: <i>t</i>
                                    <sub>22</sub> = -3.18, <i>P</i> = 0.004; [carb., protein]: <i>t</i>
                                    <sub>22</sub> = -1.96, <i>P</i> = 0.062; [carb., vitamin]: <i>t</i>
                                    <sub>22</sub> = -1.06, <i>P</i> = 0.299; and [protein, vitamin]: <i>t</i>
                                    <sub>22</sub> = 0.84, <i>P</i> = 0.408). (i) Decoding accuracies on the re-sampled food items (see the main text; <i>n</i> = 23 participants). Format of the box and whisker plots is the same as in Fig. 1c. <i>Left</i>, accuracy of fat and vitamin (one classifier was trained and tested on fat; the other classifier was on vitamin; and the accuracy scores were averaged). <i>Right</i>, accuracy in the cross-decoding analysis between fat and vitamin. Two nutrient factors in the parenthesis denote the pair used for the cross-decoding. That is, we trained a classifier on one factor and tested it on the other factor (and the reverse; and the decoding accuracy was assessed by the average across both directions). *<i>P</i> &lt; 0.05, two-tailed <i>t</i>-test against 50% ([fat, fat] &amp; [vitamin, vitamin]: <i>t</i>
                                    <sub>22</sub> = 2.10, <i>P</i> = 0.048; and [fat, vitamin]: <i>t</i>
                                    <sub>22</sub> = -1.10, <i>P</i> = 0.282). (j) Significant decoding of sugar but not sodium content in lOFC (<i>n</i> = 23 participants). The accuracies are plotted for the lOFC ROI. Format of the box and whisker plots is the same as in Fig. 1c. **<i>P</i> &lt; 0.01, <i>t</i>-test (Sodium: <i>t</i>
                                    <sub>22</sub> = 0.06, <i>P</i> = 0.474; and Sugar: <i>t</i>
                                    <sub>22</sub> = 2.67, <i>P</i> = 0.007). (k) Neither sodium nor sugar content was significantly decodable in mOFC (<i>n</i> = 23 participants). <i>t</i>-test against 50% (Sodium: <i>t</i>
                                    <sub>22</sub> = -0.15, <i>P</i> = 0.557; and Sugar: <i>t</i>
                                    <sub>22</sub> = 1.34, <i>P</i> = 0.100). mOFC, medial orbitofrontal cortex. Format of the box and whisker plots is the same as in Fig. 1c. (l) Decoding accuracies of objective nutrient factors at the time of valuation, demonstrating relatively weak effects of objective nutrient factors (<i>n</i> = 23 participants). The format is the same as in Fig. 3a (<i>left</i>). <i>Left</i>. *<i>P</i> &lt; 0.05, <i>t</i>-test against 50% (fat: <i>t</i>
                                    <sub>22</sub> = -0.33, <i>P</i> = 0.626; carb.: <i>t</i>
                                    <sub>22</sub> = 1.58, <i>P</i> = 0.064; protein: <i>t</i>
                                    <sub>22</sub> = 2.05, <i>P</i> = 0.026; and vitamin: <i>t</i>
                                    <sub>22</sub> = 2.26, <i>P</i> = 0.017). <i>Right</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = -3.10, <i>P</i> = 0.997; carb.: <i>t</i>
                                    <sub>22</sub> = 0.29, <i>P</i> = 0.387; protein: <i>t</i>
                                    <sub>22</sub> = 0.78, <i>P</i> = 0.222; and vitamin: <i>t</i>
                                    <sub>22</sub> = 0.08, <i>P</i> = 0.469).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig10"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 5 procedure and results of th" href="/articles/s41593-017-0008-x/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig10_ESM.jpg">Supplementary Figure 5 Procedure and results of the representational similarity analysis (RSA).</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(a) Procedure for construction of the voxel-wise Representational Dissimilarity Matrix (RDM). The voxel-wise RDM is created based on the correlation across the voxels’ activities for each pair of the items. See Methods for details. Corr., Pearson’s correlation coefficient. (b) Procedure for construction of the behavioral RDM. The behavioral RDM is created based on the correlation in bundles of the four subjective nutrient factors for each item pair. See Methods for details. (c) Results of the ROI analyses. Spearman’s rank correlation (z-transformed) between the voxel-wise neural and the behavioral RDMs is plotted for the lOFC and the mOFC ROIs (<i>n</i> = 23). Format of the box and whisker plots is the same as in Fig. 1c. **<i>P</i> &lt; 0.01, <i>t</i>-test (lOFC: <i>t</i>
                                    <sub>22</sub> = 2.85, <i>P</i> = 0.005; and mOFC: <i>t</i>
                                    <sub>22</sub> = 1.13, <i>P</i> = 0.135). lOFC, lateral orbitofrontal cortex; and mOFC, medial orbitofrontal cortex. (d) Results of the searchlight analysis. The RSA correlation map is thresholded at <i>P</i> &lt; 0.005 (uncorrected) for display purposes, generated by performing a <i>t</i>-test (<i>n</i> = 23 participants). Peak voxels, [MNI: x, y, z = 12, 23, -23] and [-21 38 -23] (<i>P</i> &lt; 0.05 small-volume corrected) for right and left OFC respectively. OFC, orbitofrontal cortex. (e) Pattern of fMRI response to each of the 56 food items in a space of the pair-wise correlation across voxels’ activities in the lOFC. We plot the voxel-wise neural RDM averaged over the participants (<i>top left</i>, <i>n</i> = 23). To visualize the approximate geometric structure, we also show the same data as a two-dimensional MDS plot (<i>top center</i>) and a dendrogram plot obtained by an agglomerative hierarchical clustering (<i>bottom right</i>). In the MDS plot, the digits depict the food items’ ID. See Table S2 for detailed information about the food items. MDS, multi dimensional scaling.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig11"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 6 effective connectivity betw" href="/articles/s41593-017-0008-x/figures/11" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig11_ESM.jpg">Supplementary Figure 6 Effective connectivity between OFC subregions at the time of bidding and the time of feedback.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(a) Results of an effective connectivity analysis at the time of bidding. Effect sizes of the PPI regressors are plotted (<i>n</i> = 23 participants). The format is the same as in Fig. 5ab. **<i>P</i> &lt; 0.01 for each factor. <i>Left</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = 1.37, <i>P</i> = 0.092; carb.: <i>t</i>
                                    <sub>22</sub> = 1.28, <i>P</i> = 0.108; protein: <i>t</i>
                                    <sub>22</sub> = 3.20, <i>P</i> = 0.002; and vitamin: <i>t</i>
                                    <sub>22</sub> = 1.40, <i>P</i> = 0.088). <i>Right</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = 1.59, <i>P</i> = 0.063; carb.: <i>t</i>
                                    <sub>22</sub> = 1.61, <i>P</i> = 0.060; protein: <i>t</i>
                                    <sub>22</sub> = 1.54, <i>P</i> = 0.068; and vitamin: <i>t</i>
                                    <sub>22</sub> = 1.33, <i>P</i> = 0.099). lOFC, lateral orbitofrontal cortex; mOFC, medial orbitofrontal cortex; Carb., carbohydrate; and PPI, psychophysiological interaction. <b>(b)</b> Results of an effective connectivity analysis at the time of feedback. Effect sizes of the PPI regressors are plotted (<i>n</i> = 23 participants). The format is the same as in Fig. 5ab. **<i>P</i> &lt; 0.01 for each factor. <i>Left</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = 1.30, <i>P</i> = 0.104; carb.: <i>t</i>
                                    <sub>22</sub> = 1.67, <i>P</i> = 0.055; protein: <i>t</i>
                                    <sub>22</sub> = 1.22, <i>P</i> = 0.118; and vitamin: <i>t</i>
                                    <sub>22</sub> = 1.02, <i>P</i> = 0.159). <i>Right</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = 1.64, <i>P</i> = 0.058; carb.: <i>t</i>
                                    <sub>22</sub> = 1.32, <i>P</i> = 0.100; protein: <i>t</i>
                                    <sub>22</sub> = 2.80, <i>P</i> = 0.005; and vitamin: <i>t</i>
                                    <sub>22</sub> = 1.01, <i>P</i> = 0.161).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig12"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 7 decoding of subjective valu" href="/articles/s41593-017-0008-x/figures/12" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig12_ESM.jpg">Supplementary Figure 7 Decoding of subjective value and nutrient factors in other brain regions.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(a) Anatomical ROIs used in the additional <i>post hoc</i> analyses. The ROIs are defined based on the AAL database<sup>42</sup> as follows: dmPFC, bilateral MNI_Frontal_Sup_Medial + MNI_Cingulum_Ant; dlPFC, bilateral MNI_Frontal_Mid + MNI_Frontal_Sup; vlPFC, bilateral MNI_Frontal_Inf_Oper + MNI_Frontal_Inf_Tri; PPC, bilateral MNI_Parietal_Inf + MNI_Parietal_Sup; Insula, bilateral MNI_Insula; and Amygdala, bilateral MNI_Amygdala. dmPFC, dorsomedial prefrontal cortex; dlPFC, dorsolateral prefrontal cortex; vlPFC, ventrolateral prefrontal cortex; and PPC, posterior parietal cortex. (b) Decoding accuracies of subjective value across the ROIs (<i>n</i> = 23 participants). The format is the same as in Fig. 2a (<i>left</i>). **<i>P</i> &lt; 0.01 for each region, <i>t</i>-test against 50% (dmPFC: <i>t</i>
                                    <sub>22</sub> = 4.55, <i>P</i> &lt; 0.001; dlPFC: <i>t</i>
                                    <sub>22</sub> = 7.30, <i>P</i> &lt; 0.001; vlPFC: <i>t</i>
                                    <sub>22</sub> = 4.39, <i>P</i> &lt; 0.001; PPC: <i>t</i>
                                    <sub>22</sub> = 6.52, <i>P</i> &lt; 0.001; Insula: <i>t</i>
                                    <sub>22</sub> = 3.48, <i>P</i> = 0.001; and Amygdala: <i>t</i>
                                    <sub>22</sub> = 2.92, <i>P</i> = 0.004). (c) Decoding accuracies of subjective nutrient factors (<i>n</i> = 23 participants). The format is the same as in Fig. 3a (<i>left</i>). *<i>P</i> &lt; 0.05 and **<i>P</i> &lt; 0.01 for each factor. <i>Top left</i>. <i>t</i>-test against 50% (fat: <i>t</i>
                                    <sub>22</sub> = 1.57, <i>P</i> = 0.066; carb.: <i>t</i>
                                    <sub>22</sub> = 1.12, <i>P</i> = 0.137; protein: <i>t</i>
                                    <sub>22</sub> = 0.98, <i>P</i> = 0.168; and vitamin: <i>t</i>
                                    <sub>22</sub> = 1.71, <i>P</i> = 0.050). <i>Top middle</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = 2.25, <i>P</i> = 0.018; carb.: <i>t</i>
                                    <sub>22</sub> = 1.26, <i>P</i> = 0.111; protein: <i>t</i>
                                    <sub>22</sub> = 1.52, <i>P</i> = 0.071; and vitamin: <i>t</i>
                                    <sub>22</sub> = 2.57, <i>P</i> = 0.009). <i>Top right</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = 0.51, <i>P</i> = 0.307; carb.: <i>t</i>
                                    <sub>22</sub> = 0.52, <i>P</i> = 0.305; protein: <i>t</i>
                                    <sub>22</sub> = 2.44, <i>P</i> = 0.012; and vitamin: <i>t</i>
                                    <sub>22</sub> = 0.90, <i>P</i> = 0.189). <i>Bottom left</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = 4.11, <i>P</i> &lt; 0.001; carb.: <i>t</i>
                                    <sub>22</sub> = 2.37, <i>P</i> = 0.014; protein: <i>t</i>
                                    <sub>22</sub> = 4.46, <i>P</i> &lt; 0.001; and vitamin: <i>t</i>
                                    <sub>22</sub> = 4.50, <i>P</i> &lt; 0.001). <i>Bottom middle</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = 0.65, <i>P</i> = 0.261; carb.: <i>t</i>
                                    <sub>22</sub> = 0.83, <i>P</i> = 0.209; protein: <i>t</i>
                                    <sub>22</sub> = 0.83, <i>P</i> = 0.208; and vitamin: <i>t</i>
                                    <sub>22</sub> = 0.05, <i>P</i> = 0.481). <i>Bottom right</i>. <i>t</i>-test (fat: <i>t</i>
                                    <sub>22</sub> = -0.79, <i>P</i> = 0.780; carb.: <i>t</i>
                                    <sub>22</sub> = 0.73, <i>P</i> = 0.236; protein: <i>t</i>
                                    <sub>22</sub> = 0.50, <i>P</i> = 0.312; and vitamin: <i>t</i>
                                    <sub>22</sub> = -0.23, <i>P</i> = 0.592). (d) Decoding accuracies of low-level visual features and comparison with subjective nutrient factors in lOFC, PPC and V1. The decoding accuracies of the low-level visual features (averaged over the eight features) and the subjective nutrient factors (averaged over the four factors identified as value predictors) are plotted for the lOFC, the PPC and the V1 (BA17) anatomical ROIs (<i>n</i> = 23 participants). Format of the box and whisker plots is the same as in Fig. 1c. * and ** on each plot respectively denote <i>P</i> &lt; 0.05 and <i>P</i> &lt; 0.01 for each factor, <i>t</i>-test against 50%. * and ** on the horizontal lines denote significant differences between the indicated pairs of data at <i>P</i> &lt; 0.05 and <i>P</i> &lt; 0.01 respectively, two-tailed paired <i>t</i>-test. <i>Left</i>. <i>t</i>-test (subjective nutrient factors: <i>t</i>
                                    <sub>22</sub> = 4.72, <i>P</i> &lt; 0.001; low-level visual features: <i>t</i>
                                    <sub>22</sub> = 0.70, <i>P</i> = 0.247; and subjective nutrient factors vs. low-level visual features: <i>t</i>
                                    <sub>22</sub> = 3.18, <i>P</i> = 0.004). <i>Middle</i>. <i>t</i>-test (subjective nutrient factors: <i>t</i>
                                    <sub>22</sub> = 7.04, <i>P</i> &lt; 0.001; low-level visual features: <i>t</i>
                                    <sub>22</sub> = 4.01, <i>P</i> &lt; 0.001; and subjective nutrient factors vs. low-level visual features: <i>t</i>
                                    <sub>22</sub> = 2.62, <i>P</i> = 0.0157). <i>Right</i>. <i>t</i>-test (subjective nutrient factors: <i>t</i>
                                    <sub>22</sub> = 5.85, <i>P</i> &lt; 0.001; low-level visual features: <i>t</i>
                                    <sub>22</sub> = 8.34, <i>P</i> &lt; 0.001; and subjective nutrient factors vs. low-level visual features: <i>t</i>
                                    <sub>22</sub> = -3.15, <i>P</i> = 0.005). lOFC, lateral orbitofrontal cortex; PPC, posterior parietal cortex; V1, primary visual cortex; and BA17, Brodmann area 17.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig13"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 8 a region of v1 in which all" href="/articles/s41593-017-0008-x/figures/13" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_Fig13_ESM.jpg">Supplementary Figure 8 A region of V1 in which all of the four subjective nutrient factors can be decoded.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>The decoding accuracy map obtained from the whole-brain searchlight analysis is thresholded at <i>P</i> &lt; 0.05 (cluster-level FWE correction with the cluster-forming threshold <i>P</i> = 0.001; <i>n</i> = 23 participants), conjunction-test. Peak voxel, [MNI: x, y, z = -9, -94, 7]. V1, primary visual cortex.</p></div></div></div></div></div></section><section data-title="Supplementary information"><div class="c-article-section" id="Sec45-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec45">Supplementary information</h2><div class="c-article-section__content" id="Sec45-content"><div data-test="supplementary-info"><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary text and figures" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Text and Figures</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Supplementary Figures 1–8 and Supplementary Tables 1 and 2</p></div></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="life sciences reporting summary" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-017-0008-x/MediaObjects/41593_2017_8_MOESM2_ESM.pdf" data-supp-info-image="">Life Sciences Reporting Summary</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Elucidating%20the%20underlying%20components%20of%20food%20valuation%20in%20the%20human%20orbitofrontal%20cortex&amp;author=Shinsuke%20Suzuki%20et%20al&amp;contentID=10.1038%2Fs41593-017-0008-x&amp;copyright=The%20Author%28s%29&amp;publication=1097-6256&amp;publicationDate=2017-10-23&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/s41593-017-0008-x" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41593-017-0008-x" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Suzuki, S., Cross, L. &amp; O’Doherty, J.P. Elucidating the underlying components of food valuation in the human orbitofrontal cortex.
                    <i>Nat Neurosci</i> <b>20</b>, 1780–1786 (2017). https://doi.org/10.1038/s41593-017-0008-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-017-0008-x?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-02-24">24 February 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-09-24">24 September 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-10-23">23 October 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-12">December 2017</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/s41593-017-0008-x</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>

            
        <section>
            <div class="c-article-section js-article-section" id="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Discrimination exposure impacts unhealthy processing of food cues: crosstalk between the brain and gut" href="https://doi.org/10.1038/s44220-023-00134-9">
                                        Discrimination exposure impacts unhealthy processing of food cues: crosstalk between the brain and gut
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Xiaobei Zhang</li><li>Hao Wang</li><li>Arpana Gupta</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Mental Health</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Neural mechanisms underlying the hierarchical construction of perceived aesthetic value" href="https://doi.org/10.1038/s41467-022-35654-y">
                                        Neural mechanisms underlying the hierarchical construction of perceived aesthetic value
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Kiyohito Iigaya</li><li>Sanghyun Yi</li><li>John P. O’Doherty</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Communications</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Neurons in human pre-supplementary motor area encode key computations for value-based choice" href="https://doi.org/10.1038/s41562-023-01548-2">
                                        Neurons in human pre-supplementary motor area encode key computations for value-based choice
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Tomas G. Aquino</li><li>Jeffrey Cockburn</li><li>John P. O’Doherty</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Human Behaviour</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:The influence of stress on the neural underpinnings of disinhibited eating: a systematic review and future directions for research" href="https://doi.org/10.1007/s11154-023-09814-4">
                                        The influence of stress on the neural underpinnings of disinhibited eating: a systematic review and future directions for research
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Emily Giddens</li><li>Brittany Noy</li><li>Antonio Verdejo-García</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Reviews in Endocrine and Metabolic Disorders</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Prefrontal cortex interactions with the amygdala in primates" href="https://doi.org/10.1038/s41386-021-01128-w">
                                        Prefrontal cortex interactions with the amygdala in primates
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Elisabeth A. Murray</li><li>Lesley K. Fellows</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Neuropsychopharmacology</i> (2022)</p>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </section>
    

            
        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="js-context-bar-sticky-point-desktop">
        

        
            <noscript>
                
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-017-0008-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </noscript>
            <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
                <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-right-column
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-017-0008-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </div>
        
    </div>

    
        
    

    
    
        <div class="c-article-associated-content__container">
            <section>
                <h2 class="c-article-associated-content__title u-mb-24">Associated content</h2>
                
                    
                    
                        <div class="u-full-height u-mb-24">
                            
    <article class="u-full-height c-card c-card--flush">
        <div class="c-card__layout u-full-height">
            <div class="c-card__body">
                <h3 class="c-card__title">
                    <a href="https://www.nature.com/articles/s41593-017-0016-x"
                       class="c-card__link u-link-inherit"
                       data-track="click"
                       data-track-action="view article"
                       data-track-category="associated content"
                       
                       data-track-label="news_and_views">Breaking down a meal</a>
                </h3>
                
<ul data-test="author-list" class="c-author-list c-author-list--compact">
    <li>Mathias Pessiglione</li><li>Antonius Wiehler</li>
</ul>

                
    <div class="c-card__section c-meta">
        
            <span class="c-meta__item">Nature Neuroscience</span>
        
        <span class="c-meta__item" data-test="article.type"><span class="c-meta__type">News &amp; Views</span></span>
        
        
            <time class="c-meta__item" datetime="2017-11-28">28 Nov 2017</time>
        
    </div>

            </div>
        </div>
    </article>


                        </div>
                    
                
            </section>
        </div>
        <script>
            window.dataLayer = window.dataLayer || [];
            window.dataLayer[0] = window.dataLayer[0] || {};
            window.dataLayer[0].content = window.dataLayer[0].content || {};
            window.dataLayer[0].content.associatedContentTypes = "news_and_views";
        </script>
    

    

    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>
                            
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=s41593-017-0008-x;doi=10.1038/s41593-017-0008-x;subjmeta=1409,1788,2649,378,631;kwrd=Decision,Reward">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=197083366&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-017-0008-x%26doi%3D10.1038/s41593-017-0008-x%26subjmeta%3D1409,1788,2649,378,631%26kwrd%3DDecision,Reward">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=197083366&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-017-0008-x%26doi%3D10.1038/s41593-017-0008-x%26subjmeta%3D1409,1788,2649,378,631%26kwrd%3DDecision,Reward"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>


    
        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-and-analysis"
                                   data-track="click"
                                   data-track-action="reviews &amp; analysis"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Reviews &amp; Analysis
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/video"
                                   data-track="click"
                                   data-track-action="videos"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Videos
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/current-issue"
                                   data-track="click"
                                   data-track-action="current issue"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Current issue
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>
                        
                    
                </ul>
                <ul class="c-header__list c-header__list--js-stack">
                    
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/natureneuro"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on Twitter
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;6"
                               rel="nofollow"
                               data-track="click"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/neuro.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </nav>
    
    
        
            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/aims"
                                   data-track="click"
                                   data-track-action="aims &amp; scope"
                                   data-track-label="link">
                                    Aims &amp; Scope
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-information"
                                   data-track="click"
                                   data-track-action="journal information"
                                   data-track-label="link">
                                    Journal Information
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-impact"
                                   data-track="click"
                                   data-track-action="journal metrics"
                                   data-track-label="link">
                                    Journal Metrics
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editors"
                                   data-track="click"
                                   data-track-action="about the editors"
                                   data-track-label="link">
                                    About the Editors
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/our-publishing-models"
                                   data-track="click"
                                   data-track-action="our publishing models"
                                   data-track-label="link">
                                    Our publishing models
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-values-statement"
                                   data-track="click"
                                   data-track-action="editorial values statement"
                                   data-track-label="link">
                                    Editorial Values Statement
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-policies"
                                   data-track="click"
                                   data-track-action="editorial policies"
                                   data-track-label="link">
                                    Editorial Policies
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/content"
                                   data-track="click"
                                   data-track-action="content types"
                                   data-track-label="link">
                                    Content Types
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/web-feeds"
                                   data-track="click"
                                   data-track-action="web feeds"
                                   data-track-label="link">
                                    Web Feeds
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/posters"
                                   data-track="click"
                                   data-track-action="posters"
                                   data-track-label="link">
                                    Posters
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="research cross-journal editorial team"
                                   data-track-label="link">
                                    Research Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="reviews cross-journal editorial team"
                                   data-track-label="link">
                                    Reviews Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        

        
            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/submission-guidelines"
                                   data-track="click"
                                   data-track-action="submission guidelines"
                                   data-track-label="link">
                                    Submission Guidelines
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/for-reviewers"
                                   data-track="click"
                                   data-track-action="for reviewers"
                                   data-track-label="link">
                                    For Reviewers
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="click"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://mts-nn.nature.com/cgi-bin/main.plex?form_type&#x3D;home&amp;from_idp&#x3D;1"
                                   data-track="click"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external>Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        
    


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">
                            
                                
                                    <option value="" selected>All journals</option>
                                    <option value="neuro">This journal</option>
                                
                            
                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        

        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
            

            <p class="c-meta u-ma-0 u-flex-shrink">
                <span class="c-meta__item">
                    Nature Neuroscience (<i>Nat Neurosci</i>)
                </span>
                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="onlineIssn">1546-1726</span> (online)
    </span>
    


                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="printIssn">1097-6256</span> (print)
    </span>
    

            </p>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/protocolexchange/"
                                                  data-track="click" data-track-action="protocol exchange"
                                                  data-track-label="link">Protocol Exchange</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natitaly"
                                                  data-track="click" data-track-action="nature Italy"
                                                  data-track-label="link">Nature Italy</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ko-kr"
                                                  data-track="click" data-track-action="nature korea"
                                                  data-track-label="link">Nature Korea</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>
            
        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-ea63208b81.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2024 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path id="shape" fill-rule="evenodd" clip-rule="evenodd" d="M1 3.78571C1 2.75867 1.85698 2 2.8209 2H6.1791C7.14302 2 8 2.75867 8 3.78571V4H11.1668C11.885 4 12.5585 4.42017 12.8494 5.07033C12.9893 4.98169 13.1425 4.91101 13.3056 4.86206L16.5222 3.89704C17.4454 3.62005 18.4843 4.10046 18.7794 5.08419L22.9256 18.9042C23.2207 19.8878 22.618 20.8608 21.6947 21.1378L18.4781 22.1029C17.5548 22.3799 16.516 21.8993 16.2209 20.9157L13.0001 10.1804V20.2143C13.0001 21.255 12.1231 22 11.1668 22H7.83346C7.54206 22 7.25803 21.9308 7.00392 21.8052C6.75263 21.9305 6.47077 22 6.1791 22H2.8209C1.85693 22 1 21.2412 1 20.2143V3.78571ZM3 4V15H6V4H3ZM3 20V17H6V20H3ZM18.0749 20.1358L17.2129 17.2623L20.0863 16.4002L20.9484 19.2737L18.0749 20.1358ZM19.5116 14.4846L16.6381 15.3466L14.0519 6.72624L16.9254 5.86416L19.5116 14.4846ZM8.00012 20L8.00012 6H11.0001L11.0001 20H8.00012Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</footer>




    

    

<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">

    
    <div class="c-site-messages__banner-large">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing" src="/static/images/logos/nature-briefing-logo-n150-white-d81c9da3ec.svg" width="250" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing</em> newsletter — what matters in science, free to your inbox daily.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/briefing" method="post" data-location="banner" data-track="submit||nature_briefing_sign_up" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="MainBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick">
                        <input type="hidden" value="false" name="marketing" id="marketing">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick">
                        <input type="hidden" value="MainBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">
                            
                            <input type="hidden" value="true" name="N:nature_briefing_daily" id="defaultNewsletter">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>

    
    <div class="c-site-messages__banner-small">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner">Sign up for Nature Briefing
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/s41593-017-0008-x&amp;format=js&amp;last_modified=2017-10-23" async></script>
<img src="/g4jpl8v8/article/s41593-017-0008-x" width="1" height="1" alt="" class="u-visually-hidden">
</body>
</html>