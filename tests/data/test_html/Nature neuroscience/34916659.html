<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence | Nature Neuroscience</title>
    
        
<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/neuro.rss"/>


    
        

        <script id="save-data-connection-testing">
            function hasConnection() {
                return navigator.connection || navigator.mozConnection || navigator.webkitConnection || navigator.msConnection;
            }

            function createLink(src) {
                var preloadLink = document.createElement("link");
                preloadLink.rel = "preload";
                preloadLink.href = src;
                preloadLink.as = "font";
                preloadLink.type = "font/woff2";
                preloadLink.crossOrigin = "";
                document.head.insertBefore(preloadLink, document.head.firstChild);
            }

            var connectionDetail = {
                saveDataEnabled: false,
                slowConnection: false
            };

            var connection = hasConnection();
            if (connection) {
                connectionDetail.saveDataEnabled = connection.saveData;
                if (/\slow-2g|2g/.test(connection.effectiveType)) {
                    connectionDetail.slowConnection = true;
                }
            }

            if (!(connectionDetail.saveDataEnabled || connectionDetail.slowConnection)) {
                createLink("/static/fonts/HardingText-Regular-Web-cecd90984f.woff2");
            } else {
                document.documentElement.classList.add('save-data');
            }
        </script>
    

<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"resource","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"cortex;neural-encoding;object-vision;perception","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Nature Neuroscience","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Resource"}},"article":{"doi":"10.1038/s41593-021-00962-x"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["Emily J. Allen","Ghislain St-Yves","Yihan Wu","Jesse L. Breedlove","Jacob S. Prince","Logan T. Dowdle","Matthias Nau","Brad Caron","Franco Pestilli","Ian Charest","J. Benjamin Hutchinson","Thomas Naselaris","Kendrick Kay"],"publishedAt":1639612800,"publishedAtString":"2021-12-16","title":"A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"neuro","title":"nature neuroscience","volume":"25","issue":"1"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"nature-onwards-journey","active":false},{"name":"getftr-entitled","active":false},{"name":"paywall_recommendations","active":true}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"US","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://collect.nature.com',
        imprint: 'nature'
    });
</script>

<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
 



     
    
    
        
    
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}button:focus{outline:3px solid #fece3e;will-change:transform}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:8px}.c-recommendations-list__item .c-card{flex-basis:100%}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 0 0 16px;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #cedbe0;height:auto;min-height:0;position:static}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0 0 8px;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:inherit}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}.c-card__title-recommendation .MathJax_Display{display:inline!important}.c-card__link:not(.c-card__link--no-block-link):before{z-index:1}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a,.c-article-recommendations-card__link{color:inherit}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-article-recommendations-card__meta-type,.c-meta .c-meta__item:first-child{font-weight:700}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:539px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px 13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.c-pdf-download__link{padding:13px 24px} } </style>




    
        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    
    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-122346e276.css" media="print">
    



<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">


<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h.indexOf('preview-www.nature.com') > -1) return;

            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('nature.com') > -1) {
                if (h.indexOf('test-www.nature.com') > -1) {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-26e142e9c6.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    
                        {src: '/static/js/global-article-es6-bundle-782fd09f66.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-d66d49033d.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-aca08c055a.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-4fba787158.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-1fe07484e5.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;
                        
                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-8fc1a30809.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-e0c7186f28.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-2399be388c.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">





    
    <script type="application/ld+json">{"mainEntity":{"headline":"A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence","description":"Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset (NSD), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspections of the NSD data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used NSD to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. NSD also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprecedented scale, quality and breadth, NSD opens new avenues of inquiry in cognitive neuroscience and artificial intelligence. The authors measured high-resolution fMRI activity from eight individuals who saw and memorized thousands of annotated natural images over 1 year. This massive dataset enables new paths of inquiry in cognitive neuroscience and artificial intelligence.","datePublished":"2021-12-16T00:00:00Z","dateModified":"2021-12-16T00:00:00Z","pageStart":"116","pageEnd":"126","sameAs":"https://doi.org/10.1038/s41593-021-00962-x","keywords":["Cortex","Neural encoding","Object vision","Perception","Biomedicine","general","Neurosciences","Behavioral Sciences","Biological Techniques","Neurobiology","Animal Genetics and Genomics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig4_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig5_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig6_HTML.png"],"isPartOf":{"name":"Nature Neuroscience","issn":["1546-1726","1097-6256"],"volumeNumber":"25","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Emily J. Allen","url":"http://orcid.org/0000-0002-8221-4875","affiliation":[{"name":"University of Minnesota","address":{"name":"Center for Magnetic Resonance Research (CMRR), Department of Radiology, University of Minnesota, Minneapolis, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Minnesota","address":{"name":"Department of Psychology, University of Minnesota, Minneapolis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Ghislain St-Yves","affiliation":[{"name":"Medical University of South Carolina","address":{"name":"Department of Neuroscience, Medical University of South Carolina, Charleston, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Minnesota","address":{"name":"Department of Neuroscience, University of Minnesota, Minneapolis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Yihan Wu","affiliation":[{"name":"University of Minnesota","address":{"name":"Graduate Program in Cognitive Science, University of Minnesota, Minneapolis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Jesse L. Breedlove","affiliation":[{"name":"Medical University of South Carolina","address":{"name":"Department of Neuroscience, Medical University of South Carolina, Charleston, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Minnesota","address":{"name":"Department of Psychology, University of Minnesota, Minneapolis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Jacob S. Prince","affiliation":[{"name":"Carnegie Mellon University","address":{"name":"Department of Psychology, Carnegie Mellon University, Pittsburgh, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Harvard University","address":{"name":"Department of Psychology, Harvard University, Cambridge, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Logan T. Dowdle","url":"http://orcid.org/0000-0002-1879-705X","affiliation":[{"name":"University of Minnesota","address":{"name":"Department of Neuroscience, Center for Magnetic Resonance Research (CMRR), University of Minnesota, Minneapolis, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Minnesota","address":{"name":"Department of Neurosurgery, Center for Magnetic Resonance Research (CMRR), University of Minnesota, Minneapolis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Matthias Nau","url":"http://orcid.org/0000-0003-0956-7815","affiliation":[{"name":"National Institute of Mental Health (NIMH)","address":{"name":"National Institute of Mental Health (NIMH), Bethesda MD, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Brad Caron","affiliation":[{"name":"Indiana University","address":{"name":"Program in Neuroscience, Indiana University, Bloomington IN, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Indiana University","address":{"name":"Program in Vision Science, Indiana University, Bloomington IN, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Franco Pestilli","url":"http://orcid.org/0000-0002-2469-0494","affiliation":[{"name":"University of Texas at Austin","address":{"name":"Department of Psychology, University of Texas at Austin, Austin, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Texas at Austin","address":{"name":"Center for Perceptual Systems, University of Texas at Austin, Austin, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Texas at Austin","address":{"name":"Institute for Neuroscience, University of Texas at Austin, Austin, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Ian Charest","affiliation":[{"name":"University of Birmingham","address":{"name":"Center for Human Brain Health, School of Psychology, University of Birmingham, Birmingham, UK","@type":"PostalAddress"},"@type":"Organization"},{"name":"Universit de Montral","address":{"name":"cerebrUM, Dpartement de Psychologie, Universit de Montral, Montral QC, Canada","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"J. Benjamin Hutchinson","affiliation":[{"name":"University of Oregon","address":{"name":"Department of Psychology, University of Oregon, Eugene, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Thomas Naselaris","affiliation":[{"name":"Medical University of South Carolina","address":{"name":"Department of Neuroscience, Medical University of South Carolina, Charleston, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Minnesota","address":{"name":"Department of Neuroscience, University of Minnesota, Minneapolis, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Kendrick Kay","url":"http://orcid.org/0000-0001-6604-9155","affiliation":[{"name":"University of Minnesota","address":{"name":"Center for Magnetic Resonance Research (CMRR), Department of Radiology, University of Minnesota, Minneapolis, USA","@type":"PostalAddress"},"@type":"Organization"}],"email":"kay@umn.edu","@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>




    
    
    


    
    <link rel="canonical" href="https://www.nature.com/articles/s41593-021-00962-x">
    
    
    <meta name="journal_id" content="41593"/>
    <meta name="dc.title" content="A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence"/>
    <meta name="dc.source" content="Nature Neuroscience 2021 25:1"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2021-12-16"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2021 The Author(s), under exclusive licence to Springer Nature America, Inc."/>
    <meta name="dc.rights" content="2021 The Author(s), under exclusive licence to Springer Nature America, Inc."/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset (NSD), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspections of the NSD data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used NSD to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. NSD also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprecedented scale, quality and breadth, NSD opens new avenues of inquiry in cognitive neuroscience and artificial intelligence. The authors measured high-resolution fMRI activity from eight individuals who saw and memorized thousands of annotated natural images over 1 year. This massive dataset enables new paths of inquiry in cognitive neuroscience and artificial intelligence."/>
    <meta name="prism.issn" content="1546-1726"/>
    <meta name="prism.publicationName" content="Nature Neuroscience"/>
    <meta name="prism.publicationDate" content="2021-12-16"/>
    <meta name="prism.volume" content="25"/>
    <meta name="prism.number" content="1"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="116"/>
    <meta name="prism.endingPage" content="126"/>
    <meta name="prism.copyright" content="2021 The Author(s), under exclusive licence to Springer Nature America, Inc."/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/s41593-021-00962-x"/>
    <meta name="prism.doi" content="doi:10.1038/s41593-021-00962-x"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/s41593-021-00962-x.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/s41593-021-00962-x"/>
    <meta name="citation_journal_title" content="Nature Neuroscience"/>
    <meta name="citation_journal_abbrev" content="Nat Neurosci"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="1546-1726"/>
    <meta name="citation_title" content="A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence"/>
    <meta name="citation_volume" content="25"/>
    <meta name="citation_issue" content="1"/>
    <meta name="citation_publication_date" content="2022/01"/>
    <meta name="citation_online_date" content="2021/12/16"/>
    <meta name="citation_firstpage" content="116"/>
    <meta name="citation_lastpage" content="126"/>
    <meta name="citation_article_type" content="Resource"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/s41593-021-00962-x"/>
    <meta name="DOI" content="10.1038/s41593-021-00962-x"/>
    <meta name="size" content="353793"/>
    <meta name="citation_doi" content="10.1038/s41593-021-00962-x"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/s41593-021-00962-x&amp;api_key="/>
    <meta name="description" content="Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset (NSD), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspections of the NSD data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used NSD to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. NSD also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprecedented scale, quality and breadth, NSD opens new avenues of inquiry in cognitive neuroscience and artificial intelligence. The authors measured high-resolution fMRI activity from eight individuals who saw and memorized thousands of annotated natural images over 1 year. This massive dataset enables new paths of inquiry in cognitive neuroscience and artificial intelligence."/>
    <meta name="dc.creator" content="Allen, Emily J."/>
    <meta name="dc.creator" content="St-Yves, Ghislain"/>
    <meta name="dc.creator" content="Wu, Yihan"/>
    <meta name="dc.creator" content="Breedlove, Jesse L."/>
    <meta name="dc.creator" content="Prince, Jacob S."/>
    <meta name="dc.creator" content="Dowdle, Logan T."/>
    <meta name="dc.creator" content="Nau, Matthias"/>
    <meta name="dc.creator" content="Caron, Brad"/>
    <meta name="dc.creator" content="Pestilli, Franco"/>
    <meta name="dc.creator" content="Charest, Ian"/>
    <meta name="dc.creator" content="Hutchinson, J. Benjamin"/>
    <meta name="dc.creator" content="Naselaris, Thomas"/>
    <meta name="dc.creator" content="Kay, Kendrick"/>
    <meta name="dc.subject" content="Cortex"/>
    <meta name="dc.subject" content="Neural encoding"/>
    <meta name="dc.subject" content="Object vision"/>
    <meta name="dc.subject" content="Perception"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex; citation_author=SEJ Vries; citation_volume=23; citation_publication_date=2020; citation_pages=138-151; citation_doi=10.1038/s41593-019-0550-9; citation_id=CR1"/>
    <meta name="citation_reference" content="Siegle, J. H. et al. Survey of spiking in the mouse visual system reveals functional hierarchy. Nature 592, 86&#8211;92 (2021)."/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=High-dimensional geometry of population responses in visual cortex; citation_author=C Stringer, M Pachitariu, N Steinmetz, M Carandini, KD Harris; citation_volume=571; citation_publication_date=2019; citation_pages=361-365; citation_doi=10.1038/s41586-019-1346-5; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=Cell; citation_title=Reconstruction and simulation of neocortical microcircuitry; citation_author=H Markram; citation_volume=163; citation_publication_date=2015; citation_pages=456-492; citation_doi=10.1016/j.cell.2015.09.029; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=The WU-Minn human connectome project: an overview; citation_author=DC Essen; citation_volume=80; citation_publication_date=2013; citation_pages=62-79; citation_doi=10.1016/j.neuroimage.2013.05.041; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Cell; citation_title=A complete electron microscopy volume of the brain of adult Drosophila melanogaster; citation_author=Z Zheng; citation_volume=174; citation_publication_date=2018; citation_pages=730-743; citation_doi=10.1016/j.cell.2018.06.019; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_journal_title=Vis. Res.; citation_title=Mapping visual cortex in monkeys and humans using surface-based atlases; citation_author=DC Essen; citation_volume=41; citation_publication_date=2001; citation_pages=1359-1378; citation_doi=10.1016/S0042-6989(01)00045-1; citation_id=CR7"/>
    <meta name="citation_reference" content="citation_journal_title=Annu. Rev. Neurosci.; citation_title=The human visual cortex; citation_author=K Grill-Spector, R Malach; citation_volume=27; citation_publication_date=2004; citation_pages=649-677; citation_doi=10.1146/annurev.neuro.27.070203.144220; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Memory&#8217;s echo: vivid remembering reactivates sensory-specific cortex; citation_author=ME Wheeler, SE Petersen, RL Buckner; citation_volume=97; citation_publication_date=2000; citation_pages=11125-11129; citation_doi=10.1073/pnas.97.20.11125; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Biol.; citation_title=Generative feedback explains distinct brain activity codes for seen and mental images; citation_author=JL Breedlove, G St-Yves, CA Olman, T Naselaris; citation_volume=30; citation_publication_date=2020; citation_pages=2211-2224; citation_doi=10.1016/j.cub.2020.04.014; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Biol.; citation_title=Attention reduces spatial uncertainty in human ventral temporal cortex; citation_author=KN Kay, KS Weiner, K Grill-Spector; citation_volume=25; citation_publication_date=2015; citation_pages=595-600; citation_doi=10.1016/j.cub.2014.12.050; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=A continuous semantic space describes the representation of thousands of object and action categories across the human brain; citation_author=AG Huth, S Nishimoto, AT Vu, JL Gallant; citation_volume=76; citation_publication_date=2012; citation_pages=1210-1224; citation_doi=10.1016/j.neuron.2012.10.014; citation_id=CR12"/>
    <meta name="citation_reference" content="Krizhevsky, A. Learning Multiple Layers of Features from Tiny Images. 
                  https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf
                  
                 (University of Toronto, 2009)."/>
    <meta name="citation_reference" content="Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision. 
                  https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48
                  
                , 740&#8211;755 (Springer, 2014)."/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream; citation_author=U G&#252;&#231;l&#252;, MAJ Gerven; citation_volume=35; citation_publication_date=2015; citation_pages=10005-10014; citation_doi=10.1523/JNEUROSCI.5023-14.2015; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=Deep supervised, but not unsupervised, models may explain IT cortical representation; citation_author=S-M Khaligh-Razavi, N Kriegeskorte; citation_volume=10; citation_publication_date=2014; citation_pages=e1003915; citation_doi=10.1371/journal.pcbi.1003915; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=End-to-end neural system identification with neural information flow; citation_author=K Seeliger; citation_volume=17; citation_publication_date=2021; citation_pages=e1008558; citation_doi=10.1371/journal.pcbi.1008558; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Natural scene statistics account for the representation of scene categories in human visual cortex; citation_author=DE Stansbury, T Naselaris, JL Gallant; citation_volume=79; citation_publication_date=2013; citation_pages=1025-1034; citation_doi=10.1016/j.neuron.2013.06.034; citation_id=CR18"/>
    <meta name="citation_reference" content="St-Yves, G. &amp; Naselaris, T. The feature-weighted receptive field: an interpretable encoding model for complex feature spaces. Neuroimage 180, 188&#8211;202 (2018)."/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Performance-optimized hierarchical models predict neural responses in higher visual cortex; citation_author=DLK Yamins; citation_volume=111; citation_publication_date=2014; citation_pages=8619-8624; citation_doi=10.1073/pnas.1403112111; citation_id=CR20"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Cognitive computational neuroscience: a new conference for an emerging discipline; citation_author=T Naselaris; citation_volume=22; citation_publication_date=2018; citation_pages=365-367; citation_doi=10.1016/j.tics.2018.02.008; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=Sci. Data; citation_title=BOLD5000, a public fMRI dataset while viewing 5000 visual images; citation_author=N Chang; citation_volume=6; citation_publication_date=2019; citation_doi=10.1038/s41597-019-0052-3; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Commun.; citation_title=Generic decoding of seen and imagined objects using hierarchical visual features; citation_author=T Horikawa, Y Kamitani; citation_volume=8; citation_publication_date=2017; citation_doi=10.1038/ncomms15037; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Identifying natural images from human brain activity; citation_author=KN Kay, T Naselaris, RJ Prenger, JL Gallant; citation_volume=452; citation_publication_date=2008; citation_pages=352-355; citation_doi=10.1038/nature06713; citation_id=CR24"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Comparison of physiological noise at 1.5 T, 3 T and 7 T and optimization of fMRI acquisition parameters; citation_author=C Triantafyllou; citation_volume=26; citation_publication_date=2005; citation_pages=243-250; citation_doi=10.1016/j.neuroimage.2005.01.007; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Visual long-term memory has a massive storage capacity for object details; citation_author=TF Brady, T Konkle, GA Alvarez, A Oliva; citation_volume=105; citation_publication_date=2008; citation_pages=14325-14329; citation_doi=10.1073/pnas.0803390105; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=eLife; citation_title=Hyperalignment: modeling shared information encoded in idiosyncratic cortical topographies; citation_author=JV Haxby, JS Guntupalli, SA Nastase, M Feilong; citation_volume=9; citation_publication_date=2020; citation_pages=e56601; citation_doi=10.7554/eLife.56601; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=A critical, event-related appraisal of denoising in resting-state fMRI studies; citation_author=JD Power, CJ Lynch, B Adeyemo, SE Petersen; citation_volume=30; citation_publication_date=2020; citation_pages=5544-5559; citation_doi=10.1093/cercor/bhaa139; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Biol.; citation_title=Task-related activity in human visual cortex; citation_author=ZN Roth, M Ryoo, EP Merriam; citation_volume=18; citation_publication_date=2020; citation_pages=e3000921; citation_doi=10.1371/journal.pbio.3000921; citation_id=CR29"/>
    <meta name="citation_reference" content="Benson, N. C. et al. The human connectome project 7 Tesla retinotopy dataset: description and population receptive field analysis. J. Vis. 18, 23 (2018)."/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Temporal processing capacity in high-level visual cortex is domain specific; citation_author=A Stigliani, KS Weiner, K Grill-Spector; citation_volume=35; citation_publication_date=2015; citation_pages=12412-12424; citation_doi=10.1523/JNEUROSCI.4822-14.2015; citation_id=CR31"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=A critical assessment of data quality and venous effects in sub-millimeter fMRI; citation_author=K Kay; citation_volume=189; citation_publication_date=2019; citation_pages=847-869; citation_doi=10.1016/j.neuroimage.2019.02.006; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Precision functional mapping of individual human brains; citation_author=EM Gordon; citation_volume=95; citation_publication_date=2017; citation_pages=791-807; citation_doi=10.1016/j.neuron.2017.07.011; citation_id=CR33"/>
    <meta name="citation_reference" content="citation_journal_title=Magn. Reson. Imaging; citation_title=Improving the resolution of functional brain imaging: analyzing functional data in anatomical space; citation_author=X Kang, EW Yund, TJ Herron, DL Woods; citation_volume=25; citation_publication_date=2007; citation_pages=1070-1078; citation_doi=10.1016/j.mri.2006.12.005; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Neurosci.; citation_title=GLMdenoise: a fast, automated technique for denoising task-based fMRI data; citation_author=KN Kay, A Rokem, J Winawer, RF Dougherty, B Wandell; citation_volume=7; citation_publication_date=2013; citation_pages=247; citation_doi=10.3389/fnins.2013.00247; citation_id=CR35"/>
    <meta name="citation_reference" content="Rokem, A. &amp; Kay, K. Fractional ridge regression: a fast, interpretable reparameterization of ridge regression. Gigascience 9, giaa133 (2020)."/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Striate cortex of monkey and cat: contrast response function; citation_author=DG Albrecht, DB Hamilton; citation_volume=48; citation_publication_date=1982; citation_pages=217-237; citation_doi=10.1152/jn.1982.48.1.217; citation_id=CR37"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Parietal lobe contributions to episodic memory retrieval; citation_author=AD Wagner, BJ Shannon, I Kahn, RL Buckner; citation_volume=9; citation_publication_date=2005; citation_pages=445-453; citation_doi=10.1016/j.tics.2005.07.001; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=Neuropsychologia; citation_title=Event-related fMRI studies of episodic encoding and retrieval: meta-analyses using activation likelihood estimation; citation_author=J Spaniol; citation_volume=47; citation_publication_date=2009; citation_pages=1765-1779; citation_doi=10.1016/j.neuropsychologia.2009.02.028; citation_id=CR39"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Whole-brain, time-locked activation with simple tasks revealed using massive averaging and model-free analysis; citation_author=J Gonzalez-Castillo; citation_volume=109; citation_publication_date=2012; citation_pages=5487-5492; citation_doi=10.1073/pnas.1121049109; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=J. Mach. Learn. Res.; citation_title=Visualizing data using t-SNE; citation_author=Lvander Maaten, G Hinton; citation_volume=9; citation_publication_date=2008; citation_pages=2579-2605; citation_id=CR41"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=The representation of biological classes in the human brain; citation_author=AC Connolly; citation_volume=32; citation_publication_date=2012; citation_pages=2608-2618; citation_doi=10.1523/JNEUROSCI.5547-11.2012; citation_id=CR42"/>
    <meta name="citation_reference" content="citation_journal_title=J. Physiol. Paris; citation_title=Cortical representation of animate and inanimate objects in complex natural scenes; citation_author=T Naselaris, DE Stansbury, JL Gallant; citation_volume=106; citation_publication_date=2012; citation_pages=239-249; citation_doi=10.1016/j.jphysparis.2012.02.001; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Mid-level visual features underlie the high-level categorical organization of the ventral stream; citation_author=B Long, C-P Yu, T Konkle; citation_volume=115; citation_publication_date=2018; citation_pages=E9015-E9024; citation_doi=10.1073/pnas.1719616115; citation_id=CR44"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Visual representations are dominated by intrinsic fluctuations correlated between areas; citation_author=L Henriksson, S-M Khaligh-Razavi, K Kay, N Kriegeskorte; citation_volume=114; citation_publication_date=2015; citation_pages=275-286; citation_doi=10.1016/j.neuroimage.2015.04.026; citation_id=CR45"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Encoding and decoding in fMRI; citation_author=T Naselaris, KN Kay, S Nishimoto, JL Gallant; citation_volume=56; citation_publication_date=2011; citation_pages=400-410; citation_doi=10.1016/j.neuroimage.2010.07.073; citation_id=CR46"/>
    <meta name="citation_reference" content="Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems 25 
                  https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html
                  
                , 1097&#8211;1105 (2012)."/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=Deep convolutional models improve predictions of macaque V1 responses to natural images; citation_author=SA Cadena; citation_volume=15; citation_publication_date=2019; citation_pages=e1006897; citation_doi=10.1371/journal.pcbi.1006897; citation_id=CR48"/>
    <meta name="citation_reference" content="Wang, A., Tarr, M. &amp; Wehbe, L. Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity. In Advances in Neural Information Processing Systems 32 
                  https://papers.nips.cc/paper/2019/hash/f490c742cd8318b8ee6dca10af2a163f-Abstract.html
                  
                , 15475&#8211;15485 (2019)."/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Engineering a less artificial intelligence; citation_author=FH Sinz, X Pitkow, J Reimer, M Bethge, AS Tolias; citation_volume=103; citation_publication_date=2019; citation_pages=967-979; citation_doi=10.1016/j.neuron.2019.08.034; citation_id=CR50"/>
    <meta name="citation_reference" content="citation_journal_title=Sci. Data; citation_title=A naturalistic neuroimaging database for understanding the brain using ecological stimuli; citation_author=S Aliko, J Huang, F Gheorghiu, S Meliss, JI Skipper; citation_volume=7; citation_publication_date=2020; citation_doi=10.1038/s41597-020-00680-2; citation_id=CR51"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Leveraging shared connectivity to aggregate heterogeneous datasets into a common response space; citation_author=SA Nastase, Y-F Liu, H Hillman, KA Norman, U Hasson; citation_volume=217; citation_publication_date=2020; citation_pages=116865; citation_doi=10.1016/j.neuroimage.2020.116865; citation_id=CR52"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=The cambridge centre for ageing and neuroscience (Cam-CAN) data repository: structural and functional MRI, MEG, and cognitive data from a cross-sectional adult lifespan sample; citation_author=JR Taylor; citation_volume=144; citation_publication_date=2017; citation_pages=262-269; citation_doi=10.1016/j.neuroimage.2015.09.018; citation_id=CR53"/>
    <meta name="citation_reference" content="Bellec, P. &amp; Boyle, J. A. Bridging the gap between perception and action: the case for neuroimaging, AI and video games. Preprint at 
                  https://psyarxiv.com/3epws
                  
                 (2019)."/>
    <meta name="citation_reference" content="citation_journal_title=Sci. Data; citation_title=Individual Brain Charting, a high-resolution fMRI dataset for cognitive mapping; citation_author=AL Pinho; citation_volume=5; citation_publication_date=2018; citation_doi=10.1038/sdata.2018.105; citation_id=CR55"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Commun.; citation_title=Long-term neural and physiological phenotyping of a single human; citation_author=RA Poldrack; citation_volume=6; citation_publication_date=2015; citation_doi=10.1038/ncomms9885; citation_id=CR56"/>
    <meta name="citation_reference" content="Seeliger, K., Sommers, R. P., G&#252;&#231;l&#252;, U., Bosch, S. E. &amp; van Gerven, M. A. J. A large single-participant fMRI dataset for probing brain responses to naturalistic stimuli in space and time. Preprint at 
                  https://www.biorxiv.org/content/10.1101/687681v1
                  
                 (2019)."/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Opin. Behav. Sci.; citation_title=Extensive sampling for complete models of individual brains; citation_author=T Naselaris, E Allen, K Kay; citation_volume=40; citation_publication_date=2021; citation_pages=45-51; citation_doi=10.1016/j.cobeha.2020.12.008; citation_id=CR58"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Analysis strategies for high-resolution UHF-fMRI data; citation_author=JR Polimeni, V Renvall, N Zaretskaya, B Fischl; citation_volume=168; citation_publication_date=2018; citation_pages=296-320; citation_doi=10.1016/j.neuroimage.2017.04.053; citation_id=CR59"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Extending the Human Connectome Project across ages: imaging protocols for the Lifespan Development and Aging projects; citation_author=MP Harms; citation_volume=183; citation_publication_date=2018; citation_pages=972-984; citation_doi=10.1016/j.neuroimage.2018.09.060; citation_id=CR60"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Customized head molds reduce motion during resting state fMRI scans; citation_author=JD Power; citation_volume=189; citation_publication_date=2019; citation_pages=141-149; citation_doi=10.1016/j.neuroimage.2019.01.016; citation_id=CR61"/>
    <meta name="citation_reference" content="citation_journal_title=Spat. Vis.; citation_title=The Psychophysics Toolbox; citation_author=DH Brainard; citation_volume=10; citation_publication_date=1997; citation_pages=433-436; citation_doi=10.1163/156856897X00357; citation_id=CR62"/>
    <meta name="citation_reference" content="citation_journal_title=Spat. Vis.; citation_title=The VideoToolbox software for visual psychophysics: transforming numbers into movies; citation_author=DG Pelli; citation_volume=10; citation_publication_date=1997; citation_pages=437-442; citation_doi=10.1163/156856897X00366; citation_id=CR63"/>
    <meta name="citation_reference" content="Caesar, H., Uijlings, J. &amp; Ferrari, V. COCO-Stuff: Thing and Stuff classes in context. In IEEE/CVF Conf. Computer Vision and Pattern Recognition 
                  https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00132
                  
                 1209&#8211;1218 (2018)."/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=The foveal confluence in human visual cortex; citation_author=MM Schira, CW Tyler, M Breakspear, B Spehar; citation_volume=29; citation_publication_date=2009; citation_pages=9050-9058; citation_doi=10.1523/JNEUROSCI.1760-09.2009; citation_id=CR65"/>
    <meta name="citation_reference" content="Shahid, A., Wilkinson, K., Marcu, S. &amp; Shapiro, C. M. Stanford Sleepiness Scale (SSS). In: STOP, THAT and One Hundred Other Sleep Scales (eds. Shahid, A., Wilkinson, K., Marcu, S. &amp; Shapiro, C. M.) 369&#8211;370 (Springer, 2012)."/>
    <meta name="citation_reference" content="citation_journal_title=Br. J. Psychol.; citation_title=Visual imagery differences in the recall of pictures; citation_author=DF Marks; citation_volume=64; citation_publication_date=1973; citation_pages=17-24; citation_doi=10.1111/j.2044-8295.1973.tb01322.x; citation_id=CR67"/>
    <meta name="citation_reference" content="Torgesen, J. K., Wagner, R. &amp; Rashotte, C. TOWRE-2: Test of Word Reading Efficiency (Pearson, 2012)."/>
    <meta name="citation_reference" content="citation_journal_title=Neuropsychologia; citation_title=The Cambridge Face Memory Test: results for neurologically intact individuals and an investigation of its validity using inverted face stimuli and prosopagnosic participants; citation_author=B Duchaine, K Nakayama; citation_volume=44; citation_publication_date=2006; citation_pages=576-585; citation_doi=10.1016/j.neuropsychologia.2005.07.001; citation_id=CR69"/>
    <meta name="citation_reference" content="citation_journal_title=J. Vis.; citation_title=Measuring the contrast sensitivity function in just three clicks; citation_author=J Tardif, M Watson, D Giaschi, F Gosselin; citation_volume=16; citation_publication_date=2016; citation_pages=966-966; citation_doi=10.1167/16.12.966; citation_id=CR70"/>
    <meta name="citation_reference" content="Arora, S., Liang, Y. &amp; Ma, T. A simple but tough-to-beat baseline for sentence embeddings. 
                  https://openreview.net/pdf?id=SyK00v5xx
                  
                 (2017)."/>
    <meta name="citation_reference" content="citation_journal_title=Front. Psychol.; citation_title=Inverse MDS: inferring dissimilarity structure from multiple item arrangements; citation_author=N Kriegeskorte, M Mur; citation_volume=3; citation_publication_date=2012; citation_pages=245; citation_doi=10.3389/fpsyg.2012.00245; citation_id=CR72"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Methods; citation_title=A temporal decomposition method for identifying venous effects in task-based fMRI; citation_author=K Kay, KW Jamison, R-Y Zhang, K U&#287;urbil; citation_volume=17; citation_publication_date=2020; citation_pages=1033-1039; citation_doi=10.1038/s41592-020-0941-6; citation_id=CR73"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=A reproducible evaluation of ANTs similarity metric performance in brain image registration; citation_author=BB Avants; citation_volume=54; citation_publication_date=2011; citation_pages=2033-2044; citation_doi=10.1016/j.neuroimage.2010.09.025; citation_id=CR74"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability; citation_author=PA Yushkevich; citation_volume=31; citation_publication_date=2006; citation_pages=1116-1128; citation_doi=10.1016/j.neuroimage.2006.01.015; citation_id=CR75"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Methods; citation_title=fMRIPrep: a robust preprocessing pipeline for functional MRI; citation_author=O Esteban; citation_volume=16; citation_publication_date=2019; citation_pages=111-116; citation_doi=10.1038/s41592-018-0235-4; citation_id=CR76"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion; citation_author=JD Power, KA Barnes, AZ Snyder, BL Schlaggar, SE Petersen; citation_volume=59; citation_publication_date=2012; citation_pages=2142-2154; citation_doi=10.1016/j.neuroimage.2011.10.018; citation_id=CR77"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=The continuing challenge of understanding and modeling hemodynamic variation in fMRI; citation_author=DA Handwerker, J Gonzalez-Castillo, M D&#8217;Esposito, PA Bandettini; citation_volume=62; citation_publication_date=2012; citation_pages=1017-1023; citation_doi=10.1016/j.neuroimage.2012.02.015; citation_id=CR78"/>
    <meta name="citation_reference" content="citation_journal_title=Technometrics; citation_title=Ridge regression: Biased estimation for nonorthogonal problems; citation_author=AE Hoerl, RW Kennard; citation_volume=12; citation_publication_date=1970; citation_pages=55-67; citation_doi=10.1080/00401706.1970.10488634; citation_id=CR79"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Compressive spatial summation in human visual cortex; citation_author=KN Kay, J Winawer, A Mezer, B Wandell; citation_volume=110; citation_publication_date=2013; citation_pages=481-494; citation_doi=10.1152/jn.00105.2013; citation_id=CR80"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=Methods for computing the maximum performance of computational models of fMRI responses; citation_author=A Lage-Castellanos, G Valente, E Formisano, F Martino; citation_volume=15; citation_publication_date=2019; citation_pages=e1006397; citation_doi=10.1371/journal.pcbi.1006397; citation_id=CR81"/>
    <meta name="citation_reference" content="citation_journal_title=Magn. Reson. Med.; citation_title=Functional connectivity in the motor cortex of resting human brain using echo-planar MRI; citation_author=B Biswal, FZ Yetkin, VM Haughton, JS Hyde; citation_volume=34; citation_publication_date=1995; citation_pages=537-541; citation_doi=10.1002/mrm.1910340409; citation_id=CR82"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=A toolbox for representational similarity analysis; citation_author=H Nili; citation_volume=10; citation_publication_date=2014; citation_pages=e1003553; citation_doi=10.1371/journal.pcbi.1003553; citation_id=CR83"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Syst. Neurosci.; citation_title=Representational similarity analysis&#8212;connecting the branches of systems neuroscience; citation_author=N Kriegeskorte, M Mur, P Bandettini; citation_volume=2; citation_publication_date=2008; citation_pages=4; citation_id=CR84"/>
    <meta name="citation_reference" content="citation_journal_title=J. Mach. Learn. Res.; citation_title=Scikit-learn: machine learning in Python; citation_author=F Pedregosa; citation_volume=12; citation_publication_date=2011; citation_pages=2825-2830; citation_id=CR85"/>
    <meta name="citation_reference" content="citation_journal_title=Sci. Data; citation_title=The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments; citation_author=KJ Gorgolewski; citation_volume=3; citation_publication_date=2016; citation_pages=1-9; citation_doi=10.1038/sdata.2016.44; citation_id=CR86"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Mach. Intell.; citation_title=The Algonauts Project; citation_author=RM Cichy, G Roig, A Oliva; citation_volume=1; citation_publication_date=2019; citation_pages=613; citation_doi=10.1038/s42256-019-0127-z; citation_id=CR87"/>
    <meta name="citation_author" content="Allen, Emily J."/>
    <meta name="citation_author_institution" content="Center for Magnetic Resonance Research (CMRR), Department of Radiology, University of Minnesota, Minneapolis, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of Minnesota, Minneapolis, USA"/>
    <meta name="citation_author" content="St-Yves, Ghislain"/>
    <meta name="citation_author_institution" content="Department of Neuroscience, Medical University of South Carolina, Charleston, USA"/>
    <meta name="citation_author_institution" content="Department of Neuroscience, University of Minnesota, Minneapolis, USA"/>
    <meta name="citation_author" content="Wu, Yihan"/>
    <meta name="citation_author_institution" content="Graduate Program in Cognitive Science, University of Minnesota, Minneapolis, USA"/>
    <meta name="citation_author" content="Breedlove, Jesse L."/>
    <meta name="citation_author_institution" content="Department of Neuroscience, Medical University of South Carolina, Charleston, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of Minnesota, Minneapolis, USA"/>
    <meta name="citation_author" content="Prince, Jacob S."/>
    <meta name="citation_author_institution" content="Department of Psychology, Carnegie Mellon University, Pittsburgh, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, Harvard University, Cambridge, USA"/>
    <meta name="citation_author" content="Dowdle, Logan T."/>
    <meta name="citation_author_institution" content="Department of Neuroscience, Center for Magnetic Resonance Research (CMRR), University of Minnesota, Minneapolis, USA"/>
    <meta name="citation_author_institution" content="Department of Neurosurgery, Center for Magnetic Resonance Research (CMRR), University of Minnesota, Minneapolis, USA"/>
    <meta name="citation_author" content="Nau, Matthias"/>
    <meta name="citation_author_institution" content="National Institute of Mental Health (NIMH), Bethesda MD, USA"/>
    <meta name="citation_author" content="Caron, Brad"/>
    <meta name="citation_author_institution" content="Program in Neuroscience, Indiana University, Bloomington IN, USA"/>
    <meta name="citation_author_institution" content="Program in Vision Science, Indiana University, Bloomington IN, USA"/>
    <meta name="citation_author" content="Pestilli, Franco"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of Texas at Austin, Austin, USA"/>
    <meta name="citation_author_institution" content="Center for Perceptual Systems, University of Texas at Austin, Austin, USA"/>
    <meta name="citation_author_institution" content="Institute for Neuroscience, University of Texas at Austin, Austin, USA"/>
    <meta name="citation_author" content="Charest, Ian"/>
    <meta name="citation_author_institution" content="Center for Human Brain Health, School of Psychology, University of Birmingham, Birmingham, UK"/>
    <meta name="citation_author_institution" content="cerebrUM, D&#233;partement de Psychologie, Universit&#233; de Montr&#233;al, Montr&#233;al QC, Canada"/>
    <meta name="citation_author" content="Hutchinson, J. Benjamin"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of Oregon, Eugene, USA"/>
    <meta name="citation_author" content="Naselaris, Thomas"/>
    <meta name="citation_author_institution" content="Department of Neuroscience, Medical University of South Carolina, Charleston, USA"/>
    <meta name="citation_author_institution" content="Department of Neuroscience, University of Minnesota, Minneapolis, USA"/>
    <meta name="citation_author" content="Kay, Kendrick"/>
    <meta name="citation_author_institution" content="Center for Magnetic Resonance Research (CMRR), Department of Radiology, University of Minnesota, Minneapolis, USA"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@natureneuro"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence"/>
    <meta name="twitter:description" content="Nature Neuroscience - The authors measured high-resolution fMRI activity from eight individuals who saw and memorized thousands of annotated natural images over 1 year. This massive dataset enables..."/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig1_HTML.png"/>
    

    
    
    <meta property="og:url" content="https://www.nature.com/articles/s41593-021-00962-x"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence - Nature Neuroscience"/>
    <meta property="og:description" content="The authors measured high-resolution fMRI activity from eight individuals who saw and memorized thousands of annotated natural images over 1 year. This massive dataset enables new paths of inquiry in cognitive neuroscience and artificial intelligence."/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig1_HTML.png"/>
    

    <script>
        window.eligibleForRa21 = 'true'; 
    </script>
</head>
<body class="article-page">

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>



<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
        
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>

    

    <div class="u-lazy-ad-wrapper u-mbs-0">
            <div class="deferred-placeholder" data-replace="true"
                 data-placeholder="/placeholder/v1/institutionalBanner?bpids=[bpids] #institutional-banner-container"></div>
            <aside class="c-ad c-ad--728x90">
                <div class="c-ad__inner" data-container-type="banner-advert">
                    <p class="c-ad__label">Advertisement</p>
                    
        
            
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=s41593-021-00962-x;doi=10.1038/s41593-021-00962-x;techmeta=36,57,59;subjmeta=116,1595,1723,2395,2613,2616,2618,2649,378,631;kwrd=Cortex,Neural+encoding,Object+vision,Perception">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-1043631333&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-021-00962-x%26doi%3D10.1038/s41593-021-00962-x%26techmeta%3D36,57,59%26subjmeta%3D116,1595,1723,2395,2613,2616,2618,2649,378,631%26kwrd%3DCortex,Neural+encoding,Object+vision,Perception">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=-1043631333&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41593-021-00962-x%26doi%3D10.1038/s41593-021-00962-x%26techmeta%3D36,57,59%26subjmeta%3D116,1595,1723,2395,2613,2616,2618,2649,378,631%26kwrd%3DCortex,Neural+encoding,Object+vision,Perception"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

        
    
                </div>
            </aside>
        </div>
    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#00928c">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">
                    
                    
                    <div class="c-header__logo-container">
                        
                        <a href="/neuro"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-0ccc487532906d646419e51f647ce79a.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-880e5942f43b9213989c58a04ab5c8e6.svg" height="32" alt="Nature Neuroscience">
                            </picture>
                        </a>
                    
                    </div>
                    
                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">
                            
                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41593-021-00962-x'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
                            
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        
            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">
                            
                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span><span class="c-header__show-text">Explore</span> content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                                
                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>
                                
                            
                            
                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">
                            
                                <li class="c-header__item">
                                    <a class="c-header__link"
                                       href="https://idp.nature.com/auth/personal/springernature?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Fmy-account%2Falerts%2Fsubscribe-journal%3Flist-id%3D6"
                                       rel="nofollow"
                                       data-track="click"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/neuro.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        
    </header>


    
    
        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature neuroscience"><span itemprop="name">nature neuroscience</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro/articles?type&#x3D;resource" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:resources"><span itemprop="name">resources</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>
    



    

</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
        
            <div class="c-context-bar u-hide"
                 data-test="context-bar"
                 data-context-bar
                 aria-hidden="true">
                <div class="c-context-bar__container u-container">
                    <div class="c-context-bar__title">
                        A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence
                    </div>
                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-021-00962-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                </div>
            </div>
        
        <article lang="en">
            
            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Resource</li>
    
    

                        <li class="c-article-identifiers__item">Published: <time datetime="2021-12-16">16 December 2021</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Emily_J_-Allen-Aff1-Aff2" data-author-popup="auth-Emily_J_-Allen-Aff1-Aff2" data-author-search="Allen, Emily J.">Emily J. Allen</a><span class="u-js-hide">
            <a class="js-orcid" href="http://orcid.org/0000-0002-8221-4875"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-8221-4875</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ghislain-St_Yves-Aff3-Aff17" data-author-popup="auth-Ghislain-St_Yves-Aff3-Aff17" data-author-search="St-Yves, Ghislain">Ghislain St-Yves</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup><sup class="u-js-hide"><a href="#nAff17">nAff17</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yihan-Wu-Aff4" data-author-popup="auth-Yihan-Wu-Aff4" data-author-search="Wu, Yihan">Yihan Wu</a><sup class="u-js-hide"><a href="#Aff4">4</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jesse_L_-Breedlove-Aff3-Aff18" data-author-popup="auth-Jesse_L_-Breedlove-Aff3-Aff18" data-author-search="Breedlove, Jesse L.">Jesse L. Breedlove</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup><sup class="u-js-hide"><a href="#nAff18">nAff18</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jacob_S_-Prince-Aff5-Aff19" data-author-popup="auth-Jacob_S_-Prince-Aff5-Aff19" data-author-search="Prince, Jacob S.">Jacob S. Prince</a><sup class="u-js-hide"><a href="#Aff5">5</a></sup><sup class="u-js-hide"><a href="#nAff19">nAff19</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Logan_T_-Dowdle-Aff6-Aff7" data-author-popup="auth-Logan_T_-Dowdle-Aff6-Aff7" data-author-search="Dowdle, Logan T.">Logan T. Dowdle</a><span class="u-js-hide">
            <a class="js-orcid" href="http://orcid.org/0000-0002-1879-705X"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-1879-705X</a></span><sup class="u-js-hide"><a href="#Aff6">6</a>,<a href="#Aff7">7</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Matthias-Nau-Aff8" data-author-popup="auth-Matthias-Nau-Aff8" data-author-search="Nau, Matthias">Matthias Nau</a><span class="u-js-hide">
            <a class="js-orcid" href="http://orcid.org/0000-0003-0956-7815"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-0956-7815</a></span><sup class="u-js-hide"><a href="#Aff8">8</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Brad-Caron-Aff9-Aff10" data-author-popup="auth-Brad-Caron-Aff9-Aff10" data-author-search="Caron, Brad">Brad Caron</a><sup class="u-js-hide"><a href="#Aff9">9</a>,<a href="#Aff10">10</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Franco-Pestilli-Aff11-Aff12-Aff13" data-author-popup="auth-Franco-Pestilli-Aff11-Aff12-Aff13" data-author-search="Pestilli, Franco">Franco Pestilli</a><span class="u-js-hide">
            <a class="js-orcid" href="http://orcid.org/0000-0002-2469-0494"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-2469-0494</a></span><sup class="u-js-hide"><a href="#Aff11">11</a>,<a href="#Aff12">12</a>,<a href="#Aff13">13</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ian-Charest-Aff14-Aff15" data-author-popup="auth-Ian-Charest-Aff14-Aff15" data-author-search="Charest, Ian">Ian Charest</a><sup class="u-js-hide"><a href="#Aff14">14</a>,<a href="#Aff15">15</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-J__Benjamin-Hutchinson-Aff16" data-author-popup="auth-J__Benjamin-Hutchinson-Aff16" data-author-search="Hutchinson, J. Benjamin">J. Benjamin Hutchinson</a><sup class="u-js-hide"><a href="#Aff16">16</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Thomas-Naselaris-Aff3-Aff17" data-author-popup="auth-Thomas-Naselaris-Aff3-Aff17" data-author-search="Naselaris, Thomas">Thomas Naselaris</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup><sup class="u-js-hide"><a href="#na1">na1</a></sup><sup class="u-js-hide"><a href="#nAff17">nAff17</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 13 authors for this article" title="Show all 13 authors for this article"></li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kendrick-Kay-Aff1" data-author-popup="auth-Kendrick-Kay-Aff1" data-author-search="Kay, Kendrick" data-corresp-id="c1">Kendrick Kay<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><span class="u-js-hide">
            <a class="js-orcid" href="http://orcid.org/0000-0001-6604-9155"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0001-6604-9155</a></span><sup class="u-js-hide"><a href="#Aff1">1</a></sup><sup class="u-js-hide"><a href="#na1">na1</a></sup></li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>

                    

                    <p class="c-article-info-details" data-container-section="info">
                        
    <a data-test="journal-link" href="/neuro" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Nature Neuroscience</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span>25</b>,<span class="u-visually-hidden">pages </span>116126 (<span data-test="article-publication-year">2022</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>
                    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">32k <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">74 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">243 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/articles/s41593-021-00962-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
                    
                </header>

                
    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/cortex" data-track="click" data-track-action="view subject" data-track-label="link">Cortex</a></li><li class="c-article-subject-list__subject"><a href="/subjects/neural-encoding" data-track="click" data-track-action="view subject" data-track-label="link">Neural encoding</a></li><li class="c-article-subject-list__subject"><a href="/subjects/object-vision" data-track="click" data-track-action="view subject" data-track-label="link">Object vision</a></li><li class="c-article-subject-list__subject"><a href="/subjects/perception" data-track="click" data-track-action="view subject" data-track-label="link">Perception</a></li>
        </ul>
    </div>

                
    
    

    
    

                
            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset (NSD), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspections of the NSD data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used NSD to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. NSD also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprecedented scale, quality and breadth, NSD opens new avenues of inquiry in cognitive neuroscience and artificial intelligence.</p></div></div></section>

            <noscript>
                
                    
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-021-00962-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                
            </noscript>

            
                <div class="js-context-bar-sticky-point-mobile">
                    
                        <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-entitled-mobile
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-021-00962-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                    
                </div>
            

            
                
                    
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41597-023-02471-x/MediaObjects/41597_2023_2471_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41597-023-02471-x?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1038/s41597-023-02471-x">A large-scale fMRI dataset for the visual processing of naturalistic scenes
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">23 August 2023</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Zhengxin Gong, Ming Zhou,  Zonglei Zhen</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41597-019-0052-3/MediaObjects/41597_2019_52_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41597-019-0052-3?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1038/s41597-019-0052-3">BOLD5000, a public fMRI dataset while viewing 5000 visual images
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">06 May 2019</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Nadine Chang, John A. Pyles,  Elissa M. Aminoff</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41597-020-00670-4/MediaObjects/41597_2020_670_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41597-020-00670-4?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1038/s41597-020-00670-4">Individual Brain Charting dataset extension, second release of high-resolution fMRI data for cognitive mapping
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">16 October 2020</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Ana Lusa Pinho, Alexis Amadon,  Bertrand Thirion</p>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'speedy-BootstrappedUCB',
                        timestamp: 1711578484,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                
                
                <div class="main-content">
                    <section data-title="Main"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Main</h2><div class="c-article-section__content" id="Sec1-content"><p>Neuroscience has an insatiable appetite for data. Many ongoing efforts to extensively sample brain activity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="de Vries, S. E. J. et al. A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex. Nat. Neurosci. 23, 138151 (2020)." href="#ref-CR1" id="ref-link-section-d29682704e958">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Siegle, J. H. et al. Survey of spiking in the mouse visual system reveals functional hierarchy. Nature 592, 8692 (2021)." href="#ref-CR2" id="ref-link-section-d29682704e958_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Stringer, C., Pachitariu, M., Steinmetz, N., Carandini, M. &amp; Harris, K. D. High-dimensional geometry of population responses in visual cortex. Nature 571, 361365 (2019)." href="/articles/s41593-021-00962-x#ref-CR3" id="ref-link-section-d29682704e961">3</a></sup> and structure<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Markram, H. et al. Reconstruction and simulation of neocortical microcircuitry. Cell 163, 456492 (2015)." href="#ref-CR4" id="ref-link-section-d29682704e965">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Van Essen, D. C. et al. The WU-Minn human connectome project: an overview. Neuroimage 80, 6279 (2013)." href="#ref-CR5" id="ref-link-section-d29682704e965_1">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Zheng, Z. et al. A complete electron microscopy volume of the brain of adult Drosophila melanogaster. Cell 174, 730743 (2018)." href="/articles/s41593-021-00962-x#ref-CR6" id="ref-link-section-d29682704e968">6</a></sup> are motivated, in part, by the availability of new computational methods that make analysis of massive datasets feasible. Equally as important is the growing desire to understand how the brain coordinates complex sensory and motor behaviors and the realization that the neural networks supporting such behaviors span multiple scales, from single neurons to local circuits to whole systems. Understanding massive, complex networks will inevitably require commensurately massive amounts of data.</p><p>The need for massive data is especially acute in visual neuroscience, which is a model system for understanding brain function. The network that mediates our ability to flexibly and efficiently perceive the visual world occupies approximately one-third of human cerebral cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Van Essen, D. C. et al. Mapping visual cortex in monkeys and humans using surface-based atlases. Vis. Res. 41, 13591378 (2001)." href="/articles/s41593-021-00962-x#ref-CR7" id="ref-link-section-d29682704e975">7</a></sup> and interconnects brain areas with profoundly different functional properties<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Grill-Spector, K. &amp; Malach, R. The human visual cortex. Annu. Rev. Neurosci. 27, 649677 (2004)." href="/articles/s41593-021-00962-x#ref-CR8" id="ref-link-section-d29682704e979">8</a></sup>. This network both encodes visual stimuli and interfaces visual representations into a cognitive context, including information about what one has already seen<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wheeler, M. E., Petersen, S. E. &amp; Buckner, R. L. Memorys echo: vivid remembering reactivates sensory-specific cortex. Proc. Natl Acad. Sci. USA 97, 1112511129 (2000)." href="/articles/s41593-021-00962-x#ref-CR9" id="ref-link-section-d29682704e983">9</a></sup>, might see<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Breedlove, J. L., St-Yves, G., Olman, C. A. &amp; Naselaris, T. Generative feedback explains distinct brain activity codes for seen and mental images. Curr. Biol. 30, 22112224 (2020)." href="/articles/s41593-021-00962-x#ref-CR10" id="ref-link-section-d29682704e987">10</a></sup> or is selectively attending<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Kay, K. N., Weiner, K. S. &amp; Grill-Spector, K. Attention reduces spatial uncertainty in human ventral temporal cortex. Curr. Biol. 25, 595600 (2015)." href="/articles/s41593-021-00962-x#ref-CR11" id="ref-link-section-d29682704e991">11</a></sup>. Understanding vision thus means interrogating a high-dimensional, context-dependent neural network.</p><p>Given these considerations, it is clear that extensive experimental data providing access to whole-brain responses to complex stimuli are critical in the quest to understand the human visual system. The ideal dataset should include naturalistic stimuli: the visual system is distributed widely across the brain, and natural scenes, in addition to being ecologically relevant, are effective activators of the entire system<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Huth, A. G., Nishimoto, S., Vu, A. T. &amp; Gallant, J. L. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron 76, 12101224 (2012)." href="/articles/s41593-021-00962-x#ref-CR12" id="ref-link-section-d29682704e998">12</a></sup>. Moreover, the ideal dataset should be large: to take full advantage of powerful data analysis and machine learning (ML) techniques that have recently become available, we need considerably more data than are currently available. How much? Modern ML methods used in computer vision to process natural scenes (for example, deep convolutional neural networks (CNNs)) require tens to hundreds of thousands of image samples for training<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Krizhevsky, A. Learning Multiple Layers of Features from Tiny Images. &#xA;                  https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf&#xA;                  &#xA;                 (University of Toronto, 2009)." href="/articles/s41593-021-00962-x#ref-CR13" id="ref-link-section-d29682704e1002">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision. &#xA;                  https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48&#xA;                  &#xA;                , 740755 (Springer, 2014)." href="/articles/s41593-021-00962-x#ref-CR14" id="ref-link-section-d29682704e1005">14</a></sup>. A dataset that sampled brain activity at these scales would raise the exciting possibility of exploiting these methods to develop better models of how the brain processes natural scenes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gl, U. &amp; van Gerven, M. A. J. Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. J. Neurosci. 35, 1000510014 (2015)." href="#ref-CR15" id="ref-link-section-d29682704e1009">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Khaligh-Razavi, S.-M. &amp; Kriegeskorte, N. Deep supervised, but not unsupervised, models may explain IT cortical representation. PLoS Comput. Biol. 10, e1003915 (2014)." href="#ref-CR16" id="ref-link-section-d29682704e1009_1">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Seeliger, K. et al. End-to-end neural system identification with neural information flow. PLoS Comput. Biol. 17, e1008558 (2021)." href="#ref-CR17" id="ref-link-section-d29682704e1009_2">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Stansbury, D. E., Naselaris, T. &amp; Gallant, J. L. Natural scene statistics account for the representation of scene categories in human visual cortex. Neuron 79, 10251034 (2013)." href="#ref-CR18" id="ref-link-section-d29682704e1009_3">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="St-Yves, G. &amp; Naselaris, T. The feature-weighted receptive field: an interpretable encoding model for complex feature spaces. Neuroimage 180, 188202 (2018)." href="#ref-CR19" id="ref-link-section-d29682704e1009_4">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl Acad. Sci. USA 111, 86198624 (2014)." href="/articles/s41593-021-00962-x#ref-CR20" id="ref-link-section-d29682704e1012">20</a></sup> and would accelerate efforts to bridge cognitive neuroscience and artificial intelligence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Naselaris, T. et al. Cognitive computational neuroscience: a new conference for an emerging discipline. Trends Cogn. Sci. 22, 365367 (2018)." href="/articles/s41593-021-00962-x#ref-CR21" id="ref-link-section-d29682704e1016">21</a></sup>.</p><p>In this paper, we present a dataset that achieves sampling at this ambitious scale. The NSD consists of high-resolution (1.8-mm) whole-brain 7T functional magnetic resonance imaging (fMRI) of eight carefully screened human participants who each viewed 9,00010,000 color natural scenes (22,00030,000 trials) during 3040 scan sessions distributed over the course of 1 year. Aggregated across participants, NSD includes responses to 70,566 distinct natural scene imagesthis is more than an order of magnitude larger than similar datasets involving fMRI sampling of many images<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chang, N. et al. BOLD5000, a public fMRI dataset while viewing 5000 visual images. Sci. Data 6, 49 (2019)." href="#ref-CR22" id="ref-link-section-d29682704e1023">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Horikawa, T. &amp; Kamitani, Y. Generic decoding of seen and imagined objects using hierarchical visual features. Nat. Commun. 8, 15037 (2017)." href="#ref-CR23" id="ref-link-section-d29682704e1023_1">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kay, K. N., Naselaris, T., Prenger, R. J. &amp; Gallant, J. L. Identifying natural images from human brain activity. Nature 452, 352355 (2008)." href="/articles/s41593-021-00962-x#ref-CR24" id="ref-link-section-d29682704e1026">24</a></sup>. Moreover, as we show, the high quality of the NSD dataset makes it possible to leverage the full power of modern ML methods for developing better models of visual representation. Achieving high data quality was afforded, in part, by the use of ultra-high magnetic field strength (7T) to improve signal-to-noise ratio (SNR) over what is attained at lower field strengths<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Triantafyllou, C. et al. Comparison of physiological noise at 1.5 T, 3 T and 7 T and optimization of fMRI acquisition parameters. Neuroimage 26, 243250 (2005)." href="/articles/s41593-021-00962-x#ref-CR25" id="ref-link-section-d29682704e1030">25</a></sup>.</p><p>NSD incorporates several innovations in addition to its unprecedented scale and quality. To reconcile extensive sampling with a practical time commitment, we used an aggressive rapid event-related design. This drove the development of new analysis techniques that accurately compensate for the overlap of hemodynamic responses across successive trials. To ensure participant engagement and control cognitive state, we incorporated a continuous recognition task<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Brady, T. F., Konkle, T., Alvarez, G. A. &amp; Oliva, A. Visual long-term memory has a massive storage capacity for object details. Proc. Natl Acad. Sci. USA 105, 1432514329 (2008)." href="/articles/s41593-021-00962-x#ref-CR26" id="ref-link-section-d29682704e1038">26</a></sup> in which participants were instructed to indicate whether they have seen each presented image at any point in the past. In addition to making the experiment tolerable (and even somewhat interesting) for participants, the inclusion of this task makes the NSD, to our knowledge, the longest-term continuous recognition memory fMRI study in history and, thus, a likely source of new insights into long-term memory formation and the cognitive context of vision. Finally, to ensure the broad reach of the NSD dataset, we incorporated design input from a large network of collaborators with diverse scientific interests (for example, low-level vision, high-level vision, memory, connectivity and neuroanatomy) and technical expertise (for example, mapping, multivariate pattern analysis, encoding models, representational similarity analysis and neural network modeling). This input helped precipitate a carefully curated dataset with extensive auxiliary measures.</p><p>This paper provides a comprehensive description of the design, acquisition and preparation of the NSD dataset. In particular, we detail the state-of-the-art acquisition and analysis methods that we developed for the dataset and present comprehensive assessments that evidence the high quality of the data. We also present initial analyses of the NSD dataset, demonstrating the feasibility of using data-driven analyses to reveal insights into vision and memory. We expect that the NSD will serve as a valuable resource with widespread application in neuroscience and its intersection with artificial intelligence.</p></div></div></section><section data-title="Results"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Results</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Sampling thousands of images during continuous recognition</h3><p>We obtained 73,000 color natural scenes from the richly annotated Microsoft Common Objects in Context (COCO) image dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision. &#xA;                  https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48&#xA;                  &#xA;                , 740755 (Springer, 2014)." href="/articles/s41593-021-00962-x#ref-CR14" id="ref-link-section-d29682704e1057">14</a></sup>, a dataset that is heavily used in the computer vision and ML communities. Our experimental design specified that each of eight participants would view 10,000 distinct images, and a special set of 1,000 images would be shared across participants (eight participants  9,000 unique images + 1,000 shared images = 73,000 images). This sampling strategy was chosen to maximize the number of distinct images in the NSD while also facilitating investigations of similarities and differences in brain representations across individuals<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Haxby, J. V., Guntupalli, J. S., Nastase, S. A. &amp; Feilong, M. Hyperalignment: modeling shared information encoded in idiosyncratic cortical topographies. eLife 9, e56601 (2020)." href="/articles/s41593-021-00962-x#ref-CR27" id="ref-link-section-d29682704e1061">27</a></sup>. Each image would be presented three times to a given participant. Although this is a low number, we reasoned that three trials would be sufficient to produce robust responses given our use of ultra-high field (7T) fMRI. Furthermore, images would be presented using a rapid event-related design consisting of 4-s trials (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig1">1a</a>). This was done to maximize statistical power and to create an engaging experience for the participants. In addition, the continuous nature of task engagementin contrast to slow event-related designs and block designs where engagement is likely to fluctuatehelps avoid unwanted respiratory variations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Power, J. D., Lynch, C. J., Adeyemo, B. &amp; Petersen, S. E. A critical, event-related appraisal of denoising in resting-state fMRI studies. Cereb. Cortex 30, 55445559 (2020)." href="/articles/s41593-021-00962-x#ref-CR28" id="ref-link-section-d29682704e1068">28</a></sup> and arousal-related confounds<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Roth, Z. N., Ryoo, M. &amp; Merriam, E. P. Task-related activity in human visual cortex. PLoS Biol. 18, e3000921 (2020)." href="/articles/s41593-021-00962-x#ref-CR29" id="ref-link-section-d29682704e1072">29</a></sup>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Design of the NSD experiment."><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1: Design of the NSD experiment.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="588"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p><b>a</b>, Trial design. While maintaining central fixation, participants viewed sequences of color natural scenes and judged whether each image had been previously shown at any point in the past. The scenes, taken from Microsofts COCO<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision. &#xA;                  https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48&#xA;                  &#xA;                , 740755 (Springer, 2014)." href="/articles/s41593-021-00962-x#ref-CR14" id="ref-link-section-d29682704e1091">14</a></sup>, are richly annotated with object information (as depicted). <b>b</b>, Run and session design. Each run lasted 5min and consisted of 62 or 63 stimulus trials with occasional interspersed blank trials. Each scan session consisted of 12 runs (750 stimulus trials). <b>c</b>, Timeline of 7T fMRI scan sessions. Each individual participated in an initial screening session (prffloc), 3040 NSD core sessions and two final sessions (nsdsynthetic and nsdimagery). The first NSD core session corresponds to day 0. <b>d</b>, Behavioral performance. For each of three trial types, we quantify the percentage of trials on which the participant indicated an old response.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>The NSD experiment was split across 40 scan sessions for each participant (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig1">1b</a>). To control cognitive state and encourage deep processing of the images, participants were instructed to perform a continuous recognition task in which they reported whether the current image had been presented at any previous point in the experiment. We controlled the distributions of image presentations such that both short-term and long-term repetitions were probed (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig7">1a</a>). Parameters were selected such that, even in the first scan session, images were not always new, and, even in the last scan session, images were not always old (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig7">1b</a>).</p><h3 class="c-article__sub-heading" id="Sec4">Neuroimaging data collection on carefully selected participants</h3><p>All fMRI data in the NSD were collected at 7T using a whole-brain, 1.8-mm, 1.6-s, gradient-echo, echo-planar imaging (EPI) pulse sequence. After verbally screening several potential participants with respect to basic eligibility criteria, we recruited 14 individuals to participate in an initial 7T fMRI screening session that involved population receptive field (pRF)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Benson, N. C. et al. The human connectome project 7 Tesla retinotopy dataset: description and population receptive field analysis. J. Vis. 18, 23 (2018)." href="/articles/s41593-021-00962-x#ref-CR30" id="ref-link-section-d29682704e1132">30</a></sup> and category functional localizer (fLoc)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Stigliani, A., Weiner, K. S. &amp; Grill-Spector, K. Temporal processing capacity in high-level visual cortex is domain specific. J. Neurosci. 35, 1241212424 (2015)." href="/articles/s41593-021-00962-x#ref-CR31" id="ref-link-section-d29682704e1136">31</a></sup> experiments. Based on data from this scan session, we ranked the 14 participants with respect to data quality. Specifically, we quantified BOLD variance explained in the pRF and fLoc experiments, behavioral performance in the pRF and fLoc experiments and two metrics of head motion, normalized these six measures and then averaged the measures (for details, see Rankings from the 7T fMRI screening session in the <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-021-00962-x#Sec14">Methods</a>). We then invited the top eight individuals to participate in the full NSD experiment (all individuals accepted). This selection process was conducted to ensure the best possible data quality for the NSD. Analyses conducted after completion of the NSD experiment confirm that the ranking procedure successfully identified individuals who yield high-quality data and that data quality would have suffered substantially had we omitted the selection process (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2c</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Overview of acquired data."><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2: Overview of acquired data.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="553"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p><b>a</b>, Auxiliary fMRI experiments. Data from the pRF and fLoc experiments were used to define retinotopic visual areas and category-selective regions, respectively. Resting-state data were collected before and after the NSD runs in a subset of the NSD core sessions (totaling 100 or 180min per participant). <b>b</b>, Available measures. Examples of the actual data are depicted. <b>c</b>, Participant selection. Data quality from the initial screening session was used to rank a set of 14 participants. On the right is an illustration of one measure contributing to the rankingspecifically, variance explained in the fLoc experiment (one slice per participant; identical color range). The inset compares the participant ranking against the b3 noise ceiling calculated on the full NSD dataset (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3</a>). A line fit to the eight NSD participants (gold dots) is extrapolated to predict noise ceilings for the individuals who were not selected for participation in the NSD (red circles). <b>d</b>, Metrics of data quality (for details, see Data quality metrics in the <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-021-00962-x#Sec14">Methods</a>). Results for individual participants (thin colored lines) and the median across participants (thick black line) are shown. The insets show detail on tSNR and head motion for one sample run (see Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">1</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">2</a> for more information).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Data were collected from the eight NSD participants over the course of 1 year (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig1">1c</a>). Participants consistently engaged with the task: the average response rate across scan sessions was above 99% for all participants, and the response rate never dropped below 96% in any single scan session. Moreover, all participants exhibited successful recognition performance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig1">1d</a>), issuing old responses at a higher rate for previously presented images (blue and orange lines) than for novel images (yellow lines). The full NSD dataset includes a variety of anatomical neuroimaging measures (including <i>T</i><sub>1</sub>, <i>T</i><sub>2</sub>, diffusion, venogram and angiogram), functional neuroimaging measures (including the pRF and fLoc experiments, the NSD experiment, resting-state data and two additional experiments involving synthetic stimuli and visual imagery) and behavioral measures (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2a,b</a>). In some fMRI sessions, physiological data (ten sessions per participant) and eye-tracking data (24 sessions per participant) were also collected. Analysis of the eye-tracking data indicates that participants were able to successfully maintain central fixation most of the time, with some variability in fixation performance across participants (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig10">4</a>). Regarding the core NSD experiment, we completed the full set of 40 NSD scan sessions for four of the participants, but, owing to unforeseen summer absences and scheduled decommissioning of the 7T scanner, we completed 3032 NSD scan sessions for each of the other participants. A full breakdown of data collection and analysis procedures is provided in Extended Data Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig8">2</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig9">3</a>.</p><h3 class="c-article__sub-heading" id="Sec5">Stable high-resolution imaging across scan sessions</h3><p>In our experience, although visual inspection is non-quantitative and somewhat subjective, it is still the most effective way to assess many common aspects of fMRI pre-processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Kay, K. et al. A critical assessment of data quality and venous effects in sub-millimeter fMRI. Neuroimage 189, 847869 (2019)." href="/articles/s41593-021-00962-x#ref-CR32" id="ref-link-section-d29682704e1228">32</a></sup>. Accordingly, we generated a comprehensive set of visualizations that detail the excellent quality of the raw and pre-processed NSD data. These include detailed inspections of raw time series data to confirm the presence of stimulus-evoked signals (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">3</a>); movies that assess the co-registration of the different imaging modalities (for example, <i>T</i><sub>1</sub>, <i>T</i><sub>2</sub> and EPI; Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">1</a>); movies that assess the manually edited cortical surface reconstructions generated using FreeSurfer (Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">2</a>); movies that assess the registration of the NSD participants to the fsaverage (Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">3</a>) and MNI (Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">4</a>) group spaces; movies that inspect raw and pre-processed EPI volumes (Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">5</a>); and movies that provide volume and surface visualizations of the stability of mean EPI intensity across sessions (Supplementary Videos <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">6</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">7</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">4</a>) and the stability of BOLD responses across sessions (Supplementary Videos <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">8</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">9</a>). All movies are readily viewable online (<a href="https://osf.io/zyb3t/">https://osf.io/zyb3t/</a>). The visualizationsin particular, Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">9</a>indicate that the quality of the NSD data enable precision functional mapping<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Gordon, E. M. et al. Precision functional mapping of individual human brains. Neuron 95, 791807 (2017)." href="/articles/s41593-021-00962-x#ref-CR33" id="ref-link-section-d29682704e1286">33</a></sup>: activity patterns are fine-scale and highly reliable within individual participants, and these patterns are distinct across participants.</p><p>In addition to visual inspection, quantitative data quality metrics were computed for each NSD scan session. This was in fact done on a rolling basis as the data were acquired, allowing us to monitor data quality and provide performance bonuses to the participants. Inspecting the metrics, we see that temporal signal-to-noise ratio (tSNR) is stable across scan sessions for each participant (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2d</a>, left). One participant, participant 8, exhibited low tSNR compared to the other participants; this can be attributed to higher levels of head motion for this participant (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2d</a>, middle). We also observe that BOLD responses (quantified as median variance explained across voxels and runs by a simple ONOFF general linear model (GLM)) are stable across scan sessions for each participant, although there is substantial variation in the strength of BOLD responses across participants (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2d</a>, right).</p><p>One feature that we implemented in the pre-processing of the fMRI data was to interpolate the data on a fine temporal grid and a fine spatial grid in the same steps used to correct for slice timing differences and spatial displacements (for example, head motion). This upsampling strategy preserves fine-scale detail that is present in the raw fMRI data due to the temporal jitter of the acquired fMRI volumes relative to the experimental paradigm and the spatial jitter of the acquired fMRI volumes relative to the anatomy of the brain<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Kay, K. et al. A critical assessment of data quality and venous effects in sub-millimeter fMRI. Neuroimage 189, 847869 (2019)." href="/articles/s41593-021-00962-x#ref-CR32" id="ref-link-section-d29682704e1305">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Kang, X., Yund, E. W., Herron, T. J. &amp; Woods, D. L. Improving the resolution of functional brain imaging: analyzing functional data in anatomical space. Magn. Reson. Imaging 25, 10701078 (2007)." href="/articles/s41593-021-00962-x#ref-CR34" id="ref-link-section-d29682704e1308">34</a></sup>. An illustration of the benefits of upsampling is provided in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig11">5</a>. This example highlights the existence of fine-scale detail in fMRI image intensities (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig11">5b</a>, top row) as well as in BOLD responses extracted from the fMRI data (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig11">5b</a>, bottom row, and Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig11">5c</a>). Notably, this fine-scale detail is replicable across different scan sessions (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig11">5c</a>, bottom, and Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig11">5d</a>), indicating that the upsampled preparation reveals meaningful detail that is lost under a non-upsampled approach.</p><h3 class="c-article__sub-heading" id="Sec6">Extensive auxiliary measures to complement the NSD data</h3><p>To enrich the fMRI data from the NSD experiment, we collected and prepared a large set of auxiliary measures. These measures include substantial amounts of resting-state data (minimum 100min per participant), external physiological measures during the resting-state scan sessions, diffusion data and associated derivatives (white-matter tracts and structural connectivity matrices) and an extensive collection of manually defined regions of interest (ROIs), including retinotopic and category-selective areas as well as subregions of the thalamus and medial temporal lobe. Results and discussion of these resources can be found in Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">1</a>, Extended Data Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig12">6</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig13">7</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">5</a>.</p><h3 class="c-article__sub-heading" id="Sec7">Accurate estimation of single-trial fMRI response amplitudes</h3><p>We performed a GLM analysis of the data from the NSD experiment to help streamline subsequent analyses of the data. The goal of the GLM was to obtain single-trial betasthat is, estimates of the fMRI response amplitude of each voxel to each trial conducted. Given the low SNR of fMRI and the overlap of the hemodynamic response from trial to trial, estimating accurate betas is a challenging endeavor. We thus developed a novel GLM approach consisting of three components. First, we used a library of hemodynamic response functions (HRFs) derived from an initial analysis of the dataset as an efficient and well-regularized method for estimating voxel-specific HRFs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3ac</a>). Second, we adapted the GLMdenoise technique<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Kay, K. N., Rokem, A., Winawer, J., Dougherty, R. F. &amp; Wandell, B. GLMdenoise: a fast, automated technique for denoising task-based fMRI data. Front. Neurosci. 7, 247 (2013)." href="/articles/s41593-021-00962-x#ref-CR35" id="ref-link-section-d29682704e1363">35</a></sup> to the single-trial GLM framework, thereby enabling the use of data-driven nuisance regressors (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3d</a>). Third, to address the challenge posed by highly correlated single-trial regressors, we developed an efficient implementation of ridge regression<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Rokem, A. &amp; Kay, K. Fractional ridge regression: a fast, interpretable reparameterization of ridge regression. Gigascience 9, giaa133 (2020)." href="/articles/s41593-021-00962-x#ref-CR36" id="ref-link-section-d29682704e1370">36</a></sup> and used this to regularize and improve the accuracy of the betas (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3e</a>). To assess the efficacy of these various GLM techniques, we generated three versions of the betas, reflecting increasing sophistication (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig14">8ac</a>). Beta version 1 (b1) is the result of simply using a canonical HRF for all voxels. Beta version 2 (b2) is the result of fitting an HRF to each voxel using the library-of-HRFs approach. Beta version 3 (b3) uses the library-of-HRFs approach as with b2 but also adds the use of GLMdenoise and ridge regression in an attempt to improve the accuracy of the betas.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Improving SNR through novel response estimation and denoising methods."><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3: Improving SNR through novel response estimation and denoising methods.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="628"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p><b>a</b><b>c</b>, Library of HRFs. HRFs were estimated within a subspace spanned by three PCs. Distributions of voxel-specific HRFs are shown for individual participants (<b>a</b>) and the group average (<b>b</b>). These distributions reside on the unit sphere with coordinate axes corresponding to three PC time courses (<b>b</b>, inset). We defined a series of points on the unit sphere (cyan dots), and the time courses associated with these points are used as the HRF library (<b>c</b>). <b>d</b>, GLMdenoise. Horizontal lines indicate the average number of GLMdenoise regressors identified in a scan session (1.8-mm preparation; error bars indicate bootstrapped 68% confidence intervals). <b>e</b>, Ridge regression. Optimal ridge regression fractions are shown for an example scan session (participant 5, nsd10, 1-mm preparation). <b>f</b>, Noise ceilings for the case where responses are averaged across three trials. Results from individual participants (nativesurface preparation) were mapped to fsaverage and then averaged. Right inset shows results thresholded at 15% on the inflated left hemisphere (see also Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">10</a>). <b>g</b>, Performance summary. Each bar indicates the median noise ceiling across vertices in the nsdgeneral ROI. Calc, calcarine sulcus; CGS, cingulate sulcus; CoS, collateral sulcus; CS, central sulcus; IFRS, inferior frontal sulcus; IPS, intraparietal sulcus; LS, lateral sulcus; OTS, occipitotemporal sulcus; PoCS, post-central sulcus; PrCS, precentral sulcus; SFRS, superior frontal sulcus; STS, superior temporal sulcus.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>We quantified the quality of the different beta versions (b1, b2 and b3) by calculating noise ceilings for individual voxels. The noise ceiling is a measure of trial-to-trial reliability, quantifying the percentage of variance in a voxels responses that can be attributed to the stimulus and not to measurement noise (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-021-00962-x#Sec14">Methods</a>). Surface maps of noise ceiling results reveal locations of reliable responses to the NSD stimuli: high noise ceilings are present in occipital cortex and extend into temporal and parietal cortex (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3f</a> and Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">10</a>). Notably, the maps reveal very large increases in noise ceilings from b1 to b2 to b3, indicating that the additional GLM techniques incorporated into b2 and b3 improve reliability of responses. Detailed quantifications show that these improvements are highly consistent across voxels and participants (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3g</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">6a</a>) and that noise ceiling estimates are highly reliable (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">6b</a>). For b3, the noise ceiling levels in visual cortex are, on average, 36% (calculated by computing the median across the nsdgeneral ROI and then averaging across participants). This means that a typical visual cortex voxel in the NSD dataset has associated with it a set of 10,000 responses (30,000 trials divided by 3 trials per image = 10,000 images), and a large percentage, 36%, of the variance in these 10,000 values is a signal that is, in theory, predictable. Expressed in terms of Pearsons correlation (<i>r</i>), this is equivalent to a prediction accuracy of <i>r</i>=0.60. Complementing the noise ceiling analysis, we also performed simple univariate analyses of the NSD betas (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig14">8d,e</a>); these analyses show that the NSD dataset contains high response reliability across trials within a participant as well as high response reliability across participants.</p><h3 class="c-article__sub-heading" id="Sec8">A massive increase in equivalent trials</h3><p>To put the quality of the NSD data into perspective, we propose the concept of equivalent trials, which allows comparison of different datasets that vary in SNR and trial distribution (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/s41593-021-00962-x#Sec14">Methods</a> for details). The next largest data collection effort that is similar in nature to NSD is BOLD5000 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Chang, N. et al. BOLD5000, a public fMRI dataset while viewing 5000 visual images. Sci. Data 6, 49 (2019)." href="/articles/s41593-021-00962-x#ref-CR22" id="ref-link-section-d29682704e1476">22</a></sup>). Using the same GLM analysis methods on both NSD and BOLD5000, we found that the SNR per trial is approximately 0.260 for the NSD and 0.187 for BOLD5000. Combining these values with the number of trials conducted in each dataset, we estimate that the total size of the NSD dataset is 213,000 trials  (0.260)<sup>2</sup>=14,399 equivalent trials, whereas the total size of BOLD5000 is 18,870 trials  (0.187)<sup>2</sup>=660 equivalent trials. Thus, using the metric of equivalent trials, the NSD can be viewed as 14,399/660=~22 times as large as the BOLD5000 dataset. This is a massive increase in statistical power. Note that even if we do not take into account the higher SNR per trial in the NSD dataset, the NSD still has substantially more participants (eight versus four), more trials per participant (26,625 versus 4,718, on average) and more hours of fMRI per participant (35.5 versus 13.7, on average) than BOLD5000.</p><h3 class="c-article__sub-heading" id="Sec9">Successful recovery of retinotopy</h3><p>Having demonstrated the quality of the NSD data, we now turn to example analyses that illustrate the rich scientific insights that can be derived from the data. As a simple starting example, we fit a voxel-wise pRF model that uses local contrast in the NSD images to account for the NSD betas. This simple model is expected to recover spatial tuning in early visual cortex where responses co-vary with stimulus energy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Albrecht, D. G. &amp; Hamilton, D. B. Striate cortex of monkey and cat: contrast response function. J. Neurophysiol. 48, 217237 (1982)." href="/articles/s41593-021-00962-x#ref-CR37" id="ref-link-section-d29682704e1492">37</a></sup>. Indeed, in all eight participants, high-quality maps of angle and eccentricity estimates are obtained in early visual cortex, and these estimates extend all the way to the fovea (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig15">9</a> and Supplementary Modeling Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">1</a>). These results provide a check of the validity of the NSD betas. They also show that participants were able to maintain central fixation reliably enough to support detailed mapping of visual space. This finding is consistent with our analysis of the eye-tracking data (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig10">4</a>).</p><h3 class="c-article__sub-heading" id="Sec10">Reliable and long-term recognition memory effects</h3><p>The use of a continuous recognition task establishes the NSD as one of the largest datasets relevant to human memory. Despite the challenging nature of the task, we found that participants were able to successfully discriminate old images from new images (average <i>d'</i> across participants: 1.28, maximum: 1.47, minimum: 0.94). Furthermore, recognition memory remained above chance even at long time scales between repetitions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig4">4a</a>). Specifically, for each session, we calculated a measure of recognition accuracy accounting for guessing (adjusted hit rate: hit rate minus false alarm rate) and binned this measure by the time since last exposure (considering only those trials involving a previously shown image). At the group level, participants exhibited performance levels greater than chance (adjusted hit rate &gt; 0) in all measured intervals, ranging from 1 s to 1 year. At the level of individuals, all participants showed a positive adjusted hit rate in the longest time bin for which data are available for every participant (when binning on a log scale; seven of eight participants when binning on a linear scale). These results indicate that, from its behavioral component alone, NSD is powered to address questions concerning human memory spanning short (seconds) to relatively long (months) time scales.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Reliable and long-term recognition memory effects."><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4: Reliable and long-term recognition memory effects.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="519"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p><b>a</b>, Behavioral recognition effects. Adjusted hit rate indicates recognition accuracy accounting for guessing (hit rate minus false alarm rate) and is binned by time between repetitions on a linear scale (left) or a log scale (right). Dashed line indicates chance performance. Each dot in each bin summarizes relevant trials from one scan session. Black line indicates the mean across participants, with the ribbon indicating  1 s.e.m. <b>b</b>, Neural recognition effects. We performed two-sample <i>t</i>-tests on NSD betas contrasting hits &gt; correct rejections. All results are shown on a flattened left hemisphere fsaverage surface and thresholded at |<i>t</i>|&gt; 3 (inset shows inflated surface). Tests were performed for trials taken from individual NSD scan sessions (columns 14) as well as for trials pooled across all NSD scan sessions (column 5). In addition, we performed a control in which trial labels in the pooled analysis were shuffled (column 6). Results for participant 1 (top row) and a simple average of results across participants (bottom row) are shown. Calc, calcarine sulcus; CGS, cingulate sulcus; CoS, collateral sulcus; CS, central sulcus; IFRS, inferior frontal sulcus; IPS, intraparietal sulcus; LS, lateral sulcus; OTS, occipitotemporal sulcus; PoCS, post-central sulcus; PrCS, precentral sulcus; SFRS, superior frontal sulcus; STS, superior temporal sulcus.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>But what about neural effects? To assess whether recognition effects are present in the fMRI data, we performed two-sample <i>t</i>-tests contrasting NSD betas observed for hits with NSD betas observed for correct rejections (the so-called old/new effect<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Wagner, A. D., Shannon, B. J., Kahn, I. &amp; Buckner, R. L. Parietal lobe contributions to episodic memory retrieval. Trends Cogn. Sci. 9, 445453 (2005)." href="/articles/s41593-021-00962-x#ref-CR38" id="ref-link-section-d29682704e1556">38</a></sup>). We found highly consistent old/new effects at the level of individual scan sessions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig4">4b</a>, top; see also Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">7</a>). Moreover, these effects occur in expected frontal and parietal regions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Spaniol, J. et al. Event-related fMRI studies of episodic encoding and retrieval: meta-analyses using activation likelihood estimation. Neuropsychologia 47, 17651779 (2009)." href="/articles/s41593-021-00962-x#ref-CR39" id="ref-link-section-d29682704e1566">39</a></sup> and persist at the group level (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig4">4b</a>, bottom). The scale and statistical power afforded by the NSD dataset also provide additional insight. Whereas old/new effects are typically studied using group-level analyses, the quality of the NSD dataset reveals highly statistically significant results at the level of individual participants. Indeed, when pooling trials across all NSD scan sessions, several participants exhibited statistically significant activity differentiating hits and correct rejections in nearly the entire cerebral cortex (see results for a representative participant in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig4">4b</a>, top). Reminiscent of past datasets employing extensive sampling of individuals<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Gonzalez-Castillo, J. et al. Whole-brain, time-locked activation with simple tasks revealed using massive averaging and model-free analysis. Proc. Natl Acad. Sci. USA 109, 54875492 (2012)." href="/articles/s41593-021-00962-x#ref-CR40" id="ref-link-section-d29682704e1577">40</a></sup>, the current results suggest that the extent of cortex engaged by basic memory processes is much more widespread than previously appreciated, although a careful consideration of effect sizes would be important for a full understanding of the effect.</p><h3 class="c-article__sub-heading" id="Sec11">Rich stimulus sampling for probing brain representations</h3><p>The NSD samples a large variety of natural scenes. To gain insight into the breadth of stimulus sampling available, we constructed representational dissimilarity matrices (RDMs) from the NSD betas and performed <i>t-</i>distributed stochastic neighbor embedding<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Maaten, Lvander &amp; Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. Res. 9, 25792605 (2008)." href="/articles/s41593-021-00962-x#ref-CR41" id="ref-link-section-d29682704e1592">41</a></sup> (<i>t</i>-SNE) to visualize the underlying representations. We computed <i>t</i>-SNE embeddings in different regions along the ventral visual pathway for an example participant (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5a</a>). These embeddings reflect arrangements of stimuli that are driven by the overall similarity of multi-voxel activity patterns in the brain, independent of their anatomical organization within a given ROI. Visualizing the data in this way reveals intriguing patterns of semantic representation that are clearly visible by eye. For example, by color-coding the resulting embeddings according to animacy attributes (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5b</a>), we found that, in posterior ventral temporal cortex (pVTC), there is a clear large-scale pattern progressing from images containing people (gray dots, lower left), images containing animals (red dots, middle) and images containing inanimate objects (blue dots, upper right), whereas the pattern is not present in early visual areas V1, V2 and V3. This aspect of semantic representation is consistent with previous studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Connolly, A. C. et al. The representation of biological classes in the human brain. J. Neurosci. 32, 26082618 (2012)." href="/articles/s41593-021-00962-x#ref-CR42" id="ref-link-section-d29682704e1609">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Naselaris, T., Stansbury, D. E. &amp; Gallant, J. L. Cortical representation of animate and inanimate objects in complex natural scenes. J. Physiol. Paris 106, 239249 (2012)." href="/articles/s41593-021-00962-x#ref-CR43" id="ref-link-section-d29682704e1612">43</a></sup>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Representational similarity analysis reveals transformations of representations along the ventral visual stream."><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5: Representational similarity analysis reveals transformations of representations along the ventral visual stream.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="715"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p><b>a</b>, Illustration of fsaverage ROIs used for the representational similarity analysis. <b>b</b>, <i>t</i>-SNE embedding for each ROI in an example participant (participant 1). Each dot represents a distinct image (total, 10,000). Using category labels from the COCO image dataset, we color each dot according to whether the associated image contains particular combinations of people, animals and inanimates. <b>c</b>, <i>t</i>-SNE embedding for aVTC with actual images depicted. Insets highlight an inanimate cluster (blue inset) and a cluster of people with inanimate objects (gray inset). <b>d</b>, Categorical brain repesentations. We plot the correlation between brain RDMs and a model RDM constructed from category labels in the COCO dataset. Color-shaded regions indicate within-participant error (mean and standard error across distinct groups of images), whereas the gray-shaded region indicates across-participant error (mean and standard error across participants). <b>e</b>, Similarities of brain representations across ROIs and participants. Depicted are correlations across brain RDMs obtained for different ROIs and participants. Thin white lines separate groups of eight participants. <b>f</b>, Quantitative summary. We summarize the results of <b>e</b> by averaging the upper triangle of each group of 88 participants, reflecting the correlation of RDMs from different participants. Shaded regions indicate standard errors estimated by bootstrapping participants with replacement.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Other intriguing patterns are also visible. In anterior ventral temporal cortex (aVTC), the animacy progression is present to some extent, but a different, more clustered representation emerges that presumably reflects more complex categorical and semantic clusters. Indeed, zooming in on small sections of the <i>t</i>-SNE embedding for aVTC reveals that these clusters contain images with relatively homogeneous semantic content (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5c</a>): the blue cluster is dominated by images of round edible objects, whereas the gray cluster is dominated by images of people interacting with objects. Note that the clustering of semantically related images does not necessarily mean that these representations are truly semantic in the sense of being invariant or independent of visual features; the clustering could be driven by certain visual features that are diagnostic of object categories<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Long, B., Yu, C.-P. &amp; Konkle, T. Mid-level visual features underlie the high-level categorical organization of the ventral stream. Proc. Natl Acad. Sci. USA 115, E9015E9024 (2018)." href="/articles/s41593-021-00962-x#ref-CR44" id="ref-link-section-d29682704e1672">44</a></sup>. To tease apart these possibilities, additional detailed analyses would be necessary. Overall, these findings show how simple visual inspections of the NSD dataset can be used to generate hypotheses about visual representations in the human brain.</p><p>To further characterize brain representations using a quantitative analysis, we calculated how well brain RDMs are captured by a model RDM constructed from category labels in the COCO image dataset. Consistent with the clustering observed in the <i>t</i>-SNE embeddings, we found that categorical structure is pronounced in VTC compared to early visual areas (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5d</a>). Finally, to assess the utility of the NSD for investigating similarities of brain representations across participants, we isolated images that were common across participants and created a second-order RDM that quantifies the similarity of brain RDMs across ROIs and participants (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5e</a>). In this second-order RDM, we observed high levels of consistency in each ROIs representation across participants (red outlines). We also observed distinct representations across ROIs, with the largest distinctions occurring between early visual areas and VTC. One noticeable finding is the existence of strong off-diagonal elements (white arrows); these elements indicate spatial noise correlations that are typical in fMRI and other neural measurement techniques. To counteract these noise correlations, one simple approach is to compare representations across ROIs using data from distinct trials<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Henriksson, L., Khaligh-Razavi, S.-M., Kay, K. &amp; Kriegeskorte, N. Visual representations are dominated by intrinsic fluctuations correlated between areas. Neuroimage 114, 275286 (2015)." href="/articles/s41593-021-00962-x#ref-CR45" id="ref-link-section-d29682704e1688">45</a></sup>. To further summarize the second-order RDM, we computed the average correlation of brain RDMs across all ROI pairs, restricting this calculation to distinct participants to avoid the effects of spatial noise correlations (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5f</a>). We observe that correlations are highest for brain RDMs from the same ROI (for example, a given participants V1 RDM is more correlated with other participants V1 RDMs compared to other ROIs), confirming consistencies in brain representations across participants (for a complementary univariate analysis of across-participant consistency, see Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig14">8d,e</a>).</p><h3 class="c-article__sub-heading" id="Sec12">A brain-optimized neural network model of the visual system</h3><p>One of the main motivations for the NSD was to amass sufficient sampling of brain activity to be able to drive data-hungry ML techniques. As an intriguing test case, we specifically investigated whether we could successfully use the scale of the NSD to train, from scratch, a deep CNN to accurately predict brain activity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Seeliger, K. et al. End-to-end neural system identification with neural information flow. PLoS Comput. Biol. 17, e1008558 (2021)." href="/articles/s41593-021-00962-x#ref-CR17" id="ref-link-section-d29682704e1707">17</a></sup>. Adopting the framework of encoding models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Naselaris, T., Kay, K. N., Nishimoto, S. &amp; Gallant, J. L. Encoding and decoding in fMRI. Neuroimage 56, 400410 (2011)." href="/articles/s41593-021-00962-x#ref-CR46" id="ref-link-section-d29682704e1711">46</a></sup>, we took NSD betas from visual areas V1hV4, divided these data into a training set (used for parameter tuning) and a validation set (used to assess prediction performance) and evaluated how accurately different computational models predict brain responses in the validation set based on the presented image. The primary encoding model of interest is based on a new network that we refer to as GNet, a brain-optimized CNN whose parameters are trained using imageresponse pairings observed in the training set. For comparison, we also evaluated an encoding model based on AlexNet<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems 25 &#xA;                  https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html&#xA;                  &#xA;                , 10971105 (2012)." href="/articles/s41593-021-00962-x#ref-CR47" id="ref-link-section-d29682704e1715">47</a></sup>, a task-optimized CNN whose parameters are pre-trained using explicit labels of objects taken from an image database. AlexNet has been previously shown to provide state-of-the-art performance in modeling visual responses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Gl, U. &amp; van Gerven, M. A. J. Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. J. Neurosci. 35, 1000510014 (2015)." href="/articles/s41593-021-00962-x#ref-CR15" id="ref-link-section-d29682704e1719">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="St-Yves, G. &amp; Naselaris, T. The feature-weighted receptive field: an interpretable encoding model for complex feature spaces. Neuroimage 180, 188202 (2018)." href="/articles/s41593-021-00962-x#ref-CR19" id="ref-link-section-d29682704e1722">19</a></sup>. Finally, we included a simple V1-like control model based on oriented Gabor filters<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kay, K. N., Naselaris, T., Prenger, R. J. &amp; Gallant, J. L. Identifying natural images from human brain activity. Nature 452, 352355 (2008)." href="/articles/s41593-021-00962-x#ref-CR24" id="ref-link-section-d29682704e1726">24</a></sup>. Details of modeling procedures are provided in Supplementary Modeling Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">2</a> and Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig16">10</a>.</p><p>Varying the amount of training data provided to the models, we found that the performance of the GNet-based encoding model is relatively poor when only small amounts of training data are available (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig6">6a</a>, orange arrows). This is expected because the feature extractors in GNet are not pre-trained and thus require data for tuning. However, when large amounts of training data are available, the GNet model exhibits an impressive increase in performance, achieving approximate parity with the AlexNet-based encoding model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig6">6a</a>, blue arrows). Interestingly, when we trained a single GNet model using brain activity from multiple participants, we found that the model was able to outperform the AlexNet model (two-tailed paired <i>t</i>-test across participants, <i>P</i>=0.013), albeit modestly (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig6">6a</a>, red arrows). Noticeably, the simple Gabor model accounts for substantial variance in the responses; nonetheless, the more complex CNN-based models provide additional predictive power, consistent with previous observations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Cadena, S. A. et al. Deep convolutional models improve predictions of macaque V1 responses to natural images. PLoS Comput. Biol. 15, e1006897 (2019)." href="/articles/s41593-021-00962-x#ref-CR48" id="ref-link-section-d29682704e1756">48</a></sup>. For additional insight into model performance, we compared voxel-wise performance levels of the GNet model to noise ceiling estimates (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig6">6b</a>). Across voxels, prediction accuracy is tightly correlated with the noise ceiling, suggesting that voxel-wise differences in prediction accuracy simply reflect differences in SNR. In addition, performance levels are close to, but do not reach, the noise ceiling. Finally, cortical surface maps indicate that voxel-wise performance levels vary across foveal and peripheral representations (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig6">6c</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Prediction of brain activity using a brain-optimized neural network."><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6: Prediction of brain activity using a brain-optimized neural network.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig6_HTML.png?as=webp"><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="506"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>We used encoding models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Naselaris, T., Kay, K. N., Nishimoto, S. &amp; Gallant, J. L. Encoding and decoding in fMRI. Neuroimage 56, 400410 (2011)." href="/articles/s41593-021-00962-x#ref-CR46" id="ref-link-section-d29682704e1778">46</a></sup> to predict voxel activity in V1hV4. NSD betas were divided into a training set (consisting of up to 9,000 images  3 trials = 27,000 training samples per participant) and a validation set (consisting of up to 1,000 images  3 trials = 3,000 validation samples per participant), and the accuracy of different encoding models was quantified as the voxel-wise correlation between model predictions and responses observed in the validation set. <b>a</b>, Performance as a function of amount of training data used. Models include an encoding model based on AlexNet, which is a task-optimized neural network (blue); encoding models based on GNet, which is a brain-optimized neural network trained using data from single participants (orange) or data from multiple participants (red); and a V1-like control model based on Gabor filters (purple). Plotted lines and error bars indicate mean and standard deviation across results obtained from different bootstrap samples of the data. <b>b</b>, Detailed view of the performance of the multi-participant GNet model for a representative participant. <b>c</b>, Surface maps depicting spatial distribution of validation accuracy for the multi-participant GNet model.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41593-021-00962-x/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>The demonstration that an encoding model based on a brain-optimized CNN (GNet) outperforms an encoding model based on a task-optimized CNN (AlexNet) is important for two reasons. First, it indicates that the NSD is large enough to successfully train a complex neural network architecture. Had the NSD dataset been smaller in scale or lower in quality, qualitatively different patterns of model performance would have been obtained (in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig6">6a</a>, compare orange arrows reflecting a few thousand trials to red arrows reflecting tens of thousands of trials). Second, the successful training of a brain-optimized CNN opens the possibility of new avenues of investigation into the nature of the features used in CNNs. It is an interesting open question whether the features learned by task-optimized networks like AlexNet are similar to, or diverge from, the features present in brain-optimized networks like GNet. In general, brain-optimized networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Seeliger, K. et al. End-to-end neural system identification with neural information flow. PLoS Comput. Biol. 17, e1008558 (2021)." href="/articles/s41593-021-00962-x#ref-CR17" id="ref-link-section-d29682704e1805">17</a></sup> are a useful alternative to task-optimized networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Khaligh-Razavi, S.-M. &amp; Kriegeskorte, N. Deep supervised, but not unsupervised, models may explain IT cortical representation. PLoS Comput. Biol. 10, e1003915 (2014)." href="/articles/s41593-021-00962-x#ref-CR16" id="ref-link-section-d29682704e1809">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl Acad. Sci. USA 111, 86198624 (2014)." href="/articles/s41593-021-00962-x#ref-CR20" id="ref-link-section-d29682704e1812">20</a></sup>, as the narrowly defined tasks that task-optimized networks are typically trained to solve do not necessarily respect the diversity of functions supported by the human visual system<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Wang, A., Tarr, M. &amp; Wehbe, L. Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity. In Advances in Neural Information Processing Systems 32 &#xA;                  https://papers.nips.cc/paper/2019/hash/f490c742cd8318b8ee6dca10af2a163f-Abstract.html&#xA;                  &#xA;                , 1547515485 (2019)." href="/articles/s41593-021-00962-x#ref-CR49" id="ref-link-section-d29682704e1816">49</a></sup> nor necessarily match properties found in biological visual systems<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Sinz, F. H., Pitkow, X., Reimer, J., Bethge, M. &amp; Tolias, A. S. Engineering a less artificial intelligence. Neuron 103, 967979 (2019)." href="/articles/s41593-021-00962-x#ref-CR50" id="ref-link-section-d29682704e1820">50</a></sup>.</p></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Discussion</h2><div class="c-article-section__content" id="Sec13-content"><p>In the last several years, several large-scale neuroimaging datasets have been made publicly available for re-use (for example, refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Van Essen, D. C. et al. The WU-Minn human connectome project: an overview. Neuroimage 80, 6279 (2013)." href="/articles/s41593-021-00962-x#ref-CR5" id="ref-link-section-d29682704e1833">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Gordon, E. M. et al. Precision functional mapping of individual human brains. Neuron 95, 791807 (2017)." href="/articles/s41593-021-00962-x#ref-CR33" id="ref-link-section-d29682704e1836">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Aliko, S., Huang, J., Gheorghiu, F., Meliss, S. &amp; Skipper, J. I. A naturalistic neuroimaging database for understanding the brain using ecological stimuli. Sci. Data 7, 347 (2020)." href="#ref-CR51" id="ref-link-section-d29682704e1839">51</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nastase, S. A., Liu, Y.-F., Hillman, H., Norman, K. A. &amp; Hasson, U. Leveraging shared connectivity to aggregate heterogeneous datasets into a common response space. Neuroimage 217, 116865 (2020)." href="#ref-CR52" id="ref-link-section-d29682704e1839_1">52</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Taylor, J. R. et al. The cambridge centre for ageing and neuroscience (Cam-CAN) data repository: structural and functional MRI, MEG, and cognitive data from a cross-sectional adult lifespan sample. Neuroimage 144, 262269 (2017)." href="/articles/s41593-021-00962-x#ref-CR53" id="ref-link-section-d29682704e1842">53</a></sup>). Several distinguishing aspects of the present work sets the NSD apart from past datasets. One is the unprecedented scale of the dataset. The NSD shares the motivation of recent deep (or precision) neuroimaging efforts<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Gordon, E. M. et al. Precision functional mapping of individual human brains. Neuron 95, 791807 (2017)." href="/articles/s41593-021-00962-x#ref-CR33" id="ref-link-section-d29682704e1846">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bellec, P. &amp; Boyle, J. A. Bridging the gap between perception and action: the case for neuroimaging, AI and video games. Preprint at &#xA;                  https://psyarxiv.com/3epws&#xA;                  &#xA;                 (2019)." href="#ref-CR54" id="ref-link-section-d29682704e1849">54</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pinho, A. L. et al. Individual Brain Charting, a high-resolution fMRI dataset for cognitive mapping. Sci. Data 5, 180105 (2018)." href="#ref-CR55" id="ref-link-section-d29682704e1849_1">55</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Poldrack, R. A. et al. Long-term neural and physiological phenotyping of a single human. Nat. Commun. 6, 8885 (2015)." href="#ref-CR56" id="ref-link-section-d29682704e1849_2">56</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Seeliger, K., Sommers, R. P., Gl, U., Bosch, S. E. &amp; van Gerven, M. A. J. A large single-participant fMRI dataset for probing brain responses to naturalistic stimuli in space and time. Preprint at &#xA;                  https://www.biorxiv.org/content/10.1101/687681v1&#xA;                  &#xA;                 (2019)." href="/articles/s41593-021-00962-x#ref-CR57" id="ref-link-section-d29682704e1852">57</a></sup> that are seeking to amass large amounts of data from individual subjects, as opposed to modest amounts of data from a large number of subjects. In this context of deep neuroimaging, the NSD is, to our knowledge, the most extensive fMRI data collection effort that has been performed to date. This can be gauged not only in terms of the number of hours of fMRI data acquisition per participant (3040h of data for each of eight participants on the core NSD experiment) and the high spatial resolution of the acquired data (1.8mm) but also the wealth of additional measures beyond the core experiment, including substantial amounts of resting-state and diffusion data, physiological data and functional localizers. The availability of extensive measures provides the opportunity to build complete models of how individual brains support vision and memory<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Naselaris, T., Allen, E. &amp; Kay, K. Extensive sampling for complete models of individual brains. Curr. Opin. Behav. Sci. 40, 4551 (2021)." href="/articles/s41593-021-00962-x#ref-CR58" id="ref-link-section-d29682704e1856">58</a></sup>. Of course, the emphasis on depth in individuals comes at the cost of sampling fewer individuals; datasets emphasizing large numbers of individuals, such as the Human Connectome Project<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Van Essen, D. C. et al. The WU-Minn human connectome project: an overview. Neuroimage 80, 6279 (2013)." href="/articles/s41593-021-00962-x#ref-CR5" id="ref-link-section-d29682704e1860">5</a></sup>, are better suited for studying variability in the general population and how psychological traits broadly relate to brain structure and function.</p><p>A second aspect is the unusually high quality of the data. Although the quality of neuroimaging data is more complex to assess than quantity, assessment of data quality is essential because MRI data have relatively low sensitivity and are prone to errors and artifacts. In particular, when acquiring massive datasets, there is a risk of accumulating unknown sources of noise and artifact. The work presented in this paper (and in the accompanying files in the data release) guards against this possibility by crafting a customized and highly optimized approach to pre-processing the NSD data and providing comprehensive documentation of the high data quality (see also Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">2</a>). Several factors likely contributed to the high data quality. These include (1) the use of ultra-high magnetic field strength (7T), which enhances BOLD contrast-to-noise ratio; (2) the screening, training and incentivization of participants; (3) the detailed inspection and supervision of data processing; and (4) the large network of collaborators who helped guide the design and trajectory of the dataset.</p><p>A third aspect of the present work lies in the novel analysis techniques developed for improved GLM analysis of fMRI time series data. These include (1) an efficient and robust method to estimate voxel-specific HRFs; (2) adaptation of the GLMdenoise technique<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Kay, K. N., Rokem, A., Winawer, J., Dougherty, R. F. &amp; Wandell, B. GLMdenoise: a fast, automated technique for denoising task-based fMRI data. Front. Neurosci. 7, 247 (2013)." href="/articles/s41593-021-00962-x#ref-CR35" id="ref-link-section-d29682704e1873">35</a></sup> to a single-trial GLM framework; and (3) development of ridge regression as an effective method for regularizing single-trial response estimates. These three techniques have been integrated into a toolbox that can be applied to other neuroimaging datasets and are the subject of a forthcoming paper. An important lesson stemming from our results is that well-executed data collection is important but not the only factor to consider: data preparation methods exert a major influence on the quality of a dataset and, hence, its scientific value. One can view improvements in data quality as equivalent to increases in data quantity, in the sense that analysis methods that reduce unwanted variability (noise) can be interpreted as increasing the effective amount of data collected<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Kay, K. N., Rokem, A., Winawer, J., Dougherty, R. F. &amp; Wandell, B. GLMdenoise: a fast, automated technique for denoising task-based fMRI data. Front. Neurosci. 7, 247 (2013)." href="/articles/s41593-021-00962-x#ref-CR35" id="ref-link-section-d29682704e1877">35</a></sup>. Thus, by improving data quality, the methods introduced with the NSD are contributing to the massive scale of the dataset.</p><p>The NSD dataset has many potential applications. Given its extensive sampling of natural scenes (70,566 distinct images aggregated across eight participants) and high SNR, the dataset will be useful for investigating a variety of phenomena in low-, mid- and high-level vision. In addition, the memory component of the NSD experiment provides a unique opportunity to study the neural mechanisms of both short-term and long-term memory (ranging from seconds to many months) as well as potential interactions between vision and memory. From a methodological perspective, the repeated scanning of individuals using a consistent experimental manipulation (up to 40 scan sessions of the NSD experiment per participant) provides a unique opportunity for development and evaluation of neuroimaging pipelines. Finally, perhaps the most exciting use of the NSD is as a common dataset to bridge the disciplines of cognitive science, neuroscience and artificial intelligence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Naselaris, T. et al. Cognitive computational neuroscience: a new conference for an emerging discipline. Trends Cogn. Sci. 22, 365367 (2018)." href="/articles/s41593-021-00962-x#ref-CR21" id="ref-link-section-d29682704e1884">21</a></sup>. As we have shown in the context of deep neural network modeling (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig6">6</a>), there are sufficient data in the NSD to successfully drive the training of neural network models with thousands of free parameters. This demonstration exemplifies how the NSDwith its large amounts of carefully curated fMRI data collected during a rich cognitive paradigmenables data-driven approaches toward understanding the complexities of information processing in the brain.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Methods</h2><div class="c-article-section__content" id="Sec14-content"><h3 class="c-article__sub-heading" id="Sec15">Participant recruitment</h3><p>The NSD study was advertised to the University of Minnesota community. We sought to recruit right-handed individuals (1865 years old) with no known cognitive deficits or color blindness and with normal or corrected-to-normal vision. Those who were interested in participating were contacted for a phone interview to explain the nature of the study and to screen them for eligibility. We discussed the long-term nature of the study, the time commitment that it would involve and the feasibility of traveling to the scanner on a regular basis. We paid attention to the communicativeness of potential participants and their general attitude toward study participation. Selecting participants whom we were confident would provide high-quality data was more important to us than obtaining a random sample of the general population. Based on the phone interviews, we invited 14 individuals whom we thought were strong candidates to participate in an initial 7T fMRI screening session. Of these, eight were selected to participate in the full NSD experiment.</p><h3 class="c-article__sub-heading" id="Sec16">Participants</h3><p>Eight participants (two males and six females; age range, 1932 years) participated in the NSD dataset (subj01subj08). There were six additional participants (four males and two females; age range, 2053 years) who participated in the initial 7T fMRI screening session but not in the remainder of data collection. No statistical methods were used to pre-determine the sample size; rather, our experimental approach<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Naselaris, T., Allen, E. &amp; Kay, K. Extensive sampling for complete models of individual brains. Curr. Opin. Behav. Sci. 40, 4551 (2021)." href="/articles/s41593-021-00962-x#ref-CR58" id="ref-link-section-d29682704e1911">58</a></sup> emphasizes collecting extensive data from each participant, which enables the demonstration and replication of effects in individual participants. Participants were naive to the design of the NSD dataset. All participants had normal or corrected-to-normal visual acuity. Informed written consent was obtained from all participants, and the experimental protocol was approved by the University of Minnesota institutional review board. Participants were compensated at a rate of $30 per hour, plus performance bonuses. Additional participant information, including height, weight, handedness and visual acuity, was logged and is available online.</p><p>Individuals participated in several neuroimaging and behavioral data collection sessions (a full breakdown is provided in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig8">2</a>). Neuroimaging included 3T structural scan sessions and 7T functional scan sessions. The 7T functional scan sessions included an initial screening session termed prffloc, referring to the pRF and fLoc experiments conducted in that session. The 7T sessions also included, for each participant, 3040 sessions in which the main NSD experiment was conducted (nsd01nsd40). These sessions are collectively termed the NSD core. In some of these sessions, resting-state data were acquired before and after the NSD experiment. Finally, the 7T sessions also included two sessions conducted after completion of the NSD core; these sessions, termed nsdsynthetic and nsdimagery, involved measuring responses to synthetic stimuli and cognitive task manipulations (including mental imagery), respectively. The total number of 7T fMRI scan sessions was 43, 43, 35, 33, 43, 35, 43 and 33 for subj01subj08, respectively. The average number of hours of resting-state fMRI conducted for each participant was 2.0h, and the average number of hours of task-based fMRI conducted for each participant was 38.5h. Each individual also participated in several behavioral assessments after scanning was complete. These included a variety of behavioral measures (nsdpostbehavior), a final memory test (nsdmemory) and an image-similarity assessment (nsdmeadows).</p><h3 class="c-article__sub-heading" id="Sec17">MRI data acquisition</h3><p>MRI data were collected at the Center for Magnetic Resonance Research at the University of Minnesota. Some data were collected using a combination of a 3T Siemens Prisma scanner and a standard Siemens 32-channel RF head coil. Most data were collected using a combination of a 7T Siemens Magnetom passively shielded scanner and a single-channel-transmit, 32-channel-receive RF head coil (Nova Medical). Illustrations of the different types of MRI data acquired are provided in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2b</a>. Below, we summarize the scanning protocols (full protocol printouts are available online).</p><p>At 3T, we collected several anatomical measures (<i>T</i><sub>1</sub>, <i>T</i><sub>2</sub>, diffusion and angiogram). The motivation for collecting data at 3T was to ensure acquisition of <i>T</i><sub>1</sub> volumes with good gray-matter/white-matter contrast and homogeneity, which is difficult to achieve at ultra-high field<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Polimeni, J. R., Renvall, V., Zaretskaya, N. &amp; Fischl, B. Analysis strategies for high-resolution UHF-fMRI data. Neuroimage 168, 296320 (2018)." href="/articles/s41593-021-00962-x#ref-CR59" id="ref-link-section-d29682704e1947">59</a></sup>. To increase contrast-to-noise ratio and enable the ability to assess reliability, we acquired several repetitions of <i>T</i><sub>1</sub>-weighted and <i>T</i><sub>2</sub>-weighted volumes. For each participant, we collected between six and ten scans of a whole-brain <i>T</i><sub>1</sub>-weighted MPRAGE sequence (0.8-mm isotropic resolution, TR = 2,400ms, TE = 2.22ms, TI = 1,000ms, flip angle 8, bandwidth 220Hz per pixel, no partial Fourier, in-plane acceleration factor (iPAT) 2, TA = 6.6min per scan) and 23 scans of a whole-brain <i>T</i><sub>2</sub>-weighted SPACE sequence (0.8-mm isotropic resolution, TR = 3,200ms, TE = 563ms, bandwidth 744Hz per pixel, no partial Fourier, iPAT 2, TA = 6.0min per scan). In addition to <i>T</i><sub>1</sub> and <i>T</i><sub>2</sub> data, we also acquired four high-angular-resolution, diffusion-weighted spin-echo EPI scans, using protocols from the Lifespan Human Connectome Project Development effort<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Harms, M. P. et al. Extending the Human Connectome Project across ages: imaging protocols for the Lifespan Development and Aging projects. Neuroimage 183, 972984 (2018)." href="/articles/s41593-021-00962-x#ref-CR60" id="ref-link-section-d29682704e1977">60</a></sup>. These protocols involved varying the number of diffusion directions and the phase encode direction (1.5-mm isotropic resolution, TR = 3,230ms, TE = 89.20ms, flip angle 78, refocusing flip angle 160, bandwidth 1,700 Hz per pixel, echo spacing 0.69ms, partial Fourier 6/8, no iPAT, multi-band slice acceleration factor 4, TA = 5.6min per scan for 99 directions, TA = 5.7min per scan for 100 directions). The four scans included 99 directions AP (anterior-to-posterior phase encode direction), 99 directions PA (posterior-to-anterior phase encode direction), 100 directions AP and 100 directions PA. Diffusion volumes were acquired at <i>b</i> values of 0, 1,500 or 3,000s mm<sup></sup><sup>2</sup>. We also acquired an angiogram using a time-of-flight multi-slab 3D sequence (0.39mm  0.39mm  0.5mm resolution, TR = 19.0ms, TE = 2.91ms, flip angle 18, bandwidth 186Hz per pixel, phase partial Fourier 6/8, slice partial Fourier 6/8, iPAT 2, TA = 5.5min).</p><p>At 7T, we collected functional data and associated fieldmaps and a few additional anatomical measures (venogram and high-resolution <i>T</i><sub>2</sub>). Functional data were collected using gradient-echo EPI at 1.8-mm isotropic resolution with whole-brain (including cerebellum) coverage (84 axial slices, slice thickness 1.8mm, slice gap 0mm, field-of-view 216mm (FE)  216mm (PE), phase encode direction anterior-to-posterior, matrix size 120  120, TR = 1,600ms, TE = 22.0 ms, flip angle 62, echo spacing 0.66ms, bandwidth 1,736 Hz per pixel, partial Fourier 7/8, iPAT 2, multi-band slice acceleration factor 3). The use of moderate spatial resolution capitalizes on the SNR benefits provided by ultra-high magnetic field strength. At the beginning of each 7T session, we acquired a short test EPI scan and adjusted the gain factor (FFT scale factor) accordingly to ensure large dynamic range while avoiding clipping. Empirical measurements indicate that the acoustic noise caused by the EPI sequence is 112 dBA; assuming a conservative noise reduction estimate of 26dB for the earplugs that we used, the resulting noise level is 86 dBA, which can be safely endured for approximately 816 continuous hours according to guidelines from the National Institute for Occupational Safety and Health (1998) and the Occupational Safety and Health Administration (2009).</p><p>In addition to the EPI scans, the 7T sessions also included dual-echo fieldmaps for post hoc correction of EPI spatial distortion (same overall slice slab as the EPI data, 2.2mm  2.2mm  3.6mm resolution, TR = 510ms, TE<sub>1</sub> = 8.16ms, TE<sub>2</sub> = 9.18ms, flip angle 40, bandwidth 301Hz per pixel, partial Fourier 6/8, TA = 1.3min per scan). Fieldmaps were periodically acquired over the course of each scan session to track changes in the magnetic field (details provided below). In one of the 7T sessions held for each participant, we acquired a venogram using a susceptibility-weighted imaging 3D sequence (0.5625mm  0.5625mm  0.6mm resolution, TR = 28ms, TE = 21ms, flip angle 17, bandwidth 120Hz per pixel, phase partial Fourier 6/8, slice partial Fourier 6/8, iPAT 3, TA = 10.1min). This venogram could be useful for investigating the effect of vasculature on fMRI signals<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Kay, K. et al. A critical assessment of data quality and venous effects in sub-millimeter fMRI. Neuroimage 189, 847869 (2019)." href="/articles/s41593-021-00962-x#ref-CR32" id="ref-link-section-d29682704e2002">32</a></sup>. In addition, for the purposes of hippocampal segmentation, we acquired in one of the 7T sessions a high-resolution <i>T</i><sub>2</sub>-weighted TSE scan (0.357mm  0.357mm  1.5mm resolution, 56 oblique slices oriented perpendicular to the long axis of the hippocampus, field-of-view 160mm (FE)  156.4mm (PE), TR = 16,000ms, TE = 53ms, bandwidth 100Hz per pixel, no partial Fourier, iPAT 2, turbo factor 15, TA = 4.5min).</p><p>In the prffloc 7T fMRI session, the acquisition structure was [F BWLL F BWLL F BWLL F], where F indicates a fieldmap, B indicates a multibar run of the pRF experiment (188 TRs), W indicates a wedgering run of the pRF experiment (188 TRs) and L indicates a run of the fLoc experiment (195 TRs). In the NSD 7T fMRI sessions, the acquisition structure was either [F NNNN F NNNN F NNNN F] or [F RNNNN F NNNN F NNNNR F], where F indicates a fieldmap, N indicates a run of the NSD experiment (188 TRs) and R indicates a resting-state run (188 TRs).</p><h3 class="c-article__sub-heading" id="Sec18">Stimulus display and scanner peripherals</h3><p>Ear plugs were used to reduce acoustic noise experienced by the participants. To minimize head motion, we acquired a headcase<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Power, J. D. et al. Customized head molds reduce motion during resting state fMRI scans. Neuroimage 189, 141149 (2019)." href="/articles/s41593-021-00962-x#ref-CR61" id="ref-link-section-d29682704e2022">61</a></sup> for each of the eight NSD participants (Caseforge, <a href="http://caseforge.co">http://caseforge.co</a>) and deployed the headcases starting from the second NSD core scan session (nsd02). To ensure maximal participant comfort, only the posterior half of the headcases was used (omitting the anterior half). Standard foam padding was used to mitigate head motion before that point (prffloc and nsd01).</p><p>Stimuli were presented using a Cambridge Research Systems BOLDscreen 32 LCD monitor positioned at the head of the 7T scanner bed, placed flush against the scanner bore. We chose to use an LCD monitor because it delivers a sharp, high-quality image, in contrast to typical scanner setups involving projectors and backprojection screens. The monitor operated at a resolution of 1,920 pixels  1,080 pixels at 120Hz. The size of the full monitor image was 69.84cm (width)  39.29cm (height). Participants viewed the monitor via a mirror mounted on the RF coil. The viewing distance was 5cm from the participants eyes to the mirror + 171.5cm from the mirror to the monitor image = 176.5cm total. Measurements of the display spectral power density were obtained using a PR-655 spectroradiometer (Photo Research). The BOLDscreen is designed by the manufacturer to behave as a linear display device, and our measurements confirmed this to be the case.</p><p>We determined the maximum square extent visible in both eyes given the constraints of the RF coil to be 8.4  8.4 (714 pixels  714 pixels). Thus, stimuli from the various experiments (for example, pRF, fLoc and NSD) were adjusted to fill 8.4 of visual angle (details provided below). At the beginning of each scan session, we made an effort to position the monitor in the same location relative to the scanner and to position the participants head and RF coil in the same location relative to the scanner. We also used a calibration square (8.4 in size) to determine any incidental horizontal or vertical offsets needed in that session for the participant to see the entire square in each eye, unobstructed. Given these efforts, we think that consistent and high-quality visual stimulation was achieved across scan sessions. Nonetheless, we caution that, due to limitations in positioning and/or potential drift over the course of a scan session, some slight occlusion of the corners of the 8.4  8.4 square extent might have occurred some of the time.</p><p>A Mac Pro computer controlled stimulus presentation using code based on Psychophysics Toolbox 3.0.14 (refs.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Brainard, D. H. The Psychophysics Toolbox. Spat. Vis. 10, 433436 (1997)." href="/articles/s41593-021-00962-x#ref-CR62" id="ref-link-section-d29682704e2042">62</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Pelli, D. G. The VideoToolbox software for visual psychophysics: transforming numbers into movies. Spat. Vis. 10, 437442 (1997)." href="/articles/s41593-021-00962-x#ref-CR63" id="ref-link-section-d29682704e2045">63</a></sup>). Behavioral responses were recorded using a button box (Current Designs). In some scan sessions (nsd21nsd30, the same sessions in which the primary set of resting-state data were acquired), physiological data were collected using a pulse oximeter and a respiratory belt (stock Siemens equipment). Care was taken to secure the oximeter with tape to the left index finger of the participant and to secure the respiratory belt snugly to the participants torso. Physiological data were carefully synchronized with the fMRI data and cropped but are not further analyzed in this paper.</p><p>In several scan sessions (see Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig8">2</a> for details), eye-tracking was performed using an EyeLink 1000 system (SR Research) combined with a custom infrared illuminator mounted on the RF coil. Eye-tracking was performed for the left eye, and eye-tracking data were obtained at 2,000 Hz using the Pupil-CR centroid mode. We caution that the eye-tracking data are variable in quality, as achieving sufficient pupil contrast was often difficult given the constraints of the scanner setup. For information complementary to the eye-tracking data, we also captured video recordings of the eye-tracker computer display (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2b</a>) using a cell phone secured to a mount. These video recordings are useful for checking the accuracy of the eye-tracker and provide information in scan sessions where pupil tracking and data acquisition failed completely. Details of pre-processing and analysis of eye-tracking data are provided in Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">3</a>.</p><h3 class="c-article__sub-heading" id="Sec19">Day-to-day acquisition procedures</h3><p>Participants were scanned approximately once a week, with attempts to keep a regular weekly scan time. At the beginning of each session (starting at approximately nsd07), participants were asked to rate on a five-point scale how well they slept the night before, their mood, how hungry they were and their stress level. We also asked whether they had had caffeine in the past 3 h. At the end of each scan session, participants were asked to rate how comfortable they were during the session and to provide any general feedback they had about the session. These various measures, as well as any technical issues that arose during the session, were logged onto a spreadsheet (available online).</p><p>In the first several scan sessions, we emphasized the importance of fixation and performed simple tests before scanning in which we watched the participants eyes while they attempted to fixate and while they deliberately broke fixation. This was done to help participants understand what good fixation feels like. In every scan session, we reminded participants about the importance of fixation and about the correct mapping between buttons and responses.</p><p>During data collection, we monitored aspects of data quality, including overall image quality, head motion, quality of physiological data and behavioral performance. Between functional runs, we checked in with the participant to assess their energy level, enthusiasm and compliance. If we noticed any substantial drops in response rate, we politely notified the participant and offered short breaks before continuing.</p><p>To promote participant engagement and retention, participants were given the opportunity to earn monetary bonuses that gradually increased in size over the course of the NSD study. These bonuses were contingent on achieving certain performance levels on data quality metrics, such as head motion and response rate (details available online). Information regarding performance was supplied to participants in the form of a continually updated leaderboard figure. We found that this figure greatly helped to motivate participants.</p><h3 class="c-article__sub-heading" id="Sec20">The NSD experiment</h3><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec21">Basic design</h4><p>In the NSD experiment, participants performed a long-term continuous recognition task while viewing a large number of color natural scenes. We chose this recognition task because it engages and challenges the observer and is unbiased with respect to the specific content of the images (unlike other tasks such as animacy judgment). In addition, it infuses the experiment with a rich memory dimension that is likely of interest to memory researchers. In total, 73,000 distinct images were prepared. We intended that the eight NSD participants would each view 10,000 distinct images presented three times each over the course of 40 scan sessions. We designated a special set of 1,000 images (chosen randomly from the full set of prepared images) as shared images that would be seen by all participants (referred to as the shared1000); all other images would be mutually exclusive across participants. The distribution of the three presentations of each image was tightly controlled, and participants were naive to both the number and distribution of the presentations. Note that, because some NSD participants completed only 30 of the 40 prescribed scan sessions, there are ultimately 515 images, out of the shared 1,000 images, that were viewed all three times by all eight participants (referred to as the shared515).</p><p>Images were presented using a 3-s ON/1-s OFF trial structure (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig1">1a</a>). In informal piloting, we found that this pacing made the recognition task feasible and not overly taxing. In addition, we reasoned that the relatively long stimulus duration would increase neural activity and that the rapidity of the design would allow more trials to be collected and, thereby, increase overall experimental power. Finally, we speculated that the 3/1 trial structure would yield a pleasant experience for participants, at least compared to slow event-related designs where most experimental time is spent viewing a blank screen.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec22">Image preparation</h4><p>The NSD stimuli are prepared as a single brick of RGB images with dimensionality 425 pixels  425 pixels  3 RGB channels  73,000 images and unsigned 8-bit integer format.</p><p>Images were taken from Microsofts COCO image database<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision. &#xA;                  https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48&#xA;                  &#xA;                , 740755 (Springer, 2014)." href="/articles/s41593-021-00962-x#ref-CR14" id="ref-link-section-d29682704e2109">14</a></sup>. COCO images are photographs collected from online repositories; each image is supplemented by a rich set of annotations (for example, boundary polygons around objects, natural language captions and body pose estimates). Of the 90 original COCO categories, a total of 80 COCO categories exist in the 73,000 NSD images. We used COCO images in the 2017 train/val split<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision. &#xA;                  https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48&#xA;                  &#xA;                , 740755 (Springer, 2014)." href="/articles/s41593-021-00962-x#ref-CR14" id="ref-link-section-d29682704e2113">14</a></sup> and restricted selection to the subset of images for which pixel-level annotations of stuff<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Caesar, H., Uijlings, J. &amp; Ferrari, V. COCO-Stuff: Thing and Stuff classes in context. In IEEE/CVF Conf. Computer Vision and Pattern Recognition &#xA;                  https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00132&#xA;                  &#xA;                 12091218 (2018)." href="/articles/s41593-021-00962-x#ref-CR64" id="ref-link-section-d29682704e2117">64</a></sup> (for example, sky, land, wall and road) in addition to things (for example, car, skateboard and hat) were available.</p><p>We selected only images whose smaller dimension (height or width) was at least 425 pixels. Where necessary, we squared image dimensions by cropping out pixels along the largest dimension. For example, if the original image was 425585, we cropped away 160 pixels from the larger dimension, resulting in an image that is 425425. The median number of pixels cropped per image was 160. After cropping, images were downsampled, if needed, to 425425.</p><p>Cropping an image can change the way the viewer interprets it. We refer to this effect of cropping as semantic loss. To be able to take full advantage of the rich annotations available for the COCO images, we attempted to minimize semantic loss when cropping images. For landscape-oriented images, we selected among a center, left or right crop. For portrait-oriented images, we selected among a center, top or bottom crop (finer grids of cropping options had little effect on results). Selection of crops was carefully performed based on quantitative analysis and visual inspection (details provided in the NSD Data Manual).</p><p>In addition to screening to minimize semantic loss, we implemented a screening procedure to remove duplicate images. Some of the COCO images are extremely similar to each other, differing only by a post-processing operation (that is, grayscaling or sharpening) or by a few frames in a motion-capture sequence. To remove these near-duplicates, we downsampled all images to 4040 and then computed the correlation of grayscale pixel intensities between all image pairs. We manually inspected the image pairs with the 500 highest correlation values. Of these, 38 image pairs were observed to be near-duplicates. We randomly selected another image from the COCO dataset to replace one image in each near-duplicate pair. Finally, we screened captions for all images for indications of violent or salacious content. No images were deemed too offensive to include in the experiment.</p><p>The distribution of thing categories across the final images selected for the NSD was nearly identical to distribution in the full COCO dataset. As a result, the person category was over-represented; however, with a few exceptions, all 80 COCO object categories were displayed in at least 100 images to each participant. Note that images tend to depict more than one category, so that a given object category frequently appeared in the same image with other categories. For each participants images, at least 90% of the images contained two or more of the 80 COCO categories.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec23">Distribution of image presentations</h4><p>We determined the ordering of the 10,000 images  3 trials = 30,000 trials in advance and kept the ordering fixed across participants. The idea is that these 10,000 images are actually treated as slots into which different NSD images are inserted. We designated the first 1,000 slots as corresponding to the special shared1000 images; the remaining 9,000 slots were filled with unique images for each participant. Note that because the trial ordering and repetition structure are identical across participants, the difficulty of the recognition task is similar across participants (up to the fact that some images might be more difficult to remember than others).</p><p>We controlled the distribution of image presentations to prevent the recognition task from becoming too difficult (and risking loss of participant morale). In the procedure, we conceptualized the task of determining the trial ordering as equivalent to placing image presentations on a circle that would eventually be cut and unraveled. The rationale for this circular design is to minimize the extent to which certain points in the experiment differ from others; of course, because the circle eventually becomes a line, there is some imperfection (see discussion below regarding burn-in and dead time). To determine presentation times, we created a circular probability distribution by mixing a von Mises distribution and a uniform distribution (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig7">1a</a>). Using random draws from the resulting distribution (positioning the distribution at a random location on the circle for each image), we determined three presentation times for each of the 10,000 images. After completing the placement of all 30,000 trials, we then cut the circle, unraveled it into a linear sequence of image presentations and divided this sequence into 40 consecutive segments corresponding to the 40 NSD scan sessions (750 trials per session).</p><p>To determine presentation times, we created a circular probability distribution by mixing a von Mises distribution and a uniform distribution (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig7">1a</a>). For each image, we positioned the peak of the von Mises distribution at a random position on the circle (that is, we randomly sampled the mean parameter from 180 to 180) and then randomly sampled presentation times for each of the three image repetitions from the mixture distribution. We chose specific parameters for the probability distribution: we used a von Mises distribution with a concentration parameter of 729 and a mixing ratio of 60% and 40% for the von Mises and uniform distributions, respectively. This choice of parameters yields appealing properties. First, the distribution is relatively narrow (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig7">1a</a>) and, therefore, ensures that there will be many trials involving an image that has been presented in the recent past (thus making the trials easy) while still allowing the probing of more distant memory events. Second, there is minimal burn-in time at the beginning of the experiment: even in the first scan session, there is still a substantial number of trials involving old images (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig7">1b</a>, blue line). Third, there is minimal dead time at the end of the experiment: even in the last scan session, there is still a substantial number of trials involving new images (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig7">1b</a>, blue line).</p><p>To provide a sense of the overall experimental design, we computed basic statistics on each NSD scan session. For a typical session, the total number of distinct images shown once, twice and all three times within that session is 437, 106 and 34, respectively (these numbers reflect the mean across scan sessions, rounding to the nearest integer).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec24">Trial and run design</h4><p>Each trial lasted 4s and consisted of the presentation of an image for 3s, followed by a 1-s gap. In total, 75 trials were conducted in a run; thus, each run lasted 300s. The first three trials (12s) and the last four trials (16s) were blank trials. The remaining 68 trials were divided into 63 stimulus trials and five blank trials. The blank trials were randomly positioned in each run such that the minimum and maximum continuous number of stimulus trials was nine trials (36s) and 14 trials (56s), respectively (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig1">1b</a>). For even-numbered runs, the 63rd stimulus trial was designated to be a blank trial. In total, 12 NSD runs were collected in one NSD session, yielding a total of (63+62)  6=750 stimulus trials. Moreover, this design was repeated for all 40 NSD sessions: 750 stimulus trials  40 sessions = 30,000 stimulus trials. The temporal ordering of stimulus and blank trials was generated once and kept fixed across participants.</p><p>Note that the experimental design involves minimal trial jittering: for the most part, the time interval separating consecutive stimulus images is fixed at 1s, although occasionally, due to blank trials, the time interval is 5s. This design was intended to maximize statistical power and differs from conventional fMRI practice where intervals are often chosen randomly from a fixed range.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec25">Stimulus presentation and task</h4><p>Because the BOLDscreen is calibrated to behave as a linear display device, we used a squaring luminance response when presenting the NSD experiment to simulate the typical viewing of digital images. At the time of presentation, the prepared NSD images were resized using linear interpolation from their native resolution of 425 pixels  425 pixels to 714 pixels  714 pixels to occupy 8.4  8.4 on the display. Throughout each run (including blank trials), a small semi-transparent red fixation dot with a black border (0.2  0.2, 50% opacity) was present at the center of the stimuli (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig1">1a</a>). Stimuli were shown against a gray background with an RGB value of 127, 127 and 127.</p><p>Participants were instructed to fixate the central dot and to press button 1 using the index finger of their right hand if the presented image was newthat is, if the image had never been presented beforeor button 2 using the middle finger of their right hand if the presented image was oldthat is, the image was identical to one that had been presented before, either in the current scan session or any previous scan session. Participants were additionally instructed to continue to fixate and wait for the next image in the event of blank trials.</p><p>Before the start of the NSD experiment, we showed the participants a version of the experiment involving cartoon images, for them to become familiarized with the feel and timing of the task. During the NSD experiment, minimal feedback was provided to the participants regarding their performance on the recognition task. Participants were blinded to the precise details of the NSD experiment (for example, total number of images and total number of presentations per image). Participants were informed only about their response rate (fraction of trials on which they successfully made a response) and a vague performance metric, which, unbeknownst to them, quantified their percent correct for easy trials (trials that involved the presentation of an image that had occurred earlier in the same scan session). We revealed the nature of the design in a debriefing session after the completion of the NSD experiment (details below).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec26">Details on experiment timing</h4><p>Stimulus presentation was locked to the refresh rate of the BOLDscreen monitor. Empirical measurements confirmed that the monitor refresh rate was nearly exactly 120Hz: duration of runs was highly reliable, ranging from 299.95 s to 299.98s. To compensate for the slight offset from 300s, the fMRI data were pre-processed to achieve a sampling rate of 0.999878s (high-resolution preparation) or 0.999878s  (4/3) = 1.333171s (standard-resolution preparation). For brevity, we refer to these numbers as 1.000s and 1.333s. Experimental runs were started by a trigger issued by the MR scanner. Due to input polling and monitor refresh, there was slight variability in the delay between trigger detection and the presentation of the first stimulus frame, ranging from 3 ms to 22ms. We did not attempt to compensate for this delay.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec27">Acquisition</h4><p>Due to constraints on participant availability (including unplanned out-of-town absences in the summer of 2019) and due to constraints on scanner availability (the 7T scanner was decommissioned in November 2019), we did not complete the full NSD experiment for every participant. Fortunately, we were able to collect a sizable amount of data: 40, 40, 32, 30, 40, 32, 40 and 30 NSD sessions for subj01subj08, respectively. In these collected data, each participant viewed 9,20910,000 distinct images and participated in 22,50030,000 trials. Aggregated across participants, the total number of distinct images shown was 70,566, and the total number of trials was 213,000.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec28">Debriefing</h4><p>After completion of the final memory test (details below), participants filled out a post-NSD questionnaire. This questionnaire probed topics such as strategies used for performing the NSD task and estimates for the number of images viewed and the number of image repetitions. After filling out this questionnaire, the design of the NSD experiment was then revealed to the participants.</p><h3 class="c-article__sub-heading" id="Sec29">Other experiments</h3><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec30">pRF experiment</h4><p>We adapted the experiment used in the Human Connectome Project 7T Retinotopy Dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Benson, N. C. et al. The human connectome project 7 Tesla retinotopy dataset: description and population receptive field analysis. J. Vis. 18, 23 (2018)." href="/articles/s41593-021-00962-x#ref-CR30" id="ref-link-section-d29682704e2235">30</a></sup>. Stimuli consisted of slowly moving apertures filled with a dynamic colorful texture (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2a</a>). Apertures and textures were updated at a rate of 15Hz. Two run types were used. The first, termed multibar, involves bars sweeping in multiple directions (same as RETBAR in the Human Connectome Project 7T Retinotopy Dataset). The second, termed wedgering, involves a combination of rotating wedges and expanding and contracting rings. Both run types included blank periods.</p><p>For consistency with the NSD experiment, stimuli were resized to fill a circular region with diameter 8.4. Each run lasted 300s (exact empirical timings were highly accurate and ranged between 299.95 s and 300.00s). Throughout stimulus presentation, a small semi-transparent dot (0.2  0.2) was present at the center of the stimuli. The color of the central dot switched randomly to one of three colors (black, white or red) every 15s. Participants were instructed to maintain fixation on the dot and to press a button whenever the color of the dot changed. To further aid fixation, a semi-transparent fixation grid was superimposed on the stimuli and was present throughout the experiment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Schira, M. M., Tyler, C. W., Breakspear, M. &amp; Spehar, B. The foveal confluence in human visual cortex. J. Neurosci. 29, 90509058 (2009)." href="/articles/s41593-021-00962-x#ref-CR65" id="ref-link-section-d29682704e2245">65</a></sup>. A total of six runs (three multibar and three wedgering) were collected in the first 7T fMRI session (prffloc).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec31">fLoc experiment</h4><p>This experiment was developed by the Grill-Spector laboratory<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Stigliani, A., Weiner, K. S. &amp; Grill-Spector, K. Temporal processing capacity in high-level visual cortex is domain specific. J. Neurosci. 35, 1241212424 (2015)." href="/articles/s41593-021-00962-x#ref-CR31" id="ref-link-section-d29682704e2257">31</a></sup> (stimuli and presentation code available at <a href="http://vpnl.stanford.edu/fLoc/">http://vpnl.stanford.edu/fLoc/</a>). The experiment consisted of the presentation of grayscale images depicting different stimulus categories (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2a</a>). There were ten categories, grouped into five stimulus domains: characters (word and number), bodies (body and limb), faces (adult and child), places (corridor and house) and objects (car and instrument). Stimuli were presented on a scrambled background (different backgrounds for different stimuli). Stimuli were presented in 4-s trials. In a trial, eight images from a given category were sequentially presented (image duration, 0.5s). Each run included six presentations of each of the ten categories as well as blank trials (also of 4-s duration).</p><p>For consistency with the NSD experiment, stimuli were resized to fill a square region filling 8.4  8.4 of visual extent. Each run lasted 300s (exact empirical timings were highly accurate and ranged between 300.000 s and 300.002s). Throughout stimulus presentation, a small red fixation dot was present at the center of the stimuli. Participants were instructed to maintain fixation on the dot and to press a button whenever they noticed an image in which only the background was present (oddball task). In total, six runs were collected in the first 7T fMRI session (prffloc).</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec32">Resting-state experiment</h4><p>Stimuli consisted of a white fixation cross (0.5  0.5) on a gray background (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2a</a>). Each resting-state run lasted 300s. In the second resting-state run held within a given scan session, the fixation cross turned red after 12s had elapsed and remained red for 4s before returning to white.</p><p>Resting-state data were acquired in several NSD core scan sessions: nsd21nsd38 for subj01 and subj05 and nsd21nsd30 for all other participants. Thus, a total of 100 min or 180min of resting-state data were acquired for each participant. In each session, one resting-state run was acquired at the beginning of the session (before the NSD runs), and another resting-state run was acquired at the end of the session (after the NSD runs).</p><p>In the first resting-state run, participants were instructed to stay awake and fixate the cross but otherwise rest. In the second resting-state run, participants were additionally instructed to inhale deeply when the fixation cross turned red. This instructed breath was designed to aid analysis of the physiological data collected concomitantly with the resting-state data. Before each resting-state run, participants were asked to report their current sleepiness level using the Stanford Sleepiness Scale<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Shahid, A., Wilkinson, K., Marcu, S. &amp; Shapiro, C. M. Stanford Sleepiness Scale (SSS). In: STOP, THAT and One Hundred Other Sleep Scales (eds. Shahid, A., Wilkinson, K., Marcu, S. &amp; Shapiro, C. M.) 369370 (Springer, 2012)." href="/articles/s41593-021-00962-x#ref-CR66" id="ref-link-section-d29682704e2291">66</a></sup> (17, where 1 is most active and 7 is most sleepy). After each resting-state run, participants were asked to report their sleepiness level during the run that had just completed.</p><p>After the last scan session involving resting-state data, participants filled out a post-resting-state questionnaire. This questionnaire queried what the participants were doing during the resting-state runs and whether they thought about the images from the NSD experiment.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec33">Synthetic stimuli experiment (nsdsynthetic)</h4><p>After completion of the NSD experiment, we conducted an additional 7T fMRI scan session in which responses were measured to a variety of carefully controlled synthetic (non-naturalistic) stimuli while the participant performed either a fixation task or a one-back task. These data will be described and released in a forthcoming manuscript.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec34">Visual imagery experiment (nsdimagery)</h4><p>After completion of the nsdsynthetic experiment, we conducted an additional 7T fMRI scan session in which responses were measured while participants engaged in visual imagery and other cognitive tasks. These data will be described and released in a forthcoming manuscript.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec35">Additional behavioral measures (nsdpostbehavior, nsdmemory and nsdmeadows)</h4><p>Several behavioral assessments were conducted after completion of the NSD experiment. Some of these were relatively brief and included the following (nsdpostbehavior): open-ended questions regarding language ability; the Vividness of Visual Imagery Questionnaire<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Marks, D. F. Visual imagery differences in the recall of pictures. Br. J. Psychol. 64, 1724 (1973)." href="/articles/s41593-021-00962-x#ref-CR67" id="ref-link-section-d29682704e2323">67</a></sup>; the Test of Word Reading Efficiency<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Torgesen, J. K., Wagner, R. &amp; Rashotte, C. TOWRE-2: Test of Word Reading Efficiency (Pearson, 2012)." href="/articles/s41593-021-00962-x#ref-CR68" id="ref-link-section-d29682704e2327">68</a></sup>, including both Sight Word Efficiency and Phonemic Decoding Efficiency; the Cambridge Memory Test for Faces<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Duchaine, B. &amp; Nakayama, K. The Cambridge Face Memory Test: results for neurologically intact individuals and an investigation of its validity using inverted face stimuli and prosopagnosic participants. Neuropsychologia 44, 576585 (2006)." href="/articles/s41593-021-00962-x#ref-CR69" id="ref-link-section-d29682704e2331">69</a></sup>; ultra-fast measurement of contrast sensitivity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Tardif, J., Watson, M., Giaschi, D. &amp; Gosselin, F. Measuring the contrast sensitivity function in just three clicks. J. Vis. 16, 966966 (2016)." href="/articles/s41593-021-00962-x#ref-CR70" id="ref-link-section-d29682704e2335">70</a></sup>; and an assessment of chromatic sensitivity (participants adjusted intensities of red, green and blue channels on the BOLDscreen display until minimal luminance flicker was perceived).</p><p>We also conducted a final memory test in which we collected various memory-related measures regarding the images shown to the participants during the NSD experiment (nsdmemory). These data will be described and released in a forthcoming manuscript.</p><p>Finally, using the web-based Meadows platform (<a href="http://meadows-research.com">http://meadows-research.com</a>), we conducted an assessment of how the NSD participants perceive and interpret the NSD images (nsdmeadows). First, we selected a small set of images that maximally span semantic space. This was done by isolating the shared515 images; computing shifted inverse frequency sentence embeddings for the sentence captions provided by the COCO dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Arora, S., Liang, Y. &amp; Ma, T. A simple but tough-to-beat baseline for sentence embeddings. &#xA;                  https://openreview.net/pdf?id=SyK00v5xx&#xA;                  &#xA;                 (2017)." href="/articles/s41593-021-00962-x#ref-CR71" id="ref-link-section-d29682704e2352">71</a></sup>; and using a greedy approach to determine the subset of 100 images that maximize the average distance between each images embedding and its closest neighbor. We then asked participants to perform a Multiple Arrangements Task<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Kriegeskorte, N. &amp; Mur, M. Inverse MDS: inferring dissimilarity structure from multiple item arrangements. Front. Psychol. 3, 245 (2012)." href="/articles/s41593-021-00962-x#ref-CR72" id="ref-link-section-d29682704e2356">72</a></sup> in which they arrange using a drag-and-drop interface the 100 images within a white circular arena according to the similarity of their content. Using an adaptive procedure, subsequent arrangements were conducted using subsets of the images to maximize information gain. This was done until 45min had elapsed. Using a similar interface on Meadows, participants then provided valence and arousal ratings for the 100 images as well as three additional images pulled from the shared515 images. Ratings were performed separately for valence and arousal and were accomplished by freely arranging, using a drag-and-drop interface, the images (delivered in small batches) along a one-dimensional axis ranging from low to high. This assessment took about 15min.</p><h3 class="c-article__sub-heading" id="Sec36">Overview of data analysis</h3><p>We designed custom analysis strategies to maximize the quality of derived measures from the NSD data. Several methods are based on recent work<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Kay, K. et al. A critical assessment of data quality and venous effects in sub-millimeter fMRI. Neuroimage 189, 847869 (2019)." href="/articles/s41593-021-00962-x#ref-CR32" id="ref-link-section-d29682704e2369">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Kay, K., Jamison, K. W., Zhang, R.-Y. &amp; Uurbil, K. A temporal decomposition method for identifying venous effects in task-based fMRI. Nat. Methods 17, 10331039 (2020)." href="/articles/s41593-021-00962-x#ref-CR73" id="ref-link-section-d29682704e2372">73</a></sup> where further details can be found. Data analysis and visualization were performed using custom code in MATLAB and Python as well as tools from various packages, such as FreeSurfer, SPM, FSL, ANTs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="Avants, B. B. et al. A reproducible evaluation of ANTs similarity metric performance in brain image registration. Neuroimage 54, 20332044 (2011)." href="/articles/s41593-021-00962-x#ref-CR74" id="ref-link-section-d29682704e2376">74</a></sup> and ITK-SNAP<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 75" title="Yushkevich, P. A. et al. User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability. Neuroimage 31, 11161128 (2006)." href="/articles/s41593-021-00962-x#ref-CR75" id="ref-link-section-d29682704e2380">75</a></sup>. An archive of code used is provided online (<a href="https://github.com/cvnlab/nsddatapaper/">https://github.com/cvnlab/nsddatapaper/</a>), and specific code files are referenced in the text below.</p><p>A comprehensive schematic outlining the data analysis performed in this paper is provided in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig9">3</a>. The analysis of the NSD data can be divided into three components: (1) pre-processing of the anatomical, diffusion and functional data; (2) time series analysis of the fMRI data to estimate trial-wise betas; and (3) further analyses of the trial-wise betas to answer specific scientific questions. The first two components produce the so-called prepared data that are generally useful to the community, whereas the third component refers to analyses performed for the purposes of this paper (estimation of pRFs from the NSD data, univariate memory analysis, representational similarity analysis and brain-optimized neural network training). Data collection and analysis were not performed blinded to the conditions of the experiments. No data were excluded from analyses, with the exception of a few <i>T</i><sub><i>1</i></sub> volumes (2 of 52 volumes = 4%) and certain portions of the eye-tracking data that were corrupted by noise (11 of 160 eye-tracking runs = 7%).</p><p>The pre-processing approach that we designed for the NSD dataset prioritizes accuracy and preservation of information (for example, avoiding spatial smoothing). We avoid baking in unnecessary assumptions (for example, aggressively removing signal fluctuations without careful assessment of validity), and we avoid assuming the accuracy of automated methods; care is taken to manually inspect each pre-processing step to ensure satisfactory results. Although we think our pre-processing is general and likely suitable for most downstream uses of the data, the raw data are also available for those who want to explore other pre-processing approaches, such as fmriprep<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Esteban, O. et al. fMRIPrep: a robust preprocessing pipeline for functional MRI. Nat. Methods 16, 111116 (2019)." href="/articles/s41593-021-00962-x#ref-CR76" id="ref-link-section-d29682704e2406">76</a></sup>. We note several aspects of the NSD dataset that might render the dataset challenging from a pre-processing standpoint: the relatively high spatial resolution of the fMRI data (1.8mm) places higher demands on spatial accuracy; the ultra-high field strength (7T) used for the fMRI data yields higher levels of EPI spatial distortion compared to lower field strengths; and the emphasis on many repeated scans of individuals heightens the importance of achieving consistent imaging results across scan sessions.</p><h3 class="c-article__sub-heading" id="Sec37">Pre-processing of MRI data</h3><p>Details of the pre-processing of anatomical, functional and diffusion data are provided in Supplementary Notes <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">4</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">5</a>. Functional data were pre-processed using one temporal resampling to correct for slice time differences and one spatial resampling to correct for head motion within and across scan sessions, EPI distortion and gradient non-linearities. Two versions of the functional data were prepared: a 1.8-mm standard-resolution preparation (temporal-resolution, 1.333s) and an upsampled 1.0-mm high-resolution preparation (temporal-resolution, 1.000s). Analyses of the pRF and fLoc experiments were used to define retinotopic and category-selective ROIs, respectively. Other ROIs were also defined, including an nsdgeneral ROI indicating occipital regions generally responsive in the NSD experiment and a corticalsulc ROI collection indicating major cortical sulci and gyri. Annotations for several of the corticalsulc ROIs are shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3f</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig4">4b</a>.</p><h3 class="c-article__sub-heading" id="Sec38">Data quality metrics</h3><p>Several data quality metrics were calculated (export_runmetrics.m) and summarized in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig1">1d</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2d</a>. tSNR was computed from raw fMRI volumes (no pre-processing) by first de-trending the time series data from each voxel (quadratic polynomial fit) and then dividing the mean signal intensity by the standard deviation of signal intensity values (autoqc_fmri.m). We calculated the median tSNR across voxels within a simple brain mask (mean volume thresholded at 1/10th of the 99th percentile of values) and then computed the median across runs. Head motion was quantified by calculating frame-wise displacement<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Power, J. D., Barnes, K. A., Snyder, A. Z., Schlaggar, B. L. &amp; Petersen, S. E. Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion. Neuroimage 59, 21422154 (2012)." href="/articles/s41593-021-00962-x#ref-CR77" id="ref-link-section-d29682704e2444">77</a></sup> based on motion parameter estimates (1.8-mm preparation). We calculated the mean frame-wise displacement across volumes in a run and then computed the median across runs. BOLD response was quantified by calculating the percentage of variance explained by a simple ONOFF GLM model (1.8-mm preparation). We calculated the median variance explained across voxels within the nsdgeneral ROI and then computed the median across runs. (Additional details on the ONOFF GLM can be found in the GLMsingle algorithm section.) Response rate was quantified by calculating the percentage of trials for which the participant pressed a button and then computing the mean across runs. Behavioral performance was quantified by dividing trials into easy trials (trials for which the presented image had been previously presented in the same scan session), hard trials (trials for which the presented image had been previously presented but in a previous scan session) and novel trials (trials for which the presented image had never been previously presented) and then calculating, for each trial type, the percentage of trials on which the participant indicated an old response.</p><p>To identify EPI signal dropout regions (export_signaldropout.m), we divided the <i>T</i><sub><i>2</i></sub> volume (resampled to match the EPI data) by the mean EPI volume (1-mm preparation). The resulting volume is useful as it indicates which voxels have high signal intensity in the <i>T</i><sub><i>2</i></sub> but are corrupted by signal dropout in the EPI. We mapped the volume to the cortical surface (cubic interpolation; mean across depth), transformed the result to fsaverage and then used a data-driven threshold to mark atypically high values. Vertices marked in at least four of the eight participants are indicated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3f</a>. To visualize surface imperfections, we took the voxels that were marked in the 0.8-mm anatomical space (during the manual inspection of FreeSurfer surface imperfections), smoothed this binary volume with a 3D Gaussian with full width at half maximum of 2mm, mapped the result to the cortical surface (cubic interpolation; maximum across depth) and then transformed the result to fsaverage. Vertices exceeding 0.01 in at least one of the eight participants are indicated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3f</a>.</p><h3 class="c-article__sub-heading" id="Sec39">Rankings from the 7T fMRI screening session</h3><p>Six quality measures (pRF BOLD, fLoc BOLD, pRF behavior, fLoc behavior, raw motion and de-trended motion) were computed for each of the 14 individuals who participated in the screening session. BOLD quality was quantified as the percentage of voxels for which variance explained by modeling the fMRI time series data (either pRF model fitting or GLM model fitting) exceeded 20%. Behavior quality was quantified as described above. Motion was quantified by calculating the median voxel displacement relative to the reference volume used for motion correction, computing the median of this quantity across volumes and then computing the mean across runs. This motion quantification was performed using raw motion parameter estimates (thereby providing a measure of global head displacement over the course of the session) as well as using motion parameter estimates that are linearly de-trended within each run (thereby providing a measure of within-run head instability). Each of the six measures was linearly scaled to span the range 15, where 1 corresponds to the worst performance and 5 corresponds to the best performance observed across participants. Finally, the normalized measures were averaged to produce an overall ranking for each participant, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig2">2c</a>.</p><h3 class="c-article__sub-heading" id="Sec40">Analysis of behavioral data from the NSD experiment</h3><p>The behavioral data from the NSD experiment were lightly reformatted for the convenience of subsequent analyses (analyzebehavior_nsd.m). We first checked whether the participant had accidentally positioned their fingers on incorrect buttons on the button box and compensated for this if necessary. (In a few instances, we deliberately instructed participants to use alternative buttons due to hardware malfunction of the button box.) We then recorded, for each stimulus trial, several quantities, including time of image presentation, whether the image presented was new or old, whether the response was correct or incorrect and the reaction time. Button responses were extracted from a time window extending 2504,250ms after image onset. In the case of multiple buttons pressed during a trial, we scored the final button pressed, excluding any redundant presses of that button (participants sometimes repeated button presses for good measure).</p><h3 class="c-article__sub-heading" id="Sec41">GLM analysis of the NSD experiment</h3><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec42">Overview of approach</h4><p>We performed a GLM analysis of the pre-processed time series data from the NSD experiment. To maximize flexibility for subsequent analyses, the GLM approach was designed to provide estimates of BOLD response amplitudes (betas) for single trials. Due to low SNR, single-trial estimation in fMRI is challenging. We, therefore, developed several analysis components to optimize the quality of single-trial betas. These components are packaged into a tool called GLMsingle, which is the subject of a forthcoming manuscript where additional details and discussion can be found.</p><p>The first analysis component of GLMsingle is the use of a library of HRFs, whereby the best-fitting HRF from the library is chosen for each voxel. This simple approach for compensating for differences in hemodynamic time courses across voxels<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 78" title="Handwerker, D. A., Gonzalez-Castillo, J., DEsposito, M. &amp; Bandettini, P. A. The continuing challenge of understanding and modeling hemodynamic variation in fMRI. Neuroimage 62, 10171023 (2012)." href="/articles/s41593-021-00962-x#ref-CR78" id="ref-link-section-d29682704e2504">78</a></sup> has several appealing features: it is efficient and can be executed with little computational cost (and, hence, can accommodate the massive scale of the NSD); and it invariably provides well-regularized HRF estimates. The second analysis component is an adaptation of GLMdenoise to a single-trial GLM framework. GLMdenoise<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Kay, K. N., Rokem, A., Winawer, J., Dougherty, R. F. &amp; Wandell, B. GLMdenoise: a fast, automated technique for denoising task-based fMRI data. Front. Neurosci. 7, 247 (2013)." href="/articles/s41593-021-00962-x#ref-CR35" id="ref-link-section-d29682704e2508">35</a></sup> is a technique in which data-derived nuisance regressors are identified and used to remove noise fromand, therefore, improve the accuracy ofbeta estimates. The third component is an application of ridge regression<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 79" title="Hoerl, A. E. &amp; Kennard, R. W. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics 12, 5567 (1970)." href="/articles/s41593-021-00962-x#ref-CR79" id="ref-link-section-d29682704e2512">79</a></sup> as a method for dampening the noise inflation caused by correlated single-trial GLM predictors. To determine the optimal level of regularization for each voxel, we make use of a recently developed efficient re-parameterization of ridge regression called fractional ridge regression<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Rokem, A. &amp; Kay, K. Fractional ridge regression: a fast, interpretable reparameterization of ridge regression. Gigascience 9, giaa133 (2020)." href="/articles/s41593-021-00962-x#ref-CR36" id="ref-link-section-d29682704e2516">36</a></sup>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec43">Derivation of the library of HRFs</h4><p>To generate a library of HRFs that accurately capture empirically occurring time course variation, we performed an initial analysis of data from the first NSD core session (nsd01). This library was fixed and used for the analysis of all subsequent NSD sessions. The first step was to create a comprehensive summary of observed time courses (hrf_derivecanonicalpcs.m). The time series data from each participants nsd01 session was fit using a finite impulse response model (030s) where all of the stimulus trials are treated as instances of a single experimental condition (this simplification is necessary to make estimation feasible). We identified voxels for which model variance explained (<i>R</i><sup>2</sup>) was greater than 10%, and, from these voxels, we randomly drew 20,000 voxels (with replacement). Pooling across participants, time course estimates from the resulting 160,000 voxels were subjected to singular value decomposition to determine the top three PCs (shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3b</a>, inset). To fine-tune time course estimates, we re-fit the time series data from the nsd01 session using these three PCs as the basis (as opposed to the finite impulse response basis). Finally, adopting the visualization approach of the Temporal Decomposition Method<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Kay, K., Jamison, K. W., Zhang, R.-Y. &amp; Uurbil, K. A temporal decomposition method for identifying venous effects in task-based fMRI. Nat. Methods 17, 10331039 (2020)." href="/articles/s41593-021-00962-x#ref-CR73" id="ref-link-section-d29682704e2535">73</a></sup>, we projected voxel time course estimates onto the unit sphere (using the same voxel selection criterion of <i>R</i><sup>2</sup>&gt;10%) and constructed a 2D histogram for each participant (shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3a</a>).</p><p>The second step was to define a set of time courses that span the observed time course variation (hrf_constructmanifold.m). To do this, we converted the 2D histograms to units of relative frequency and then averaged the histograms across participants. Inspecting the group average histogram (shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3b</a>), we manually clicked a sequence of points on the unit sphere that follow the data density as closely as possible. We then parameterized the path traced by these points (a simple one-dimensional manifold) by positioning regularly spaced points where successive points are separated by six angular degrees (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3b</a>, cyan dots). The time courses corresponding to the resulting set of 20 points were cubic interpolated to a sampling rate of 0.1s and normalized to peak at 1 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3c</a>). Finally, we fit each time course using a double-gamma function as implemented in SPMs spm_hrf.m (hrf_fitspmhrftomanifold.m). This yielded a library of 20 canonical HRFs that might be useful for application to other experimental datasets (getcanonicalhrflibrary.m). We note that variation in time course shape is likely due to the influence of macrovasculature on BOLD temporal dynamics<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Kay, K., Jamison, K. W., Zhang, R.-Y. &amp; Uurbil, K. A temporal decomposition method for identifying venous effects in task-based fMRI. Nat. Methods 17, 10331039 (2020)." href="/articles/s41593-021-00962-x#ref-CR73" id="ref-link-section-d29682704e2559">73</a></sup>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec44">Cross-validation framework for single-trial GLM</h4><p>The GLMdenoise and ridge regression analysis components of GLMsingle both require tuning of hyperparameters. To determine the optimal setting of hyperparameters, we use a cross-validation approach in which out-of-sample predictions are made for single-trial beta estimates, as opposed to time series data. This simplifies and reduces the computational requirements of the cross-validation procedure. Note that, because of cross-validation, although GLMsingle produces estimates of responses to single trials, it does require the existence of and information regarding repeated trialsthat is, trials for which the stimulus is the same.</p><p>The first step of the cross-validation procedure is to analyze all of the available data using no regularization. In the case of GLMdenoise, this amounts to the inclusion of zero nuisance regressors; in the case of ridge regression, this amounts to the use of a shrinkage fraction of 1, indicating ordinary least squares regression. In both cases, the analysis produces a full set of unregularized single-trial betas (for example, in one NSD session, there are 750 single-trial betas distributed across 12 runs). The second step of the procedure is to perform a grid search over values of the hyperparameter (for example, number of nuisance regressors and shrinkage fraction). For each value, we assess how well the resulting beta estimates generalize to left-out runs. For example, in leave-one-run-out cross-validation, one run is held out as the validation run; stimuli that occur in both the training runs and the validation run are identified; and squared errors between the regularized beta estimates from the training runs and the unregularized beta estimates from the validation run are calculated. This procedure is iterated with each run serving as the validation run, and errors are summed across iterations.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec45">GLMsingle algorithm</h4><p>Having described the essential aspects of the estimation framework above, we now turn to the steps in the GLMsingle algorithm. GLMsingle involves fitting several different GLM variants. Each variant includes polynomial regressors to characterize the baseline signal level: for each run, we include polynomials of degrees 0 through round (<i>L</i>/2), where <i>L</i> is the duration in minutes of the run.</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Fit a simple ONOFF GLM. In this model, stimulus trials are treated as instances of a single experimental condition, and a canonical HRF is used (getcanonicalhrf.m). Thus, there is a single ONOFF predictor that attempts to capture signals driven by the experiment. The utility of this simple model is to provide variance explained (<i>R</i><sup>2</sup>) values that help indicate which voxels carry experiment-driven signals.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Fit a baseline single-trial GLM. In this model, each stimulus trial is modeled separately using the canonical HRF. This model provides a useful baseline for comparison.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Identify HRF for each voxel. We fit the data multiple times with a single-trial GLM, each time using a different HRF from the library of HRFs. For each voxel, we identify which HRF provides the best fit to the data (highest variance explained) and inherit the single-trial betas associated with that HRF. Note that the final model for each voxel involves a single chosen HRF from the library (not a weighted sum of HRFs).</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>Use GLMdenoise to determine nuisance regressors to include in the model. We define a pool of noise voxels (brain voxels that have low ONOFF <i>R</i><sup>2</sup>) and then perform principal component (PC) analysis on the time series data associated with these voxels. The top PCs are added one at a time to the GLM until cross-validation performance is maximized on average across voxels.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">5.</span>
                      
                        <p>Use fractional ridge regression to regularize single-trial betas. With the nuisance regressors determined, we use fractional ridge regression (fracridge<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Rokem, A. &amp; Kay, K. Fractional ridge regression: a fast, interpretable reparameterization of ridge regression. Gigascience 9, giaa133 (2020)." href="/articles/s41593-021-00962-x#ref-CR36" id="ref-link-section-d29682704e2650">36</a></sup>) to estimate the single-trial betas, systematically evaluating different shrinkage fractions. For each voxel, in the context of a GLM that incorporates the specific HRF chosen for that voxel, cross-validation is used to select an optimal shrinkage fraction for that voxel. To mitigate bias on the overall scale of betas, we apply a post hoc scaling and offset on betas obtained for a given voxel to match, in a least squares sense, the unregularized betas obtained for that voxel.</p>
                      
                    </li>
                  </ol><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec46">Application of GLMsingle to the NSD data</h4><p>We used GLMsingle to analyze the time series data independently for each NSD scan session (glm_nsd.m). Major algorithmic parameters included the following: we evaluated up to ten nuisance regressors; we evaluated shrinkage fractions from 0.05 to 0.90 in increments of 0.05 and from 0.91 to 1 in increments of 0.01 (representing a finer grain for voxels with the best SNR); we performed six-fold cross-validation (consecutive pairs of runs) for Steps 4 and 5; and we used an ONOFF <i>R</i><sup>2</sup> threshold of 5% in Step 4.</p><p>Three different versions of the single-trial betas were computed and saved. The first beta version (b1, betas_assumehrf) is the result of Step 2 and reflects the use of a canonical HRF. The second beta version (b2, betas_fithrf) is the result of Step 3 and reflects the result of voxel-wise HRF estimation. The third beta version (b3, betas_fithrf_GLMdenoise_RR) is the result of Step 5 and reflects the additional GLMdenoise and ridge regression procedures. Betas were converted to units of percent BOLD signal change by dividing amplitudes by the mean signal intensity observed at each voxel and multiplying by 100. Although we provide betas in units of percent signal change, we suggest that users might want to <i>z</i>-score the responses of each voxel within each scan session to eliminate potential non-stationarities and to equalize units across voxels.</p><p>For user convenience, we created preparations of the single-trial betas in additional spaces other than the native 1.8-mm and 1.0-mm functional spaces. For the nativesurface preparation, we performed cubic interpolation of the 1.0-mm betas onto each of the three cortical surface depths and averaged across depths (analysis_transformfsaverage.m). The result was then mapped using nearest neighbor interpolation to fsaverage space to create the fsaverage preparation. For the MNI preparation, we mapped the 1.0-mm betas to MNI space using cubic interpolation (analysis_transformMNI.m).</p><h3 class="c-article__sub-heading" id="Sec47">GLM analysis of the resting-state experiment</h3><p>As an optional resource, we fit the time series data from the resting-state experiment using methods that parallel those used for the NSD experiment (glm_nsdresting.m). For each scan session involving resting-state, we took the two resting-state runs (first and last run acquired) and analyzed the data using the design matrix of the neighboring NSD runs and the same voxel-wise HRFs determined from analyzing the NSD runs in that scan session (this is analogous to beta version b2). Although there is no reason to think that spontaneous resting-state activity conforms to the 4-s trial structure of the NSD experiment, these resting-state betas might be useful as a direct comparison for the NSD betas.</p><h3 class="c-article__sub-heading" id="Sec48">Noise ceiling estimation</h3><p>To obtain a measure of data quality, noise ceilings were estimated for the NSD betas (export_noiseceiling.m). The noise ceiling for a given voxel is defined as the maximum percentage of variance in the voxels responses that can, in theory, be explained, given the presence of measurement noise. Our method for estimating the noise ceiling follows the general framework laid out in previous studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 80" title="Kay, K. N., Winawer, J., Mezer, A. &amp; Wandell, B. Compressive spatial summation in human visual cortex. J. Neurophysiol. 110, 481494 (2013)." href="/articles/s41593-021-00962-x#ref-CR80" id="ref-link-section-d29682704e2696">80</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Lage-Castellanos, A., Valente, G., Formisano, E. &amp; De Martino, F. Methods for computing the maximum performance of computational models of fMRI responses. PLoS Comput. Biol. 15, e1006397 (2019)." href="/articles/s41593-021-00962-x#ref-CR81" id="ref-link-section-d29682704e2699">81</a></sup>. Several assumptions are made: (1) the signal contained in the voxels response is determined solely by the presented image; (2) the variability of the signal across different images is Gaussian distributed; (3) the noise is Gaussian distributed with zero mean; and (4) the response to an image is equal to the signal plus noise. Given these assumptions, any observed response is a sample from a sum of Gaussian distributions:</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\rm{RESP}} \sim {{{\mathcal{N}}}}\left( {\mu _{\rm{signal}},\sigma _{\rm{signal}}} \right) + {{{\mathcal{N}}}}\left( {0,\sigma _{{\rm{noise}}}} \right)$$</span></div></div><p>where RESP indicates the NSD beta observed on a given trial, <i></i><sub>signal</sub> is the mean signal across different images, <i></i><sub>signal</sub> is the standard deviation of the signal across different images and <i></i><sub>noise</sub> is the standard deviation of the noise (for illustration of these concepts, see Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig14">8c</a>). Note that the first Gaussian distribution characterizes true signal variability, whereas the second Gaussian characterizes variability due to noise. Also, note that this framework treats response variability unrelated to the stimulus as noise, but such variability might, in fact, reflect signal from the perspective of functional connectivity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 82" title="Biswal, B., Yetkin, F. Z., Haughton, V. M. &amp; Hyde, J. S. Functional connectivity in the motor cortex of resting human brain using echo-planar MRI. Magn. Reson. Med. 34, 537541 (1995)." href="/articles/s41593-021-00962-x#ref-CR82" id="ref-link-section-d29682704e2811">82</a></sup>.</p><p>To compute the noise ceiling, we first take the trial-wise NSD betas for each voxel and <i>z</i>-score these betas within each scan session. This simple normalization compensates for non-stationarities that might exist across sessions. We then calculate the variance of the betas across the three presentations of each image (using the unbiased estimator that normalizes by <i>n</i>1 where <i>n</i> is the sample size), average this variance across images and then compute the square root of the result. This produces an estimate of the noise standard deviation:</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\hat \sigma _{{\rm{noise}}} = \sqrt {{{{\mathrm{mean}}}}\left( {\beta _\sigma ^2} \right)}$$</span></div></div><p>where <span class="mathjax-tex">\(\beta _\sigma ^2\)</span> indicates the variance across the betas obtained for a given image. Next, given that the variance of the <i>z</i>-scored betas is 1, we estimate the signal standard deviation as follows:</p><div id="Equc" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\hat \sigma _{\rm{signal}} = \sqrt {\left| {1 - \hat \sigma _{\rm{noise}}^2} \right|_ + }$$</span></div></div><p>where ||<sub>+</sub> indicates positive half-wave rectification. Finally, we simplify by calculating a single scalar quantity:</p><div id="Equd" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\rm{ncsnr}} = \frac{{\hat \sigma _{\rm{signal}}}}{{\hat \sigma _{\rm{noise}}}}$$</span></div></div><p>where ncsnr indicates the noise ceiling SNR.</p><p>Given the framework described above, the noise ceiling can be calculated as the amount of variance contributed by the signal expressed as a percentage of the total amount of variance in the data:</p><div id="Eque" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\rm{NC}} = 100 \times \frac{{\sigma _{_{\rm{signal}}}^2}}{{\sigma _{_{\rm{signal}}}^2 + \sigma _{\rm{noise}}^2}}$$</span></div></div><p>where NC indicates the noise ceiling. We would like to be able to calculate the noise ceiling based on the single scalar ncsnr. Moreover, because a researcher might want to average across multiple presentations of each image before attempting to explain the NSD betas, we would like a method for flexibly expressing the noise ceiling for different levels of trial averaging. With some algebra, it can be shown that the noise ceiling can be expressed as follows:</p><div id="Equf" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\rm{NC}} = 100 \times \frac{{\rm{ncsnr}^2}}{{\rm{ncsnr^2}} + \frac{1}{n}}$$</span></div></div><p>where <i>n</i> indicates the number of trials that are averaged together (see the NSD Data Manual for the derivation and additional details). We note that there is a direct relationship between the commonly used metric of split-half reliability and the noise ceiling: if a voxel has two sets of responses that reflect the same image presentations, then the correlation between the two sets of responses multiplied by 100 is equal to the noise ceiling for single-trial responses expressed in percent variance explained.</p><p>Using the above methods, we calculated noise ceilings for each of the beta versions and for each of various spatial preparations (1.8-mm, 1-mm, fsaverage and nativesurface). For simplicity, noise ceiling estimates were calculated using betas associated with images with all three presentations available. To assess stability, we also computed split-half noise ceiling estimates. This was achieved by splitting the available images into two mutually exclusive groups and computing noise ceiling estimates independently for each group. The noise ceiling results shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3f,g</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">6</a> were computed assuming <i>n =</i> 3, reflecting the scenario in which trial-wise betas are averaged across three trials for each image. The noise ceiling results shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig6">6a,b</a> were computed assuming <i>n</i> = 1 and are expressed in correlation units (square root of percent variance explained).</p><p>We include a few important notes as follows. Even though the NSD consists of only up to three trials for a given image, the estimate of response variability for each voxel (that is, the noise standard deviation) is averaged across a very large number of images, thus stabilizing the noise ceiling estimate. Also, note that our noise ceiling metric refers to activity levels in individual voxels in individual participants. It is thus quite different from, for example, noise ceiling metrics computed for group average representational dissimilarity matrices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Nili, H. et al. A toolbox for representational similarity analysis. PLoS Comput. Biol. 10, e1003553 (2014)." href="/articles/s41593-021-00962-x#ref-CR83" id="ref-link-section-d29682704e3487">83</a></sup>. The latter are more abstracted away from the data given that they summarize properties observed across a collection of voxels, reflect second-order computations on activity levels and not activity levels themselves and probe responses at the group level and not at the individual level.</p><h3 class="c-article__sub-heading" id="Sec49">Calculation of equivalent trials</h3><p>To provide a common basis for comparing different datasets, we define the number of equivalent trials present in a dataset as <i>N</i>  ncsnr<sup>2</sup>, where <i>N</i> indicates the number of trials conducted and ncsnr is the noise ceiling SNR (as defined above). The assumptions here are that (1) every trial has equal value, irrespective of whether it is used to measure brain responses to an image that has already been shown or a new image (for example, two trials for one image is equivalent to one trial for two distinct images); and (2) increases in SNR are equivalent to the collection of additional trials. For an illustrative example of the second assumption, suppose an experimenter chooses to improve SNR by averaging the response to a given image across <i>p</i> repetitions of that image. This effectively reduces the noise standard deviation by a factor of <i>p</i>, and ncsnr will thus increase by a factor of <i>p</i>. Alternatively, the experimenter could choose to not average and instead use the <i>p</i> trials as is. In the former case, the number of equivalent trials is 1  (<i>p</i>  ncsnr)<sup>2</sup> = <i>p</i>  ncsnr<sup>2</sup>, whereas, in the latter case, the number of equivalent trials is <i>p</i>  ncsnr<sup>2</sup>. Thus, the two cases correspond to the same number of equivalent trials.</p><p>We conducted an auxiliary analysis that directly compares the NSD against the BOLD5000 dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Chang, N. et al. BOLD5000, a public fMRI dataset while viewing 5000 visual images. Sci. Data 6, 49 (2019)." href="/articles/s41593-021-00962-x#ref-CR22" id="ref-link-section-d29682704e3539">22</a></sup>. The goal of this analysis was to calculate a summary ncsnr value for each dataset, so that the number of equivalent trials can be calculated. For fair comparison, both NSD and BOLD5000 were analyzed using the same GLM methods described in this paper (beta version b3). We then defined a common brain region on which data quality can be compared. This was done by transforming the nsdgeneral ROI to MNI space and then mapping the resulting MNI mask to each participant in the two datasets. Finally, we computed the median ncsnr observed across voxels in the mask in each participant.</p><p>The median ncsnr, averaged across participants, was 0.260 for the NSD (averaged across the first four NSD participants) and 0.187 for BOLD5000 (averaged across the four participants in BOLD5000). This indicates that, despite the longer time duration allocated per trial in BOLD5000 (10s) compared to the NSD (4s), the quality of a single-trial beta in the NSD is higher than that in BOLD5000. Specifically, one NSD trial is approximately equivalent to (0.260)<sup>2</sup>/(0.187)<sup>2</sup> = 1.93 BOLD5000 trials. This increase in quality is likely due, in part, to the screening of participants and the ultra-high magnetic field strength (7T) used in the NSD. Note that the ncsnr metric quantifies the SNR per trial and is expected to be unbiased with respect to the number of repeated trials used to calculate it. Thus, although the exact number of trials per image is different in the NSD and BOLD5000 datasets, the ncsnr values can still be directly compared.</p><h3 class="c-article__sub-heading" id="Sec50">Univariate analysis of memory recognition</h3><p>For this analysis (results shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig4">4b</a>), we used version 3 of the NSD betas (b3) in the fsaverage preparation. Betas for each surface vertex were kept in percent signal change units. Using the behavioral responses, we identified trials involving hits (participants responded old to a previously presented image) and trials involving correct rejections (participants responded new to a novel image). Then, for each participant, we calculated two-sample <i>t-</i>values at each surface vertex. This was done both for trials pooled within individual NSD scan sessions as well as for trials pooled across all sessions.</p><h3 class="c-article__sub-heading" id="Sec51">Representational similarity analysis</h3><p>For this analysis (results shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5</a>), we used version 3 of the NSD betas (b3) in the fsaverage preparation. Betas for each surface vertex were <i>z</i>-scored within each scan session, concatenated across sessions and averaged across repeated trials for each distinct image. To support the representational similarity analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysisconnecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/s41593-021-00962-x#ref-CR84" id="ref-link-section-d29682704e3579">84</a></sup>, we defined a set of ROIs (V1, V2, V3, pVTC and aVTC) on the fsaverage surface. This was done by mapping the manually defined V1, V2 and V3 from each participant to fsaverage, averaging across participant and using the result to guide the definition of group-level ROIs. We also defined a posterior and anterior division of ventral temporal cortex (pVTC and aVTC, respectively) based on anatomical criteria. For each participant, we extracted betas for vertices within each ROI (concatenating across hemispheres). We then computed Pearsons correlation between beta patterns across all possible pairs of images. This yielded RDMs with rows and columns indexing distinct images (for example, the RDMs for participant 1 have dimensionality 10,000  10,000 with correlations corresponding to 49,995,000 possible pairs).</p><p>To help visualize and interpret these large dissimilarity matrices, we performed <i>t</i>-SNE embedding<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Maaten, Lvander &amp; Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. Res. 9, 25792605 (2008)." href="/articles/s41593-021-00962-x#ref-CR41" id="ref-link-section-d29682704e3589">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 85" title="Pedregosa, F. et al. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, 28252830 (2011)." href="/articles/s41593-021-00962-x#ref-CR85" id="ref-link-section-d29682704e3592">85</a></sup> using a perplexity level of 100 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5b,c</a>). This projects the high-dimensional representations onto a 2D plane such that the distance of a given pair on the plane reflects that pairs distance in the high-dimensional representation as accurately as possible. To verify the strong categorical structure visible in pVTC and aVTC (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5b</a>), we quantified the similarity of the brain RDMs to a model RDM constructed from the category labels in the COCO dataset. Specifically, we constructed an RDM from a binary matrix indicating the presence or absence of each of the 80 COCO categories (cosine distance metric) and correlated this model RDM with each brain RDM. This process was performed for mutually exclusive groups of 100 images drawn from all images presented three times to a given participant (the number of groups was 100, 100, 62, 54, 100, 62, 100 and 54 for the eight participants, respectively). We calculated the mean and standard error across results obtained for different groups of images (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5d</a>). Finally, we investigated similarity of brain representations across ROIs and participants. This was done by isolating the shared515 images, constructing brain RDMs for these images and correlating brain RDMs across ROIs and participants. The resulting second-order RDM is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5e</a>, with further quantification of this matrix shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig5">5f</a>.</p><h3 class="c-article__sub-heading" id="Sec52">Reporting Summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM2">Nature Research Reporting Summary</a> linked to this article.</p></div></div></section>
                </div>
            

            <div>
                <section data-title="Data availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data availability</h2><div class="c-article-section__content" id="data-availability-content">
              
              <p>The NSD dataset is freely available at <a href="http://naturalscenesdataset.org">http://naturalscenesdataset.org</a>. The data are hosted in the cloud, allowing researchers to exploit high-performance cloud computing to efficiently analyze the dataset. We provide both raw data in BIDS format<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="Gorgolewski, K. J. et al. The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Sci. Data 3, 19 (2016)." href="/articles/s41593-021-00962-x#ref-CR86" id="ref-link-section-d29682704e3726">86</a></sup> and prepared data files, along with extensive technical documentation in the NSD Data Manual. To ensure strict validation for an upcoming Algonauts prediction challenge<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Cichy, R. M., Roig, G. &amp; Oliva, A. The Algonauts Project. Nat. Mach. Intell. 1, 613 (2019)." href="/articles/s41593-021-00962-x#ref-CR87" id="ref-link-section-d29682704e3730">87</a></sup>, the initial public release will withhold the last three NSD scan sessions from each participant (approximately 8.4% of the NSD data). Images used for the NSD were taken from the Common Objects in Context database<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision. &#xA;                  https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48&#xA;                  &#xA;                , 740755 (Springer, 2014)." href="/articles/s41593-021-00962-x#ref-CR14" id="ref-link-section-d29682704e3734">14</a></sup> (<a href="https://cocodataset.org">https://cocodataset.org</a>).</p>
            </div></div></section><section data-title="Code availability"><div class="c-article-section" id="code-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="code-availability">Code availability</h2><div class="c-article-section__content" id="code-availability-content">
              
              <p>We provide an archive of code used in this study (<a href="https://github.com/cvnlab/nsddatapaper/">https://github.com/cvnlab/nsddatapaper/</a>) as well as utility functions for working with the prepared NSD data (<a href="https://github.com/cvnlab/nsdcode/">https://github.com/cvnlab/nsdcode/</a>). Custom algorithms developed for this study include GLMsingle (<a href="https://github.com/cvnlab/GLMsingle/">https://github.com/cvnlab/GLMsingle/</a>) and fracridge (<a href="https://github.com/nrdg/fracridge/">https://github.com/nrdg/fracridge/</a>). Example scripts demonstrating scientific analyses of the NSD data are available (<a href="https://github.com/cvnlab/nsdexamples/">https://github.com/cvnlab/nsdexamples/</a>); these scripts might be useful for teaching purposes.</p>
            </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">de Vries, S. E. J. et al. A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex. <i>Nat. Neurosci.</i> <b>23</b>, 138151 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41593-019-0550-9" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41593-019-0550-9" aria-label="Article reference 1" data-doi="10.1038/s41593-019-0550-9">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31844315" aria-label="PubMed reference 1">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20large-scale%20standardized%20physiological%20survey%20reveals%20functional%20organization%20of%20the%20mouse%20visual%20cortex&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fs41593-019-0550-9&amp;volume=23&amp;pages=138-151&amp;publication_year=2020&amp;author=Vries%2CSEJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Siegle, J. H. et al. Survey of spiking in the mouse visual system reveals functional hierarchy. <i>Nature</i> <b>592</b>, 8692 (2021).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Stringer, C., Pachitariu, M., Steinmetz, N., Carandini, M. &amp; Harris, K. D. High-dimensional geometry of population responses in visual cortex. <i>Nature</i> <b>571</b>, 361365 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41586-019-1346-5" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-019-1346-5" aria-label="Article reference 3" data-doi="10.1038/s41586-019-1346-5">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1MXht1yktL%2FM" aria-label="CAS reference 3">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31243367" aria-label="PubMed reference 3">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6642054" aria-label="PubMed Central reference 3">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=High-dimensional%20geometry%20of%20population%20responses%20in%20visual%20cortex&amp;journal=Nature&amp;doi=10.1038%2Fs41586-019-1346-5&amp;volume=571&amp;pages=361-365&amp;publication_year=2019&amp;author=Stringer%2CC&amp;author=Pachitariu%2CM&amp;author=Steinmetz%2CN&amp;author=Carandini%2CM&amp;author=Harris%2CKD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Markram, H. et al. Reconstruction and simulation of neocortical microcircuitry. <i>Cell</i> <b>163</b>, 456492 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.cell.2015.09.029" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cell.2015.09.029" aria-label="Article reference 4" data-doi="10.1016/j.cell.2015.09.029">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXhs1yksbzN" aria-label="CAS reference 4">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26451489" aria-label="PubMed reference 4">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Reconstruction%20and%20simulation%20of%20neocortical%20microcircuitry&amp;journal=Cell&amp;doi=10.1016%2Fj.cell.2015.09.029&amp;volume=163&amp;pages=456-492&amp;publication_year=2015&amp;author=Markram%2CH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Van Essen, D. C. et al. The WU-Minn human connectome project: an overview. <i>Neuroimage</i> <b>80</b>, 6279 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2013.05.041" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2013.05.041" aria-label="Article reference 5" data-doi="10.1016/j.neuroimage.2013.05.041">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23684880" aria-label="PubMed reference 5">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20WU-Minn%20human%20connectome%20project%3A%20an%20overview&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2013.05.041&amp;volume=80&amp;pages=62-79&amp;publication_year=2013&amp;author=Essen%2CDC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Zheng, Z. et al. A complete electron microscopy volume of the brain of adult <i>Drosophila melanogaster</i>. <i>Cell</i> <b>174</b>, 730743 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.cell.2018.06.019" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cell.2018.06.019" aria-label="Article reference 6" data-doi="10.1016/j.cell.2018.06.019">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1cXhtlGltbzP" aria-label="CAS reference 6">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30033368" aria-label="PubMed reference 6">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6063995" aria-label="PubMed Central reference 6">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20complete%20electron%20microscopy%20volume%20of%20the%20brain%20of%20adult%20Drosophila%20melanogaster&amp;journal=Cell&amp;doi=10.1016%2Fj.cell.2018.06.019&amp;volume=174&amp;pages=730-743&amp;publication_year=2018&amp;author=Zheng%2CZ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Van Essen, D. C. et al. Mapping visual cortex in monkeys and humans using surface-based atlases. <i>Vis. Res.</i> <b>41</b>, 13591378 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0042-6989(01)00045-1" data-track-action="article reference" href="https://doi.org/10.1016%2FS0042-6989%2801%2900045-1" aria-label="Article reference 7" data-doi="10.1016/S0042-6989(01)00045-1">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11322980" aria-label="PubMed reference 7">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Mapping%20visual%20cortex%20in%20monkeys%20and%20humans%20using%20surface-based%20atlases&amp;journal=Vis.%20Res.&amp;doi=10.1016%2FS0042-6989%2801%2900045-1&amp;volume=41&amp;pages=1359-1378&amp;publication_year=2001&amp;author=Essen%2CDC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Grill-Spector, K. &amp; Malach, R. The human visual cortex. <i>Annu. Rev. Neurosci.</i> <b>27</b>, 649677 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1146/annurev.neuro.27.070203.144220" data-track-action="article reference" href="https://doi.org/10.1146%2Fannurev.neuro.27.070203.144220" aria-label="Article reference 8" data-doi="10.1146/annurev.neuro.27.070203.144220">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2cXmslantLc%3D" aria-label="CAS reference 8">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15217346" aria-label="PubMed reference 8">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20human%20visual%20cortex&amp;journal=Annu.%20Rev.%20Neurosci.&amp;doi=10.1146%2Fannurev.neuro.27.070203.144220&amp;volume=27&amp;pages=649-677&amp;publication_year=2004&amp;author=Grill-Spector%2CK&amp;author=Malach%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Wheeler, M. E., Petersen, S. E. &amp; Buckner, R. L. Memorys echo: vivid remembering reactivates sensory-specific cortex. <i>Proc. Natl Acad. Sci. USA</i> <b>97</b>, 1112511129 (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.97.20.11125" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.97.20.11125" aria-label="Article reference 9" data-doi="10.1073/pnas.97.20.11125">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3cXnt1aht7c%3D" aria-label="CAS reference 9">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11005879" aria-label="PubMed reference 9">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC27159" aria-label="PubMed Central reference 9">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Memory%E2%80%99s%20echo%3A%20vivid%20remembering%20reactivates%20sensory-specific%20cortex&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.97.20.11125&amp;volume=97&amp;pages=11125-11129&amp;publication_year=2000&amp;author=Wheeler%2CME&amp;author=Petersen%2CSE&amp;author=Buckner%2CRL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Breedlove, J. L., St-Yves, G., Olman, C. A. &amp; Naselaris, T. Generative feedback explains distinct brain activity codes for seen and mental images. <i>Curr. Biol.</i> <b>30</b>, 22112224 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.cub.2020.04.014" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cub.2020.04.014" aria-label="Article reference 10" data-doi="10.1016/j.cub.2020.04.014">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXosVyiu70%3D" aria-label="CAS reference 10">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32359428" aria-label="PubMed reference 10">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Generative%20feedback%20explains%20distinct%20brain%20activity%20codes%20for%20seen%20and%20mental%20images&amp;journal=Curr.%20Biol.&amp;doi=10.1016%2Fj.cub.2020.04.014&amp;volume=30&amp;pages=2211-2224&amp;publication_year=2020&amp;author=Breedlove%2CJL&amp;author=St-Yves%2CG&amp;author=Olman%2CCA&amp;author=Naselaris%2CT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Kay, K. N., Weiner, K. S. &amp; Grill-Spector, K. Attention reduces spatial uncertainty in human ventral temporal cortex. <i>Curr. Biol.</i> <b>25</b>, 595600 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.cub.2014.12.050" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cub.2014.12.050" aria-label="Article reference 11" data-doi="10.1016/j.cub.2014.12.050">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXjt1amtr4%3D" aria-label="CAS reference 11">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25702580" aria-label="PubMed reference 11">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4348205" aria-label="PubMed Central reference 11">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20reduces%20spatial%20uncertainty%20in%20human%20ventral%20temporal%20cortex&amp;journal=Curr.%20Biol.&amp;doi=10.1016%2Fj.cub.2014.12.050&amp;volume=25&amp;pages=595-600&amp;publication_year=2015&amp;author=Kay%2CKN&amp;author=Weiner%2CKS&amp;author=Grill-Spector%2CK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Huth, A. G., Nishimoto, S., Vu, A. T. &amp; Gallant, J. L. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. <i>Neuron</i> <b>76</b>, 12101224 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2012.10.014" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2012.10.014" aria-label="Article reference 12" data-doi="10.1016/j.neuron.2012.10.014">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhvVKqu77F" aria-label="CAS reference 12">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23259955" aria-label="PubMed reference 12">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3556488" aria-label="PubMed Central reference 12">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20continuous%20semantic%20space%20describes%20the%20representation%20of%20thousands%20of%20object%20and%20action%20categories%20across%20the%20human%20brain&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2012.10.014&amp;volume=76&amp;pages=1210-1224&amp;publication_year=2012&amp;author=Huth%2CAG&amp;author=Nishimoto%2CS&amp;author=Vu%2CAT&amp;author=Gallant%2CJL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Krizhevsky, A. <i>Learning Multiple Layers of Features from Tiny Images</i>. <a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" data-track="click" data-track-action="external reference" data-track-label="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a> (University of Toronto, 2009).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision. <a href="https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48" data-track="click" data-track-action="external reference" data-track-label="https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48">https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48</a>, 740755 (Springer, 2014).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Gl, U. &amp; van Gerven, M. A. J. Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. <i>J. Neurosci.</i> <b>35</b>, 1000510014 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.5023-14.2015" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.5023-14.2015" aria-label="Article reference 15" data-doi="10.1523/JNEUROSCI.5023-14.2015">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26157000" aria-label="PubMed reference 15">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6605414" aria-label="PubMed Central reference 15">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20neural%20networks%20reveal%20a%20gradient%20in%20the%20complexity%20of%20neural%20representations%20across%20the%20ventral%20stream&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.5023-14.2015&amp;volume=35&amp;pages=10005-10014&amp;publication_year=2015&amp;author=G%C3%BC%C3%A7l%C3%BC%2CU&amp;author=Gerven%2CMAJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Khaligh-Razavi, S.-M. &amp; Kriegeskorte, N. Deep supervised, but not unsupervised, models may explain IT cortical representation. <i>PLoS Comput. Biol.</i> <b>10</b>, e1003915 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1003915" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1003915" aria-label="Article reference 16" data-doi="10.1371/journal.pcbi.1003915">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25375136" aria-label="PubMed reference 16">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4222664" aria-label="PubMed Central reference 16">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20supervised%2C%20but%20not%20unsupervised%2C%20models%20may%20explain%20IT%20cortical%20representation&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1003915&amp;volume=10&amp;publication_year=2014&amp;author=Khaligh-Razavi%2CS-M&amp;author=Kriegeskorte%2CN">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Seeliger, K. et al. End-to-end neural system identification with neural information flow. <i>PLoS Comput. Biol.</i> <b>17</b>, e1008558 (2021).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1008558" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1008558" aria-label="Article reference 17" data-doi="10.1371/journal.pcbi.1008558">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3MXkvVGgs7k%3D" aria-label="CAS reference 17">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33539366" aria-label="PubMed reference 17">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7888598" aria-label="PubMed Central reference 17">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=End-to-end%20neural%20system%20identification%20with%20neural%20information%20flow&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1008558&amp;volume=17&amp;publication_year=2021&amp;author=Seeliger%2CK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Stansbury, D. E., Naselaris, T. &amp; Gallant, J. L. Natural scene statistics account for the representation of scene categories in human visual cortex. <i>Neuron</i> <b>79</b>, 10251034 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2013.06.034" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2013.06.034" aria-label="Article reference 18" data-doi="10.1016/j.neuron.2013.06.034">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXht1Gmu7%2FE" aria-label="CAS reference 18">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23932491" aria-label="PubMed reference 18">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5464350" aria-label="PubMed Central reference 18">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Natural%20scene%20statistics%20account%20for%20the%20representation%20of%20scene%20categories%20in%20human%20visual%20cortex&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2013.06.034&amp;volume=79&amp;pages=1025-1034&amp;publication_year=2013&amp;author=Stansbury%2CDE&amp;author=Naselaris%2CT&amp;author=Gallant%2CJL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">St-Yves, G. &amp; Naselaris, T. The feature-weighted receptive field: an interpretable encoding model for complex feature spaces. <i>Neuroimage</i> <b>180</b>, 188202 (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. <i>Proc. Natl Acad. Sci. USA</i> <b>111</b>, 86198624 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1403112111" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1403112111" aria-label="Article reference 20" data-doi="10.1073/pnas.1403112111">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXnslWnsb4%3D" aria-label="CAS reference 20">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24812127" aria-label="PubMed reference 20">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4060707" aria-label="PubMed Central reference 20">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Performance-optimized%20hierarchical%20models%20predict%20neural%20responses%20in%20higher%20visual%20cortex&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1403112111&amp;volume=111&amp;pages=8619-8624&amp;publication_year=2014&amp;author=Yamins%2CDLK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Naselaris, T. et al. Cognitive computational neuroscience: a new conference for an emerging discipline. <i>Trends Cogn. Sci.</i> <b>22</b>, 365367 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2018.02.008" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2018.02.008" aria-label="Article reference 21" data-doi="10.1016/j.tics.2018.02.008">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29500078" aria-label="PubMed reference 21">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5911192" aria-label="PubMed Central reference 21">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20computational%20neuroscience%3A%20a%20new%20conference%20for%20an%20emerging%20discipline&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2018.02.008&amp;volume=22&amp;pages=365-367&amp;publication_year=2018&amp;author=Naselaris%2CT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Chang, N. et al. BOLD5000, a public fMRI dataset while viewing 5000 visual images. <i>Sci. Data</i> <b>6</b>, 49 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41597-019-0052-3" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41597-019-0052-3" aria-label="Article reference 22" data-doi="10.1038/s41597-019-0052-3">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31061383" aria-label="PubMed reference 22">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6502931" aria-label="PubMed Central reference 22">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=BOLD5000%2C%20a%20public%20fMRI%20dataset%20while%20viewing%205000%20visual%20images&amp;journal=Sci.%20Data&amp;doi=10.1038%2Fs41597-019-0052-3&amp;volume=6&amp;publication_year=2019&amp;author=Chang%2CN">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Horikawa, T. &amp; Kamitani, Y. Generic decoding of seen and imagined objects using hierarchical visual features. <i>Nat. Commun.</i> <b>8</b>, 15037 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/ncomms15037" data-track-action="article reference" href="https://doi.org/10.1038%2Fncomms15037" aria-label="Article reference 23" data-doi="10.1038/ncomms15037">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2sXot1yqtL0%3D" aria-label="CAS reference 23">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28530228" aria-label="PubMed reference 23">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5458127" aria-label="PubMed Central reference 23">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Generic%20decoding%20of%20seen%20and%20imagined%20objects%20using%20hierarchical%20visual%20features&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fncomms15037&amp;volume=8&amp;publication_year=2017&amp;author=Horikawa%2CT&amp;author=Kamitani%2CY">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Kay, K. N., Naselaris, T., Prenger, R. J. &amp; Gallant, J. L. Identifying natural images from human brain activity. <i>Nature</i> <b>452</b>, 352355 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature06713" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature06713" aria-label="Article reference 24" data-doi="10.1038/nature06713">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1cXjsFCnsL8%3D" aria-label="CAS reference 24">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18322462" aria-label="PubMed reference 24">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3556484" aria-label="PubMed Central reference 24">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Identifying%20natural%20images%20from%20human%20brain%20activity&amp;journal=Nature&amp;doi=10.1038%2Fnature06713&amp;volume=452&amp;pages=352-355&amp;publication_year=2008&amp;author=Kay%2CKN&amp;author=Naselaris%2CT&amp;author=Prenger%2CRJ&amp;author=Gallant%2CJL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Triantafyllou, C. et al. Comparison of physiological noise at 1.5 T, 3 T and 7 T and optimization of fMRI acquisition parameters. <i>Neuroimage</i> <b>26</b>, 243250 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2005.01.007" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2005.01.007" aria-label="Article reference 25" data-doi="10.1016/j.neuroimage.2005.01.007">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD2M3ivFCgug%3D%3D" aria-label="CAS reference 25">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15862224" aria-label="PubMed reference 25">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparison%20of%20physiological%20noise%20at%201.5%20T%2C%203%20T%20and%207%20T%20and%20optimization%20of%20fMRI%20acquisition%20parameters&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2005.01.007&amp;volume=26&amp;pages=243-250&amp;publication_year=2005&amp;author=Triantafyllou%2CC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Brady, T. F., Konkle, T., Alvarez, G. A. &amp; Oliva, A. Visual long-term memory has a massive storage capacity for object details. <i>Proc. Natl Acad. Sci. USA</i> <b>105</b>, 1432514329 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.0803390105" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.0803390105" aria-label="Article reference 26" data-doi="10.1073/pnas.0803390105">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1cXht1SgtrjK" aria-label="CAS reference 26">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18787113" aria-label="PubMed reference 26">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2533687" aria-label="PubMed Central reference 26">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20long-term%20memory%20has%20a%20massive%20storage%20capacity%20for%20object%20details&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.0803390105&amp;volume=105&amp;pages=14325-14329&amp;publication_year=2008&amp;author=Brady%2CTF&amp;author=Konkle%2CT&amp;author=Alvarez%2CGA&amp;author=Oliva%2CA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Haxby, J. V., Guntupalli, J. S., Nastase, S. A. &amp; Feilong, M. Hyperalignment: modeling shared information encoded in idiosyncratic cortical topographies. <i>eLife</i> <b>9</b>, e56601 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.7554/eLife.56601" data-track-action="article reference" href="https://doi.org/10.7554%2FeLife.56601" aria-label="Article reference 27" data-doi="10.7554/eLife.56601">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXhslamtLrE" aria-label="CAS reference 27">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32484439" aria-label="PubMed reference 27">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7266639" aria-label="PubMed Central reference 27">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Hyperalignment%3A%20modeling%20shared%20information%20encoded%20in%20idiosyncratic%20cortical%20topographies&amp;journal=eLife&amp;doi=10.7554%2FeLife.56601&amp;volume=9&amp;publication_year=2020&amp;author=Haxby%2CJV&amp;author=Guntupalli%2CJS&amp;author=Nastase%2CSA&amp;author=Feilong%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Power, J. D., Lynch, C. J., Adeyemo, B. &amp; Petersen, S. E. A critical, event-related appraisal of denoising in resting-state fMRI studies. <i>Cereb. Cortex</i> <b>30</b>, 55445559 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/bhaa139" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhaa139" aria-label="Article reference 28" data-doi="10.1093/cercor/bhaa139">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32494823" aria-label="PubMed reference 28">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20critical%2C%20event-related%20appraisal%20of%20denoising%20in%20resting-state%20fMRI%20studies&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhaa139&amp;volume=30&amp;pages=5544-5559&amp;publication_year=2020&amp;author=Power%2CJD&amp;author=Lynch%2CCJ&amp;author=Adeyemo%2CB&amp;author=Petersen%2CSE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Roth, Z. N., Ryoo, M. &amp; Merriam, E. P. Task-related activity in human visual cortex. <i>PLoS Biol.</i> <b>18</b>, e3000921 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1371/journal.pbio.3000921" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pbio.3000921" aria-label="Article reference 29" data-doi="10.1371/journal.pbio.3000921">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXisVSqtLrF" aria-label="CAS reference 29">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33156829" aria-label="PubMed reference 29">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7673548" aria-label="PubMed Central reference 29">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Task-related%20activity%20in%20human%20visual%20cortex&amp;journal=PLoS%20Biol.&amp;doi=10.1371%2Fjournal.pbio.3000921&amp;volume=18&amp;publication_year=2020&amp;author=Roth%2CZN&amp;author=Ryoo%2CM&amp;author=Merriam%2CEP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Benson, N. C. et al. The human connectome project 7 Tesla retinotopy dataset: description and population receptive field analysis. <i>J. Vis</i>. <b>18</b>, 23 (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Stigliani, A., Weiner, K. S. &amp; Grill-Spector, K. Temporal processing capacity in high-level visual cortex is domain specific. <i>J. Neurosci.</i> <b>35</b>, 1241212424 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.4822-14.2015" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.4822-14.2015" aria-label="Article reference 31" data-doi="10.1523/JNEUROSCI.4822-14.2015">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC28XkslKqug%3D%3D" aria-label="CAS reference 31">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26354910" aria-label="PubMed reference 31">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4563034" aria-label="PubMed Central reference 31">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Temporal%20processing%20capacity%20in%20high-level%20visual%20cortex%20is%20domain%20specific&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.4822-14.2015&amp;volume=35&amp;pages=12412-12424&amp;publication_year=2015&amp;author=Stigliani%2CA&amp;author=Weiner%2CKS&amp;author=Grill-Spector%2CK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Kay, K. et al. A critical assessment of data quality and venous effects in sub-millimeter fMRI. <i>Neuroimage</i> <b>189</b>, 847869 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2019.02.006" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2019.02.006" aria-label="Article reference 32" data-doi="10.1016/j.neuroimage.2019.02.006">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30731246" aria-label="PubMed reference 32">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20critical%20assessment%20of%20data%20quality%20and%20venous%20effects%20in%20sub-millimeter%20fMRI&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2019.02.006&amp;volume=189&amp;pages=847-869&amp;publication_year=2019&amp;author=Kay%2CK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Gordon, E. M. et al. Precision functional mapping of individual human brains. <i>Neuron</i> <b>95</b>, 791807 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2017.07.011" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2017.07.011" aria-label="Article reference 33" data-doi="10.1016/j.neuron.2017.07.011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2sXht1CmsLjF" aria-label="CAS reference 33">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28757305" aria-label="PubMed reference 33">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5576360" aria-label="PubMed Central reference 33">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Precision%20functional%20mapping%20of%20individual%20human%20brains&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2017.07.011&amp;volume=95&amp;pages=791-807&amp;publication_year=2017&amp;author=Gordon%2CEM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Kang, X., Yund, E. W., Herron, T. J. &amp; Woods, D. L. Improving the resolution of functional brain imaging: analyzing functional data in anatomical space. <i>Magn. Reson. Imaging</i> <b>25</b>, 10701078 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.mri.2006.12.005" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.mri.2006.12.005" aria-label="Article reference 34" data-doi="10.1016/j.mri.2006.12.005">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17707169" aria-label="PubMed reference 34">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Improving%20the%20resolution%20of%20functional%20brain%20imaging%3A%20analyzing%20functional%20data%20in%20anatomical%20space&amp;journal=Magn.%20Reson.%20Imaging&amp;doi=10.1016%2Fj.mri.2006.12.005&amp;volume=25&amp;pages=1070-1078&amp;publication_year=2007&amp;author=Kang%2CX&amp;author=Yund%2CEW&amp;author=Herron%2CTJ&amp;author=Woods%2CDL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Kay, K. N., Rokem, A., Winawer, J., Dougherty, R. F. &amp; Wandell, B. GLMdenoise: a fast, automated technique for denoising task-based fMRI data. <i>Front. Neurosci.</i> <b>7</b>, 247 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3389/fnins.2013.00247" data-track-action="article reference" href="https://doi.org/10.3389%2Ffnins.2013.00247" aria-label="Article reference 35" data-doi="10.3389/fnins.2013.00247">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24381539" aria-label="PubMed reference 35">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3865440" aria-label="PubMed Central reference 35">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=GLMdenoise%3A%20a%20fast%2C%20automated%20technique%20for%20denoising%20task-based%20fMRI%20data&amp;journal=Front.%20Neurosci.&amp;doi=10.3389%2Ffnins.2013.00247&amp;volume=7&amp;publication_year=2013&amp;author=Kay%2CKN&amp;author=Rokem%2CA&amp;author=Winawer%2CJ&amp;author=Dougherty%2CRF&amp;author=Wandell%2CB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Rokem, A. &amp; Kay, K. Fractional ridge regression: a fast, interpretable reparameterization of ridge regression. <i>Gigascience</i> <b>9</b>, giaa133 (2020).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Albrecht, D. G. &amp; Hamilton, D. B. Striate cortex of monkey and cat: contrast response function. <i>J. Neurophysiol.</i> <b>48</b>, 217237 (1982).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.1982.48.1.217" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.1982.48.1.217" aria-label="Article reference 37" data-doi="10.1152/jn.1982.48.1.217">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaL3s%2FgvFahtw%3D%3D" aria-label="CAS reference 37">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=7119846" aria-label="PubMed reference 37">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Striate%20cortex%20of%20monkey%20and%20cat%3A%20contrast%20response%20function&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.1982.48.1.217&amp;volume=48&amp;pages=217-237&amp;publication_year=1982&amp;author=Albrecht%2CDG&amp;author=Hamilton%2CDB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Wagner, A. D., Shannon, B. J., Kahn, I. &amp; Buckner, R. L. Parietal lobe contributions to episodic memory retrieval. <i>Trends Cogn. Sci.</i> <b>9</b>, 445453 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2005.07.001" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2005.07.001" aria-label="Article reference 38" data-doi="10.1016/j.tics.2005.07.001">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16054861" aria-label="PubMed reference 38">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Parietal%20lobe%20contributions%20to%20episodic%20memory%20retrieval&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2005.07.001&amp;volume=9&amp;pages=445-453&amp;publication_year=2005&amp;author=Wagner%2CAD&amp;author=Shannon%2CBJ&amp;author=Kahn%2CI&amp;author=Buckner%2CRL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Spaniol, J. et al. Event-related fMRI studies of episodic encoding and retrieval: meta-analyses using activation likelihood estimation. <i>Neuropsychologia</i> <b>47</b>, 17651779 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuropsychologia.2009.02.028" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuropsychologia.2009.02.028" aria-label="Article reference 39" data-doi="10.1016/j.neuropsychologia.2009.02.028">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19428409" aria-label="PubMed reference 39">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Event-related%20fMRI%20studies%20of%20episodic%20encoding%20and%20retrieval%3A%20meta-analyses%20using%20activation%20likelihood%20estimation&amp;journal=Neuropsychologia&amp;doi=10.1016%2Fj.neuropsychologia.2009.02.028&amp;volume=47&amp;pages=1765-1779&amp;publication_year=2009&amp;author=Spaniol%2CJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Gonzalez-Castillo, J. et al. Whole-brain, time-locked activation with simple tasks revealed using massive averaging and model-free analysis. <i>Proc. Natl Acad. Sci. USA</i> <b>109</b>, 54875492 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1121049109" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1121049109" aria-label="Article reference 40" data-doi="10.1073/pnas.1121049109">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XlslOjtr8%3D" aria-label="CAS reference 40">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22431587" aria-label="PubMed reference 40">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3325687" aria-label="PubMed Central reference 40">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Whole-brain%2C%20time-locked%20activation%20with%20simple%20tasks%20revealed%20using%20massive%20averaging%20and%20model-free%20analysis&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1121049109&amp;volume=109&amp;pages=5487-5492&amp;publication_year=2012&amp;author=Gonzalez-Castillo%2CJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Maaten, Lvander &amp; Hinton, G. Visualizing data using t-SNE. <i>J. Mach. Learn. Res.</i> <b>9</b>, 25792605 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Visualizing%20data%20using%20t-SNE&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=9&amp;pages=2579-2605&amp;publication_year=2008&amp;author=Maaten%2CLvander&amp;author=Hinton%2CG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Connolly, A. C. et al. The representation of biological classes in the human brain. <i>J. Neurosci.</i> <b>32</b>, 26082618 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.5547-11.2012" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.5547-11.2012" aria-label="Article reference 42" data-doi="10.1523/JNEUROSCI.5547-11.2012">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38Xjt1Ckurs%3D" aria-label="CAS reference 42">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22357845" aria-label="PubMed reference 42">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3532035" aria-label="PubMed Central reference 42">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20representation%20of%20biological%20classes%20in%20the%20human%20brain&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.5547-11.2012&amp;volume=32&amp;pages=2608-2618&amp;publication_year=2012&amp;author=Connolly%2CAC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Naselaris, T., Stansbury, D. E. &amp; Gallant, J. L. Cortical representation of animate and inanimate objects in complex natural scenes. <i>J. Physiol. Paris</i> <b>106</b>, 239249 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.jphysparis.2012.02.001" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.jphysparis.2012.02.001" aria-label="Article reference 43" data-doi="10.1016/j.jphysparis.2012.02.001">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22472178" aria-label="PubMed reference 43">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3407302" aria-label="PubMed Central reference 43">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Cortical%20representation%20of%20animate%20and%20inanimate%20objects%20in%20complex%20natural%20scenes&amp;journal=J.%20Physiol.%20Paris&amp;doi=10.1016%2Fj.jphysparis.2012.02.001&amp;volume=106&amp;pages=239-249&amp;publication_year=2012&amp;author=Naselaris%2CT&amp;author=Stansbury%2CDE&amp;author=Gallant%2CJL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Long, B., Yu, C.-P. &amp; Konkle, T. Mid-level visual features underlie the high-level categorical organization of the ventral stream. <i>Proc. Natl Acad. Sci. USA</i> <b>115</b>, E9015E9024 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1719616115" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1719616115" aria-label="Article reference 44" data-doi="10.1073/pnas.1719616115">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1cXitVKhu7rE" aria-label="CAS reference 44">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30171168" aria-label="PubMed reference 44">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6156638" aria-label="PubMed Central reference 44">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Mid-level%20visual%20features%20underlie%20the%20high-level%20categorical%20organization%20of%20the%20ventral%20stream&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1719616115&amp;volume=115&amp;pages=E9015-E9024&amp;publication_year=2018&amp;author=Long%2CB&amp;author=Yu%2CC-P&amp;author=Konkle%2CT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Henriksson, L., Khaligh-Razavi, S.-M., Kay, K. &amp; Kriegeskorte, N. Visual representations are dominated by intrinsic fluctuations correlated between areas. <i>Neuroimage</i> <b>114</b>, 275286 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2015.04.026" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2015.04.026" aria-label="Article reference 45" data-doi="10.1016/j.neuroimage.2015.04.026">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25896934" aria-label="PubMed reference 45">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20representations%20are%20dominated%20by%20intrinsic%20fluctuations%20correlated%20between%20areas&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2015.04.026&amp;volume=114&amp;pages=275-286&amp;publication_year=2015&amp;author=Henriksson%2CL&amp;author=Khaligh-Razavi%2CS-M&amp;author=Kay%2CK&amp;author=Kriegeskorte%2CN">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Naselaris, T., Kay, K. N., Nishimoto, S. &amp; Gallant, J. L. Encoding and decoding in fMRI. <i>Neuroimage</i> <b>56</b>, 400410 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2010.07.073" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2010.07.073" aria-label="Article reference 46" data-doi="10.1016/j.neuroimage.2010.07.073">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20691790" aria-label="PubMed reference 46">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Encoding%20and%20decoding%20in%20fMRI&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2010.07.073&amp;volume=56&amp;pages=400-410&amp;publication_year=2011&amp;author=Naselaris%2CT&amp;author=Kay%2CKN&amp;author=Nishimoto%2CS&amp;author=Gallant%2CJL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems 25 <a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html" data-track="click" data-track-action="external reference" data-track-label="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a>, 10971105 (2012).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">Cadena, S. A. et al. Deep convolutional models improve predictions of macaque V1 responses to natural images. <i>PLoS Comput. Biol.</i> <b>15</b>, e1006897 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1006897" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1006897" aria-label="Article reference 48" data-doi="10.1371/journal.pcbi.1006897">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1MXhtlWls7fI" aria-label="CAS reference 48">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31013278" aria-label="PubMed reference 48">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6499433" aria-label="PubMed Central reference 48">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20convolutional%20models%20improve%20predictions%20of%20macaque%20V1%20responses%20to%20natural%20images&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1006897&amp;volume=15&amp;publication_year=2019&amp;author=Cadena%2CSA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">Wang, A., Tarr, M. &amp; Wehbe, L. Neural Taskonomy: Inferring the Similarity of Task-Derived Representations from Brain Activity. In <i>Advances in Neural Information Processing Systems 32</i> <a href="https://papers.nips.cc/paper/2019/hash/f490c742cd8318b8ee6dca10af2a163f-Abstract.html" data-track="click" data-track-action="external reference" data-track-label="https://papers.nips.cc/paper/2019/hash/f490c742cd8318b8ee6dca10af2a163f-Abstract.html">https://papers.nips.cc/paper/2019/hash/f490c742cd8318b8ee6dca10af2a163f-Abstract.html</a>, 1547515485 (2019).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">Sinz, F. H., Pitkow, X., Reimer, J., Bethge, M. &amp; Tolias, A. S. Engineering a less artificial intelligence. <i>Neuron</i> <b>103</b>, 967979 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2019.08.034" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2019.08.034" aria-label="Article reference 50" data-doi="10.1016/j.neuron.2019.08.034">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1MXhvVOlsbrN" aria-label="CAS reference 50">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31557461" aria-label="PubMed reference 50">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Engineering%20a%20less%20artificial%20intelligence&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2019.08.034&amp;volume=103&amp;pages=967-979&amp;publication_year=2019&amp;author=Sinz%2CFH&amp;author=Pitkow%2CX&amp;author=Reimer%2CJ&amp;author=Bethge%2CM&amp;author=Tolias%2CAS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">Aliko, S., Huang, J., Gheorghiu, F., Meliss, S. &amp; Skipper, J. I. A naturalistic neuroimaging database for understanding the brain using ecological stimuli. <i>Sci. Data</i> <b>7</b>, 347 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41597-020-00680-2" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41597-020-00680-2" aria-label="Article reference 51" data-doi="10.1038/s41597-020-00680-2">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33051448" aria-label="PubMed reference 51">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7555491" aria-label="PubMed Central reference 51">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20naturalistic%20neuroimaging%20database%20for%20understanding%20the%20brain%20using%20ecological%20stimuli&amp;journal=Sci.%20Data&amp;doi=10.1038%2Fs41597-020-00680-2&amp;volume=7&amp;publication_year=2020&amp;author=Aliko%2CS&amp;author=Huang%2CJ&amp;author=Gheorghiu%2CF&amp;author=Meliss%2CS&amp;author=Skipper%2CJI">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">Nastase, S. A., Liu, Y.-F., Hillman, H., Norman, K. A. &amp; Hasson, U. Leveraging shared connectivity to aggregate heterogeneous datasets into a common response space. <i>Neuroimage</i> <b>217</b>, 116865 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2020.116865" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2020.116865" aria-label="Article reference 52" data-doi="10.1016/j.neuroimage.2020.116865">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32325212" aria-label="PubMed reference 52">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Leveraging%20shared%20connectivity%20to%20aggregate%20heterogeneous%20datasets%20into%20a%20common%20response%20space&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2020.116865&amp;volume=217&amp;publication_year=2020&amp;author=Nastase%2CSA&amp;author=Liu%2CY-F&amp;author=Hillman%2CH&amp;author=Norman%2CKA&amp;author=Hasson%2CU">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">Taylor, J. R. et al. The cambridge centre for ageing and neuroscience (Cam-CAN) data repository: structural and functional MRI, MEG, and cognitive data from a cross-sectional adult lifespan sample. <i>Neuroimage</i> <b>144</b>, 262269 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2015.09.018" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2015.09.018" aria-label="Article reference 53" data-doi="10.1016/j.neuroimage.2015.09.018">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26375206" aria-label="PubMed reference 53">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20cambridge%20centre%20for%20ageing%20and%20neuroscience%20%28Cam-CAN%29%20data%20repository%3A%20structural%20and%20functional%20MRI%2C%20MEG%2C%20and%20cognitive%20data%20from%20a%20cross-sectional%20adult%20lifespan%20sample&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2015.09.018&amp;volume=144&amp;pages=262-269&amp;publication_year=2017&amp;author=Taylor%2CJR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR54">Bellec, P. &amp; Boyle, J. A. Bridging the gap between perception and action: the case for neuroimaging, AI and video games. Preprint at <a href="https://psyarxiv.com/3epws" data-track="click" data-track-action="external reference" data-track-label="https://psyarxiv.com/3epws">https://psyarxiv.com/3epws</a> (2019).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55."><p class="c-article-references__text" id="ref-CR55">Pinho, A. L. et al. Individual Brain Charting, a high-resolution fMRI dataset for cognitive mapping. <i>Sci. Data</i> <b>5</b>, 180105 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/sdata.2018.105" data-track-action="article reference" href="https://doi.org/10.1038%2Fsdata.2018.105" aria-label="Article reference 55" data-doi="10.1038/sdata.2018.105">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29893753" aria-label="PubMed reference 55">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5996851" aria-label="PubMed Central reference 55">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Individual%20Brain%20Charting%2C%20a%20high-resolution%20fMRI%20dataset%20for%20cognitive%20mapping&amp;journal=Sci.%20Data&amp;doi=10.1038%2Fsdata.2018.105&amp;volume=5&amp;publication_year=2018&amp;author=Pinho%2CAL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56."><p class="c-article-references__text" id="ref-CR56">Poldrack, R. A. et al. Long-term neural and physiological phenotyping of a single human. <i>Nat. Commun.</i> <b>6</b>, 8885 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/ncomms9885" data-track-action="article reference" href="https://doi.org/10.1038%2Fncomms9885" aria-label="Article reference 56" data-doi="10.1038/ncomms9885">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2MXhvFyktrjM" aria-label="CAS reference 56">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26648521" aria-label="PubMed reference 56">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Long-term%20neural%20and%20physiological%20phenotyping%20of%20a%20single%20human&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fncomms9885&amp;volume=6&amp;publication_year=2015&amp;author=Poldrack%2CRA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57."><p class="c-article-references__text" id="ref-CR57">Seeliger, K., Sommers, R. P., Gl, U., Bosch, S. E. &amp; van Gerven, M. A. J. A large single-participant fMRI dataset for probing brain responses to naturalistic stimuli in space and time. Preprint at <a href="https://www.biorxiv.org/content/10.1101/687681v1" data-track="click" data-track-action="external reference" data-track-label="https://www.biorxiv.org/content/10.1101/687681v1">https://www.biorxiv.org/content/10.1101/687681v1</a> (2019).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58."><p class="c-article-references__text" id="ref-CR58">Naselaris, T., Allen, E. &amp; Kay, K. Extensive sampling for complete models of individual brains. <i>Curr. Opin. Behav. Sci.</i> <b>40</b>, 4551 (2021).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.cobeha.2020.12.008" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cobeha.2020.12.008" aria-label="Article reference 58" data-doi="10.1016/j.cobeha.2020.12.008">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Extensive%20sampling%20for%20complete%20models%20of%20individual%20brains&amp;journal=Curr.%20Opin.%20Behav.%20Sci.&amp;doi=10.1016%2Fj.cobeha.2020.12.008&amp;volume=40&amp;pages=45-51&amp;publication_year=2021&amp;author=Naselaris%2CT&amp;author=Allen%2CE&amp;author=Kay%2CK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59."><p class="c-article-references__text" id="ref-CR59">Polimeni, J. R., Renvall, V., Zaretskaya, N. &amp; Fischl, B. Analysis strategies for high-resolution UHF-fMRI data. <i>Neuroimage</i> <b>168</b>, 296320 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2017.04.053" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2017.04.053" aria-label="Article reference 59" data-doi="10.1016/j.neuroimage.2017.04.053">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28461062" aria-label="PubMed reference 59">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Analysis%20strategies%20for%20high-resolution%20UHF-fMRI%20data&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2017.04.053&amp;volume=168&amp;pages=296-320&amp;publication_year=2018&amp;author=Polimeni%2CJR&amp;author=Renvall%2CV&amp;author=Zaretskaya%2CN&amp;author=Fischl%2CB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60."><p class="c-article-references__text" id="ref-CR60">Harms, M. P. et al. Extending the Human Connectome Project across ages: imaging protocols for the Lifespan Development and Aging projects. <i>Neuroimage</i> <b>183</b>, 972984 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2018.09.060" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2018.09.060" aria-label="Article reference 60" data-doi="10.1016/j.neuroimage.2018.09.060">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30261308" aria-label="PubMed reference 60">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Extending%20the%20Human%20Connectome%20Project%20across%20ages%3A%20imaging%20protocols%20for%20the%20Lifespan%20Development%20and%20Aging%20projects&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2018.09.060&amp;volume=183&amp;pages=972-984&amp;publication_year=2018&amp;author=Harms%2CMP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61."><p class="c-article-references__text" id="ref-CR61">Power, J. D. et al. Customized head molds reduce motion during resting state fMRI scans. <i>Neuroimage</i> <b>189</b>, 141149 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2019.01.016" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2019.01.016" aria-label="Article reference 61" data-doi="10.1016/j.neuroimage.2019.01.016">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30639840" aria-label="PubMed reference 61">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Customized%20head%20molds%20reduce%20motion%20during%20resting%20state%20fMRI%20scans&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2019.01.016&amp;volume=189&amp;pages=141-149&amp;publication_year=2019&amp;author=Power%2CJD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62."><p class="c-article-references__text" id="ref-CR62">Brainard, D. H. The Psychophysics Toolbox. <i>Spat. Vis.</i> <b>10</b>, 433436 (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1163/156856897X00357" data-track-action="article reference" href="https://doi.org/10.1163%2F156856897X00357" aria-label="Article reference 62" data-doi="10.1163/156856897X00357">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK2szitVSlug%3D%3D" aria-label="CAS reference 62">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9176952" aria-label="PubMed reference 62">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Psychophysics%20Toolbox&amp;journal=Spat.%20Vis.&amp;doi=10.1163%2F156856897X00357&amp;volume=10&amp;pages=433-436&amp;publication_year=1997&amp;author=Brainard%2CDH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63."><p class="c-article-references__text" id="ref-CR63">Pelli, D. G. The VideoToolbox software for visual psychophysics: transforming numbers into movies. <i>Spat. Vis.</i> <b>10</b>, 437442 (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1163/156856897X00366" data-track-action="article reference" href="https://doi.org/10.1163%2F156856897X00366" aria-label="Article reference 63" data-doi="10.1163/156856897X00366">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK2szitVSluw%3D%3D" aria-label="CAS reference 63">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9176953" aria-label="PubMed reference 63">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 63" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20VideoToolbox%20software%20for%20visual%20psychophysics%3A%20transforming%20numbers%20into%20movies&amp;journal=Spat.%20Vis.&amp;doi=10.1163%2F156856897X00366&amp;volume=10&amp;pages=437-442&amp;publication_year=1997&amp;author=Pelli%2CDG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64."><p class="c-article-references__text" id="ref-CR64">Caesar, H., Uijlings, J. &amp; Ferrari, V. COCO-Stuff: Thing and Stuff classes in context. In <i>IEEE/CVF Conf. Computer Vision and Pattern Recognition</i> <a href="https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00132" data-track="click" data-track-action="external reference" data-track-label="https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00132">https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00132</a> 12091218 (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65."><p class="c-article-references__text" id="ref-CR65">Schira, M. M., Tyler, C. W., Breakspear, M. &amp; Spehar, B. The foveal confluence in human visual cortex. <i>J. Neurosci.</i> <b>29</b>, 90509058 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.1760-09.2009" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.1760-09.2009" aria-label="Article reference 65" data-doi="10.1523/JNEUROSCI.1760-09.2009">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXovFegurk%3D" aria-label="CAS reference 65">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19605642" aria-label="PubMed reference 65">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6665445" aria-label="PubMed Central reference 65">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20foveal%20confluence%20in%20human%20visual%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.1760-09.2009&amp;volume=29&amp;pages=9050-9058&amp;publication_year=2009&amp;author=Schira%2CMM&amp;author=Tyler%2CCW&amp;author=Breakspear%2CM&amp;author=Spehar%2CB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66."><p class="c-article-references__text" id="ref-CR66">Shahid, A., Wilkinson, K., Marcu, S. &amp; Shapiro, C. M. Stanford Sleepiness Scale (SSS). In: <i>STOP, THAT and One Hundred Other Sleep Scales</i> (eds. Shahid, A., Wilkinson, K., Marcu, S. &amp; Shapiro, C. M.) 369370 (Springer, 2012).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67."><p class="c-article-references__text" id="ref-CR67">Marks, D. F. Visual imagery differences in the recall of pictures. <i>Br. J. Psychol.</i> <b>64</b>, 1724 (1973).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1111/j.2044-8295.1973.tb01322.x" data-track-action="article reference" href="https://doi.org/10.1111%2Fj.2044-8295.1973.tb01322.x" aria-label="Article reference 67" data-doi="10.1111/j.2044-8295.1973.tb01322.x">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaE2c%2FgsVyltQ%3D%3D" aria-label="CAS reference 67">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=4742442" aria-label="PubMed reference 67">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 67" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20imagery%20differences%20in%20the%20recall%20of%20pictures&amp;journal=Br.%20J.%20Psychol.&amp;doi=10.1111%2Fj.2044-8295.1973.tb01322.x&amp;volume=64&amp;pages=17-24&amp;publication_year=1973&amp;author=Marks%2CDF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68."><p class="c-article-references__text" id="ref-CR68">Torgesen, J. K., Wagner, R. &amp; Rashotte, C. <i>TOWRE-2: Test of Word Reading Efficiency</i> (Pearson, 2012).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69."><p class="c-article-references__text" id="ref-CR69">Duchaine, B. &amp; Nakayama, K. The Cambridge Face Memory Test: results for neurologically intact individuals and an investigation of its validity using inverted face stimuli and prosopagnosic participants. <i>Neuropsychologia</i> <b>44</b>, 576585 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuropsychologia.2005.07.001" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuropsychologia.2005.07.001" aria-label="Article reference 69" data-doi="10.1016/j.neuropsychologia.2005.07.001">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16169565" aria-label="PubMed reference 69">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 69" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Cambridge%20Face%20Memory%20Test%3A%20results%20for%20neurologically%20intact%20individuals%20and%20an%20investigation%20of%20its%20validity%20using%20inverted%20face%20stimuli%20and%20prosopagnosic%20participants&amp;journal=Neuropsychologia&amp;doi=10.1016%2Fj.neuropsychologia.2005.07.001&amp;volume=44&amp;pages=576-585&amp;publication_year=2006&amp;author=Duchaine%2CB&amp;author=Nakayama%2CK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70."><p class="c-article-references__text" id="ref-CR70">Tardif, J., Watson, M., Giaschi, D. &amp; Gosselin, F. Measuring the contrast sensitivity function in just three clicks. <i>J. Vis.</i> <b>16</b>, 966966 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1167/16.12.966" data-track-action="article reference" href="https://doi.org/10.1167%2F16.12.966" aria-label="Article reference 70" data-doi="10.1167/16.12.966">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20the%20contrast%20sensitivity%20function%20in%20just%20three%20clicks&amp;journal=J.%20Vis.&amp;doi=10.1167%2F16.12.966&amp;volume=16&amp;pages=966-966&amp;publication_year=2016&amp;author=Tardif%2CJ&amp;author=Watson%2CM&amp;author=Giaschi%2CD&amp;author=Gosselin%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="71."><p class="c-article-references__text" id="ref-CR71">Arora, S., Liang, Y. &amp; Ma, T. A simple but tough-to-beat baseline for sentence embeddings. <a href="https://openreview.net/pdf?id=SyK00v5xx" data-track="click" data-track-action="external reference" data-track-label="https://openreview.net/pdf?id=SyK00v5xx">https://openreview.net/pdf?id=SyK00v5xx</a> (2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="72."><p class="c-article-references__text" id="ref-CR72">Kriegeskorte, N. &amp; Mur, M. Inverse MDS: inferring dissimilarity structure from multiple item arrangements. <i>Front. Psychol.</i> <b>3</b>, 245 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3389/fpsyg.2012.00245" data-track-action="article reference" href="https://doi.org/10.3389%2Ffpsyg.2012.00245" aria-label="Article reference 72" data-doi="10.3389/fpsyg.2012.00245">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22848204" aria-label="PubMed reference 72">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3404552" aria-label="PubMed Central reference 72">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 72" href="http://scholar.google.com/scholar_lookup?&amp;title=Inverse%20MDS%3A%20inferring%20dissimilarity%20structure%20from%20multiple%20item%20arrangements&amp;journal=Front.%20Psychol.&amp;doi=10.3389%2Ffpsyg.2012.00245&amp;volume=3&amp;publication_year=2012&amp;author=Kriegeskorte%2CN&amp;author=Mur%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="73."><p class="c-article-references__text" id="ref-CR73">Kay, K., Jamison, K. W., Zhang, R.-Y. &amp; Uurbil, K. A temporal decomposition method for identifying venous effects in task-based fMRI. <i>Nat. Methods</i> <b>17</b>, 10331039 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41592-020-0941-6" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41592-020-0941-6" aria-label="Article reference 73" data-doi="10.1038/s41592-020-0941-6">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3cXhslKhu7vE" aria-label="CAS reference 73">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32895538" aria-label="PubMed reference 73">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7721302" aria-label="PubMed Central reference 73">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20temporal%20decomposition%20method%20for%20identifying%20venous%20effects%20in%20task-based%20fMRI&amp;journal=Nat.%20Methods&amp;doi=10.1038%2Fs41592-020-0941-6&amp;volume=17&amp;pages=1033-1039&amp;publication_year=2020&amp;author=Kay%2CK&amp;author=Jamison%2CKW&amp;author=Zhang%2CR-Y&amp;author=U%C4%9Furbil%2CK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="74."><p class="c-article-references__text" id="ref-CR74">Avants, B. B. et al. A reproducible evaluation of ANTs similarity metric performance in brain image registration. <i>Neuroimage</i> <b>54</b>, 20332044 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2010.09.025" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2010.09.025" aria-label="Article reference 74" data-doi="10.1016/j.neuroimage.2010.09.025">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20851191" aria-label="PubMed reference 74">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 74" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20reproducible%20evaluation%20of%20ANTs%20similarity%20metric%20performance%20in%20brain%20image%20registration&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2010.09.025&amp;volume=54&amp;pages=2033-2044&amp;publication_year=2011&amp;author=Avants%2CBB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="75."><p class="c-article-references__text" id="ref-CR75">Yushkevich, P. A. et al. User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability. <i>Neuroimage</i> <b>31</b>, 11161128 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2006.01.015" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2006.01.015" aria-label="Article reference 75" data-doi="10.1016/j.neuroimage.2006.01.015">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16545965" aria-label="PubMed reference 75">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 75" href="http://scholar.google.com/scholar_lookup?&amp;title=User-guided%203D%20active%20contour%20segmentation%20of%20anatomical%20structures%3A%20significantly%20improved%20efficiency%20and%20reliability&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2006.01.015&amp;volume=31&amp;pages=1116-1128&amp;publication_year=2006&amp;author=Yushkevich%2CPA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="76."><p class="c-article-references__text" id="ref-CR76">Esteban, O. et al. fMRIPrep: a robust preprocessing pipeline for functional MRI. <i>Nat. Methods</i> <b>16</b>, 111116 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s41592-018-0235-4" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41592-018-0235-4" aria-label="Article reference 76" data-doi="10.1038/s41592-018-0235-4">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1cXisVyhurnN" aria-label="CAS reference 76">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30532080" aria-label="PubMed reference 76">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 76" href="http://scholar.google.com/scholar_lookup?&amp;title=fMRIPrep%3A%20a%20robust%20preprocessing%20pipeline%20for%20functional%20MRI&amp;journal=Nat.%20Methods&amp;doi=10.1038%2Fs41592-018-0235-4&amp;volume=16&amp;pages=111-116&amp;publication_year=2019&amp;author=Esteban%2CO">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="77."><p class="c-article-references__text" id="ref-CR77">Power, J. D., Barnes, K. A., Snyder, A. Z., Schlaggar, B. L. &amp; Petersen, S. E. Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion. <i>Neuroimage</i> <b>59</b>, 21422154 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2011.10.018" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2011.10.018" aria-label="Article reference 77" data-doi="10.1016/j.neuroimage.2011.10.018">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22019881" aria-label="PubMed reference 77">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 77" href="http://scholar.google.com/scholar_lookup?&amp;title=Spurious%20but%20systematic%20correlations%20in%20functional%20connectivity%20MRI%20networks%20arise%20from%20subject%20motion&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2011.10.018&amp;volume=59&amp;pages=2142-2154&amp;publication_year=2012&amp;author=Power%2CJD&amp;author=Barnes%2CKA&amp;author=Snyder%2CAZ&amp;author=Schlaggar%2CBL&amp;author=Petersen%2CSE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="78."><p class="c-article-references__text" id="ref-CR78">Handwerker, D. A., Gonzalez-Castillo, J., DEsposito, M. &amp; Bandettini, P. A. The continuing challenge of understanding and modeling hemodynamic variation in fMRI. <i>Neuroimage</i> <b>62</b>, 10171023 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2012.02.015" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2012.02.015" aria-label="Article reference 78" data-doi="10.1016/j.neuroimage.2012.02.015">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22366081" aria-label="PubMed reference 78">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 78" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20continuing%20challenge%20of%20understanding%20and%20modeling%20hemodynamic%20variation%20in%20fMRI&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2012.02.015&amp;volume=62&amp;pages=1017-1023&amp;publication_year=2012&amp;author=Handwerker%2CDA&amp;author=Gonzalez-Castillo%2CJ&amp;author=D%E2%80%99Esposito%2CM&amp;author=Bandettini%2CPA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="79."><p class="c-article-references__text" id="ref-CR79">Hoerl, A. E. &amp; Kennard, R. W. Ridge regression: Biased estimation for nonorthogonal problems. <i>Technometrics</i> <b>12</b>, 5567 (1970).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1080/00401706.1970.10488634" data-track-action="article reference" href="https://doi.org/10.1080%2F00401706.1970.10488634" aria-label="Article reference 79" data-doi="10.1080/00401706.1970.10488634">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 79" href="http://scholar.google.com/scholar_lookup?&amp;title=Ridge%20regression%3A%20Biased%20estimation%20for%20nonorthogonal%20problems&amp;journal=Technometrics&amp;doi=10.1080%2F00401706.1970.10488634&amp;volume=12&amp;pages=55-67&amp;publication_year=1970&amp;author=Hoerl%2CAE&amp;author=Kennard%2CRW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="80."><p class="c-article-references__text" id="ref-CR80">Kay, K. N., Winawer, J., Mezer, A. &amp; Wandell, B. Compressive spatial summation in human visual cortex. <i>J. Neurophysiol.</i> <b>110</b>, 481494 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.00105.2013" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.00105.2013" aria-label="Article reference 80" data-doi="10.1152/jn.00105.2013">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23615546" aria-label="PubMed reference 80">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3727075" aria-label="PubMed Central reference 80">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 80" href="http://scholar.google.com/scholar_lookup?&amp;title=Compressive%20spatial%20summation%20in%20human%20visual%20cortex&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.00105.2013&amp;volume=110&amp;pages=481-494&amp;publication_year=2013&amp;author=Kay%2CKN&amp;author=Winawer%2CJ&amp;author=Mezer%2CA&amp;author=Wandell%2CB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="81."><p class="c-article-references__text" id="ref-CR81">Lage-Castellanos, A., Valente, G., Formisano, E. &amp; De Martino, F. Methods for computing the maximum performance of computational models of fMRI responses. <i>PLoS Comput. Biol.</i> <b>15</b>, e1006397 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1006397" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1006397" aria-label="Article reference 81" data-doi="10.1371/journal.pcbi.1006397">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30849071" aria-label="PubMed reference 81">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6426260" aria-label="PubMed Central reference 81">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 81" href="http://scholar.google.com/scholar_lookup?&amp;title=Methods%20for%20computing%20the%20maximum%20performance%20of%20computational%20models%20of%20fMRI%20responses&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1006397&amp;volume=15&amp;publication_year=2019&amp;author=Lage-Castellanos%2CA&amp;author=Valente%2CG&amp;author=Formisano%2CE&amp;author=Martino%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="82."><p class="c-article-references__text" id="ref-CR82">Biswal, B., Yetkin, F. Z., Haughton, V. M. &amp; Hyde, J. S. Functional connectivity in the motor cortex of resting human brain using echo-planar MRI. <i>Magn. Reson. Med.</i> <b>34</b>, 537541 (1995).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1002/mrm.1910340409" data-track-action="article reference" href="https://doi.org/10.1002%2Fmrm.1910340409" aria-label="Article reference 82" data-doi="10.1002/mrm.1910340409">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK287gslKrsg%3D%3D" aria-label="CAS reference 82">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8524021" aria-label="PubMed reference 82">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 82" href="http://scholar.google.com/scholar_lookup?&amp;title=Functional%20connectivity%20in%20the%20motor%20cortex%20of%20resting%20human%20brain%20using%20echo-planar%20MRI&amp;journal=Magn.%20Reson.%20Med.&amp;doi=10.1002%2Fmrm.1910340409&amp;volume=34&amp;pages=537-541&amp;publication_year=1995&amp;author=Biswal%2CB&amp;author=Yetkin%2CFZ&amp;author=Haughton%2CVM&amp;author=Hyde%2CJS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="83."><p class="c-article-references__text" id="ref-CR83">Nili, H. et al. A toolbox for representational similarity analysis. <i>PLoS Comput. Biol.</i> <b>10</b>, e1003553 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1003553" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1003553" aria-label="Article reference 83" data-doi="10.1371/journal.pcbi.1003553">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24743308" aria-label="PubMed reference 83">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3990488" aria-label="PubMed Central reference 83">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 83" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20toolbox%20for%20representational%20similarity%20analysis&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1003553&amp;volume=10&amp;publication_year=2014&amp;author=Nili%2CH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="84."><p class="c-article-references__text" id="ref-CR84">Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysisconnecting the branches of systems neuroscience. <i>Front. Syst. Neurosci.</i> <b>2</b>, 4 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19104670" aria-label="PubMed reference 84">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405" aria-label="PubMed Central reference 84">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 84" href="http://scholar.google.com/scholar_lookup?&amp;title=Representational%20similarity%20analysis%E2%80%94connecting%20the%20branches%20of%20systems%20neuroscience&amp;journal=Front.%20Syst.%20Neurosci.&amp;volume=2&amp;publication_year=2008&amp;author=Kriegeskorte%2CN&amp;author=Mur%2CM&amp;author=Bandettini%2CP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="85."><p class="c-article-references__text" id="ref-CR85">Pedregosa, F. et al. Scikit-learn: machine learning in Python. <i>J. Mach. Learn. Res.</i> <b>12</b>, 28252830 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 85" href="http://scholar.google.com/scholar_lookup?&amp;title=Scikit-learn%3A%20machine%20learning%20in%20Python&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=12&amp;pages=2825-2830&amp;publication_year=2011&amp;author=Pedregosa%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="86."><p class="c-article-references__text" id="ref-CR86">Gorgolewski, K. J. et al. The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. <i>Sci. Data</i> <b>3</b>, 19 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/sdata.2016.44" data-track-action="article reference" href="https://doi.org/10.1038%2Fsdata.2016.44" aria-label="Article reference 86" data-doi="10.1038/sdata.2016.44">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 86" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20brain%20imaging%20data%20structure%2C%20a%20format%20for%20organizing%20and%20describing%20outputs%20of%20neuroimaging%20experiments&amp;journal=Sci.%20Data&amp;doi=10.1038%2Fsdata.2016.44&amp;volume=3&amp;pages=1-9&amp;publication_year=2016&amp;author=Gorgolewski%2CKJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="87."><p class="c-article-references__text" id="ref-CR87">Cichy, R. M., Roig, G. &amp; Oliva, A. The Algonauts Project. <i>Nat. Mach. Intell.</i> <b>1</b>, 613 (2019).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/s42256-019-0127-z" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42256-019-0127-z" aria-label="Article reference 87" data-doi="10.1038/s42256-019-0127-z">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 87" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Algonauts%20Project&amp;journal=Nat.%20Mach.%20Intell.&amp;doi=10.1038%2Fs42256-019-0127-z&amp;volume=1&amp;publication_year=2019&amp;author=Cichy%2CRM&amp;author=Roig%2CG&amp;author=Oliva%2CA">
                    Google Scholar</a>
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-021-00962-x?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank the NSD participants for their time and endurance; E. Aminoff, J. Pyles, M. Tarr, M. Hebart and C. Baker for advice on experimental design and data collection; J. Power and A. Schapiro for consultation on resting-state and physiological data; V. Carr and R. Olsen for consultation on hippocampal subfield scanning protocols; A. Grant for assistance with scanner peripherals; F. Gosselin and J. Tardif for contrast sensitivity analysis; B. Klimes-Dougan and K. Cullen for designing the valence/arousal assessment; W. Guo for segmentations of the medial temporal lobe; M. Arcaro, A. Bratch, D. Finzi, A. White and J. Winawer for assistance with ROI definition; C. Gorgolewski and R. Poldrack for discussion of BIDS and data sharing; R. Cichy, E. Yacoub, K. Grill-Spector, K. Jamison, A. Rokem, A. Huth, S. Anzellotti, N. Kriegeskorte and J. Winawer for general discussions; and K. Ugurbil for overall project advice. We also thank our NSD collaborators for shaping the trajectory of the project. This work was supported by NSF CRCNS grants IIS-1822683 (K.K.) and IIS-1822929 (T.N.); NIH grants P41 EB015894, P30 NS076408, S10 RR026783 and S10 OD017974-01, the W. M. Keck Foundation and the NIMH Intramural Research Program ZIAMH002909 (M.N.); and NSF BCS-1734853, NIH NIBIB R01EB030896, NIH NIBIB R01EB029272 and NIH IIS-1912270 (F.P.).</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><span class="c-article-author-information__subtitle u-visually-hidden" id="author-notes">Author notes</span><ol class="c-article-author-information__list"><li class="c-article-author-information__item" id="nAff17"><p class="c-article-author-information__authors-list">Ghislain St-Yves&amp;Thomas Naselaris</p><p class="js-present-address">Present address: Department of Neuroscience, University of Minnesota, Minneapolis, MN, USA</p></li><li class="c-article-author-information__item" id="nAff18"><p class="c-article-author-information__authors-list">Jesse L. Breedlove</p><p class="js-present-address">Present address: Department of Psychology, University of Minnesota, Minneapolis, MN, USA</p></li><li class="c-article-author-information__item" id="nAff19"><p class="c-article-author-information__authors-list">Jacob S. Prince</p><p class="js-present-address">Present address: Department of Psychology, Harvard University, Cambridge, MA, USA</p></li><li class="c-article-author-information__item" id="na1"><p>These authors jointly supervised this work: Thomas Naselaris, Kendrick Kay.</p></li></ol><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Center for Magnetic Resonance Research (CMRR), Department of Radiology, University of Minnesota, Minneapolis, MN, USA</p><p class="c-article-author-affiliation__authors-list">Emily J. Allen&amp;Kendrick Kay</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Psychology, University of Minnesota, Minneapolis, MN, USA</p><p class="c-article-author-affiliation__authors-list">Emily J. Allen</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Neuroscience, Medical University of South Carolina, Charleston, SC, USA</p><p class="c-article-author-affiliation__authors-list">Ghislain St-Yves,Jesse L. Breedlove&amp;Thomas Naselaris</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Graduate Program in Cognitive Science, University of Minnesota, Minneapolis, MN, USA</p><p class="c-article-author-affiliation__authors-list">Yihan Wu</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">Department of Psychology, Carnegie Mellon University, Pittsburgh, PA, USA</p><p class="c-article-author-affiliation__authors-list">Jacob S. Prince</p></li><li id="Aff6"><p class="c-article-author-affiliation__address">Department of Neuroscience, Center for Magnetic Resonance Research (CMRR), University of Minnesota, Minneapolis, MN, USA</p><p class="c-article-author-affiliation__authors-list">Logan T. Dowdle</p></li><li id="Aff7"><p class="c-article-author-affiliation__address">Department of Neurosurgery, Center for Magnetic Resonance Research (CMRR), University of Minnesota, Minneapolis, MN, USA</p><p class="c-article-author-affiliation__authors-list">Logan T. Dowdle</p></li><li id="Aff8"><p class="c-article-author-affiliation__address">National Institute of Mental Health (NIMH), Bethesda MD, USA</p><p class="c-article-author-affiliation__authors-list">Matthias Nau</p></li><li id="Aff9"><p class="c-article-author-affiliation__address">Program in Neuroscience, Indiana University, Bloomington IN, USA</p><p class="c-article-author-affiliation__authors-list">Brad Caron</p></li><li id="Aff10"><p class="c-article-author-affiliation__address">Program in Vision Science, Indiana University, Bloomington IN, USA</p><p class="c-article-author-affiliation__authors-list">Brad Caron</p></li><li id="Aff11"><p class="c-article-author-affiliation__address">Department of Psychology, University of Texas at Austin, Austin, TX, USA</p><p class="c-article-author-affiliation__authors-list">Franco Pestilli</p></li><li id="Aff12"><p class="c-article-author-affiliation__address">Center for Perceptual Systems, University of Texas at Austin, Austin, TX, USA</p><p class="c-article-author-affiliation__authors-list">Franco Pestilli</p></li><li id="Aff13"><p class="c-article-author-affiliation__address">Institute for Neuroscience, University of Texas at Austin, Austin, TX, USA</p><p class="c-article-author-affiliation__authors-list">Franco Pestilli</p></li><li id="Aff14"><p class="c-article-author-affiliation__address">Center for Human Brain Health, School of Psychology, University of Birmingham, Birmingham, UK</p><p class="c-article-author-affiliation__authors-list">Ian Charest</p></li><li id="Aff15"><p class="c-article-author-affiliation__address">cerebrUM, Dpartement de Psychologie, Universit de Montral, Montral QC, Canada</p><p class="c-article-author-affiliation__authors-list">Ian Charest</p></li><li id="Aff16"><p class="c-article-author-affiliation__address">Department of Psychology, University of Oregon, Eugene, OR, USA</p><p class="c-article-author-affiliation__authors-list">J. Benjamin Hutchinson</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Emily_J_-Allen-Aff1-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Emily J. Allen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Emily%20J.%20Allen" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Emily%20J.%20Allen" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Emily%20J.%20Allen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Ghislain-St_Yves-Aff3-Aff17"><span class="c-article-authors-search__title u-h3 js-search-name">Ghislain St-Yves</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Ghislain%20St-Yves" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ghislain%20St-Yves" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ghislain%20St-Yves%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Yihan-Wu-Aff4"><span class="c-article-authors-search__title u-h3 js-search-name">Yihan Wu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Yihan%20Wu" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yihan%20Wu" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yihan%20Wu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Jesse_L_-Breedlove-Aff3-Aff18"><span class="c-article-authors-search__title u-h3 js-search-name">Jesse L. Breedlove</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Jesse%20L.%20Breedlove" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jesse%20L.%20Breedlove" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jesse%20L.%20Breedlove%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Jacob_S_-Prince-Aff5-Aff19"><span class="c-article-authors-search__title u-h3 js-search-name">Jacob S. Prince</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Jacob%20S.%20Prince" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jacob%20S.%20Prince" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jacob%20S.%20Prince%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Logan_T_-Dowdle-Aff6-Aff7"><span class="c-article-authors-search__title u-h3 js-search-name">Logan T. Dowdle</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Logan%20T.%20Dowdle" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Logan%20T.%20Dowdle" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Logan%20T.%20Dowdle%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Matthias-Nau-Aff8"><span class="c-article-authors-search__title u-h3 js-search-name">Matthias Nau</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Matthias%20Nau" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Matthias%20Nau" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Matthias%20Nau%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Brad-Caron-Aff9-Aff10"><span class="c-article-authors-search__title u-h3 js-search-name">Brad Caron</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Brad%20Caron" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Brad%20Caron" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Brad%20Caron%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Franco-Pestilli-Aff11-Aff12-Aff13"><span class="c-article-authors-search__title u-h3 js-search-name">Franco Pestilli</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Franco%20Pestilli" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Franco%20Pestilli" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Franco%20Pestilli%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Ian-Charest-Aff14-Aff15"><span class="c-article-authors-search__title u-h3 js-search-name">Ian Charest</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Ian%20Charest" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ian%20Charest" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ian%20Charest%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-J__Benjamin-Hutchinson-Aff16"><span class="c-article-authors-search__title u-h3 js-search-name">J. Benjamin Hutchinson</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=J.%20Benjamin%20Hutchinson" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=J.%20Benjamin%20Hutchinson" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22J.%20Benjamin%20Hutchinson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Thomas-Naselaris-Aff3-Aff17"><span class="c-article-authors-search__title u-h3 js-search-name">Thomas Naselaris</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Thomas%20Naselaris" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Thomas%20Naselaris" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Thomas%20Naselaris%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Kendrick-Kay-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Kendrick Kay</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Kendrick%20Kay" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kendrick%20Kay" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kendrick%20Kay%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>E.J.A. collected the neuroimaging data, coordinated the data collection effort and performed manual brain segmentations. G.S.-Y. performed neural network analyses. Y.W. performed participant recruitment, assisted with scanning and prepared eye-tracking videos. J.L.B. assisted in data analysis. J.S.P. performed the equivalent trials analysis on the NSD and BOLD5000. L.T.D. organized and prepared data in BIDS format. M.N. analyzed the eye-tracking data. B.C. and F.P. analyzed the diffusion data. I.C. performed representational similarity analyses. J.B.H. analyzed the behavioral data. K.K. and T.N. conceived of the project and designed the main experiment. J.B.H. and I.C. designed the nsdmeadows and nsdmemory behavioral assessments. K.K. developed analysis methods, analyzed the neuroimaging data and directed the overall project. K.K., T.N., E.J.A., M.N., B.C., F.P., I.C. and J.B.H. wrote the paper. All authors discussed and edited the manuscript.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:kay@umn.edu">Kendrick Kay</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading" id="FPar3">Competing interests</h3>
                <p>The authors declare no competing financial interests.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p><b>Peer review information</b> <i>Nature Neuroscience</i> thanks Evan Gordon, Andrew Zalesky, and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p><p><b>Publishers note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Extended data"><div class="c-article-section" id="Sec54-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec54">Extended data</h2><div class="c-article-section__content" id="Sec54-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig7"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 1 design of the nsd experiment." href="/articles/s41593-021-00962-x/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig7_ESM.jpg">Extended Data Fig. 1 Design of the NSD experiment.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, Image presentations. Each of 10,000 distinct images was placed 3 times on a circle according to a probability distribution created by mixing a relatively narrow von Mises distribution and a uniform distribution. The resulting image sequence was divided into 40 equally-sized segments for the 40 NSD scan sessions. <b>b</b>, Basic statistics of image repetitions. We define <i>novel trial</i> as a trial involving an image never shown before, <i>old trial</i> as a trial that is not a novel trial, and <i>easy trial</i> as an old trial for which the presented image had been shown previously in the same scan session.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig8"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 2 overview of data collection." href="/articles/s41593-021-00962-x/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig8_ESM.jpg">Extended Data Fig. 2 Overview of data collection.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>This table summarizes the overall NSD data collection effort. Structural and diffusion MRI data were collected at 3T. Functional MRI data were collected at 7T. The breakdown of the 7T fMRI scan sessions is indicated: for example, subject 2 participated in 1 (prffloc) + 40 (nsd01nsd40) + 1 (nsdsynthetic) + 1 (nsdimagery) = 43 7T fMRI scan sessions. Additional behavioral data were acquired outside of the scanner (nsdpostbehavior, nsdmemory, nsdmeadows). Note that scan sessions were occasionally split across multiple magnet entries (see aquamarine and yellow cells). For simplicity, we treat these cases as if they represent single scan sessions.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig9"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 3 overview of data analysis." href="/articles/s41593-021-00962-x/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig9_ESM.jpg">Extended Data Fig. 3 Overview of data analysis.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Analyses conducted in this paper can be divided into three parts. Part 1 consists of pre-processing, in which raw functional, anatomical, diffusion, and eyetracking data are transformed into various useful intermediate outcomes. In addition, coordinate transformations between various spaces are estimated and incorporated into the nsd_mapdata utility. Part 2 consists of analyses of the pre-processed fMRI data. The GLMsingle algorithm introduced in this paper is used to analyze the fMRI data from the NSD experiment (Part 2a), and standard methods are used to analyze the fMRI data from the pRF and fLoc experiments (Part 2b). Part 3 consists of specific scientific analyses demonstrated in this paper that make use of the data prepared in Parts 1 and 2. Given the extensive data preparation procedures (Parts 12), it is useful to comment on which aspects are fairly typical in MRI processing and which are more customized or unique to the present work. With respect to the pre-processing steps in Part 1, the general outcomes that these steps achieve are typical in MRI and are necessary for basic interpretation of the data. For example, small shifts in head position over the course of a scan session necessitate some motion compensation in order to interpret the signal from a given voxel in terms of a single brain location. The specific methods by which we execute these pre-processing steps may differ from what is performed in commonly used software packages (for example, SPM, FSL, AFNI). However, the outcomes are similar at a conceptual level: for example, the fMRI data are pre-processed using temporal interpolation of voxel-wise time-series data and spatial interpolation of brain volumes. With respect to the additional preparation procedures in Part 2, the procedures in Part 2b are fairly typical analyses used to functionally localize brain regions. More customized and unique to the present work are the procedures in Part 2a, which are designed to improve the accuracy of single-trial fMRI amplitude estimates. We provide evidence that these procedures do in fact perform as intended (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig3">3</a> and Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41593-021-00962-x#Fig14">8</a>).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig10"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 4 eyetracking results." href="/articles/s41593-021-00962-x/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig10_ESM.jpg">Extended Data Fig. 4 Eyetracking results.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, Pre-processing of eyetracking data. Blinks and tracking noise were removed, followed by linear detrending, median-centering, downsampling, and smoothing. Runs with less than 1/3 valid samples after these cleaning procedures were excluded from further analysis (see Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">5</a>). Shown are results for an example run (subject 1, nsd31 scan session, run 6). Pre-processing reduced noise without obscuring potential eye movements. <b>b</b>, Fraction of time during which deviation from central fixation was less than a specific threshold. Results are shown for a range of thresholds (left) and for a threshold of 1 (right). <b>c</b>, 2D histograms of gaze positions. The main images show histogram results on a linear scale; the inset images show results on a log scale. To summarize the results, we overlay a gray ellipse marking the central 90% of a multivariate 2D Gaussian distribution that has been fit to the gaze positions, as well as a blue circle containing 90% of the gaze positions. Both the parametric and non-parametric approaches yield similar results and indicate that gaze positions of all subjects clustered around central fixation. The level of precision varied across subjects. The number of usable eyetracking runs for each subject is indicated by the white text. <b>d</b>, Example of accurate fixation behavior (subject 1, nsd31 scan session, run 8). Shown are pre-processed vertical gaze coordinates (top left), normalized pupil area (bottom left), and a 2D scatter plot of gaze positions (right). <b>e</b>, Example of eye movements (subject 5, nsd29 scan session, run 11). Same format as <b>d</b>. Notice that eye movements manifest as staircase structure in the vertical gaze coordinates and as dispersed gaze positions in the scatter plot. <b>f</b>, Trial-wise time-resolved analysis. Relative to stimulus trial onsets, we plot the across-trial median deviation from central fixation (top), as well as the across-trial median pupil size after mean-centering the pupil size within each trial (bottom). Results for subjects 3 and 8 are not available for this analysis. Overall, the results show that subjects were able to maintain fixation most of the time: gaze positions were within 1 of central fixation 6897% of the time (see <b>b</b>). Three subjects are worth further discussion. Subject 4 exhibited eye movements after stimulus onset (see <b>f</b>, top); however, this is of minor concern given that these movements were small. Subject 5 exhibited more substantial eye movements (see <b>c</b>, <b>e</b>, and <b>f</b>); we suggest exclusion of this subject from analyses of the NSD fMRI data that are contingent on strict central fixation. Finally, while our results indicate fixation instability for subject 8 (see <b>b</b> and <b>c</b>), careful inspection of the eyetracking video recordings (available online) suggests this reflects pupil tracking noise rather than actual eye movements made by the subject.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig11"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 5 improvements in spatial detai" href="/articles/s41593-021-00962-x/figures/11" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig11_ESM.jpg">Extended Data Fig. 5 Improvements in spatial detail through upsampling.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, Comparison of approaches. For an example coronal slice in Subject 1, we compare the non-upsampled 1.8-mm preparation of the data (left), the upsampled 1-mm preparation of the data (right), and a version of the 1.8-mm results that has been post-hoc upsampled to 1-mm resolution to enable direct comparison (middle). Two quantities are shown: mean signal intensity and variance explained by an ON-OFF GLM model. <b>b</b>, Zoomed view of white rectangle marked in <b>a</b>. <b>c</b>, Profile view of blue dotted horizontal line marked in <b>b</b>. Error bars in the bottom plot indicate  1 SEM across 40 scan sessions (error bars are small and nearly invisible). <b>d</b>, Timecourse estimates for voxels marked by orange arrowheads at the bottom of <b>c</b>. Each colored trace corresponds to an estimate of the hemodynamic timecourse for a single voxel in one NSD scan session from the upsampled 1-mm data preparation. The beginning of the timecourses (first vertical line) corresponds to the onset of the 3-s image presentation. The results shown in this figure support the idea that the upsampled data preparation preserves fine-scale spatial detail that is lost (blurred away) under a non-upsampled data preparation. While the effects are small, preserving as much detail as possible may be critical for certain neuroscientific questions.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig12"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 6 reliable diffusion derivative" href="/articles/s41593-021-00962-x/figures/12" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig12_ESM.jpg">Extended Data Fig. 6 Reliable diffusion derivatives facilitate investigation of white-matter connectivity.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, Fractional anisotropy (FA). The left shows tractography and FA results for the optic radiation identified in subject 7. The right shows reliability of FA results for 61 white-matter tracts identified using the atlas from Bullock et al.<sup>114</sup> For other measures, see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">5ce</a>. <b>b</b>, Structural connectivity. Using 43 visual areas  2 hemispheres = 86 regions from the HCP-MMP1 atlas<sup>109</sup> (left), we construct group-average connectivity matrices indicating the density of fibers connecting pairs of regions (right). <b>c</b>, Quantitative summary. Each dot represents fiber density between a pair of regions (as in <b>b</b>). Dot colors reflect different region pairs but are otherwise arbitrary. Group-average results (main figure) and results for an individual subject (inset) are shown.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig13"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 7 regions of interest (rois) pr" href="/articles/s41593-021-00962-x/figures/13" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig13_ESM.jpg">Extended Data Fig. 7 Regions of interest (ROIs) provided with NSD.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>A variety of ROIs were defined based on auxiliary fMRI experiments (pRF, fLoc). In <b>a</b><b>c</b>, we show example results for subject 3, right hemisphere. <b>a</b>, Early visual areas. Results are shown on FreeSurfers sphere surface as well as in the 0.8-mm anatomical volume space. <b>b</b>, Eccentricity-based regions. Similar format to <b>a</b>. Note that the total stimulus extent is 8.4  8.4 in the pRF, fLoc, and NSD experiments. <b>c</b>, Face-selective regions. Regions were defined based on <i>t</i>-values computed for the contrast of faces against all other categories. Results are shown on FreeSurfers inflated surface as well as in the 0.8-mm anatomical space. <b>d</b>, Probabilistic maps of ROI locations. For each of three example ROIs, we map the location of the ROI in each subject to fsaverage and then compute, for each fsaverage vertex, the fraction of subjects labeled at that vertex. Notice there is reasonable consistency across subjects in fsaverage space.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig14"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 8 detailed visualization of nsd" href="/articles/s41593-021-00962-x/figures/14" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig14_ESM.jpg">Extended Data Fig. 8 Detailed visualization of NSD betas.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>We prepared three beta versions (b1, b2, b3) reflecting GLM analyses of increasing sophistication. <b>a</b>, Inspection of NSD betas. The full set of estimated single-trial responses (1.8-mm preparation, beta version b1) is shown for voxels in subject 1 right hemisphere region of interest (ROI) FFA-1 (fusiform face area subdivision 1). We observe horizontal stripes, indicative of gross variation in percent BOLD signal change across voxels. <b>b</b>, Zoomed view of one scan session. Shown are all three beta versions, as well as the result of <i>z</i>-scoring betas within each scan session (in general, we suggest that users may wish to <i>z</i>-score each voxels responses within each scan session in order to eliminate potential non-stationarities and to equalize units across voxels). The different beta versions generally resemble one another (left column), implying that the variations in GLM methods do not drastically change the data. Vertical stripes visible in the visualizations tend to decrease from b1 to b2, suggesting that fitting voxel-wise HRFs reduces artifacts. Vertical stripes also tend to decrease from b2 to b3, which might reflect the reduction of correlated noise achieved by GLMdenoise. <b>c</b>, Detailed inspection of one voxel. To assess the reliability of evoked responses, we group trials according to the image presented. The estimated signal standard deviation (<sub>signal</sub>) and noise standard deviation (<sub>noise</sub>) are illustrated at the right of each subplot. Notice that b2 and b3 reduce variability of betas across the 3 trials associated with each image. <b>d</b>, Response reliability. Here we plot single-trial responses observed in two example ROIs (1.8-mm preparation, beta version b2, right hemisphere FFA-1 and PPA (parahippocampal place area), response averaged across voxels in each ROI), showing the first 50 of the shared515 images. The left column shows responses for different trials in subject 1; the right column shows trial-averaged responses in different subjects. Lines connecting consecutive images are used to aid visualization but do not indicate specific temporal relationships between images. Thick black lines indicate the mean across trials (left) or subjects (right). Notice that reliability is reasonably high both within and across subjects. <b>e</b>, Quantitative summary. To summarize results shown in <b>d</b>, we plot the correlation between responses to the shared515 images across all trials and all subjects. Thin white horizontal and vertical lines separate different subjects (each having 3 trials). Notice there is high reliability within each ROI, and responses are highly dissimilar across ROIs. The strong off-diagonal elements (white arrows) indicate the presence of spatial noise correlations that occur on individual trials, which is typical in fMRI<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Henriksson, L., Khaligh-Razavi, S.-M., Kay, K. &amp; Kriegeskorte, N. Visual representations are dominated by intrinsic fluctuations correlated between areas. Neuroimage 114, 275286 (2015)." href="/articles/s41593-021-00962-x#ref-CR45" id="ref-link-section-d29682704e4134">45</a></sup>. Noise correlations likely reflect a combination of measurement noise (for example, head motion) and real neural activity variability (for example, arousal effects). In some cases, correlations are larger across subjects than within subjects; one explanation is that there is, to some degree, a common ROI representation and a noisy measurement of this representation obtained in one subject might actually be better correlated with a less noisy measurement of this representation obtained in a different subject. Also, the results indicate the existence of temporal ordering effects (for example, trial 1 in a given subject tends to be more correlated with trial 1 in other subjects as opposed to trials 2 or 3). This likely indicates the presence of adaptation- and/or memory-related effects in the NSD data, given that the temporal ordering of trials was fixed across subjects.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig15"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 9 angle and eccentricity estima" href="/articles/s41593-021-00962-x/figures/15" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig15_ESM.jpg">Extended Data Fig. 9 Angle and eccentricity estimates from the NSD data.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Here we show results from the analysis of the pRF experiment and results from an analogous analysis performed on trial-averaged NSD betas (see Supplementary Modeling Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41593-021-00962-x#MOESM1">1</a> for details). Each panel shows an occipital view of FreeSurfers sphere surface, and white lines indicate borders of visual areas V1hV4 (defined based on results of the pRF experiment). Angle and eccentricity estimates are plotted using the same colormaps as in Benson et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Benson, N. C. et al. The human connectome project 7 Tesla retinotopy dataset: description and population receptive field analysis. J. Vis. 18, 23 (2018)." href="/articles/s41593-021-00962-x#ref-CR30" id="ref-link-section-d29682704e4163">30</a></sup> We also plot the amount of time-series variance explained in the pRF data (variance relative to the mean signal level) and the amount of variance explained in the NSD betas (variance relative to 0% BOLD signal change). Clear retinotopic maps in early visual cortex are visible in the NSD results, including robust angle estimates even in foveal regions. In addition, there is high consistency of retinotopic estimates across the pRF and NSD datasets. There is some discrepancy in absolute eccentricity estimates at peripheral locations; this is likely due to technical differences in how modeling procedures behave for voxels near the stimulus edge.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig16"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 10 design of alexnet- and gnet-" href="/articles/s41593-021-00962-x/figures/16" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_Fig16_ESM.jpg">Extended Data Fig. 10 Design of AlexNet- and GNet-based encoding models.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>a</b>, Illustration of an encoding model that predicts brain activity in a given voxel (<i>r</i><sub><i>tv</i></sub>) in response to images (<i>x</i><sub><i>t</i></sub>). Images are passed to nonlinear feature extractors, <i></i><sub><i>l</i></sub> (trapezoids), that output feature maps (grey cuboids). Feature maps are grouped, passed through an element-wise nonlinearity, <i>f</i>(), and then multiplied pixel-wise by a spatial pooling field (<i>g</i><sup>1</sup>,,<i>g</i><sup><i>N</i></sup> where superscripts index distinct groups of feature maps) that determines the region of visual space that drives voxel activity. The weighted pixel values in each feature map are then summed, reducing each feature map to a scalar value. These scalar values are concatenated across all feature maps, forming a single feature vector that is passed through another element-wise nonlinearity (left black rectangle) and then weighted by a set of feature weights, <i>w</i> (right black rectangle), to yield predicted voxel activity. Note that for each type of encoding model (for example, AlexNet-based encoding model, GNet-based encoding model), the feature extractors are identical for all voxels, but the spatial pooling fields and feature weights are optimized and may vary across voxels. For the AlexNet-based encoding model, the feature extractors were pre-specified, the spatial pooling fields were optimized via line search, and the feature weights <i>w</i> were optimized via ridge regression. For the GNet-based encoding model, stochastic gradient descent with early stopping was used to optimize the parameters of the feature extractors <i></i><sub><i>l</i></sub>, the spatial pooling fields <i>g</i><sup>1</sup>,,<i>g</i><sup><i>N</i></sup>, and the feature weights <i>w</i>. <b>b</b>, Illustration of spatial pooling fields. For the AlexNet model, a single isotropic 2D Gaussian pooling field (middle) selected from a set of candidates (right) was applied to all feature maps. For the GNet model, an independent, flexible pooling field (left) was applied to each group of feature maps. Applying flexible pooling fields to AlexNet leads to lower prediction accuracy overall, so we present the version that uses isotropic 2D Gaussian fields. <b>c</b>, Comparative architecture of AlexNet and GNet. AlexNet and GNet are both deep convolutional neural networks, but differ in the types and sequencing of layers (rows of the table). The first three layers are the same for both networks and correspond to the first three layers of an AlexNet trained to classify objects in the ImageNet dataset. For both networks, these shared pre-filtering layers are followed by sequences of convolutional layers (rows labeled conv; values indicate feature depth and convolutional filter resolution; str = filter stride, pad = convolutional padding), max-pooling layers (maxpool), batch-normalization and weight-dropout layers (batchnorm + dropout), adaptive averaging layers (adaptive avg), and fully-connected layers (fully con.; value indicates number of units). Feature maps in the convolutional or fully connected layers (indicated by red arrows; resolution of the feature maps in parentheses) are used as predictors of brain activity in the context of an encoding model (see <b>a</b>).</p></div></div></div></div></div></section><section data-title="Supplementary information"><div class="c-article-section" id="Sec55-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec55">Supplementary information</h2><div class="c-article-section__content" id="Sec55-content"><div data-test="supplementary-info"><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary information" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Information</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Supplementary Notes 15, Supplementary Modeling Notes 1 and 2, Supplementary Figs.17, Supplementary Videos 110 and Supplementary References</p></div></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="reporting summary" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41593-021-00962-x/MediaObjects/41593_2021_962_MOESM2_ESM.pdf" data-supp-info-image="">Reporting Summary</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20massive%207T%20fMRI%20dataset%20to%20bridge%20cognitive%20neuroscience%20and%20artificial%20intelligence&amp;author=Emily%20J.%20Allen%20et%20al&amp;contentID=10.1038%2Fs41593-021-00962-x&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer%20Nature%20America%2C%20Inc.&amp;publication=1097-6256&amp;publicationDate=2021-12-16&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/s41593-021-00962-x" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41593-021-00962-x" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Allen, E.J., St-Yves, G., Wu, Y. <i>et al.</i> A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence.
                    <i>Nat Neurosci</i> <b>25</b>, 116126 (2022). https://doi.org/10.1038/s41593-021-00962-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-021-00962-x?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-03-01">01 March 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-10-12">12 October 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-12-16">16 December 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2022-01">January 2022</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/s41593-021-00962-x</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>

            
        <section>
            <div class="c-article-section js-article-section" id="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Elevating the field for applying neuroimaging to individual patients in psychiatry" href="https://doi.org/10.1038/s41398-024-02781-7">
                                        Elevating the field for applying neuroimaging to individual patients in psychiatry
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>David R. Roalf</li><li>Martijn Figee</li><li>Desmond J. Oathes</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Translational Psychiatry</i> (2024)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:What comparing deep neural networks can teach us about human vision" href="https://doi.org/10.1038/s42256-024-00789-8">
                                        What comparing deep neural networks can teach us about human vision
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Katja Seeliger</li><li>Martin N. Hebart</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Machine Intelligence</i> (2024)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Driving and suppressing the human language network using large language models" href="https://doi.org/10.1038/s41562-023-01783-7">
                                        Driving and suppressing the human language network using large language models
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Greta Tuckute</li><li>Aalok Sathe</li><li>Evelina Fedorenko</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Human Behaviour</i> (2024)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Functional neuroimaging as a catalyst for integrated neuroscience" href="https://doi.org/10.1038/s41586-023-06670-9">
                                        Functional neuroimaging as a catalyst for integrated neuroscience
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Emily S. Finn</li><li>Russell A. Poldrack</li><li>James M. Shine</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:A large-scale fMRI dataset for the visual processing of naturalistic scenes" href="https://doi.org/10.1038/s41597-023-02471-x">
                                        A large-scale fMRI dataset for the visual processing of naturalistic scenes
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Zhengxin Gong</li><li>Ming Zhou</li><li>Zonglei Zhen</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Scientific Data</i> (2023)</p>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </section>
    

            
        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="js-context-bar-sticky-point-desktop">
        

        
            <noscript>
                
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-021-00962-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </noscript>
            <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
                <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-right-column
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41593-021-00962-x.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </div>
        
    </div>

    
        
    

    
    
        <div class="c-article-associated-content__container">
            <section>
                <h2 class="c-article-associated-content__title u-mb-24">Associated content</h2>
                
                    
                    
                        <div class="u-full-height u-mb-24">
                            
    <article class="u-full-height c-card c-card--flush">
        <div class="c-card__layout u-full-height">
            <div class="c-card__body">
                <h3 class="c-card__title">
                    <a href="https://www.nature.com/articles/s41593-021-00966-7"
                       class="c-card__link u-link-inherit"
                       data-track="click"
                       data-track-action="view article"
                       data-track-category="associated content"
                       
                       data-track-label="news_and_views">A deeper look at vision and memory</a>
                </h3>
                
<ul data-test="author-list" class="c-author-list c-author-list--compact">
    <li>Thomas L. Botch</li><li>Caroline E. Robertson</li><li>Emily S. Finn</li>
</ul>

                
    <div class="c-card__section c-meta">
        
            <span class="c-meta__item">Nature Neuroscience</span>
        
        <span class="c-meta__item" data-test="article.type"><span class="c-meta__type">News &amp; Views</span></span>
        
        
            <time class="c-meta__item" datetime="2021-12-16">16 Dec 2021</time>
        
    </div>

            </div>
        </div>
    </article>


                        </div>
                    
                
            </section>
        </div>
        <script>
            window.dataLayer = window.dataLayer || [];
            window.dataLayer[0] = window.dataLayer[0] || {};
            window.dataLayer[0].content = window.dataLayer[0].content || {};
            window.dataLayer[0].content.associatedContentTypes = "news_and_views";
        </script>
    

    

    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>
                            
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=s41593-021-00962-x;doi=10.1038/s41593-021-00962-x;techmeta=36,57,59;subjmeta=116,1595,1723,2395,2613,2616,2618,2649,378,631;kwrd=Cortex,Neural+encoding,Object+vision,Perception">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=1671107380&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-021-00962-x%26doi%3D10.1038/s41593-021-00962-x%26techmeta%3D36,57,59%26subjmeta%3D116,1595,1723,2395,2613,2616,2618,2649,378,631%26kwrd%3DCortex,Neural+encoding,Object+vision,Perception">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=1671107380&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41593-021-00962-x%26doi%3D10.1038/s41593-021-00962-x%26techmeta%3D36,57,59%26subjmeta%3D116,1595,1723,2395,2613,2616,2618,2649,378,631%26kwrd%3DCortex,Neural+encoding,Object+vision,Perception"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>


    
        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-and-analysis"
                                   data-track="click"
                                   data-track-action="reviews &amp; analysis"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Reviews &amp; Analysis
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/video"
                                   data-track="click"
                                   data-track-action="videos"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Videos
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/current-issue"
                                   data-track="click"
                                   data-track-action="current issue"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Current issue
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>
                        
                    
                </ul>
                <ul class="c-header__list c-header__list--js-stack">
                    
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/natureneuro"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on Twitter
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;6"
                               rel="nofollow"
                               data-track="click"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/neuro.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </nav>
    
    
        
            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/aims"
                                   data-track="click"
                                   data-track-action="aims &amp; scope"
                                   data-track-label="link">
                                    Aims &amp; Scope
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-information"
                                   data-track="click"
                                   data-track-action="journal information"
                                   data-track-label="link">
                                    Journal Information
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-impact"
                                   data-track="click"
                                   data-track-action="journal metrics"
                                   data-track-label="link">
                                    Journal Metrics
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editors"
                                   data-track="click"
                                   data-track-action="about the editors"
                                   data-track-label="link">
                                    About the Editors
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/our-publishing-models"
                                   data-track="click"
                                   data-track-action="our publishing models"
                                   data-track-label="link">
                                    Our publishing models
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-values-statement"
                                   data-track="click"
                                   data-track-action="editorial values statement"
                                   data-track-label="link">
                                    Editorial Values Statement
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-policies"
                                   data-track="click"
                                   data-track-action="editorial policies"
                                   data-track-label="link">
                                    Editorial Policies
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/content"
                                   data-track="click"
                                   data-track-action="content types"
                                   data-track-label="link">
                                    Content Types
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/web-feeds"
                                   data-track="click"
                                   data-track-action="web feeds"
                                   data-track-label="link">
                                    Web Feeds
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/posters"
                                   data-track="click"
                                   data-track-action="posters"
                                   data-track-label="link">
                                    Posters
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="research cross-journal editorial team"
                                   data-track-label="link">
                                    Research Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="reviews cross-journal editorial team"
                                   data-track-label="link">
                                    Reviews Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        

        
            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/submission-guidelines"
                                   data-track="click"
                                   data-track-action="submission guidelines"
                                   data-track-label="link">
                                    Submission Guidelines
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/for-reviewers"
                                   data-track="click"
                                   data-track-action="for reviewers"
                                   data-track-label="link">
                                    For Reviewers
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="click"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://mts-nn.nature.com/cgi-bin/main.plex?form_type&#x3D;home&amp;from_idp&#x3D;1"
                                   data-track="click"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external>Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        
    


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">
                            
                                
                                    <option value="" selected>All journals</option>
                                    <option value="neuro">This journal</option>
                                
                            
                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        

        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
            

            <p class="c-meta u-ma-0 u-flex-shrink">
                <span class="c-meta__item">
                    Nature Neuroscience (<i>Nat Neurosci</i>)
                </span>
                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="onlineIssn">1546-1726</span> (online)
    </span>
    


                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="printIssn">1097-6256</span> (print)
    </span>
    

            </p>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/protocolexchange/"
                                                  data-track="click" data-track-action="protocol exchange"
                                                  data-track-label="link">Protocol Exchange</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natitaly"
                                                  data-track="click" data-track-action="nature Italy"
                                                  data-track-label="link">Nature Italy</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ko-kr"
                                                  data-track="click" data-track-action="nature korea"
                                                  data-track-label="link">Nature Korea</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>
            
        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-ea63208b81.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2024 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path id="shape" fill-rule="evenodd" clip-rule="evenodd" d="M1 3.78571C1 2.75867 1.85698 2 2.8209 2H6.1791C7.14302 2 8 2.75867 8 3.78571V4H11.1668C11.885 4 12.5585 4.42017 12.8494 5.07033C12.9893 4.98169 13.1425 4.91101 13.3056 4.86206L16.5222 3.89704C17.4454 3.62005 18.4843 4.10046 18.7794 5.08419L22.9256 18.9042C23.2207 19.8878 22.618 20.8608 21.6947 21.1378L18.4781 22.1029C17.5548 22.3799 16.516 21.8993 16.2209 20.9157L13.0001 10.1804V20.2143C13.0001 21.255 12.1231 22 11.1668 22H7.83346C7.54206 22 7.25803 21.9308 7.00392 21.8052C6.75263 21.9305 6.47077 22 6.1791 22H2.8209C1.85693 22 1 21.2412 1 20.2143V3.78571ZM3 4V15H6V4H3ZM3 20V17H6V20H3ZM18.0749 20.1358L17.2129 17.2623L20.0863 16.4002L20.9484 19.2737L18.0749 20.1358ZM19.5116 14.4846L16.6381 15.3466L14.0519 6.72624L16.9254 5.86416L19.5116 14.4846ZM8.00012 20L8.00012 6H11.0001L11.0001 20H8.00012Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</footer>




    

    

<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">

    
    <div class="c-site-messages__banner-large">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing" src="/static/images/logos/nature-briefing-logo-n150-white-d81c9da3ec.svg" width="250" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing</em> newsletter  what matters in science, free to your inbox daily.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/briefing" method="post" data-location="banner" data-track="submit||nature_briefing_sign_up" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="MainBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick">
                        <input type="hidden" value="false" name="marketing" id="marketing">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick">
                        <input type="hidden" value="MainBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">
                            
                            <input type="hidden" value="true" name="N:nature_briefing_daily" id="defaultNewsletter">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>

    
    <div class="c-site-messages__banner-small">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner">Sign up for Nature Briefing
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/s41593-021-00962-x&amp;format=js&amp;last_modified=2021-12-16" async></script>
<img src="/a8f3n6g8/article/s41593-021-00962-x" width="1" height="1" alt="" class="u-visually-hidden">
</body>
</html>