<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices | Nature Neuroscience</title>
    
        
<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/neuro.rss"/>


    
        

        <script id="save-data-connection-testing">
            function hasConnection() {
                return navigator.connection || navigator.mozConnection || navigator.webkitConnection || navigator.msConnection;
            }

            function createLink(src) {
                var preloadLink = document.createElement("link");
                preloadLink.rel = "preload";
                preloadLink.href = src;
                preloadLink.as = "font";
                preloadLink.type = "font/woff2";
                preloadLink.crossOrigin = "";
                document.head.insertBefore(preloadLink, document.head.firstChild);
            }

            var connectionDetail = {
                saveDataEnabled: false,
                slowConnection: false
            };

            var connection = hasConnection();
            if (connection) {
                connectionDetail.saveDataEnabled = connection.saveData;
                if (/\slow-2g|2g/.test(connection.effectiveType)) {
                    connectionDetail.slowConnection = true;
                }
            }

            if (!(connectionDetail.saveDataEnabled || connectionDetail.slowConnection)) {
                createLink("/static/fonts/HardingText-Regular-Web-cecd90984f.woff2");
            } else {
                document.documentElement.classList.add('save-data');
            }
        </script>
    

<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"attention;extrastriate-cortex;neural-encoding","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Nature Neuroscience","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article"}},"article":{"doi":"10.1038/nn.3574"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["Thomas C Sprague","John T Serences"],"publishedAt":1384041600,"publishedAtString":"2013-11-10","title":"Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"neuro","title":"nature neuroscience","volume":"16","issue":"12"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"nature-onwards-journey","active":false},{"name":"getftr-entitled","active":false},{"name":"paywall_recommendations","active":true}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"US","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://collect.nature.com',
        imprint: 'nature'
    });
</script>

<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
 



     
    
    
        
    
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}button:focus{outline:3px solid #fece3e;will-change:transform}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:8px}.c-recommendations-list__item .c-card{flex-basis:100%}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 0 0 16px;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #cedbe0;height:auto;min-height:0;position:static}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0 0 8px;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:inherit}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}.c-card__title-recommendation .MathJax_Display{display:inline!important}.c-card__link:not(.c-card__link--no-block-link):before{z-index:1}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a,.c-article-recommendations-card__link{color:inherit}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-article-recommendations-card__meta-type,.c-meta .c-meta__item:first-child{font-weight:700}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:539px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px 13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.c-pdf-download__link{padding:13px 24px} } </style>




    
        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    
    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-122346e276.css" media="print">
    



<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">


<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h.indexOf('preview-www.nature.com') > -1) return;

            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('nature.com') > -1) {
                if (h.indexOf('test-www.nature.com') > -1) {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-26e142e9c6.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    
                        {src: '/static/js/global-article-es6-bundle-782fd09f66.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-d66d49033d.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-aca08c055a.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-4fba787158.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-1fe07484e5.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;
                        
                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-8fc1a30809.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-e0c7186f28.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-2399be388c.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">





    
    <script type="application/ld+json">{"mainEntity":{"headline":"Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices","description":"Attention alters neural responses that encode different aspects of visual stimuli, but exactly how these changes together modulate the encoded spatial representation of a scene remains unclear. Here the authors look at spatial priority maps of attended to and ignored stimuli and find that attention increases the gain but not the size of stimulus representations. Computational theories propose that attention modulates the topographical landscape of spatial 'priority' maps in regions of the visual cortex so that the location of an important object is associated with higher activation levels. Although studies of single-unit recordings have demonstrated attention-related increases in the gain of neural responses and changes in the size of spatial receptive fields, the net effect of these modulations on the topography of region-level priority maps has not been investigated. Here we used functional magnetic resonance imaging and a multivariate encoding model to reconstruct spatial representations of attended and ignored stimuli using activation patterns across entire visual areas. These reconstructed spatial representations reveal the influence of attention on the amplitude and size of stimulus representations within putative priority maps across the visual hierarchy. Our results suggest that attention increases the amplitude of stimulus representations in these spatial maps, particularly in higher visual areas, but does not substantively change their size.","datePublished":"2013-11-10T00:00:00Z","dateModified":"2013-11-10T00:00:00Z","pageStart":"1879","pageEnd":"1887","sameAs":"https://doi.org/10.1038/nn.3574","keywords":["Attention","Extrastriate cortex","Neural encoding","Biomedicine","general","Neurosciences","Behavioral Sciences","Biological Techniques","Neurobiology","Animal Genetics and Genomics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig1_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig2_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig3_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig4_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig5_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig6_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig7_HTML.jpg"],"isPartOf":{"name":"Nature Neuroscience","issn":["1546-1726","1097-6256"],"volumeNumber":"16","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Thomas C Sprague","affiliation":[{"name":"Neuroscience Graduate Program, University of California San Diego","address":{"name":"Neuroscience Graduate Program, University of California San Diego, La Jolla, USA","@type":"PostalAddress"},"@type":"Organization"}],"email":"tsprague@ucsd.edu","@type":"Person"},{"name":"John T Serences","affiliation":[{"name":"Neuroscience Graduate Program, University of California San Diego","address":{"name":"Neuroscience Graduate Program, University of California San Diego, La Jolla, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of California San Diego","address":{"name":"Department of Psychology, University of California San Diego, La Jolla, USA","@type":"PostalAddress"},"@type":"Organization"}],"email":"jserences@ucsd.edu","@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>




    
    
    


    
    <link rel="canonical" href="https://www.nature.com/articles/nn.3574">
    
    
    <meta name="journal_id" content="41593"/>
    <meta name="dc.title" content="Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices"/>
    <meta name="dc.source" content="Nature Neuroscience 2013 16:12"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2013-11-10"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2013 Springer Nature America, Inc."/>
    <meta name="dc.rights" content="2013 Springer Nature America, Inc."/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="Attention alters neural responses that encode different aspects of visual stimuli, but exactly how these changes together modulate the encoded spatial representation of a scene remains unclear. Here the authors look at spatial priority maps of attended to and ignored stimuli and find that attention increases the gain but not the size of stimulus representations. Computational theories propose that attention modulates the topographical landscape of spatial &#39;priority&#39; maps in regions of the visual cortex so that the location of an important object is associated with higher activation levels. Although studies of single-unit recordings have demonstrated attention-related increases in the gain of neural responses and changes in the size of spatial receptive fields, the net effect of these modulations on the topography of region-level priority maps has not been investigated. Here we used functional magnetic resonance imaging and a multivariate encoding model to reconstruct spatial representations of attended and ignored stimuli using activation patterns across entire visual areas. These reconstructed spatial representations reveal the influence of attention on the amplitude and size of stimulus representations within putative priority maps across the visual hierarchy. Our results suggest that attention increases the amplitude of stimulus representations in these spatial maps, particularly in higher visual areas, but does not substantively change their size."/>
    <meta name="prism.issn" content="1546-1726"/>
    <meta name="prism.publicationName" content="Nature Neuroscience"/>
    <meta name="prism.publicationDate" content="2013-11-10"/>
    <meta name="prism.volume" content="16"/>
    <meta name="prism.number" content="12"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="1879"/>
    <meta name="prism.endingPage" content="1887"/>
    <meta name="prism.copyright" content="2013 Springer Nature America, Inc."/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/nn.3574"/>
    <meta name="prism.doi" content="doi:10.1038/nn.3574"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/nn.3574.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/nn.3574"/>
    <meta name="citation_journal_title" content="Nature Neuroscience"/>
    <meta name="citation_journal_abbrev" content="Nat Neurosci"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="1546-1726"/>
    <meta name="citation_title" content="Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices"/>
    <meta name="citation_volume" content="16"/>
    <meta name="citation_issue" content="12"/>
    <meta name="citation_publication_date" content="2013/12"/>
    <meta name="citation_online_date" content="2013/11/10"/>
    <meta name="citation_firstpage" content="1879"/>
    <meta name="citation_lastpage" content="1887"/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/nn.3574"/>
    <meta name="DOI" content="10.1038/nn.3574"/>
    <meta name="size" content="259417"/>
    <meta name="citation_doi" content="10.1038/nn.3574"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/nn.3574&amp;api_key="/>
    <meta name="description" content="Attention alters neural responses that encode different aspects of visual stimuli, but exactly how these changes together modulate the encoded spatial representation of a scene remains unclear. Here the authors look at spatial priority maps of attended to and ignored stimuli and find that attention increases the gain but not the size of stimulus representations. Computational theories propose that attention modulates the topographical landscape of spatial &#39;priority&#39; maps in regions of the visual cortex so that the location of an important object is associated with higher activation levels. Although studies of single-unit recordings have demonstrated attention-related increases in the gain of neural responses and changes in the size of spatial receptive fields, the net effect of these modulations on the topography of region-level priority maps has not been investigated. Here we used functional magnetic resonance imaging and a multivariate encoding model to reconstruct spatial representations of attended and ignored stimuli using activation patterns across entire visual areas. These reconstructed spatial representations reveal the influence of attention on the amplitude and size of stimulus representations within putative priority maps across the visual hierarchy. Our results suggest that attention increases the amplitude of stimulus representations in these spatial maps, particularly in higher visual areas, but does not substantively change their size."/>
    <meta name="dc.creator" content="Sprague, Thomas C"/>
    <meta name="dc.creator" content="Serences, John T"/>
    <meta name="dc.subject" content="Attention"/>
    <meta name="dc.subject" content="Extrastriate cortex"/>
    <meta name="dc.subject" content="Neural encoding"/>
    <meta name="citation_reference" content="citation_journal_title=Hum. Neurobiol.; citation_title=Shifts in selective visual attention: towards the underlying neural circuitry; citation_author=C Koch, S Ullman; citation_volume=4; citation_publication_date=1985; citation_pages=219-227; citation_id=CR1"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans. Pattern Anal. Mach. Intell.; citation_title=A model of saliency-based visual attention for rapid scene analysis; citation_author=L Itti, C Koch, E Niebur; citation_volume=20; citation_publication_date=1998; citation_pages=1254-1259; citation_doi=10.1109/34.730558; citation_id=CR2"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Rev. Neurosci.; citation_title=Computational modelling of visual attention; citation_author=L Itti, C Koch; citation_volume=2; citation_publication_date=2001; citation_pages=194-203; citation_doi=10.1038/35058500; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Selective visual attention and perceptual coherence; citation_author=JT Serences, S Yantis; citation_volume=10; citation_publication_date=2006; citation_pages=38-45; citation_doi=10.1016/j.tics.2005.11.008; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Salience, relevance, and firing: a priority map for target selection; citation_author=JH Fecteau, DP Munoz; citation_volume=10; citation_publication_date=2006; citation_pages=382-390; citation_doi=10.1016/j.tics.2006.06.011; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Effects of similarity and history on neural mechanisms of visual selection; citation_author=NP Bichot, JD Schall; citation_volume=2; citation_publication_date=1999; citation_pages=549-554; citation_doi=10.1038/9205; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex; citation_author=SJ Luck, L Chelazzi, SA Hillyard, R Desimone; citation_volume=77; citation_publication_date=1997; citation_pages=24-42; citation_doi=10.1152/jn.1997.77.1.24; citation_id=CR7"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Competitive mechanisms subserve attention in macaque areas V2 and V4; citation_author=JH Reynolds, L Chelazzi, R Desimone; citation_volume=19; citation_publication_date=1999; citation_pages=1736-1753; citation_doi=10.1523/JNEUROSCI.19-05-01736.1999; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4; citation_author=CJ McAdams, JHR Maunsell; citation_volume=19; citation_publication_date=1999; citation_pages=431-441; citation_doi=10.1523/JNEUROSCI.19-01-00431.1999; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Spatial attention effects in macaque area V4; citation_author=CE Connor, DC Preddie, JL Gallant, DC Van Essen; citation_volume=17; citation_publication_date=1997; citation_pages=3201-3214; citation_doi=10.1523/JNEUROSCI.17-09-03201.1997; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Attentional modulation of visual motion processing in cortical areas MT and MST; citation_author=S Treue, JHR Maunsell; citation_volume=382; citation_publication_date=1996; citation_pages=539-541; citation_doi=10.1038/382539a0; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Attention increases sensitivity of V4 neurons; citation_author=JH Reynolds, T Pasternak, R Desimone; citation_volume=26; citation_publication_date=2000; citation_pages=703-714; citation_doi=10.1016/S0896-6273(00)81206-4; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Attention to both space and feature modulates neuronal responses in macaque area V4; citation_author=CJ McAdams, JHR Maunsell; citation_volume=83; citation_publication_date=2000; citation_pages=1751-1755; citation_doi=10.1152/jn.2000.83.3.1751; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Effects of attention on the processing of motion in macaque middle temporal and medial superior temporal visual cortical areas; citation_author=S Treue, JHR Maunsell; citation_volume=19; citation_publication_date=1999; citation_pages=7591-7602; citation_doi=10.1523/JNEUROSCI.19-17-07591.1999; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Effect of spatial attention on the responses of area MT neurons; citation_author=E Seidemann, WT Newsome; citation_volume=81; citation_publication_date=1999; citation_pages=1783-1794; citation_doi=10.1152/jn.1999.81.4.1783; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Focal attention produces spatially selective processing in visual cortical areas V1, V2, and V4 in the presence of competing stimuli; citation_author=BC Motter; citation_volume=70; citation_publication_date=1993; citation_pages=909-919; citation_doi=10.1152/jn.1993.70.3.909; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Selective attention gates visual processing in the extrastriate cortex; citation_author=J Moran, R Desimone; citation_volume=229; citation_publication_date=1985; citation_pages=782-784; citation_doi=10.1126/science.4023713; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Spatial attention improves the quality of population codes in human visual cortex; citation_author=S Saproo, JT Serences; citation_volume=104; citation_publication_date=2010; citation_pages=885-895; citation_doi=10.1152/jn.00369.2010; citation_id=CR18"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Dynamic shifts of visual receptive fields in cortical area MT by spatial attention; citation_author=T Womelsdorf, K Anton-Erxleben, F Pieper, S Treue; citation_volume=9; citation_publication_date=2006; citation_pages=1156-1160; citation_doi=10.1038/nn1748; citation_id=CR19"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation; citation_author=T Womelsdorf, K Anton-Erxleben, S Treue; citation_volume=28; citation_publication_date=2008; citation_pages=8934-8944; citation_doi=10.1523/JNEUROSCI.4030-07.2008; citation_id=CR20"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Attention reshapes center-surround receptive field structure in macaque cortical area MT; citation_author=K Anton-Erxleben, VM Stephan, S Treue; citation_volume=19; citation_publication_date=2009; citation_pages=2466-2478; citation_doi=10.1093/cercor/bhp002; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Expansion of MT neurons excitatory receptive fields during covert attentive tracking; citation_author=R Niebergall, PS Khayat, S Treue, JC Martinez-Trujillo; citation_volume=31; citation_publication_date=2011; citation_pages=15499-15510; citation_doi=10.1523/JNEUROSCI.2822-11.2011; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Rev. Neurosci.; citation_title=Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence; citation_author=K Anton-Erxleben, M Carrasco; citation_volume=14; citation_publication_date=2013; citation_pages=188-200; citation_doi=10.1038/nrn3443; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Transient attention enhances perceptual performance and FMRI response in human visual cortex; citation_author=T Liu, F Pestilli, M Carrasco; citation_volume=45; citation_publication_date=2005; citation_pages=469-477; citation_doi=10.1016/j.neuron.2004.12.039; citation_id=CR24"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Spatial attention affects brain activity in human primary visual cortex; citation_author=SP Gandhi, DJ Heeger, GM Boynton; citation_volume=96; citation_publication_date=1999; citation_pages=3314-3319; citation_doi=10.1073/pnas.96.6.3314; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Increased activity in human visual cortex during directed attention in the absence of visual stimulation; citation_author=S Kastner, MA Pinsk, P De Weerd, R Desimone, LG Ungerleider; citation_volume=22; citation_publication_date=1999; citation_pages=751-761; citation_doi=10.1016/S0896-6273(00)80734-5; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=A physiological correlate of the &#8220;spotlight&#8221; of visual attention; citation_author=JA Brefczynski, EA DeYoe; citation_volume=2; citation_publication_date=1999; citation_pages=370-374; citation_doi=10.1038/7280; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Neural correlates of sustained spatial attention in human early visual cortex; citation_author=MA Silver, D Ress, DJ Heeger; citation_volume=97; citation_publication_date=2007; citation_pages=229-237; citation_doi=10.1152/jn.00677.2006; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=The retinotopy of visual spatial attention; citation_author=RB Tootell; citation_volume=21; citation_publication_date=1998; citation_pages=1409-1422; citation_doi=10.1016/S0896-6273(00)80659-5; citation_id=CR29"/>
    <meta name="citation_reference" content="citation_journal_title=J. Vis.; citation_title=The effects of spatial attention in early human visual cortex are stimulus independent; citation_author=SO Murray; citation_volume=8; citation_publication_date=2008; citation_pages=2.1-2.11; citation_doi=10.1167/8.10.2; citation_id=CR30"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Topographic maps of visual spatial attention in human parietal cortex; citation_author=MA Silver, D Ress, DJ Heeger; citation_volume=94; citation_publication_date=2005; citation_pages=1358-1371; citation_doi=10.1152/jn.01316.2004; citation_id=CR31"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Prioritized maps of space in human frontoparietal cortex; citation_author=TA Jerde, EP Merriam, AC Riggall, JH Hedges, CE Curtis; citation_volume=32; citation_publication_date=2012; citation_pages=17382-17390; citation_doi=10.1523/JNEUROSCI.3810-12.2012; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Attention improves encoding of task-relevant features in the human visual cortex; citation_author=JFM Jehee, DK Brady, F Tong; citation_volume=31; citation_publication_date=2011; citation_pages=8210-8219; citation_doi=10.1523/JNEUROSCI.6153-09.2011; citation_id=CR33"/>
    <meta name="citation_reference" content="citation_journal_title=Neuropsychologia; citation_title=Computational advances towards linking BOLD and behavior; citation_author=JT Serences, S Saproo; citation_volume=50; citation_publication_date=2012; citation_pages=435-446; citation_doi=10.1016/j.neuropsychologia.2011.07.013; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Overlapping mechanisms of attention and spatial working memory; citation_author=E Awh, J Jonides; citation_volume=5; citation_publication_date=2001; citation_pages=119-126; citation_doi=10.1016/S1364-6613(00)01593-X; citation_id=CR35"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Decoding and reconstructing color from responses in human visual cortex; citation_author=GJ Brouwer, DJ Heeger; citation_volume=29; citation_publication_date=2009; citation_pages=13992-14003; citation_doi=10.1523/JNEUROSCI.3577-09.2009; citation_id=CR36"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Encoding and decoding in fMRI; citation_author=T Naselaris, K Kay, S Nishimoto, J Gallant; citation_volume=56; citation_publication_date=2011; citation_pages=400-410; citation_doi=10.1016/j.neuroimage.2010.07.073; citation_id=CR37"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Optimal deployment of attentional gain during fine discriminations; citation_author=M Scolari, A Byers, JT Serences; citation_volume=32; citation_publication_date=2012; citation_pages=7723-7733; citation_doi=10.1523/JNEUROSCI.5558-11.2012; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=Phil. Trans. R. Soc. Lond. B; citation_title=Cortical visual areas in monkeys: location, topography, connections, columns, plasticity and cortical dynamics; citation_author=R Gattass; citation_volume=360; citation_publication_date=2005; citation_pages=709-731; citation_doi=10.1098/rstb.2005.1629; citation_id=CR39"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Visual receptive field modulation in the lateral intraparietal area during attentive fixation and free gaze; citation_author=S Ben Hamed, JR Duhamel, F Bremmer, W Graf; citation_volume=12; citation_publication_date=2002; citation_pages=234-245; citation_doi=10.1093/cercor/12.3.234; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=Brain Res.; citation_title=Visual receptive fields of frontal eye field neurons; citation_author=CW Mohler, ME Goldberg, RH Wurtz; citation_volume=61; citation_publication_date=1973; citation_pages=385-389; citation_doi=10.1016/0006-8993(73)90543-X; citation_id=CR41"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Population receptive field estimates in human visual cortex; citation_author=SO Dumoulin, BA Wandell; citation_volume=39; citation_publication_date=2008; citation_pages=647-660; citation_doi=10.1016/j.neuroimage.2007.09.034; citation_id=CR42"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Mapping of contralateral space in retinotopic coordinates by a parietal cortical area in humans; citation_author=MI Sereno, S Pitzalis, A Martinez; citation_volume=294; citation_publication_date=2001; citation_pages=1350-1354; citation_doi=10.1126/science.1063695; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Visual topography of human intraparietal sulcus; citation_author=JD Swisher, MA Halko, LB Merabet, SA McMains, DC Somers; citation_volume=27; citation_publication_date=2007; citation_pages=5326-5337; citation_doi=10.1523/JNEUROSCI.0991-07.2007; citation_id=CR44"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Retinotopy and attention in human occipital, temporal, parietal, and frontal cortex; citation_author=AP Saygin, MI Sereno; citation_volume=18; citation_publication_date=2008; citation_pages=2158-2168; citation_doi=10.1093/cercor/bhm242; citation_id=CR45"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Modulation of sensory suppression: implications for receptive field sizes in the human visual cortex; citation_author=S Kastner; citation_volume=86; citation_publication_date=2001; citation_pages=1398-1411; citation_doi=10.1152/jn.2001.86.3.1398; citation_id=CR46"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Persistent neural activity during the maintenance of spatial position in working memory; citation_author=R Srimal, CE Curtis; citation_volume=39; citation_publication_date=2008; citation_pages=455-468; citation_doi=10.1016/j.neuroimage.2007.08.040; citation_id=CR47"/>
    <meta name="citation_reference" content="citation_journal_title=Neuropsychologia; citation_title=Location and function of the human frontal eye-field: a selective review; citation_author=T Paus; citation_volume=34; citation_publication_date=1996; citation_pages=475-483; citation_doi=10.1016/0028-3932(95)00134-4; citation_id=CR48"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Topographic maps in human frontal cortex revealed in memory-guided saccade and spatial working-memory tasks; citation_author=S Kastner; citation_volume=97; citation_publication_date=2007; citation_pages=3494-3507; citation_doi=10.1152/jn.00010.2007; citation_id=CR49"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Biol.; citation_title=Attention narrows position tuning of population responses in V1; citation_author=J Fischer, D Whitney; citation_volume=19; citation_publication_date=2009; citation_pages=1356-1361; citation_doi=10.1016/j.cub.2009.06.059; citation_id=CR50"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=fMRI of human visual cortex; citation_author=SA Engel; citation_volume=369; citation_publication_date=1994; citation_pages=525; citation_doi=10.1038/369525a0; citation_id=CR51"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging; citation_author=MI Sereno; citation_volume=268; citation_publication_date=1995; citation_pages=889-893; citation_doi=10.1126/science.7754376; citation_id=CR52"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Functional analysis of human MT and related visual cortical areas using magnetic resonance imaging; citation_author=RB Tootell; citation_volume=15; citation_publication_date=1995; citation_pages=3215-3230; citation_doi=10.1523/JNEUROSCI.15-04-03215.1995; citation_id=CR53"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=The representation of behavioral choice for motion in human visual cortex; citation_author=JT Serences, GM Boynton; citation_volume=27; citation_publication_date=2007; citation_pages=12893-12899; citation_doi=10.1523/JNEUROSCI.4021-07.2007; citation_id=CR54"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Identifying natural images from human brain activity; citation_author=KN Kay, T Naselaris, RJ Prenger, JL Gallant; citation_volume=452; citation_publication_date=2008; citation_pages=352-355; citation_doi=10.1038/nature06713; citation_id=CR55"/>
    <meta name="citation_reference" content="citation_journal_title=Technometrics; citation_title=Ridge regression: biased estimation for nonorthogonal problems; citation_author=AE Hoerl, RW Kennard; citation_volume=12; citation_publication_date=1970; citation_pages=55-67; citation_doi=10.1080/00401706.1970.10488634; citation_id=CR56"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=A new method for estimating population receptive field topography in visual cortex; citation_author=S Lee, A Papanikolaou, NK Logothetis, SM Smirnakis, GA Keliris; citation_volume=81; citation_publication_date=2013; citation_pages=144-157; citation_doi=10.1016/j.neuroimage.2013.05.026; citation_id=CR57"/>
    <meta name="citation_reference" content="citation_journal_title=Ann. Stat.; citation_title=Estimating the dimension of a model; citation_author=G Schwarz; citation_volume=6; citation_publication_date=1978; citation_pages=461-464; citation_doi=10.1214/aos/1176344136; citation_id=CR58"/>
    <meta name="citation_reference" content="citation_journal_title=Ann. Stat.; citation_title=The control of the false discovery rate in multiple testing under dependency; citation_author=Y Benjamini, D Yekutieli; citation_volume=29; citation_publication_date=2001; citation_pages=1165-1188; citation_doi=10.1214/aos/1013699998; citation_id=CR59"/>
    <meta name="citation_author" content="Sprague, Thomas C"/>
    <meta name="citation_author_institution" content="Neuroscience Graduate Program, University of California San Diego, La Jolla, USA"/>
    <meta name="citation_author" content="Serences, John T"/>
    <meta name="citation_author_institution" content="Neuroscience Graduate Program, University of California San Diego, La Jolla, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of California San Diego, La Jolla, USA"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@natureneuro"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices"/>
    <meta name="twitter:description" content="Nature Neuroscience - Attention alters neural responses that encode different aspects of visual stimuli, but exactly how these changes together modulate the encoded spatial representation of a..."/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig1_HTML.jpg"/>
    

    
    
    <meta property="og:url" content="https://www.nature.com/articles/nn.3574"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices - Nature Neuroscience"/>
    <meta property="og:description" content="Attention alters neural responses that encode different aspects of visual stimuli, but exactly how these changes together modulate the encoded spatial representation of a scene remains unclear. Here the authors look at spatial priority maps of attended to and ignored stimuli and find that attention increases the gain but not the size of stimulus representations."/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig1_HTML.jpg"/>
    

    <script>
        window.eligibleForRa21 = 'true'; 
    </script>
</head>
<body class="article-page">

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>



<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
        
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>

    

    <div class="u-lazy-ad-wrapper u-mbs-0">
            <div class="deferred-placeholder" data-replace="true"
                 data-placeholder="/placeholder/v1/institutionalBanner?bpids=[bpids] #institutional-banner-container"></div>
            <aside class="c-ad c-ad--728x90">
                <div class="c-ad__inner" data-container-type="banner-advert">
                    <p class="c-ad__label">Advertisement</p>
                    
        
            
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=nn.3574;doi=10.1038/nn.3574;techmeta=36,59;subjmeta=116,1310,2395,2613,2614,2649,378,631;kwrd=Attention,Extrastriate+cortex,Neural+encoding">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=1328900956&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Dnn.3574%26doi%3D10.1038/nn.3574%26techmeta%3D36,59%26subjmeta%3D116,1310,2395,2613,2614,2649,378,631%26kwrd%3DAttention,Extrastriate+cortex,Neural+encoding">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=1328900956&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Dnn.3574%26doi%3D10.1038/nn.3574%26techmeta%3D36,59%26subjmeta%3D116,1310,2395,2613,2614,2649,378,631%26kwrd%3DAttention,Extrastriate+cortex,Neural+encoding"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

        
    
                </div>
            </aside>
        </div>
    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#00928c">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">
                    
                    
                    <div class="c-header__logo-container">
                        
                        <a href="/neuro"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-0ccc487532906d646419e51f647ce79a.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-880e5942f43b9213989c58a04ab5c8e6.svg" height="32" alt="Nature Neuroscience">
                            </picture>
                        </a>
                    
                    </div>
                    
                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">
                            
                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/nn.3574'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
                            
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        
            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">
                            
                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span><span class="c-header__show-text">Explore</span> content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                                
                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>
                                
                            
                            
                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">
                            
                                <li class="c-header__item">
                                    <a class="c-header__link"
                                       href="https://idp.nature.com/auth/personal/springernature?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Fmy-account%2Falerts%2Fsubscribe-journal%3Flist-id%3D6"
                                       rel="nofollow"
                                       data-track="click"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/neuro.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        
    </header>


    
    
        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature neuroscience"><span itemprop="name">nature neuroscience</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro/articles?type&#x3D;article" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="name">articles</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>
    



    

</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
        
            <div class="c-context-bar u-hide"
                 data-test="context-bar"
                 data-context-bar
                 aria-hidden="true">
                <div class="c-context-bar__container u-container">
                    <div class="c-context-bar__title">
                        Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices
                    </div>
                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3574.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                </div>
            </div>
        
        <article lang="en">
            
            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Article</li>
    
    

                        <li class="c-article-identifiers__item">Published: <time datetime="2013-11-10">10 November 2013</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Thomas_C-Sprague-Aff1" data-author-popup="auth-Thomas_C-Sprague-Aff1" data-author-search="Sprague, Thomas C" data-corresp-id="c1">Thomas C Sprague<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff1">1</a></sup> &amp; </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-John_T-Serences-Aff1-Aff2" data-author-popup="auth-John_T-Serences-Aff1-Aff2" data-author-search="Serences, John T" data-corresp-id="c2">John T Serences<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a></sup></li></ul>

                    

                    <p class="c-article-info-details" data-container-section="info">
                        
    <a data-test="journal-link" href="/neuro" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Nature Neuroscience</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span>16</b>,<span class="u-visually-hidden">pages </span>18791887 (<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>
                    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8230 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">146 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">11 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/articles/nn.3574/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
                    
                </header>

                
    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/attention" data-track="click" data-track-action="view subject" data-track-label="link">Attention</a></li><li class="c-article-subject-list__subject"><a href="/subjects/extrastriate-cortex" data-track="click" data-track-action="view subject" data-track-label="link">Extrastriate cortex</a></li><li class="c-article-subject-list__subject"><a href="/subjects/neural-encoding" data-track="click" data-track-action="view subject" data-track-label="link">Neural encoding</a></li>
        </ul>
    </div>

                
    
    

    
    

                
            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs2" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs2">Abstract</h2><div class="c-article-section__content" id="Abs2-content"><p>Computational theories propose that attention modulates the topographical landscape of spatial 'priority' maps in regions of the visual cortex so that the location of an important object is associated with higher activation levels. Although studies of single-unit recordings have demonstrated attention-related increases in the gain of neural responses and changes in the size of spatial receptive fields, the net effect of these modulations on the topography of region-level priority maps has not been investigated. Here we used functional magnetic resonance imaging and a multivariate encoding model to reconstruct spatial representations of attended and ignored stimuli using activation patterns across entire visual areas. These reconstructed spatial representations reveal the influence of attention on the amplitude and size of stimulus representations within putative priority maps across the visual hierarchy. Our results suggest that attention increases the amplitude of stimulus representations in these spatial maps, particularly in higher visual areas, but does not substantively change their size.</p></div></div></section>

            <noscript>
                
                    
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3574.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                
            </noscript>

            
                <div class="js-context-bar-sticky-point-mobile">
                    
                        <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-entitled-mobile
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3574.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                    
                </div>
            

            
                
                    
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42003-020-01423-0/MediaObjects/42003_2020_1423_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s42003-020-01423-0?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1038/s42003-020-01423-0">Parallel fast and slow recurrent cortical processing mediates target and distractor selection in visual search
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">19 November 2020</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Sarah E. Donohue, Mircea A. Schoenfeld &amp; Jens-Max Hopf</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41583-022-00582-9/MediaObjects/41583_2022_582_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41583-022-00582-9?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1038/s41583-022-00582-9">Priority coding in the visual system
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">11 April 2022</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Nicole C. Rust &amp; Marlene R. Cohen</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42003-021-02294-9/MediaObjects/42003_2021_2294_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s42003-021-02294-9?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1038/s42003-021-02294-9">The transverse occipital sulcus and intraparietal sulcus show neural selectivity to object-scene size relationships
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">22 June 2021</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Lauren E. Welbourne, Aditya Jonnalagadda,  Miguel P. Eckstein</p>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'speedy-BootstrappedUCB',
                        timestamp: 1711582740,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                
                
                <div class="main-content">
                    <section data-title="Main"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Main</h2><div class="c-article-section__content" id="Sec1-content"><p>Prominent computational theories of selective attention posit that basic properties of visual stimuli are encoded in a series of interacting priority maps that are found at each stage of the visual system<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Koch, C. &amp; Ullman, S. Shifts in selective visual attention: towards the underlying neural circuitry. Hum. Neurobiol. 4, 219227 (1985)." href="/articles/nn.3574#ref-CR1" id="ref-link-section-d31016971e381">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Itti, L., Koch, C. &amp; Niebur, E. A model of saliency-based visual attention for rapid scene analysis. IEEE Trans. Pattern Anal. Mach. Intell. 20, 12541259 (1998)." href="/articles/nn.3574#ref-CR2" id="ref-link-section-d31016971e384">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Itti, L. &amp; Koch, C. Computational modelling of visual attention. Nat. Rev. Neurosci. 2, 194203 (2001)." href="/articles/nn.3574#ref-CR3" id="ref-link-section-d31016971e387">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Serences, J.T. &amp; Yantis, S. Selective visual attention and perceptual coherence. Trends Cogn. Sci. 10, 3845 (2006)." href="/articles/nn.3574#ref-CR4" id="ref-link-section-d31016971e390">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Fecteau, J.H. &amp; Munoz, D.P. Salience, relevance, and firing: a priority map for target selection. Trends Cogn. Sci. 10, 382390 (2006)." href="/articles/nn.3574#ref-CR5" id="ref-link-section-d31016971e393">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bichot, N.P. &amp; Schall, J.D. Effects of similarity and history on neural mechanisms of visual selection. Nat. Neurosci. 2, 549554 (1999)." href="/articles/nn.3574#ref-CR6" id="ref-link-section-d31016971e396">6</a></sup>. The maps in different areas are thought to encode different stimulus features (for example, orientation, color or motion) on the basis of the selectivity of component neurons. Two general themes governing the organization of these maps have emerged. First, accurately encoding the spatial location of relevant stimuli is the fundamental goal of these priority maps, as spatial position is necessary to guide saccadic eye movements (and other exploratory and reflexive motor responses). Second, priority maps early in the visual system reflect primarily the physical salience of stimuli in the visual field, whereas priority maps in later areas increasingly index the behavioral relevance of stimuli independent of physical salience<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Serences, J.T. &amp; Yantis, S. Selective visual attention and perceptual coherence. Trends Cogn. Sci. 10, 3845 (2006)." href="/articles/nn.3574#ref-CR4" id="ref-link-section-d31016971e400">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Fecteau, J.H. &amp; Munoz, D.P. Salience, relevance, and firing: a priority map for target selection. Trends Cogn. Sci. 10, 382390 (2006)." href="/articles/nn.3574#ref-CR5" id="ref-link-section-d31016971e403">5</a></sup>.</p><p>Although many studies have investigated the influence of spatial attention on single-unit neural activity over the last several decades<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Luck, S.J., Chelazzi, L., Hillyard, S.A. &amp; Desimone, R. Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex. J. Neurophysiol. 77, 2442 (1997)." href="/articles/nn.3574#ref-CR7" id="ref-link-section-d31016971e410">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Reynolds, J.H., Chelazzi, L. &amp; Desimone, R. Competitive mechanisms subserve attention in macaque areas V2 and V4. J. Neurosci. 19, 17361753 (1999)." href="/articles/nn.3574#ref-CR8" id="ref-link-section-d31016971e413">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="McAdams, C.J. &amp; Maunsell, J.H.R. Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4. J. Neurosci. 19, 431441 (1999)." href="/articles/nn.3574#ref-CR9" id="ref-link-section-d31016971e416">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Connor, C.E., Preddie, D.C., Gallant, J.L. &amp; Van Essen, D.C. Spatial attention effects in macaque area V4. J. Neurosci. 17, 32013214 (1997)." href="/articles/nn.3574#ref-CR10" id="ref-link-section-d31016971e419">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Treue, S. &amp; Maunsell, J.H.R. Attentional modulation of visual motion processing in cortical areas MT and MST. Nature 382, 539541 (1996)." href="/articles/nn.3574#ref-CR11" id="ref-link-section-d31016971e422">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Reynolds, J.H., Pasternak, T. &amp; Desimone, R. Attention increases sensitivity of V4 neurons. Neuron 26, 703714 (2000)." href="/articles/nn.3574#ref-CR12" id="ref-link-section-d31016971e425">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="McAdams, C.J. &amp; Maunsell, J.H.R. Attention to both space and feature modulates neuronal responses in macaque area V4. J. Neurophysiol. 83, 17511755 (2000)." href="/articles/nn.3574#ref-CR13" id="ref-link-section-d31016971e429">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Treue, S. &amp; Maunsell, J.H.R. Effects of attention on the processing of motion in macaque middle temporal and medial superior temporal visual cortical areas. J. Neurosci. 19, 75917602 (1999)." href="/articles/nn.3574#ref-CR14" id="ref-link-section-d31016971e432">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Seidemann, E. &amp; Newsome, W.T. Effect of spatial attention on the responses of area MT neurons. J. Neurophysiol. 81, 17831794 (1999)." href="/articles/nn.3574#ref-CR15" id="ref-link-section-d31016971e435">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Motter, B.C. Focal attention produces spatially selective processing in visual cortical areas V1, V2, and V4 in the presence of competing stimuli. J. Neurophysiol. 70, 909919 (1993)." href="/articles/nn.3574#ref-CR16" id="ref-link-section-d31016971e438">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Moran, J. &amp; Desimone, R. Selective attention gates visual processing in the extrastriate cortex. Science 229, 782784 (1985)." href="/articles/nn.3574#ref-CR17" id="ref-link-section-d31016971e441">17</a></sup>, directly examining the impact of attention on the topographic profile across an entire spatial priority map is a major challenge because single units have access to a limited window of the spatial scene<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Fecteau, J.H. &amp; Munoz, D.P. Salience, relevance, and firing: a priority map for target selection. Trends Cogn. Sci. 10, 382390 (2006)." href="/articles/nn.3574#ref-CR5" id="ref-link-section-d31016971e445">5</a></sup>. This is a key limitation, as the relationship between changes in the size and amplitude of individual spatial receptive fields (or voxel-level receptive fields across populations of neurons) and changes in the fidelity of population-level spatial encoding are not related in a straightforward manner (ref. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Saproo, S. &amp; Serences, J.T. Spatial attention improves the quality of population codes in human visual cortex. J. Neurophysiol. 104, 885895 (2010)." href="/articles/nn.3574#ref-CR18" id="ref-link-section-d31016971e448">18</a> discusses this issue with respect to population codes for orientation). For example, if spatial receptive fields are uniformly shrunk by attention while viewing a stimulus, the population-level spatial representation (or priority map) carried by all of those neurons might shrink or become sharper, but the code may be more vulnerable to uncorrelated noise (as there is less redundant coding of any given spatial position by the population). Alternatively, a uniform increase in spatial receptive field size might blur or increase the size of a spatial representation encoded by a population, but such a representation might be more robust to uncorrelated neural noise because of increased redundancy.</p><p>Further complicating matters is the observation that spatial receptive fields have been shown to both increase and decrease in size with attention as a function of where the spatial receptive field is positioned relative to the attended stimulus. Spatial receptive fields tuned near an attended stimulus grow, and spatial receptive fields that fully encompass an attended stimulus shrink<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Connor, C.E., Preddie, D.C., Gallant, J.L. &amp; Van Essen, D.C. Spatial attention effects in macaque area V4. J. Neurosci. 17, 32013214 (1997)." href="/articles/nn.3574#ref-CR10" id="ref-link-section-d31016971e455">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Womelsdorf, T., Anton-Erxleben, K., Pieper, F. &amp; Treue, S. Dynamic shifts of visual receptive fields in cortical area MT by spatial attention. Nat. Neurosci. 9, 11561160 (2006)." href="/articles/nn.3574#ref-CR19" id="ref-link-section-d31016971e458">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Womelsdorf, T., Anton-Erxleben, K. &amp; Treue, S. Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation. J. Neurosci. 28, 89348944 (2008)." href="/articles/nn.3574#ref-CR20" id="ref-link-section-d31016971e461">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Anton-Erxleben, K., Stephan, V.M. &amp; Treue, S. Attention reshapes center-surround receptive field structure in macaque cortical area MT. Cereb. Cortex 19, 24662478 (2009)." href="/articles/nn.3574#ref-CR21" id="ref-link-section-d31016971e464">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Niebergall, R., Khayat, P.S., Treue, S. &amp; Martinez-Trujillo, J.C. Expansion of MT neurons excitatory receptive fields during covert attentive tracking. J. Neurosci. 31, 1549915510 (2011)." href="/articles/nn.3574#ref-CR22" id="ref-link-section-d31016971e467">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Anton-Erxleben, K. &amp; Carrasco, M. Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence. Nat. Rev. Neurosci. 14, 188200 (2013)." href="/articles/nn.3574#ref-CR23" id="ref-link-section-d31016971e470">23</a></sup>. These receptive field size changes occur in parallel to changes in the amplitude (gain) of neural responses with attention<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Luck, S.J., Chelazzi, L., Hillyard, S.A. &amp; Desimone, R. Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex. J. Neurophysiol. 77, 2442 (1997)." href="/articles/nn.3574#ref-CR7" id="ref-link-section-d31016971e474">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Reynolds, J.H., Chelazzi, L. &amp; Desimone, R. Competitive mechanisms subserve attention in macaque areas V2 and V4. J. Neurosci. 19, 17361753 (1999)." href="/articles/nn.3574#ref-CR8" id="ref-link-section-d31016971e477">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="McAdams, C.J. &amp; Maunsell, J.H.R. Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4. J. Neurosci. 19, 431441 (1999)." href="/articles/nn.3574#ref-CR9" id="ref-link-section-d31016971e480">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Connor, C.E., Preddie, D.C., Gallant, J.L. &amp; Van Essen, D.C. Spatial attention effects in macaque area V4. J. Neurosci. 17, 32013214 (1997)." href="/articles/nn.3574#ref-CR10" id="ref-link-section-d31016971e483">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Treue, S. &amp; Maunsell, J.H.R. Attentional modulation of visual motion processing in cortical areas MT and MST. Nature 382, 539541 (1996)." href="/articles/nn.3574#ref-CR11" id="ref-link-section-d31016971e486">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Reynolds, J.H., Pasternak, T. &amp; Desimone, R. Attention increases sensitivity of V4 neurons. Neuron 26, 703714 (2000)." href="/articles/nn.3574#ref-CR12" id="ref-link-section-d31016971e489">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="McAdams, C.J. &amp; Maunsell, J.H.R. Attention to both space and feature modulates neuronal responses in macaque area V4. J. Neurophysiol. 83, 17511755 (2000)." href="/articles/nn.3574#ref-CR13" id="ref-link-section-d31016971e493">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Treue, S. &amp; Maunsell, J.H.R. Effects of attention on the processing of motion in macaque middle temporal and medial superior temporal visual cortical areas. J. Neurosci. 19, 75917602 (1999)." href="/articles/nn.3574#ref-CR14" id="ref-link-section-d31016971e496">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Seidemann, E. &amp; Newsome, W.T. Effect of spatial attention on the responses of area MT neurons. J. Neurophysiol. 81, 17831794 (1999)." href="/articles/nn.3574#ref-CR15" id="ref-link-section-d31016971e499">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Motter, B.C. Focal attention produces spatially selective processing in visual cortical areas V1, V2, and V4 in the presence of competing stimuli. J. Neurophysiol. 70, 909919 (1993)." href="/articles/nn.3574#ref-CR16" id="ref-link-section-d31016971e502">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Moran, J. &amp; Desimone, R. Selective attention gates visual processing in the extrastriate cortex. Science 229, 782784 (1985)." href="/articles/nn.3574#ref-CR17" id="ref-link-section-d31016971e505">17</a></sup>. Thus, the net impact of all of these changes on the fidelity of population-level spatial representations is unclear, and addressing this issue requires assessing how attention changes the profile of spatial representations encoded by the joint, region-level pattern of activity.</p><p>Here we assessed the modulatory role of attention on the spatial information content of putative priority maps by using an encoding model to reconstruct spatial representations of attended and unattended visual stimuli on the basis of multivariate blood oxygen leveldependent functional magnetic resonance imaging (BOLD fMRI) activation patterns within visually responsive regions of the occipital, parietal and frontal cortices. These reconstructions can be considered to reflect region-level spatial representations, and they allowed us to quantitatively track changes in parameters that characterize the topography of spatial maps within each region of interest (ROI). Notably, this technique exploits the full multivariate pattern of BOLD signal across an entire region to evaluate the manner in which spatial representations are modulated by attention rather than comparing multivariate decoding accuracies or considering the univariate response of each voxel in isolation. This approach can be used to examine mechanisms of attentional modulation that cannot be easily characterized by measuring changes in either the univariate mean BOLD signal or the decoding accuracy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Liu, T., Pestilli, F. &amp; Carrasco, M. Transient attention enhances perceptual performance and FMRI response in human visual cortex. Neuron 45, 469477 (2005)." href="/articles/nn.3574#ref-CR24" id="ref-link-section-d31016971e512">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Gandhi, S.P., Heeger, D.J. &amp; Boynton, G.M. Spatial attention affects brain activity in human primary visual cortex. Proc. Natl. Acad. Sci. USA 96, 33143319 (1999)." href="/articles/nn.3574#ref-CR25" id="ref-link-section-d31016971e515">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Kastner, S., Pinsk, M.A., De Weerd, P., Desimone, R. &amp; Ungerleider, L.G. Increased activity in human visual cortex during directed attention in the absence of visual stimulation. Neuron 22, 751761 (1999)." href="/articles/nn.3574#ref-CR26" id="ref-link-section-d31016971e518">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Brefczynski, J.A. &amp; DeYoe, E.A. A physiological correlate of the spotlight of visual attention. Nat. Neurosci. 2, 370374 (1999)." href="/articles/nn.3574#ref-CR27" id="ref-link-section-d31016971e521">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Silver, M.A., Ress, D. &amp; Heeger, D.J. Neural correlates of sustained spatial attention in human early visual cortex. J. Neurophysiol. 97, 229237 (2007)." href="/articles/nn.3574#ref-CR28" id="ref-link-section-d31016971e524">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Tootell, R.B. et al. The retinotopy of visual spatial attention. Neuron 21, 14091422 (1998)." href="/articles/nn.3574#ref-CR29" id="ref-link-section-d31016971e527">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Murray, S.O. The effects of spatial attention in early human visual cortex are stimulus independent. J. Vis. 8, 2.12.11 (2008)." href="/articles/nn.3574#ref-CR30" id="ref-link-section-d31016971e531">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Silver, M.A., Ress, D. &amp; Heeger, D.J. Topographic maps of visual spatial attention in human parietal cortex. J. Neurophysiol. 94, 13581371 (2005)." href="/articles/nn.3574#ref-CR31" id="ref-link-section-d31016971e534">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Jerde, T.A., Merriam, E.P., Riggall, A.C., Hedges, J.H. &amp; Curtis, C.E. Prioritized maps of space in human frontoparietal cortex. J. Neurosci. 32, 1738217390 (2012)." href="/articles/nn.3574#ref-CR32" id="ref-link-section-d31016971e537">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Jehee, J.F.M., Brady, D.K. &amp; Tong, F. Attention improves encoding of task-relevant features in the human visual cortex. J. Neurosci. 31, 82108219 (2011)." href="/articles/nn.3574#ref-CR33" id="ref-link-section-d31016971e540">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Serences, J.T. &amp; Saproo, S. Computational advances towards linking BOLD and behavior. Neuropsychologia 50, 435446 (2012)." href="/articles/nn.3574#ref-CR34" id="ref-link-section-d31016971e543">34</a></sup> (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig1">Fig. 1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="The effects of spatial attention on region-level priority maps."><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 1: The effects of spatial attention on region-level priority maps.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3574/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig1_HTML.jpg?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig1_HTML.jpg" alt="figure 1" loading="lazy" width="685" height="526"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Spatial attention might act through one of several mechanisms to change the spatial representation of a stimulus within a putative priority map. (<b>a</b>) The hypothetical spatial representation carried across an entire region in response to an unattended circular stimulus. (<b>b</b>) Under one hypothetical scenario, attention might enhance the spatial representation of the same stimulus by amplifying the gain of the spatial representation (i.e., multiplying the representation by a constant greater than 1). (<b>c</b>) Alternatively, attention might act through a combination of multiple mechanisms such as increasing the gain, decreasing the size and increasing the baseline activity of the entire region (i.e., adding a constant to the response across all areas of the priority map). (<b>d</b>) Cross-sections of <b>a</b><b>c</b>. This is not meant as an exhaustive description of different attentional modulations. (<b>e</b>) The different types of attentional modulation can give rise to identical responses when the mean BOLD response is measured across the entire expanse of a priority map. Simple Cartesian representations, such as those in <b>a</b><b>c</b>, may be visualized in early visual areas where retinotopy is well defined at the spatial resolution of the BOLD response. However, later areas might still encode precise spatial representations of a stimulus even when clear retinotopic organization is not evident, so using alternative methods for reconstructing stimulus representations, such as the approach described in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Figure 3</a>, is necessary to evaluate the fidelity of information encoded in putative attentional priority maps. Unattn, unattended; attn, attended.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3574/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Our results reveal that spatial attention increases the amplitude of region-level stimulus representations within putative priority maps carried by areas of the occipital, parietal and frontal cortices. However, we found little evidence that attention changes the size of stimulus representations in region-level priority maps, even though we observed increases in spatial filter size at the single-voxel level. In addition, the reconstructed spatial representations that are based on activation patterns in later regions of the occipital, parietal and frontal cortices showed larger attentional modulation than those from early areas, which is consistent with the hypothesis that the representations in later regions increasingly transition to more selectively represent relevant stimuli<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Serences, J.T. &amp; Yantis, S. Selective visual attention and perceptual coherence. Trends Cogn. Sci. 10, 3845 (2006)." href="/articles/nn.3574#ref-CR4" id="ref-link-section-d31016971e605">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Fecteau, J.H. &amp; Munoz, D.P. Salience, relevance, and firing: a priority map for target selection. Trends Cogn. Sci. 10, 382390 (2006)." href="/articles/nn.3574#ref-CR5" id="ref-link-section-d31016971e608">5</a></sup>. These changes in the gain of spatial representations should theoretically increase the efficiency with which information about relevant objects in the visual field can be processed and subsequently used to guide perceptual decisions and motor plans<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Saproo, S. &amp; Serences, J.T. Spatial attention improves the quality of population codes in human visual cortex. J. Neurophysiol. 104, 885895 (2010)." href="/articles/nn.3574#ref-CR18" id="ref-link-section-d31016971e612">18</a></sup>.</p></div></div></section><section data-title="Results"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Results</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Manipulating attentional demands</h3><p>To evaluate how task demands influence the topography of spatial representations within different areas of the visual system, we designed a BOLD fMRI experiment that required participants to perform one of three tasks using an identical stimulus display (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2a</a>). In each trial, participants (<i>n</i> = 8) maintained fixation at the center of the screen (Online Methods and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig8">Supplementary Fig. 1</a>) while a full-contrast flickering checkerboard was presented in 1 of 36 spatial locations that sampled 6 discrete eccentricities (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2b</a>). Participants reported either a faint contrast change at the fixation point (the attend fixation condition) or a faint contrast change of the flickering checkerboard stimulus (the attend stimulus condition) or performed a spatial working memory task in which they compared the location of a probe stimulus, T2, with the remembered location of a target stimulus, T1, presented within the radius of the flickering checkerboard (the spatial working memory condition; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2c</a>). We included the spatial working memory task as an alternate means of inducing focused and sustained spatial attention around the stimulus position<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Awh, E. &amp; Jonides, J. Overlapping mechanisms of attention and spatial working memory. Trends Cogn. Sci. 5, 119126 (2001)." href="/articles/nn.3574#ref-CR35" id="ref-link-section-d31016971e644">35</a></sup>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Task design and behavioral results."><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 2: Task design and behavioral results.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3574/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig2_HTML.jpg?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig2_HTML.jpg" alt="figure 2" loading="lazy" width="685" height="208"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>(<b>a</b>) Each trial consisted of a 500-ms target stimulus (T1), a 3000-ms flickering checkerboard (6 Hz, full contrast, 2.33 (or 2.326) diameter) and a 500-ms probe stimulus (T2). T1 and T2 were at the same location in 50% of trials and were slightly offset in the remaining 50% of trials. During the stimulus presentation period, the stimulus dimmed briefly in 50% of trials, and the fixation point dimmed in 50% of trials (each independently randomly chosen). Participants maintained fixation throughout the experiment, and eye position measured during scanning did not vary as a function of either task demands or stimulus position (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig8">Supplementary Fig. 1</a>). ITI, intertrial interval. (<b>b</b>) In each trial, a single checkerboard stimulus appeared at 1 of 36 overlapping spatial locations with a slight spatial offset between runs (Online Methods). Each spatial location was sampled once per run. This six-by-six grid of stimulus locations probes six unique eccentricities, as indicated by the color code of the dots (which is not present in the actual stimulus display). (<b>c</b>) In alternating blocks of trials, participants detected either a dimming of the fixation point (attend fixation) or a dimming of the checkerboard stimulus (attend stimulus) or they indicated if the spatial position of T1 and T2 matched (spatial working memory). Notably, all tasks used a physically identical stimulus displayonly the task demands varied. Each participant completed between four and six scanning runs of each of the three tasks. (<b>d</b>) For the attend fixation task, performance was better when the stimulus was presented at peripheral locations. In contrast, performance declined with increasing stimulus eccentricity in the attend stimulus and spatial working memory conditions. Error bars, s.e.m.</p><p>
                          <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM65">Source data</a>
                        </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3574/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>On average, performance in the attend fixation task was slightly, although nonsignificantly, higher than that in the attend stimulus or spatial working memory task (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2d</a>; main effect of condition, <i>F</i>(2,14) = 0.951, <i>P</i> = 0.41; attend fixation, 87.37  6.46% accuracy (mean  s.e.m.); attend stimulus, 81.00  6.67%; spatial working memory, 80.00  2.09%). However, we observed a different pattern of response errors across the three task demands: accuracy in the attend fixation condition was lowest in trials in which the flickering checkerboard stimulus was presented near the fixation, whereas accuracy dropped off with increasing stimulus eccentricity in the attend stimulus and spatial working memory tasks (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2d</a>; condition  eccentricity interaction, <i>F</i>(10,70) = 7.235, <i>P</i> &lt; 0.0001).</p><h3 class="c-article__sub-heading" id="Sec4">Reconstructed spatial representations of visual stimuli</h3><p>To compare spatial representations carried within different brain regions as a function of task demands, we first functionally identified seven ROIs in each hemisphere of each participant using independent localizer techniques (Online Methods and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM73">Supplementary Table 1</a>).</p><p>Next we used an encoding model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Serences, J.T. &amp; Saproo, S. Computational advances towards linking BOLD and behavior. Neuropsychologia 50, 435446 (2012)." href="/articles/nn.3574#ref-CR34" id="ref-link-section-d31016971e725">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Brouwer, G.J. &amp; Heeger, D.J. Decoding and reconstructing color from responses in human visual cortex. J. Neurosci. 29, 1399214003 (2009)." href="/articles/nn.3574#ref-CR36" id="ref-link-section-d31016971e728">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Naselaris, T., Kay, K., Nishimoto, S. &amp; Gallant, J. Encoding and decoding in fMRI. Neuroimage 56, 400410 (2011)." href="/articles/nn.3574#ref-CR37" id="ref-link-section-d31016971e731">37</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Scolari, M., Byers, A. &amp; Serences, J.T. Optimal deployment of attentional gain during fine discriminations. J. Neurosci. 32, 77237733 (2012)." href="/articles/nn.3574#ref-CR38" id="ref-link-section-d31016971e734">38</a></sup> to reconstruct a spatial representation of the stimulus that was presented during each trial using activation patterns from each ROI (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig9">Supplementary Figs. 2</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig10">3</a>). This method results in a spatial representation of the entire visual field measured during each trial that is constrained by activation across all voxels within each ROI. As a result, we obtained average spatial representations for each stimulus position for each ROI for each task condition that accurately reflected the stimulus viewed by the observer (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Fig. 4a</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig11">Supplementary Fig. 4</a>). This method linearly maps high-dimensional voxel space to a lower-dimensional information space that corresponds to visual field coordinates (Online Methods).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="The encoding model that was used to reconstruct spatial representations of visual stimuli."><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 3: The encoding model that was used to reconstruct spatial representations of visual stimuli.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3574/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig3_HTML.jpg?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig3_HTML.jpg" alt="figure 3" loading="lazy" width="685" height="539"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Spatial representations of stimuli in each of the 36 possible positions were estimated separately for each ROI. (<b>a</b>) Training the encoding model. Shown is a set of linear spatial filters that forms the basis set, or information channels, that we used to estimate the spatial selectivity of the BOLD responses in each voxel (Online Methods and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig9">Supplementary Figs. 2</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig10">3</a>). The shape of these filters determines how each information channel should respond in each trial given the position of the stimulus that was presented (thus forming a set of regressors or predicted channel responses). Then we constructed a design matrix by concatenating the regressors generated for each trial. This design matrix, in combination with the measured BOLD signal amplitude in each trial, was then used to estimate a weight for each channel in each voxel using a standard general linear model (GLM). (<b>b</b>) Estimating channel responses. Given the known spatial selectivity (or weight) profile of each voxel as computed in <b>a</b>, we then used the pattern of responses across all voxels in each trial in the test set to estimate the magnitude of the response in each of the 36 information channels in that trial. This estimate of the channel responses is thus constrained by the multivariate pattern of responses across all voxels in each trial in the test set and results in a mapping from voxel space (hundreds of dimensions) onto a lower-dimensional channel space (36 dimensions; Online Methods). We then produced a smooth reconstructed spatial representation for every trial by summing the responses of all 36 filters after weighting them by the respective channel responses in each trial. An example of a spatial representation computed from a single trial using data from V1 when the stimulus was presented at the location depicted in <b>a</b> is shown on the lower right.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3574/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Task demands modulate spatial representations."><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 4: Task demands modulate spatial representations.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3574/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig4_HTML.jpg?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig4_HTML.jpg" alt="figure 4" loading="lazy" width="685" height="722"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>(<b>a</b>) Reconstructed spatial representations of each of 36 flickering checkerboard stimuli presented in a six-by-six grid. All 36 stimulus locations are shown, with each location's representation averaged across participants (<i>n</i> = 8) using data from bilateral V1 during attend stimulus runs. One participant was not included in this analysis (AG3; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig11">Supplementary Fig. 4</a>). Each small image represents the reconstructed spatial representation of the entire visual field, and the position of the image corresponds to the location of the presented stimulus. (<b>b</b>) A subset of representations (corresponding to the upper left quadrant of the visual field, represented by the dashed box in <b>a</b>) for each ROI and each task condition. The results were similar for the other quadrants (data not shown; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Fig. 5</a> shows the aggregate quantification of all reconstructions). All reconstructions in <b>a</b> and <b>b</b> are shown on the same color scale.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3574/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>As a point of terminological clarification, we emphasize that we report estimates of the spatial representation of a stimulus display on the basis of the distributed activation pattern across all voxels within a ROI. Throughout the Results section, we therefore refer to our actual measurements as reconstructed spatial representations. However, in the Discussion, we interpret these measurements in the context of putative attentional priority maps that are thought to have a key role in shaping perception and decision making<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Koch, C. &amp; Ullman, S. Shifts in selective visual attention: towards the underlying neural circuitry. Hum. Neurobiol. 4, 219227 (1985)." href="/articles/nn.3574#ref-CR1" id="ref-link-section-d31016971e841">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Itti, L., Koch, C. &amp; Niebur, E. A model of saliency-based visual attention for rapid scene analysis. IEEE Trans. Pattern Anal. Mach. Intell. 20, 12541259 (1998)." href="/articles/nn.3574#ref-CR2" id="ref-link-section-d31016971e844">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Itti, L. &amp; Koch, C. Computational modelling of visual attention. Nat. Rev. Neurosci. 2, 194203 (2001)." href="/articles/nn.3574#ref-CR3" id="ref-link-section-d31016971e847">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Serences, J.T. &amp; Yantis, S. Selective visual attention and perceptual coherence. Trends Cogn. Sci. 10, 3845 (2006)." href="/articles/nn.3574#ref-CR4" id="ref-link-section-d31016971e850">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Fecteau, J.H. &amp; Munoz, D.P. Salience, relevance, and firing: a priority map for target selection. Trends Cogn. Sci. 10, 382390 (2006)." href="/articles/nn.3574#ref-CR5" id="ref-link-section-d31016971e853">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bichot, N.P. &amp; Schall, J.D. Effects of similarity and history on neural mechanisms of visual selection. Nat. Neurosci. 2, 549554 (1999)." href="/articles/nn.3574#ref-CR6" id="ref-link-section-d31016971e856">6</a></sup>.</p><p>Reconstructed spatial representations based on activation patterns in each ROI exhibited several qualitative differences as a function of stimulus eccentricity, task demands and ROI (which we quantify more formally below). First, representations were very precise in the primary visual cortex (V1) (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Fig. 4a</a>) and became successively coarser and more diffuse in areas of the extrastriate, parietal and frontal cortices (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Fig. 4b</a>). Similarly, representations of more eccentric stimuli were more diffuse compared to those of more foveal stimuli (for example, when comparing eccentric to foveal representations within each ROI). We also observed higher-fidelity representations of the upper visual field when using only voxels from the ventral aspects of V2 and V3 and higher-fidelity representations of the lower visual field when using only voxels from the dorsal aspects of these regions (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig12">Supplementary Fig. 5a</a>). These observations, which are consistent with known receptive field properties in nonhuman primates, confirm that our encoding-model method recovered known properties of these visual subregions and these reconstructions were not merely the result of fitting idiosyncratic aspects of our particular data set (i.e., overfitting noise). We further demonstrated this point by using the model to reconstruct representations of completely new stimuli (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig12">Supplementary Fig. 5b</a>).</p><p>Second, the profile of reconstructed spatial representations within many regions also varied with task demands, which is consistent with the notion that these spatial representations reflect spatial maps of attentional priority. Notably, especially in human visual area V4 (hV4), the human middle temporal cortex (hMT+), the intraparietal sulcus (IPS) and the superior precentral sulcus (sPCS), the magnitude of the spatial representations increased when the participant was either attending to the flickering checkerboard stimulus or performing the spatial working memory task compared to when they were performing a task at fixation.</p><h3 class="c-article__sub-heading" id="Sec5">Size of spatial representations across eccentricities and ROIs</h3><p>Before formally evaluating the effects of attention on the profile of spatial representations, we first sought to quantify changes in the size of these representations due to stimulus eccentricity and ROI for comparison with known properties of the primate visual system. To this end, we fit a smooth surface to the spatial representations associated with each of the three task conditions separately for each of the 36 possible stimulus locations in each ROI (Online Methods and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig9">Supplementary Fig. 2</a>). These fits generated an estimate of the amplitude, baseline offset and size of the represented stimulus within each reconstructed spatial representation. We averaged the fit parameters obtained from each ROI across stimulus locations that were at equivalent eccentricities and then across participants (yielding six sets of fit parameters, with one set for each of the six possible stimulus eccentricities; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2b</a>). We then used these fit parameters to make inferences about how the magnitudes and shapes of the spatial representations of stimuli from each ROI varied across stimulus positions.</p><p>First we quantified the accuracy of fits by computing the Euclidean distance between the centroid of the fit function and the actual location of the stimulus across all eccentricities and task conditions. The estimated centroids were generally accurate and closely tracked changes in stimulus location (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Fig. 4a</a>). However, the distances between the fit centroids and the actual stimulus positions in sPCS were nearly double those of the next least-accurate region, hMT+ (sPCS, 3.01  0.077 (mean  s.e.m.); hMT+, 1.68  0.17). The error distances in all other areas were relatively small (V1, 0.67  0.084; V2, 0.77  0.12; V3, 0.75  0.095; hV4, 1.16  0.13; IPS, 1.46  0.20). Thus, the relatively low correspondence between the estimated and actual stimulus positions on the basis of data from the sPCS suggests that the resulting fit parameters should be interpreted with caution (addressed further in the Discussion).</p><p>In early visual ROIs of V1, V2, V3 and hV4, the size of the reconstructed spatial representations increased with increasing eccentricity regardless of task condition (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Fig. 5</a> and Online Methods; main effect of eccentricity, two-way analysis of variance (ANOVA) within each ROI, all <i>P</i> &lt; 0.0004; unless otherwise specified, all statistical tests on fit parameters to spatial representations employed a nonparametric permutation procedure and were corrected for multiple comparisons). This increase in size with eccentricity was expected, given the use of a constant stimulus size and the well-documented increase in the size of spatial receptive fields in early visual areas with increasing eccentricity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Gattass, R. et al. Cortical visual areas in monkeys: location, topography, connections, columns, plasticity and cortical dynamics. Phil. Trans. R. Soc. Lond. B 360, 709731 (2005)." href="/articles/nn.3574#ref-CR39" id="ref-link-section-d31016971e908">39</a></sup>. In addition, the size of the reconstructed stimulus representations also increased systematically from V1 to sPCS, which is consistent with the known expansion of mean spatial receptive field sizes in the parietal and frontal cortices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Ben Hamed, S., Duhamel, J.R., Bremmer, F. &amp; Graf, W. Visual receptive field modulation in the lateral intraparietal area during attentive fixation and free gaze. Cereb. Cortex 12, 234245 (2002)." href="/articles/nn.3574#ref-CR40" id="ref-link-section-d31016971e912">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Mohler, C.W., Goldberg, M.E. &amp; Wurtz, R.H. Visual receptive fields of frontal eye field neurons. Brain Res. 61, 385389 (1973)." href="/articles/nn.3574#ref-CR41" id="ref-link-section-d31016971e915">41</a></sup> (three-way ANOVA, significant main effect of ROI on fit size, <i>P</i> &lt; 0.0001).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Fit parameters to reconstructed spatial representations averaged across like eccentricities."><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 5: Fit parameters to reconstructed spatial representations averaged across like eccentricities.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3574/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig5_HTML.jpg?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig5_HTML.jpg" alt="figure 5" loading="lazy" width="685" height="336"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>For each participant, we fit a smooth two-dimensional surface (Online Methods) to the average reconstructed stimulus representation in all 36 locations separately for each task condition and ROI. We allowed the amplitude, baseline, size and center (<i>x</i>, <i>y</i> coordinate) of the fit basis function to vary freely during the fitting. Fit parameters were averaged within each participant across like eccentricities and then averaged across participants. The size of the best-fitting surface varied systematically with stimulus eccentricity and ROI but did not vary as a function of task condition. In contrast, the amplitude of the best fitting surface increased with attention in hV4, hMT+ and sPCS (with a marginal effect in IPS). Shown are the main effect of task condition (*), eccentricity () and interaction between task and eccentricity () at the <i>P</i> &lt; 0.05 level corrected for multiple comparisons (Online Methods). Gray symbols indicate trends at the <i>P</i> &lt; 0.025 level uncorrected for multiple comparisons. Error bars, within-participant s.e.m.</p><p>
                          <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM66">Source data</a>
                        </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3574/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>One alternative explanation is that the size of the represented stimulus increases with eccentricity because there is more trial-to-trial variability in the center point of the represented stimulus within reconstructions at more peripheral stimulus locations. In turn, this increase in trial-to-trial variability would 'smear' the spatial representations, leading to larger size estimates. However, our data speak against this possibility, as increased variability in the reconstructed stimulus locations would also result in lower estimated amplitudes, so increases in fit size and decreases in fit amplitude across conditions would always be yoked, and correlating the change in amplitude and the change in size within each eccentricity across each condition pair would reveal a negative correlation (for example, if the size of the spatial representation measured at a given eccentricity increased with attention, then the amplitude would decrease). No combinations of condition pair, eccentricity and ROI revealed a significant correlation between change in amplitude and change in size (all <i>P</i> &gt; 0.05, corrected using the false discovery rate (FDR); Online Methods). Furthermore, in a follow-up analysis, we computed the population receptive field (pRF) for each voxel<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Dumoulin, S.O. &amp; Wandell, B.A. Population receptive field estimates in human visual cortex. Neuroimage 39, 647660 (2008)." href="/articles/nn.3574#ref-CR42" id="ref-link-section-d31016971e967">42</a></sup>, which revealed that voxels tuned to more eccentric visual field positions have a larger pRF size (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM73">Supplementary Table 2</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM73">Supplementary Results</a> and Online Methods). This combination of analyses supports the conclusion that increases in fit size with increases in stimulus eccentricity are not due solely to increased variability in reconstructed spatial representations.</p><h3 class="c-article__sub-heading" id="Sec6">Effects of attention on spatial representations</h3><p>Despite being sensitive to expected changes in representation size on the basis of anatomical properties of the visual system, task demands exerted a negligible influence on the size of the reconstructed spatial representations, with no areas showing a significant effect (hV4 was closest at <i>P</i> = 0.033, but this did not survive correction for multiple comparisons, and <i>P</i> values in all other regions were &gt;0.147).</p><p>In contrast, the fit amplitudes in hV4, hMT+, IPS and sPCS were significantly modulated by task condition, with a higher amplitude in the attention and working memory conditions than in the fixation condition (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Fig. 5</a>; three-way ANOVA, main effect of task condition, <i>P</i> = 0.0003). For example, in hV4, the amplitude of the best fitting surface to the spatial representations of attended stimuli was higher during the attend stimulus and spatial working memory conditions as compared to the attend fixation condition (two-way ANOVA, main effect of task condition, <i>P</i> &lt; 0.0001). We observed similar effects in hMT+ (two-way ANOVA, <i>P</i> = 0.0007) and sPCS (two-way ANOVA, <i>P</i> = 0.0007). A similar pattern was evident in IPS as well, but it did not survive correction for multiple comparisons (two-way ANOVA, uncorrected <i>P</i> = 0.011). Within individual ROIs, there was a significant interaction between task condition and eccentricity in hMT+ (<i>P</i> = 0.0003), with larger increases in amplitude observed for more eccentric stimuli. It is notable that this increase in the amplitude of spatial representations with attention corresponds to a focal gain modulation that is restricted to the portion of visual space in the immediate neighborhood of the attended stimulus. Changes in fit amplitude do not result from a uniform, region-wide increase in BOLD signal that equally influences the response across an entire ROI; such a general and widespread modulation would be accounted for by an increase in the baseline fit parameter (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig9">Supplementary Fig. 2</a>). In addition, the impact of task condition on the amplitude of reconstructed spatial representations was more pronounced in later visual areas (hV4, hMT+, IPS and sPCS) compared to earlier areas (V1, V2 and V3) (three-way interaction between ROI, condition and eccentricity, <i>P</i> = 0.043).</p><p>In addition to an increase in the fit amplitude of the reconstructed spatial representations, IPS and sPCS also exhibited a spatially global increase in baseline response levels across the entire measured spatial representation in the attend stimulus and spatial working memory conditions compared to the attend fixation condition (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Fig. 5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig13">Supplementary Fig. 6</a>; two-way ANOVA, main effect of condition, IPS, <i>P</i> = 0.0014; sPCS, <i>P</i> = 0.0012). The spatially nonselective increases may reflect the fact that spatial receptive fields in these regions are often large enough to encompass the entire stimulus display<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Ben Hamed, S., Duhamel, J.R., Bremmer, F. &amp; Graf, W. Visual receptive field modulation in the lateral intraparietal area during attentive fixation and free gaze. Cereb. Cortex 12, 234245 (2002)." href="/articles/nn.3574#ref-CR40" id="ref-link-section-d31016971e1037">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Mohler, C.W., Goldberg, M.E. &amp; Wurtz, R.H. Visual receptive fields of frontal eye field neurons. Brain Res. 61, 385389 (1973)." href="/articles/nn.3574#ref-CR41" id="ref-link-section-d31016971e1040">41</a></sup>, so all stimuli might drive some increase in the response irrespective of spatial position.</p><h3 class="c-article__sub-heading" id="Sec7">Controlling difficulty across task conditions</h3><p>Slight differences in task difficulty in the first experiment (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2d</a>) might have contributed to the observed changes in spatial representations. To address this possibility, we ran four participants from the original cohort in a second experimental session while carefully equating behavioral performance across all three tasks (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">Fig. 6a</a>). Overall accuracy during this second session did not differ significantly across the three conditions, although we observed a similar interaction between task condition and stimulus eccentricity (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">Fig. 6a</a>; two-way repeated-measures ANOVA, main effect of condition, <i>F</i>(2,6) = 0.043, <i>P</i> = 0.96; condition  eccentricity interaction, <i>F</i>(10,30) = 3.28, <i>P</i> = 0.005; attend fixation, 78.8  2.80% (mean  s.e.m.); attend stimulus, 80.0  2.60%; spatial working memory, 79.8  1.76%). In addition, we also identified IPS visual field maps 03 (IPS0IPS3) using standard procedures so that we could more precisely characterize the effects of attention on stimulus representations in subregions of our larger IPS ROI<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Silver, M.A., Ress, D. &amp; Heeger, D.J. Topographic maps of visual spatial attention in human parietal cortex. J. Neurophysiol. 94, 13581371 (2005)." href="/articles/nn.3574#ref-CR31" id="ref-link-section-d31016971e1075">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Jerde, T.A., Merriam, E.P., Riggall, A.C., Hedges, J.H. &amp; Curtis, C.E. Prioritized maps of space in human frontoparietal cortex. J. Neurosci. 32, 1738217390 (2012)." href="/articles/nn.3574#ref-CR32" id="ref-link-section-d31016971e1078">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Sereno, M.I., Pitzalis, S. &amp; Martinez, A. Mapping of contralateral space in retinotopic coordinates by a parietal cortical area in humans. Science 294, 13501354 (2001)." href="/articles/nn.3574#ref-CR43" id="ref-link-section-d31016971e1081">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Swisher, J.D., Halko, M.A., Merabet, L.B., McMains, S.A. &amp; Somers, D.C. Visual topography of human intraparietal sulcus. J. Neurosci. 27, 53265337 (2007)." href="/articles/nn.3574#ref-CR44" id="ref-link-section-d31016971e1084">44</a></sup> (Online Methods and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig14">Supplementary Fig. 7</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Results are consistent when task difficulty is matched."><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 6: Results are consistent when task difficulty is matched.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3574/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig6_HTML.jpg?as=webp"><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig6_HTML.jpg" alt="figure 6" loading="lazy" width="685" height="745"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>(<b>a</b>) Performance of four participants who were rescanned while carefully matching task difficulty across all three experimental conditions. As in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Figure 2d</a>, the participants' performance is better in the attend fixation task when the checkerboard is presented in the periphery, and performance in the attend stimulus and spatial working memory tasks is better when the stimulus is presented near the fovea. (<b>b</b>) A subset of illustrative reconstructed stimulus representations from V1, hV4, hMT+ and IPS0 and IPS1 (IPS0/1) averaged across like eccentricities (correct trials only, with the number of averaged trials indicated as insets). <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig14">Supplementary Figure 7</a> includes details about IPS subregion identification.</p><p>
                          <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM67">Source data</a>
                        </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3574/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>To ensure that behavioral performance was not unduly biasing our results, we reconstructed spatial representations using only correct trials (<span class="stix"></span>80% of total trials; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">Fig. 6a</a>). All representations were co-registered on the basis of stimulus eccentricity before averaging (the corresponding eccentricity points are shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2b</a>). Even though our sample size was smaller (<i>n</i> = 4 as compared to <i>n</i> = 8), the influence of attention on the topography of the spatial representations was similar to that in our initial observations (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">Fig. 6b</a>). In addition, mapping out retinotopic subregions of the IPS revealed that the functionally defined IPS ROI (shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Fig. 5</a>) corresponds primarily to IPS0 and IPS1 (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig14">Supplementary Fig. 7a,b</a>).</p><p>When examining best-fit surfaces to the spatial representations from this experiment (we computed the fits using co-registered representations and only correct trials for each participant; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">Fig. 7</a> and Online Methods), we found that attention significantly modulated amplitude across all regions (three-way ANOVA, main effect of task condition, <i>P</i> = 0.0162). When considered in isolation, only hV4 showed a significant change in amplitude with attention after correction for multiple comparisons (two-way repeated-measures ANOVA, <i>P</i> = 0.0022). However, we observed similar trends in V1, V2 and V3 (uncorrected <i>P</i> = 0.0243, 0.042 and 0.031, respectively). We found no significant main effect of task condition on the size of the representations (all <i>P</i> &gt; 0.135, with the minimum <i>P</i> found for hMT+), and the overall baseline levels significantly increased as a function of task condition in hMT+ only (<i>P</i> = 0.00197). Across all ROIs, there was a main effect of eccentricity on fit size (three-way ANOVA, <i>P</i> = 0.0016) but no main effect of task condition on fit size (three-way ANOVA, <i>P</i> = 0.423).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7" data-title="Fit parameters to spatial representations after controlling for task difficulty."><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 7: Fit parameters to spatial representations after controlling for task difficulty.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.3574/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig7_HTML.jpg?as=webp"><img aria-describedby="Fig7" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig7_HTML.jpg" alt="figure 7" loading="lazy" width="685" height="335"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>As in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figure 5</a>, a surface was fit to the averaged, co-registered spatial representations for each participant. However, in this case, task difficulty was carefully matched between conditions, and representations were based solely on trials in which the participant made a correct behavioral response (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">Fig. 6b</a>). The results are similar to those reported in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figure 5</a>: attention acts to increase the fit amplitude of spatial representations in hV4 but does not act to decrease size. In hMT+, attention also acted in a nonlocalized manner to increase the baseline parameter. The statistics and symbols are as in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figure 5</a>. Error bars, within-participant s.e.m.</p><p>
                          <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM68">Source data</a>
                        </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.3574/figures/7" data-track-dest="link:Figure7 Full size image" aria-label="Full size image figure 7" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec8">pRFs expand with attention</h3><p>For these same four participants, we computed the pRF<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Dumoulin, S.O. &amp; Wandell, B.A. Population receptive field estimates in human visual cortex. Neuroimage 39, 647660 (2008)." href="/articles/nn.3574#ref-CR42" id="ref-link-section-d31016971e1231">42</a></sup> for each voxel in V1, hV4, hMT+ and IPS0 using data from the behaviorally controlled replication experiment. We computed pRFs by first using the initial step of our encoding-model estimation procedure (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3a</a>) to determine the response of each voxel to each position in the visual field (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Figs. 8</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">9</a> and Online Methods). We then fit each voxel's response profile with the same surface that we used to characterize the spatial representations. By comparing pRFs computed using data from each condition independently, we found that a majority of the pRFs in hV4, hMT+ and IPS0 increased in size during either the attend stimulus or spatial working memory condition as compared to the attend fixation condition. In contrast, pRF size in V1 was not significantly modulated by attention (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">Supplementary Fig. 9</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM73">Supplementary Results</a>).</p><p>To reconcile the results that voxel-level pRFs expanded with attention yet region-level spatial representations remained at a constant size, we simulated data using estimated pRF parameters from hV4 (a region for which spatial representations increase in amplitude and pRFs increase in size; Online Methods) under different pRF modulation conditions. In the first condition, we generated simulated data using pRFs with sizes centered around two mean values, which resulted in a pRF scaling across all simulated voxels (the average size across voxels increased, but some voxels decreased in size and others increased). Under these conditions, spatial representations increased in size (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">Supplementary Fig. 10a,b</a>). In a second pRF modulation scenario, we used the fit pRF values from one participant's hV4 ROI (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Fig. 8</a>) to simulate data. In this case, spatial representations remained the same size but increased in amplitude, which is consistent with our observations using real data (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figs. 5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">Supplementary Fig. 10c,d</a>; this conclusion was also supported when we used pRF data from the other three observers to seed the simulation). Thus, the pattern of pRF modulations across all voxels enhances the amplitude of spatial representations while preserving their size.</p></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Discussion</h2><div class="c-article-section__content" id="Sec9-content"><p>Spatial attention has previously been shown to alter the gain of single-unit responses that are associated with relevant visual features such as orientation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Luck, S.J., Chelazzi, L., Hillyard, S.A. &amp; Desimone, R. Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex. J. Neurophysiol. 77, 2442 (1997)." href="/articles/nn.3574#ref-CR7" id="ref-link-section-d31016971e1278">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Reynolds, J.H., Chelazzi, L. &amp; Desimone, R. Competitive mechanisms subserve attention in macaque areas V2 and V4. J. Neurosci. 19, 17361753 (1999)." href="/articles/nn.3574#ref-CR8" id="ref-link-section-d31016971e1281">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="McAdams, C.J. &amp; Maunsell, J.H.R. Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4. J. Neurosci. 19, 431441 (1999)." href="/articles/nn.3574#ref-CR9" id="ref-link-section-d31016971e1284">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Reynolds, J.H., Pasternak, T. &amp; Desimone, R. Attention increases sensitivity of V4 neurons. Neuron 26, 703714 (2000)." href="/articles/nn.3574#ref-CR12" id="ref-link-section-d31016971e1287">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="McAdams, C.J. &amp; Maunsell, J.H.R. Attention to both space and feature modulates neuronal responses in macaque area V4. J. Neurophysiol. 83, 17511755 (2000)." href="/articles/nn.3574#ref-CR13" id="ref-link-section-d31016971e1290">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Motter, B.C. Focal attention produces spatially selective processing in visual cortical areas V1, V2, and V4 in the presence of competing stimuli. J. Neurophysiol. 70, 909919 (1993)." href="/articles/nn.3574#ref-CR16" id="ref-link-section-d31016971e1293">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Moran, J. &amp; Desimone, R. Selective attention gates visual processing in the extrastriate cortex. Science 229, 782784 (1985)." href="/articles/nn.3574#ref-CR17" id="ref-link-section-d31016971e1297">17</a></sup> and motion direction<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Treue, S. &amp; Maunsell, J.H.R. Attentional modulation of visual motion processing in cortical areas MT and MST. Nature 382, 539541 (1996)." href="/articles/nn.3574#ref-CR11" id="ref-link-section-d31016971e1301">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Treue, S. &amp; Maunsell, J.H.R. Effects of attention on the processing of motion in macaque middle temporal and medial superior temporal visual cortical areas. J. Neurosci. 19, 75917602 (1999)." href="/articles/nn.3574#ref-CR14" id="ref-link-section-d31016971e1304">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Seidemann, E. &amp; Newsome, W.T. Effect of spatial attention on the responses of area MT neurons. J. Neurophysiol. 81, 17831794 (1999)." href="/articles/nn.3574#ref-CR15" id="ref-link-section-d31016971e1307">15</a></sup>, as well as modulate the size of spatial receptive fields<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Connor, C.E., Preddie, D.C., Gallant, J.L. &amp; Van Essen, D.C. Spatial attention effects in macaque area V4. J. Neurosci. 17, 32013214 (1997)." href="/articles/nn.3574#ref-CR10" id="ref-link-section-d31016971e1311">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Womelsdorf, T., Anton-Erxleben, K., Pieper, F. &amp; Treue, S. Dynamic shifts of visual receptive fields in cortical area MT by spatial attention. Nat. Neurosci. 9, 11561160 (2006)." href="/articles/nn.3574#ref-CR19" id="ref-link-section-d31016971e1314">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Womelsdorf, T., Anton-Erxleben, K. &amp; Treue, S. Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation. J. Neurosci. 28, 89348944 (2008)." href="/articles/nn.3574#ref-CR20" id="ref-link-section-d31016971e1317">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Anton-Erxleben, K., Stephan, V.M. &amp; Treue, S. Attention reshapes center-surround receptive field structure in macaque cortical area MT. Cereb. Cortex 19, 24662478 (2009)." href="/articles/nn.3574#ref-CR21" id="ref-link-section-d31016971e1320">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Niebergall, R., Khayat, P.S., Treue, S. &amp; Martinez-Trujillo, J.C. Expansion of MT neurons excitatory receptive fields during covert attentive tracking. J. Neurosci. 31, 1549915510 (2011)." href="/articles/nn.3574#ref-CR22" id="ref-link-section-d31016971e1323">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Anton-Erxleben, K. &amp; Carrasco, M. Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence. Nat. Rev. Neurosci. 14, 188200 (2013)." href="/articles/nn.3574#ref-CR23" id="ref-link-section-d31016971e1326">23</a></sup>. Here we show that these local modulations operate jointly to increase the overall amplitude of the region-level spatial representation of an attended stimulus without changing its represented size. Furthermore, these amplitude modulations were especially apparent in later areas of the visual system such as hV4, hMT+ and IPS, which is consistent with predictions made by computational theories of attentional priority maps<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Serences, J.T. &amp; Yantis, S. Selective visual attention and perceptual coherence. Trends Cogn. Sci. 10, 3845 (2006)." href="/articles/nn.3574#ref-CR4" id="ref-link-section-d31016971e1330">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Fecteau, J.H. &amp; Munoz, D.P. Salience, relevance, and firing: a priority map for target selection. Trends Cogn. Sci. 10, 382390 (2006)." href="/articles/nn.3574#ref-CR5" id="ref-link-section-d31016971e1333">5</a></sup>.</p><p>We were able to reconstruct robust spatial representations across a range of eccentricities and for all three task conditions in all measured ROIs. Notably, even though we used an identical reconstruction procedure in all areas, the size of the reconstructed spatial representations increased from early to later visual areas (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Fig. 5</a>). Single-unit receptive field sizes across cortical regions are thought to increase in a similar manner<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Gattass, R. et al. Cortical visual areas in monkeys: location, topography, connections, columns, plasticity and cortical dynamics. Phil. Trans. R. Soc. Lond. B 360, 709731 (2005)." href="/articles/nn.3574#ref-CR39" id="ref-link-section-d31016971e1343">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Ben Hamed, S., Duhamel, J.R., Bremmer, F. &amp; Graf, W. Visual receptive field modulation in the lateral intraparietal area during attentive fixation and free gaze. Cereb. Cortex 12, 234245 (2002)." href="/articles/nn.3574#ref-CR40" id="ref-link-section-d31016971e1346">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Mohler, C.W., Goldberg, M.E. &amp; Wurtz, R.H. Visual receptive fields of frontal eye field neurons. Brain Res. 61, 385389 (1973)." href="/articles/nn.3574#ref-CR41" id="ref-link-section-d31016971e1349">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Saygin, A.P. &amp; Sereno, M.I. Retinotopy and attention in human occipital, temporal, parietal, and frontal cortex. Cereb. Cortex 18, 21582168 (2008)." href="/articles/nn.3574#ref-CR45" id="ref-link-section-d31016971e1352">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Kastner, S. et al. Modulation of sensory suppression: implications for receptive field sizes in the human visual cortex. J. Neurophysiol. 86, 13981411 (2001)." href="/articles/nn.3574#ref-CR46" id="ref-link-section-d31016971e1355">46</a></sup>. In addition, representations of stimuli presented at higher eccentricities were larger than representations of stimuli presented near the fovea, which also corresponds to known changes in receptive field size with eccentricity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Gattass, R. et al. Cortical visual areas in monkeys: location, topography, connections, columns, plasticity and cortical dynamics. Phil. Trans. R. Soc. Lond. B 360, 709731 (2005)." href="/articles/nn.3574#ref-CR39" id="ref-link-section-d31016971e1359">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Dumoulin, S.O. &amp; Wandell, B.A. Population receptive field estimates in human visual cortex. Neuroimage 39, 647660 (2008)." href="/articles/nn.3574#ref-CR42" id="ref-link-section-d31016971e1362">42</a></sup>. Furthermore, simulating data under conditions in which we uniformly scaled the mean size of voxel-level pRFs revealed that such changes are detectable using our analysis method (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">Supplementary Fig. 10a,b</a>). Thus, this technique is sensitive to detecting changes in the size of spatial representations of stimuli that are driven by known neural constraints such as relative differences in receptive field size across cortical ROIs and eccentricities, even though these factors are not built in to the spatial encoding model. Together these empirical and modeling results suggest that at the level of region-wide priority maps, the representation of a stimulus does not expand or contract under the attentional conditions tested here, and they underscore the importance of incorporating response changes across all encoding units when evaluating attentional modulations.</p><p>The quantification method we implemented for measuring changes in spatial representations across tasks, eccentricities and ROIs involved fitting a surface that was defined by several parameters: center location, amplitude, baseline offset and size (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig9">Supplementary Fig. 2</a>). Changes in activation that carry no information about stimulus location (such as changes in general arousal or responsiveness to stimuli presented in all locations because of large receptive fields) will influence the baseline parameter, as such changes reflect increased or decreased signal across an entire region. In contrast, a change in the spatial representation that changes the representation of a visual stimulus would result in a change in the amplitude or size parameter (or both). Here we demonstrated that attention operates primarily by selectively increasing the amplitude of stimulus representations in several putative priority maps (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figs. 5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a>) rather than increasing the overall BOLD signal more generally across entire regions.</p><p>Notably, spatial reconstructions based on activation patterns from sPCS were relatively inaccurate compared to other ROIs, and this ROI primarily exhibited increases in the fit baseline parameter (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Fig. 5</a>). This region, which may be a human homolog of the functionally defined macaque frontal eye field<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Srimal, R. &amp; Curtis, C.E. Persistent neural activity during the maintenance of spatial position in working memory. Neuroimage 39, 455468 (2008)." href="/articles/nn.3574#ref-CR47" id="ref-link-section-d31016971e1387">47</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Paus, T. Location and function of the human frontal eye-field: a selective review. Neuropsychologia 34, 475483 (1996)." href="/articles/nn.3574#ref-CR48" id="ref-link-section-d31016971e1390">48</a></sup> (FEF), might have showed degraded spatial selectivity in the present study because of the relatively large size of spatial receptive fields observed in many FEF neurons (typically 20 diameter<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Mohler, C.W., Goldberg, M.E. &amp; Wurtz, R.H. Visual receptive fields of frontal eye field neurons. Brain Res. 61, 385389 (1973)." href="/articles/nn.3574#ref-CR41" id="ref-link-section-d31016971e1394">41</a></sup>) and the small area subtended by our stimulus display (9.31 across horizontally). Consistent with this possibility, previous reports of retinotopic organization in the human frontal cortex used stimuli that were presented at higher eccentricities in order to resolve spatial maps (10 (ref. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Kastner, S. et al. Topographic maps in human frontal cortex revealed in memory-guided saccade and spatial working-memory tasks. J. Neurophysiol. 97, 34943507 (2007)." href="/articles/nn.3574#ref-CR49" id="ref-link-section-d31016971e1397">49</a>) to 25 (ref. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Saygin, A.P. &amp; Sereno, M.I. Retinotopy and attention in human occipital, temporal, parietal, and frontal cortex. Cereb. Cortex 18, 21582168 (2008)." href="/articles/nn.3574#ref-CR45" id="ref-link-section-d31016971e1400">45</a>)).</p><h3 class="c-article__sub-heading" id="Sec10">Attentional priority maps</h3><p>The extensive literature on spatial salience or priority maps<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Koch, C. &amp; Ullman, S. Shifts in selective visual attention: towards the underlying neural circuitry. Hum. Neurobiol. 4, 219227 (1985)." href="/articles/nn.3574#ref-CR1" id="ref-link-section-d31016971e1412">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Itti, L., Koch, C. &amp; Niebur, E. A model of saliency-based visual attention for rapid scene analysis. IEEE Trans. Pattern Anal. Mach. Intell. 20, 12541259 (1998)." href="/articles/nn.3574#ref-CR2" id="ref-link-section-d31016971e1415">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Itti, L. &amp; Koch, C. Computational modelling of visual attention. Nat. Rev. Neurosci. 2, 194203 (2001)." href="/articles/nn.3574#ref-CR3" id="ref-link-section-d31016971e1418">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Serences, J.T. &amp; Yantis, S. Selective visual attention and perceptual coherence. Trends Cogn. Sci. 10, 3845 (2006)." href="/articles/nn.3574#ref-CR4" id="ref-link-section-d31016971e1421">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Fecteau, J.H. &amp; Munoz, D.P. Salience, relevance, and firing: a priority map for target selection. Trends Cogn. Sci. 10, 382390 (2006)." href="/articles/nn.3574#ref-CR5" id="ref-link-section-d31016971e1424">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bichot, N.P. &amp; Schall, J.D. Effects of similarity and history on neural mechanisms of visual selection. Nat. Neurosci. 2, 549554 (1999)." href="/articles/nn.3574#ref-CR6" id="ref-link-section-d31016971e1427">6</a></sup> postulates the existence of one or several maps of visual space, each carrying information about behaviorally relevant objects within the visual scene. Furthermore, priority maps in early visual areas (for example, V1) are thought to encode primarily low-level stimulus features (for example, contrast), whereas priority maps in later regions are thought to increasingly weight behavioral relevance over low-level stimulus attributes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Serences, J.T. &amp; Yantis, S. Selective visual attention and perceptual coherence. Trends Cogn. Sci. 10, 3845 (2006)." href="/articles/nn.3574#ref-CR4" id="ref-link-section-d31016971e1431">4</a></sup>. Although many important insights have stemmed from observing single-unit responses as a function of changes in attentional priority (reviewed in ref. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Fecteau, J.H. &amp; Munoz, D.P. Salience, relevance, and firing: a priority map for target selection. Trends Cogn. Sci. 10, 382390 (2006)." href="/articles/nn.3574#ref-CR5" id="ref-link-section-d31016971e1434">5</a>), these results provide information about how isolated pixels in a priority map change under different task conditions.</p><p>A previous fMRI study used multivariate decoding (classification) analyses to identify several frontal and parietal ROIs that exhibit similar activation patterns during covert attention, spatial working memory and saccade generation tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Jerde, T.A., Merriam, E.P., Riggall, A.C., Hedges, J.H. &amp; Curtis, C.E. Prioritized maps of space in human frontoparietal cortex. J. Neurosci. 32, 1738217390 (2012)." href="/articles/nn.3574#ref-CR32" id="ref-link-section-d31016971e1441">32</a></sup>. These results provide strong support for the notion that common priority maps support representations of attentional priority across multiple tasks. Here we assessed how the holistic landscape across these priority maps measured using fMRI changed as attention was systematically varied. Our demonstration that spatial representation amplitude was enhanced with attention in later, but not earlier, ROIs supports the hypothesis that priority maps in higher areas are increasingly dominated by attentional factors and suggests that these attentional modulations of priority maps operate by scaling the amplitude of the behaviorally relevant item without changing its represented size.</p><h3 class="c-article__sub-heading" id="Sec11">pRFs</h3><p>In addition to measuring spatial representations that are carried by the pattern of activation across entire visual regions, we also estimated the voxel-level pRFs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Dumoulin, S.O. &amp; Wandell, B.A. Population receptive field estimates in human visual cortex. Neuroimage 39, 647660 (2008)." href="/articles/nn.3574#ref-CR42" id="ref-link-section-d31016971e1453">42</a></sup> for a subset of participants and ROIs by adding constraints to our encoding-model estimation procedure (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Figs. 8</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">9</a> and Online Methods). This alternative tool has been used previously to evaluate the aggregate spatial receptive field profile across all neural populations within voxels that belong to different visual ROIs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Dumoulin, S.O. &amp; Wandell, B.A. Population receptive field estimates in human visual cortex. Neuroimage 39, 647660 (2008)." href="/articles/nn.3574#ref-CR42" id="ref-link-section-d31016971e1463">42</a></sup>.</p><p>Changes in voxel-level pRFs can inform how a region dynamically adjusts the spatial sensitivity of its constituent filters in order to modulate its overall spatial priority map. First, we replicated the typical results that voxel-level pRFs tuned for more eccentric visual field positions are larger in size (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM73">Supplementary Table 2</a>) and that pRFs for later visual regions tend to be larger than pRFs for earlier visual regions (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">Supplementary Fig. 9</a>). Second, results from this complementary analysis revealed that in regions that showed enhanced spatial representation amplitude with attention (hV4, hMT+ and IPS0), pRF size increased (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Figs. 8</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">9</a>), even though the corresponding region-level spatial representations did not increase in size (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">Fig. 7</a>). This may seem like a disconnect, given that the particular pattern of pRF changes across all voxels within a region jointly shapes how the spatial priority map changes with attention. However, there is not necessarily a monotonic mapping between the size of the constituent filters and the size of population-level spatial representations. Indeed, simulations based on the observed pattern of pRF changes with attention give rise to region-level increases in representation amplitude in the absence of changes in representation size, as we observed in our data (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">Supplementary Fig. 10</a>). This finding, together with our primary results concerning region-level spatial representations, provides evidence that attentional modulation of spatial information encoding is a process that strongly benefits from study at the large-scale population level.</p><h3 class="c-article__sub-heading" id="Sec12">Comparison to previous results</h3><p>At the level of single-unit recordings, attention has been shown to decrease the size of MT spatial receptive fields when an animal is attending to a stimulus that is encompassed by the recorded neuron's receptive field<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Womelsdorf, T., Anton-Erxleben, K., Pieper, F. &amp; Treue, S. Dynamic shifts of visual receptive fields in cortical area MT by spatial attention. Nat. Neurosci. 9, 11561160 (2006)." href="/articles/nn.3574#ref-CR19" id="ref-link-section-d31016971e1497">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Womelsdorf, T., Anton-Erxleben, K. &amp; Treue, S. Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation. J. Neurosci. 28, 89348944 (2008)." href="/articles/nn.3574#ref-CR20" id="ref-link-section-d31016971e1500">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Anton-Erxleben, K., Stephan, V.M. &amp; Treue, S. Attention reshapes center-surround receptive field structure in macaque cortical area MT. Cereb. Cortex 19, 24662478 (2009)." href="/articles/nn.3574#ref-CR21" id="ref-link-section-d31016971e1503">21</a></sup> and to increase the size of spatial receptive fields when an animal is attending nearby the recorded neuron's receptive field<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Womelsdorf, T., Anton-Erxleben, K. &amp; Treue, S. Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation. J. Neurosci. 28, 89348944 (2008)." href="/articles/nn.3574#ref-CR20" id="ref-link-section-d31016971e1507">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Anton-Erxleben, K., Stephan, V.M. &amp; Treue, S. Attention reshapes center-surround receptive field structure in macaque cortical area MT. Cereb. Cortex 19, 24662478 (2009)." href="/articles/nn.3574#ref-CR21" id="ref-link-section-d31016971e1510">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Niebergall, R., Khayat, P.S., Treue, S. &amp; Martinez-Trujillo, J.C. Expansion of MT neurons excitatory receptive fields during covert attentive tracking. J. Neurosci. 31, 1549915510 (2011)." href="/articles/nn.3574#ref-CR22" id="ref-link-section-d31016971e1513">22</a></sup>. In V4, spatial receptive fields appear to shift toward the attended region of space in a subset of neurons<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Connor, C.E., Preddie, D.C., Gallant, J.L. &amp; Van Essen, D.C. Spatial attention effects in macaque area V4. J. Neurosci. 17, 32013214 (1997)." href="/articles/nn.3574#ref-CR10" id="ref-link-section-d31016971e1517">10</a></sup>. With respect to cortical space, these single-unit attentional modulations of spatial receptive fields suggest that unifocal attention may act to increase the cortical surface area that is responsive to a stimulus of constant size. Consistent with this prediction, our measured pRFs for extrastriate regions of hV4, hMT+ and IPS0 increased in size with attention.</p><p>In contrast, one previous report suggested that spatial attention instead narrows the activation profile along the cortical surface of the visual cortex in response to a visual stimulus<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Fischer, J. &amp; Whitney, D. Attention narrows position tuning of population responses in V1. Curr. Biol. 19, 13561361 (2009)." href="/articles/nn.3574#ref-CR50" id="ref-link-section-d31016971e1524">50</a></sup>. However, this inference was based on patterns of intertrial correlations between BOLD activation patterns that were associated with dividing attention between four stimuli (one presented in each quadrant). These patterns were suggested to result from a combination of attention-related gain and narrowing of population-level responses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Fischer, J. &amp; Whitney, D. Attention narrows position tuning of population responses in V1. Curr. Biol. 19, 13561361 (2009)." href="/articles/nn.3574#ref-CR50" id="ref-link-section-d31016971e1528">50</a></sup>, that is, a narrower response along the cortical surface with attention.</p><p>We did not observe any significant attention-related changes in the size of the reconstructed spatial representations in either the primary visual cortex or other areas in the extrastriate, parietal or frontal cortices. However, the tasks performed by observers and the analysis techniques implemented were very different between these studies. Most notably, observers in the present study and in previous fMRI<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Liu, T., Pestilli, F. &amp; Carrasco, M. Transient attention enhances perceptual performance and FMRI response in human visual cortex. Neuron 45, 469477 (2005)." href="/articles/nn.3574#ref-CR24" id="ref-link-section-d31016971e1535">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Gandhi, S.P., Heeger, D.J. &amp; Boynton, G.M. Spatial attention affects brain activity in human primary visual cortex. Proc. Natl. Acad. Sci. USA 96, 33143319 (1999)." href="/articles/nn.3574#ref-CR25" id="ref-link-section-d31016971e1538">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Kastner, S., Pinsk, M.A., De Weerd, P., Desimone, R. &amp; Ungerleider, L.G. Increased activity in human visual cortex during directed attention in the absence of visual stimulation. Neuron 22, 751761 (1999)." href="/articles/nn.3574#ref-CR26" id="ref-link-section-d31016971e1541">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Brefczynski, J.A. &amp; DeYoe, E.A. A physiological correlate of the spotlight of visual attention. Nat. Neurosci. 2, 370374 (1999)." href="/articles/nn.3574#ref-CR27" id="ref-link-section-d31016971e1544">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Silver, M.A., Ress, D. &amp; Heeger, D.J. Neural correlates of sustained spatial attention in human early visual cortex. J. Neurophysiol. 97, 229237 (2007)." href="/articles/nn.3574#ref-CR28" id="ref-link-section-d31016971e1547">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Tootell, R.B. et al. The retinotopy of visual spatial attention. Neuron 21, 14091422 (1998)." href="/articles/nn.3574#ref-CR29" id="ref-link-section-d31016971e1550">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Murray, S.O. The effects of spatial attention in early human visual cortex are stimulus independent. J. Vis. 8, 2.12.11 (2008)." href="/articles/nn.3574#ref-CR30" id="ref-link-section-d31016971e1554">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Silver, M.A., Ress, D. &amp; Heeger, D.J. Topographic maps of visual spatial attention in human parietal cortex. J. Neurophysiol. 94, 13581371 (2005)." href="/articles/nn.3574#ref-CR31" id="ref-link-section-d31016971e1557">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Jerde, T.A., Merriam, E.P., Riggall, A.C., Hedges, J.H. &amp; Curtis, C.E. Prioritized maps of space in human frontoparietal cortex. J. Neurosci. 32, 1738217390 (2012)." href="/articles/nn.3574#ref-CR32" id="ref-link-section-d31016971e1560">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Jehee, J.F.M., Brady, D.K. &amp; Tong, F. Attention improves encoding of task-relevant features in the human visual cortex. J. Neurosci. 31, 82108219 (2011)." href="/articles/nn.3574#ref-CR33" id="ref-link-section-d31016971e1563">33</a></sup> and single-unit studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Connor, C.E., Preddie, D.C., Gallant, J.L. &amp; Van Essen, D.C. Spatial attention effects in macaque area V4. J. Neurosci. 17, 32013214 (1997)." href="/articles/nn.3574#ref-CR10" id="ref-link-section-d31016971e1567">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Womelsdorf, T., Anton-Erxleben, K., Pieper, F. &amp; Treue, S. Dynamic shifts of visual receptive fields in cortical area MT by spatial attention. Nat. Neurosci. 9, 11561160 (2006)." href="/articles/nn.3574#ref-CR19" id="ref-link-section-d31016971e1570">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Womelsdorf, T., Anton-Erxleben, K. &amp; Treue, S. Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation. J. Neurosci. 28, 89348944 (2008)." href="/articles/nn.3574#ref-CR20" id="ref-link-section-d31016971e1573">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Anton-Erxleben, K., Stephan, V.M. &amp; Treue, S. Attention reshapes center-surround receptive field structure in macaque cortical area MT. Cereb. Cortex 19, 24662478 (2009)." href="/articles/nn.3574#ref-CR21" id="ref-link-section-d31016971e1576">21</a></sup> were typically required to attend to a single stimulus, whereas population-level activation narrowing was observed when participants simultaneously attended to the precise spatial position of four Gabor stimuli, one presented in each visual quadrant<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Fischer, J. &amp; Whitney, D. Attention narrows position tuning of population responses in V1. Curr. Biol. 19, 13561361 (2009)." href="/articles/nn.3574#ref-CR50" id="ref-link-section-d31016971e1580">50</a></sup>. Furthermore, our observation that pRFs increased in size during the attend stimulus and spatial working memory conditions is compatible with the pattern of spatial receptive field changes in single units<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Connor, C.E., Preddie, D.C., Gallant, J.L. &amp; Van Essen, D.C. Spatial attention effects in macaque area V4. J. Neurosci. 17, 32013214 (1997)." href="/articles/nn.3574#ref-CR10" id="ref-link-section-d31016971e1584">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Womelsdorf, T., Anton-Erxleben, K., Pieper, F. &amp; Treue, S. Dynamic shifts of visual receptive fields in cortical area MT by spatial attention. Nat. Neurosci. 9, 11561160 (2006)." href="/articles/nn.3574#ref-CR19" id="ref-link-section-d31016971e1587">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Womelsdorf, T., Anton-Erxleben, K. &amp; Treue, S. Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation. J. Neurosci. 28, 89348944 (2008)." href="/articles/nn.3574#ref-CR20" id="ref-link-section-d31016971e1590">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Anton-Erxleben, K., Stephan, V.M. &amp; Treue, S. Attention reshapes center-surround receptive field structure in macaque cortical area MT. Cereb. Cortex 19, 24662478 (2009)." href="/articles/nn.3574#ref-CR21" id="ref-link-section-d31016971e1593">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Niebergall, R., Khayat, P.S., Treue, S. &amp; Martinez-Trujillo, J.C. Expansion of MT neurons excitatory receptive fields during covert attentive tracking. J. Neurosci. 31, 1549915510 (2011)." href="/articles/nn.3574#ref-CR22" id="ref-link-section-d31016971e1596">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Anton-Erxleben, K. &amp; Carrasco, M. Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence. Nat. Rev. Neurosci. 14, 188200 (2013)." href="/articles/nn.3574#ref-CR23" id="ref-link-section-d31016971e1599">23</a></sup>, and our data and simulations show that these local changes can result in a region-level representation that changes only in amplitude and not in size (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">Supplementary Fig. 10</a>).</p><p>Collectively, it seems probable that the exact task demands (unifocal as compared to multifocal attention) and stimulus properties (single stimulus as compared to multiple stimuli) may have a key role in determining how attention influences the profile of spatial representations. Future work using analysis methods that are sensitive to region-level differences in spatial representations (for example, applying encoding models similar to that described here to data acquired when participants perform different tasks) in conjunction with careful identification of neural receptive field properties across those task-demand conditions (for example, from simultaneous multiunit electrophysiological recordings or <i>in vivo</i> two-photon Ca<sup>2+</sup> imaging in rodents and primates) may provide complementary insights into when and how attention changes the shape and/or amplitude of stimulus representations in spatial priority maps and how those changes are implemented in the neural circuitry.</p><p>Notably, although our observations are largely consistent with measured receptive field changes at the single-unit level<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Connor, C.E., Preddie, D.C., Gallant, J.L. &amp; Van Essen, D.C. Spatial attention effects in macaque area V4. J. Neurosci. 17, 32013214 (1997)." href="/articles/nn.3574#ref-CR10" id="ref-link-section-d31016971e1618">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Womelsdorf, T., Anton-Erxleben, K., Pieper, F. &amp; Treue, S. Dynamic shifts of visual receptive fields in cortical area MT by spatial attention. Nat. Neurosci. 9, 11561160 (2006)." href="/articles/nn.3574#ref-CR19" id="ref-link-section-d31016971e1621">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Womelsdorf, T., Anton-Erxleben, K. &amp; Treue, S. Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation. J. Neurosci. 28, 89348944 (2008)." href="/articles/nn.3574#ref-CR20" id="ref-link-section-d31016971e1624">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Anton-Erxleben, K., Stephan, V.M. &amp; Treue, S. Attention reshapes center-surround receptive field structure in macaque cortical area MT. Cereb. Cortex 19, 24662478 (2009)." href="/articles/nn.3574#ref-CR21" id="ref-link-section-d31016971e1627">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Niebergall, R., Khayat, P.S., Treue, S. &amp; Martinez-Trujillo, J.C. Expansion of MT neurons excitatory receptive fields during covert attentive tracking. J. Neurosci. 31, 1549915510 (2011)." href="/articles/nn.3574#ref-CR22" id="ref-link-section-d31016971e1630">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Anton-Erxleben, K. &amp; Carrasco, M. Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence. Nat. Rev. Neurosci. 14, 188200 (2013)." href="/articles/nn.3574#ref-CR23" id="ref-link-section-d31016971e1633">23</a></sup>, we cannot make direct inferences that such single-unit changes are in fact occurring. A number of mechanisms, including one in which only the gain of different populations is modulated by attention, could also account for the pattern of results we saw in both our region-level spatial representations (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figs. 5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a>) and our pRF measurements (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Figs. 8</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">9</a>). We note, however, that some neural mechanisms are highly unlikely given our measured spatial representations and pRFs. For example, we would not observe an increase in pRF size if spatial receptive fields of neurons within those voxels were to exclusively narrow with attention. As a result of these interpretational concerns, we restrict the inferences we draw from our results to the role of attention in modulating region-level spatial priority maps measured with fMRI and make no direct claims about spatial information coding at a neural level.</p><h3 class="c-article__sub-heading" id="Sec13">Information content of attentional priority maps</h3><p>One consequence of an observed increase in the amplitude of reconstructed priority maps is that the mutual information between the stimulus position and the observed BOLD responses should increase (a more complete discussion is provided in ref. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Saproo, S. &amp; Serences, J.T. Spatial attention improves the quality of population codes in human visual cortex. J. Neurophysiol. 104, 885895 (2010)." href="/articles/nn.3574#ref-CR18" id="ref-link-section-d31016971e1656">18</a>). This increase can occur, in theory, because mutual information reflects the ratio of signal entropy (the variability in neural responses that is tied systematically to changes in the stimulus) to noise entropy (the variability in neural responses that is not tied to changes in the stimulus). Thus a multiplicative increase in the gain of the neural responses that are associated with an attended stimulus should increase mutual information because it will increase the variability of responses that are associated with an attended stimulus location, which will in turn increase signal entropy. In contrast, a purely additive shift in all neural responses (reflected by an increase in the fit baseline parameter) will not increase the dynamic range of responses that are associated with an attended stimulus location, causing the mutual information to either remain constant (under a constant additive noise model) or even decrease (under a Poisson noise model, in which noise increases with the mean). Previous fMRI work on spatial attention has not attempted to disentangle these two potential sources of increases in the BOLD signal, highlighting the utility of approaches that can support more precise inferences about how task demands influence region-level neural codes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Liu, T., Pestilli, F. &amp; Carrasco, M. Transient attention enhances perceptual performance and FMRI response in human visual cortex. Neuron 45, 469477 (2005)." href="/articles/nn.3574#ref-CR24" id="ref-link-section-d31016971e1660">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Gandhi, S.P., Heeger, D.J. &amp; Boynton, G.M. Spatial attention affects brain activity in human primary visual cortex. Proc. Natl. Acad. Sci. USA 96, 33143319 (1999)." href="/articles/nn.3574#ref-CR25" id="ref-link-section-d31016971e1663">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Kastner, S., Pinsk, M.A., De Weerd, P., Desimone, R. &amp; Ungerleider, L.G. Increased activity in human visual cortex during directed attention in the absence of visual stimulation. Neuron 22, 751761 (1999)." href="/articles/nn.3574#ref-CR26" id="ref-link-section-d31016971e1666">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Brefczynski, J.A. &amp; DeYoe, E.A. A physiological correlate of the spotlight of visual attention. Nat. Neurosci. 2, 370374 (1999)." href="/articles/nn.3574#ref-CR27" id="ref-link-section-d31016971e1669">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Silver, M.A., Ress, D. &amp; Heeger, D.J. Neural correlates of sustained spatial attention in human early visual cortex. J. Neurophysiol. 97, 229237 (2007)." href="/articles/nn.3574#ref-CR28" id="ref-link-section-d31016971e1672">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Tootell, R.B. et al. The retinotopy of visual spatial attention. Neuron 21, 14091422 (1998)." href="/articles/nn.3574#ref-CR29" id="ref-link-section-d31016971e1675">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Murray, S.O. The effects of spatial attention in early human visual cortex are stimulus independent. J. Vis. 8, 2.12.11 (2008)." href="/articles/nn.3574#ref-CR30" id="ref-link-section-d31016971e1679">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Silver, M.A., Ress, D. &amp; Heeger, D.J. Topographic maps of visual spatial attention in human parietal cortex. J. Neurophysiol. 94, 13581371 (2005)." href="/articles/nn.3574#ref-CR31" id="ref-link-section-d31016971e1682">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Jerde, T.A., Merriam, E.P., Riggall, A.C., Hedges, J.H. &amp; Curtis, C.E. Prioritized maps of space in human frontoparietal cortex. J. Neurosci. 32, 1738217390 (2012)." href="/articles/nn.3574#ref-CR32" id="ref-link-section-d31016971e1685">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Jehee, J.F.M., Brady, D.K. &amp; Tong, F. Attention improves encoding of task-relevant features in the human visual cortex. J. Neurosci. 31, 82108219 (2011)." href="/articles/nn.3574#ref-CR33" id="ref-link-section-d31016971e1688">33</a></sup>.</p><p>The information content of a neural code is not necessarily monotonically related to the size of the constituent neural filters<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Saproo, S. &amp; Serences, J.T. Spatial attention improves the quality of population codes in human visual cortex. J. Neurophysiol. 104, 885895 (2010)." href="/articles/nn.3574#ref-CR18" id="ref-link-section-d31016971e1695">18</a></sup>. Extremely small (pinpoint) or extremely large (flat) spatial filters each individually carry very little information about the spatial arrangement of stimuli within the visual field. Accordingly, the optimal filter size lies somewhere between these two extremes, and thus it is not straightforward to infer whether a change in filter size results in a more or less optimal neural code (in terms of information encoding capacity). By simultaneously estimating changes in filter size across an entire ROI subtending the entire stimulated visual field, we were able to demonstrate that the synergistic pattern of spatial filter (pRF) modulations with attention jointly constrains the region-level spatial representation to maintain a constant represented stimulus size, despite most voxels exhibiting an increase in pRF size (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Figs. 8</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">9</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">10</a>). Together our results demonstrate the importance of incorporating all available information across entire ROIs when evaluating the modulatory role of attention on the information content of spatial priority maps.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Methods</h2><div class="c-article-section__content" id="Sec14-content"><h3 class="c-article__sub-heading" id="Sec15">Participants.</h3><p>Ten neurologically healthy volunteers (five female, 25  2.11 years of age (mean  s.d.)) with normal or corrected-to-normal vision were recruited from the University of California, San Diego (UCSD). All participants provided written informed consent in accordance with the human participants Institutional Review Board at UCSD and were monetarily compensated for their participation. For the original experiment, participants participated in two to three scanning sessions, each lasting 2 h. Data from two participants (one female) were excluded from the main analysis because of excessive head movement (AJ3) or unusually noisy reconstructions during attend fixation runs (AG3).</p><p>In the follow-up experiment in which behavioral performance was carefully controlled and IPS subregions were retinotopically mapped, four participants of our original cohort were scanned for an additional two sessions, each lasting 1.52 h.</p><h3 class="c-article__sub-heading" id="Sec16">Stimulus.</h3><p>Stimuli were rear projected on a screen (90-cm width) located 380 cm from the participant's eyes at the foot of the scanner table. The screen was viewed using a mirror attached to the headcoil.</p><p>We presented an identical stimulus sequence during all imaging runs while asking observers to perform several different tasks. Each trial began with the presentation of a small red dot (T1) that was presented for 500 ms, followed by a flickering circular checkerboard stimulus at full contrast (stimulus radius (<i>r</i><sub>stim</sub>) = 1.163, 1.47 cycles per degree) that was presented for 3 s and then a probe stimulus (T2) that was identical to T1. A 2-s intertrial interval separated each trial (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2a</a>). T1 was presented between 0.176 and 1.104 from the center of the checkerboard stimulus along a vector of random orientation (in polar coordinates, <i></i><sub>1</sub> was randomly chosen along the range 0 to 360, and <i>r</i><sub>1</sub> was uniformly sampled from the range 0.176 to 1.104). This ensured that the location of T1 was not precisely predictive of the checkerboard location. In 50% of the trials, T2 was presented in the same location as T1, and in the remaining trials, T2 was presented between 0.176 and 1.104 from the center of the checkerboard along a vector oriented at least 90 from the vector along which T1 was plotted (<i>r</i><sub>2</sub> was uniformly sampled from the range 0.176 to 1.104, and <i></i><sub>2</sub> was randomly chosen by adding between 90 and 270, uniformly sampled, to <i></i><sub>1</sub>). Polar coordinates used the center of the checkerboard stimulus as the origin. During the working memory condition (see below), participants based their response on whether T1 and T2 were presented in the exact same spatial position.</p><p>The location of the checkerboard stimulus was chosen pseudorandomly in each trial from a grid of 36 potential stimulus locations spaced by 1.163, or <i>r</i><sub>stim</sub>. The stimulus location grid was jittered by 0.827 diagonally either up and to the left or down and to the right in each run, allowing for an improved sampling of space. All figures were presented aligned to a common space by removing jitter (see below).</p><p>In each run, there were 36 trials (1 trial for each stimulus location) and 9 null trials in which participants passively fixated for the duration of a normal trial (6 s). We scanned participants for between four and six runs of each task, always ensuring each task was repeated an equal number of times.</p><h3 class="c-article__sub-heading" id="Sec17">Tasks.</h3><p>Participants performed one of three tasks during each functional run (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Fig. 2c</a>). During attend fixation runs, participants responded when they detected a brief contrast dimming of the fixation point (0.33 s), which occurred in 50% of trials. During attend stimulus runs, participants responded when they detected a brief contrast dimming of the flickering checkerboard stimulus (0.33 s), which occurred in 50% of trials. During spatial working memory runs, participants made a button press response to indicate whether T2 was in the same location or a different location as T1. Notably, all three events (T1, checkerboard and T2) occurred during all runs, ensuring that the sensory display remained identical and that we were measuring changes in spatial representations as a function of task demands rather than changes as a result of inconsistent visual stimulation. For the follow-up behavioral control experiment, we dynamically adjusted the difficulty (contrast dimming or T1-T2 separation distance) to achieve a consistent accuracy of <span class="stix"></span>75% across tasks.</p><h3 class="c-article__sub-heading" id="Sec18">Eye tracking.</h3><p>Participants were instructed to maintain fixation during all runs. Fixation was monitored during scanning for four participants using an ASL LRO-R long-range eye-tracking system (Applied Science Laboratories) with a sampling rate of 240 Hz. We recorded mean gaze as a function of stimulus location and task demands after excluding any samples in which neither the pupil nor the corneal reflection were reliably detected (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig8">Supplementary Fig. 1</a>).</p><h3 class="c-article__sub-heading" id="Sec19">Imaging.</h3><p>We scanned all participants on a 3T GE MR750 research-dedicated scanner at UCSD. Functional images were collected using a gradient echo planar imaging (EPI) pulse sequence and an eight-channel head coil (19.2  19.2 cm FOV, 96  96 matrix size, 31 3-mm-thick slices with 0-mm gap, TR = 2,250 ms, TE = 30 ms, flip angle = 90), which yielded a voxel size of 2  2  3 mm. We acquired oblique slices with coverage extending from the superior portion of parietal cortex to the ventral occipital cortex.</p><p>We also acquired a high-resolution anatomical scan (FSPGR T1-weighted sequence, TR/TE = 11/3.3 ms, TI = 1,100 ms, 172 slices, flip angle = 18, 1 mm<sup>3</sup> resolution). Functional images were co-registered to this scan. Images were preprocessed using FSL (Oxford, UK) and BrainVoyager 2.3 (BrainInnovations). Preprocessing included unwarping the EPI images using routines provided by FSL, slice-time correction, three-dimensional motion correction (six-parameter affine transform), temporal high-pass filtering (to remove first-, second- and third-order drift), transformation to Talairach space and normalization of signal amplitudes by converting to <i>z</i> scores. We did not perform any spatial smoothing beyond the smoothing introduced by resampling during the co-registration of the functional images, motion correction and transformation to Talairach space. When mapping IPS subregions, we scanned the participants using an identical pulse sequence but instead used a 32-channel Nova Medical headcoil.</p><h3 class="c-article__sub-heading" id="Sec20">Functional localizers.</h3><p>All the ROIs used were identified using independent localizer runs acquired across multiple scanning sessions.</p><p>Early visual areas were defined using standard retinotopic procedures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Engel, S.A. et al. fMRI of human visual cortex. Nature 369, 525 (1994)." href="/articles/nn.3574#ref-CR51" id="ref-link-section-d31016971e1823">51</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Sereno, M.I. et al. Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging. Science 268, 889893 (1995)." href="/articles/nn.3574#ref-CR52" id="ref-link-section-d31016971e1826">52</a></sup>. We identified the horizontal and vertical meridians using functional data projected onto gray- and white-matter boundary surface reconstructions for each hemisphere. Using these meridians, we defined the areas V1, V2v, V3v, hV4, V2d and V3d. Unless otherwise indicated, data were concatenated across hemispheres and across the dorsal and ventral aspects of each respective visual area. We scanned each participant for between two and four retinotopic mapping runs (<i>n</i> = 3 completed two runs, <i>n</i> = 3 completed three runs, and <i>n</i> = 2 completed four runs).</p><p>hMT+ was defined using a functional localizer in which a field of dots either moved with 100% coherence in a pseudorandomly selected direction or were randomly replotted on each frame to produce a visual 'snow' display<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Tootell, R.B. et al. Functional analysis of human MT and related visual cortical areas using magnetic resonance imaging. J. Neurosci. 15, 32153230 (1995)." href="/articles/nn.3574#ref-CR53" id="ref-link-section-d31016971e1842">53</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Serences, J.T. &amp; Boynton, G.M. The representation of behavioral choice for motion in human visual cortex. J. Neurosci. 27, 1289312899 (2007)." href="/articles/nn.3574#ref-CR54" id="ref-link-section-d31016971e1845">54</a></sup>. Dots were each 0.081 in diameter and were presented in an annulus of between 0.63 and 2.26 around the fixation. During coherent dot motion, all dots moved at a constant velocity of 2.71 s<sup>1</sup>. Participants attended the dot display for transient changes in velocity (during coherent motion) or replotting frequency (snow). Participants completed between one and three runs of this localizer (<i>n</i> = 2 completed one run, <i>n</i> = 3 completed two runs, and <i>n</i> = 3 completed three runs).</p><p>IPS and sPCS ROIs were defined using a functional localizer that required maintenance of a spatial location in working memory, a task that is commonly used to isolate IPS and sPCS, which is the putative human FEF<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Srimal, R. &amp; Curtis, C.E. Persistent neural activity during the maintenance of spatial position in working memory. Neuroimage 39, 455468 (2008)." href="/articles/nn.3574#ref-CR47" id="ref-link-section-d31016971e1863">47</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Kastner, S. et al. Topographic maps in human frontal cortex revealed in memory-guided saccade and spatial working-memory tasks. J. Neurophysiol. 97, 34943507 (2007)." href="/articles/nn.3574#ref-CR49" id="ref-link-section-d31016971e1866">49</a></sup>. A flickering checkerboard subtending half of the visual field appeared for 12 s, during which time two spatial working memory trials were presented. During the flickering checkerboard presentation, we presented a red target dot for 500 ms, which was followed 2 s later by a green probe dot for 500 ms. After the probe dot appeared, participants indicated whether the probe dot was in the same location or a different location as the red target dot. Here we limited our definition of IPS to the posterior aspect (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM73">Supplementary Table 1</a>). ROIs were functionally defined with a threshold of FDR-corrected <i>P</i> &lt; 0.05 or a more stringent threshold when patches of activation abutted one another. Participants completed between one (<i>n</i> = 2) and two (<i>n</i> = 6) runs of this scan.</p><p>We also used data from these IPS and sPCS localizer scans to identify voxels in all other ROIs that were responsive to the portion of the visual field in which stimuli were presented in the main tasks, as the large checkerboard stimuli subtended the same visual area as the stimulus array used in the main task. All ROIs were masked on a participant-by-participant basis such that further analyses only included voxels with significant responses during this localizer task (FDR corrected <i>P</i> &lt; 0.05).</p><h3 class="c-article__sub-heading" id="Sec21">Mapping IPS subregions.</h3><p>To determine the likely relative contributions of different IPS subregions to the localized ROI measured for all participants, we scanned the four participants who made up the behaviorally controlled cohort presented in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">Figures 6</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a> using a polar angle mapping stimulus and an attentionally demanding task.</p><p>We used two stimulus types and behavioral tasks to define the borders between IPS subregions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Silver, M.A., Ress, D. &amp; Heeger, D.J. Topographic maps of visual spatial attention in human parietal cortex. J. Neurophysiol. 94, 13581371 (2005)." href="/articles/nn.3574#ref-CR31" id="ref-link-section-d31016971e1906">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Jerde, T.A., Merriam, E.P., Riggall, A.C., Hedges, J.H. &amp; Curtis, C.E. Prioritized maps of space in human frontoparietal cortex. J. Neurosci. 32, 1738217390 (2012)." href="/articles/nn.3574#ref-CR32" id="ref-link-section-d31016971e1909">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Sereno, M.I., Pitzalis, S. &amp; Martinez, A. Mapping of contralateral space in retinotopic coordinates by a parietal cortical area in humans. Science 294, 13501354 (2001)." href="/articles/nn.3574#ref-CR43" id="ref-link-section-d31016971e1912">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Swisher, J.D., Halko, M.A., Merabet, L.B., McMains, S.A. &amp; Somers, D.C. Visual topography of human intraparietal sulcus. J. Neurosci. 27, 53265337 (2007)." href="/articles/nn.3574#ref-CR44" id="ref-link-section-d31016971e1915">44</a></sup>. In all runs, we used a wedge stimulus spanning a 72 polar angle and presented between 1.75 and 8.75 eccentricity rotating with a period of 24.75 s. In alternating runs, the wedge was either a 4-Hz flickering checkerboard stimulus (black-white, red-green or blue-yellow) or a field of moving black dots (0.3, 13 dots per square degree<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Itti, L., Koch, C. &amp; Niebur, E. A model of saliency-based visual attention for rapid scene analysis. IEEE Trans. Pattern Anal. Mach. Intell. 20, 12541259 (1998)." href="/articles/nn.3574#ref-CR2" id="ref-link-section-d31016971e1919">2</a></sup>, moving at 5 s<sup>1</sup>, changing direction every 8 s). During checkerboard runs, participants quickly responded after detecting a brief (250 ms) contrast dimming of a portion of the checkerboard. During moving dots runs, participants quickly responded after detecting a brief (417 ms) increase in dot speed. Targets appeared with 20% probability every 1.5 s. Difficulty was adjusted to achieve approximately 75% correct performance by changing the magnitude of the contrast dimming (checkerboard) or dot speed increment (moving dots) between runs. On average, participants performed with 84.1% accuracy in the contrast dimming task and 75.4% accuracy in the moving dots task. Two participants completed 14 runs (8 clockwise and 6 counterclockwise), and one participant completed 10 runs (AC; 5 clockwise and 5 counterclockwise). One participant was scanned with two different stimulus setups: half of all runs used the parameters described above and half used a wedge that spanned 60 of polar angle and rotated with a period of 36.00 s (AB; 6 runs clockwise, 6 runs counterclockwise).</p><p>Preprocessing procedures were identical to those used for the main task. To compute the best visual field angle for each voxel in IPS, we shifted the signals from counterclockwise runs earlier in time by twice the estimated hemodynamic response function (HRF) delay (2  6.75 s = 13.5 s), removed the first and last full cycle of data (we removed 22 TRs for all participants except AB, for which we removed 32 TRs) and then reversed the time series so that all runs correspond to the clockwise stimulus presentation. We then averaged these time-inverted counterclockwise runs with the clockwise runs. We computed the power and phase at the stimulus frequency (1/24.75 Hz or 1/36 Hz, participant AB) and subtracted the estimated HRF delay (6.75 s) to align the signal phase in each voxel with the visual stimulus position. We then projected maps onto the reconstructed cortical surfaces for each subject and defined IPS0IPS3 by identifying the upper and lower vertical meridian responses (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig14">Supplementary Fig. 7a</a>). Low statistical thresholds were used (computed using normalized power at the stimulus frequency) to identify the borders of IPS subregions. Voxels were selected for further analysis by thresholding their activation during the same independent localizer task that was used to functionally define IPS and sPCS.</p><h3 class="c-article__sub-heading" id="Sec22">Encoding model.</h3><p>To measure changes in spatial representations under different task demands, we implemented an encoding model to reconstruct spatial representations of each stimulus used in the main task<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Serences, J.T. &amp; Saproo, S. Computational advances towards linking BOLD and behavior. Neuropsychologia 50, 435446 (2012)." href="/articles/nn.3574#ref-CR34" id="ref-link-section-d31016971e1939">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Brouwer, G.J. &amp; Heeger, D.J. Decoding and reconstructing color from responses in human visual cortex. J. Neurosci. 29, 1399214003 (2009)." href="/articles/nn.3574#ref-CR36" id="ref-link-section-d31016971e1942">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Naselaris, T., Kay, K., Nishimoto, S. &amp; Gallant, J. Encoding and decoding in fMRI. Neuroimage 56, 400410 (2011)." href="/articles/nn.3574#ref-CR37" id="ref-link-section-d31016971e1945">37</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Scolari, M., Byers, A. &amp; Serences, J.T. Optimal deployment of attentional gain during fine discriminations. J. Neurosci. 32, 77237733 (2012)." href="/articles/nn.3574#ref-CR38" id="ref-link-section-d31016971e1948">38</a></sup>. This technique assumes that the signal measured in each voxel can be modeled as the weighted sum of different discrete neural populations, or information channels, that have different tuning properties<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Brouwer, G.J. &amp; Heeger, D.J. Decoding and reconstructing color from responses in human visual cortex. J. Neurosci. 29, 1399214003 (2009)." href="/articles/nn.3574#ref-CR36" id="ref-link-section-d31016971e1952">36</a></sup>. Using an independent set of training data, we estimated weights that approximate the degree to which each underlying neural population contributed to the observed BOLD response in each voxel (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3a</a>). Next, an independent set of test data was used to estimate the activation within these information channels on the basis of the activation pattern across all voxels within an ROI in each test trial using the information channel weights in each voxel that were estimated during the training phase (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3b</a>).</p><p>This approach requires specifying an explicit model for how neural populations encode information. Here we assumed a simple model for visual encoding within each ROI that focused exclusively on the spatial selectivity of visually responsive neural populations. To this end, we built a basis set of 36 two-dimensional spatial filters. We modeled these filters as cosine functions raised to a high power: <i>f</i>(<i>r</i>) = (0.5 cos(<i>r</i>/<i>s</i>) + 0.5)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Luck, S.J., Chelazzi, L., Hillyard, S.A. &amp; Desimone, R. Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex. J. Neurophysiol. 77, 2442 (1997)." href="/articles/nn.3574#ref-CR7" id="ref-link-section-d31016971e1977">7</a></sup> for <i>r</i> &lt; <i>s</i> and 0 elsewhere (<i>r</i> is the distance from that filter's center; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig9">Supplementary Fig. 2</a>). This allowed the filters to maintain an approximately Gaussian shape while reaching 0 at a fixed distance from the center (<i>s</i>), which helped constrain curve-fitting solutions (below). The <i>s</i> (size constant) parameter was fixed at 5<i>r</i><sub>stim</sub>, which is 5.8153. The 36 identical filters formed a six-by-six grid spanning the visual space subtended by the stimuli. Filters were separated by 2.094, with the centers tiled uniformly from 5.234 above, below, left and right of the fixation (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3a</a>). The full-width half-maximum (FWHM) of all filters was 2.3103 (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig9">Supplementary Figs. 2</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig10">3</a>). This ratio of filter size to spacing was chosen to avoid high correlations between predicted channel responses (caused by too much overlap between channels, which can result in a rank-deficient design matrix) and to accomplish smooth reconstructions (if filters are too small, reconstructed spatial representations are 'patchy'; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig10">Supplementary Fig. 3</a> shows an illustration of reconstruction smoothness as a function of the filter size to spacing ratio). All filters were assigned identical FWHMs so that known properties of the visual system, such as increasing receptive field size with eccentricity and along the visual stream<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Gattass, R. et al. Cortical visual areas in monkeys: location, topography, connections, columns, plasticity and cortical dynamics. Phil. Trans. R. Soc. Lond. B 360, 709731 (2005)." href="/articles/nn.3574#ref-CR39" id="ref-link-section-d31016971e2018">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Ben Hamed, S., Duhamel, J.R., Bremmer, F. &amp; Graf, W. Visual receptive field modulation in the lateral intraparietal area during attentive fixation and free gaze. Cereb. Cortex 12, 234245 (2002)." href="/articles/nn.3574#ref-CR40" id="ref-link-section-d31016971e2021">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Mohler, C.W., Goldberg, M.E. &amp; Wurtz, R.H. Visual receptive fields of frontal eye field neurons. Brain Res. 61, 385389 (1973)." href="/articles/nn.3574#ref-CR41" id="ref-link-section-d31016971e2024">41</a></sup>, could be recovered without being built in to the analysis.</p><p>To avoid circularity in our analysis, we used a cross-validation approach to compute channel responses for every trial. First we used all runs but three (one run for each task condition) to create a training set that had an equal number of trials in each condition. Using this training set, we estimated channel weights within each voxel across all task conditions (i.e., runs one through five of the attend fixation, attend stimulus and spatial working memory tasks were used together to estimate channel weights, which were used to compute channel responses for run six of each task condition). The use of an equal number of trials from each condition in the training set ensures that channel weight estimation is not biased by any changes in BOLD response across task demands. Next the weights estimated across all task demand conditions were used to compute channel response amplitudes for each trial individually. Trials were then sorted according to their task condition and spatial location.</p><p>During the training phase, we created a design matrix that contained the predicted channel response for all 36 channels in every trial (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3a</a>). The predicted response for each channel was computed by filtering a mask over the portion of the display subtended by the stimulus on that trial by the channel's basis function. The resulting design matrix was normalized to 1, such that reconstruction amplitudes are in units of BOLD <i>z</i> scores.</p><p>To extract relevant portions of the BOLD signal in every trial for computing channel responses, we took an average of the signal over two TRs beginning 6.75 s after trial onset. This range was chosen by examination of BOLD HRFs and was the same across all participants. Qualitatively, results do not change when other reasonable HRF lags are used, such as using two TRs starting 4.5 s after the stimulus.</p><p>Using this approach, we modeled voxel BOLD responses as a weighted sum of channel responses comprising each voxel<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Brouwer, G.J. &amp; Heeger, D.J. Decoding and reconstructing color from responses in human visual cortex. J. Neurosci. 29, 1399214003 (2009)." href="/articles/nn.3574#ref-CR36" id="ref-link-section-d31016971e2047">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Scolari, M., Byers, A. &amp; Serences, J.T. Optimal deployment of attentional gain during fine discriminations. J. Neurosci. 32, 77237733 (2012)." href="/articles/nn.3574#ref-CR38" id="ref-link-section-d31016971e2050">38</a></sup>. This can be written as a general linear model of the form</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/lw440/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Equ1_HTML.gif" class="u-display-block" alt=""></div></div><p>where <i>B</i><sub>1</sub> is the BOLD response in each voxel measured during every trial (<i>m</i> voxels  <i>n</i> trials), <i>W</i> is a matrix that maps channel space to voxel space (<i>m</i> voxels  <i>k</i> channels), and <i>C</i><sub>1</sub> is a design matrix of predicted channel responses in each trial (<i>k</i> channels  <i>n</i> trials). The weight matrix <img src="//media.springernature.com/lw13/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_IEq1_HTML.gif" style="width:13px;max-width:none;" alt=""> was estimated by</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/lw440/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Equ2_HTML.gif" class="u-display-block" alt=""></div></div><p>Then, using data from the held out test data set (<i>B</i><sub>2</sub>), the weight matrix estimated above was used to compute channel responses for every trial <img src="//media.springernature.com/lw22/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_IEq2_HTML.gif" style="width:22px;max-width:none;" alt="">, which were then sorted by task condition and spatial position:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/lw440/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Equ3_HTML.gif" class="u-display-block" alt=""></div></div><h3 class="c-article__sub-heading" id="Sec23">Reconstructing spatial representations.</h3><p>To reconstruct the region-wide representation of the visual stimulus viewed in every trial, we computed a weighted sum of the basis set using each channel response as the weight for the corresponding basis function (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3b</a>). Reconstructions were computed out to a 5.234 eccentricity across the horizontal and vertical meridians, although visual stimuli only subtended at a maximum 4.523 eccentricity across the horizontal or vertical meridians. This was done to avoid edge artifacts in the reconstructions. Additionally, at this stage, the reconstructed visual fields were shifted to account for the slight jitter introduced in the presented stimulus locations and to align reconstructions from all trials. Runs in which stimuli were jittered up and to the left were reconstructed by moving the centers of the basis functions down and to the right, and runs in which stimuli were jittered down and to the right were reconstructed by moving the centers of the basis functions up and to the left. These shifts serve to counter the spatial jitter of stimulus presentation for visualization and quantification. By including spatial jitter during stimulus presentation, we are able to attain a more nuanced estimate of channel weights by sampling 72 stimulus locations rather than 36.</p><p>We averaged each participant's reconstructions at all 36 spatial locations for each task condition across trials. For <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Figure 4</a>, all <i>n</i> = 8 participants' average reconstructions for each task condition were averaged, and reconstructions from all ROIs and task conditions were visualized on a common color scale to illustrate differences in spatial representations across the different task conditions and spatial locations. The three-by-three subset of reconstructions shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Figure 4b</a> was chosen as it is representative and results were similar for all quadrants.</p><p>For the follow-up control experiment, we plotted reconstructed spatial representations from only correct trials by co-registering all representations for trials at matching eccentricities and then averaging across all co-registered representations for each participant at each eccentricity. We co-registered representations for like eccentricities to the top left quadrant (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">Fig. 6b</a>). Representations were rotated in 90 steps and flipped across the diagonal (equivalent to a matrix transpose operation on pixel values) as necessary.</p><p>Notably, this analysis depends on two necessary conditions. First, individual voxels must respond to certain spatial positions more than others, although the shape of these spatial selectivity profiles is not constrained to follow any particular distribution (for example, it need not resemble a Gaussian distribution). Second, the spatial selectivity profile for each voxel must be stable across time, such that spatial selectivity estimated on the basis of data in the training set can generalize to the held-out test set.</p><h3 class="c-article__sub-heading" id="Sec24">Curve fitting.</h3><p>To quantify the effects of attention on visual field reconstructions, we fit a basis function to all 36 average reconstructions for each participant for each task condition for each ROI using fminsearch as implemented in MATLAB 2012b (which uses the Nelder-Mead simplex search method; Mathworks, Inc).</p><p>The error function used for fitting was the sum of squared errors between the reconstructed visual stimulus and the function</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/lw440/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Equ4_HTML.gif" class="u-display-block" alt=""></div></div><p>where <i>r</i> is computed as the Euclidean distance from the center of the fit function. We allowed the baseline (<i>b</i>), amplitude (<i>a</i>), location (<i>x</i>, <i>y</i>) and size (<i>s</i>) to vary as free parameters. The size <i>s</i> was restricted so as not to be too large or too small (confined to 0.5815 &lt; <i>s</i> &lt; 26.17), and the location was restricted around the region of visual stimulation (<i>x</i> and <i>y</i> lie within stimulus extent borders +1.36 on each side).</p><p>Because of the number of free parameters in this function, we performed a two-step stochastic curve-fitting procedure to find the approximate best-fit function for each reconstructed stimulus. First we averaged reconstructions for each spatial location across all three task conditions and performed 50 fits with random starting points. The fit with the smallest sum squared error was used as the starting point around which all other starting points were randomly drawn when fitting to reconstructions from each task condition individually. When fitting individual task condition reconstructions, we performed 150 fits for each condition. We used parameters from the fit with the smallest sum squared error as a quantitative characterization of the reconstructed visual stimulus. Then we averaged the fit parameters across like eccentricities within each task condition, ROI and participant. For the follow-up control experiment, we performed an identical fitting procedure on each of the co-registered representations to directly estimate the best fit parameters at each eccentricity.</p><h3 class="c-article__sub-heading" id="Sec25">Excluded participant.</h3><p>For one participant (AG3), reconstructions from the attend fixation runs were unusually noisy and could not be well approximated by the basis function used for fitting. However, both the attend stimulus and spatial working memory runs for this individual exhibited successful reconstructions (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig11">Supplementary Fig. 4</a>). As the estimated channel weights used to compute these stimulus reconstructions were identical across the three task conditions, only changes in information coding across task demands could account for this radical shift in reconstruction fidelity. Because this participant's reconstructions could not be accurately quantified for the attend fixation condition, the reconstructions and fit parameters for this individual for all conditions have been left out of the data presented in the Results. However, as noted above, data from this participant are consistent with our main conclusion that attentional demands influence the quality of spatial representations.</p><h3 class="c-article__sub-heading" id="Sec26">Evaluating the relationship between amplitude and size.</h3><p>It may be the case that our observation of increasing spatial representation size with increasing stimulus eccentricity is purely a result of intertrial variability in the reconstructed stimulus position. That is, the same representation could be jittered across trials, and the resulting average representation across trials would appear 'smeared' and would be fit with a larger size and smaller amplitude. If this were true, changes in these parameters would always be negatively correlated with one anotheran increase in size across conditions would always occur with a decrease in amplitude.</p><p>To evaluate this possibility, for each eccentricity, ROI and condition pair (attend stimulus and attend fixation, spatial working memory and attend stimulus, and spatial working memory and attend fixation), we correlated the change in size with the change in amplitude (each correlation contained eight observations, corresponding to <i>n</i> = 8 participants). To evaluate the statistical significance of these correlations, we repeated this procedure 10,000 times, each time shuffling the condition labels separately for size and amplitude, recomputing the difference and then recomputing the correlation between changes in size and changes in amplitude. This resulted in a null distribution of chance correlation values against which we determined the probability of obtaining the true correlation value by chance. After correction for FDR, no correlations were significant (all <i>P</i> &gt; 0.05; of note, FDR is more liberal than Bonferroni correction).</p><h3 class="c-article__sub-heading" id="Sec27">Representations from the ventral and dorsal aspects of V2 and V3.</h3><p>For <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig12">Supplementary Figure 5a</a>, we generated reconstructions using a procedure identical to that used for <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Figure 4</a>, except we only used voxels that were assigned to the dorsal or ventral aspects of V2 and V3 instead of combining voxels across the dorsal and ventral aspects, as was done in the main analysis.</p><h3 class="c-article__sub-heading" id="Sec28">Reconstructions of untrained (new) stimuli.</h3><p>For <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig12">Supplementary Figure 5b</a>, we estimated channel weights using all runs of all task conditions from the main task as a training set. We used these weights to estimate channel responses from the BOLD data taken from an entirely new data set, which consisted of responses to a hemi-annulusshaped radial checkerboard (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig12">Supplementary Fig. 5b</a>).</p><p>This new experiment featured four stimulus conditions: left-in, left-out, right-in and right-out. The inner hemi-annuli subtended at 0.633 to 2.262 eccentricity. The outer hemi-annuli subtended at 2.262 to 4.523 eccentricity. Stimuli were flickered at 6 Hz for 12 s in each trial while the participants performed a spatial working memory task on small probe stimuli presented at different points within the displayed stimulus.</p><p>The BOLD signal used for reconstruction was taken as the average of four TRs beginning 4.5 s after stimulus onset. These data were used as the test set. Otherwise, the reconstruction process was identical to that in the main experiment, as were all other scan parameters and preprocessing steps.</p><h3 class="c-article__sub-heading" id="Sec29">pRF estimation.</h3><p>To determine whether the spatial sensitivity of each voxel across all trials and all runs changed across conditions, we implemented a new version of a pRF analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Dumoulin, S.O. &amp; Wandell, B.A. Population receptive field estimates in human visual cortex. Neuroimage 39, 647660 (2008)." href="/articles/nn.3574#ref-CR42" id="ref-link-section-d31016971e2290">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Kay, K.N., Naselaris, T., Prenger, R.J. &amp; Gallant, J.L. Identifying natural images from human brain activity. Nature 452, 352355 (2008)." href="/articles/nn.3574#ref-CR55" id="ref-link-section-d31016971e2293">55</a></sup>. For this analysis, we estimated the unimodal, isotropic pRF that best accounts for the BOLD responses to each stimulus position within every single voxel. This analysis is complementary to the primary analyses described above.</p><p>For four participants (those presented in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">Figs. 6</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig14">Supplementary Fig. 7</a>) and four ROIs for each participant (V1, hV4, hMT+ and IPS0, which were chosen because this set includes both ROIs with (hV4, hMT+ and IPS0) and without (V1) attentional modulation), we used data across all runs within each task condition and ridge regression<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Hoerl, A.E. &amp; Kennard, R.W. Ridge regression: biased estimation for nonorthogonal problems. Technometrics 12, 5567 (1970)." href="/articles/nn.3574#ref-CR56" id="ref-link-section-d31016971e2309">56</a></sup> to identify pRFs for each voxel under each task condition. We computed these pRFs using a method similar to that used to compute channel weights in the encoding model analysis (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3a</a> and Online Methods; the univariate step 1 of the encoding model, see equation (1)). We generated predicted responses with the same information channels that were used for the encoding model analysis (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3a</a>), and reconstructed pRFs for each task condition for a given voxel were defined as the corresponding spatial filters weighted by the computed weight for each channel (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Fig. 8a</a>).</p><p>In the main analysis in which we computed spatial reconstructions on the basis of activation patterns across an entire ROI (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Figs. 4</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">6c</a>), any spatial information encoded by a voxel's response could be exploited; this is true even if the voxel's response to different locations was not unimodal (it need not follow any set distribution, as long as it responds consistently). However, univariate pRFs computed on a voxel-by-voxel basis cannot be well characterized by an isotropic function if they are not unimodal<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Lee, S., Papanikolaou, A., Logothetis, N.K., Smirnakis, S.M. &amp; Keliris, G.A. A new method for estimating population receptive field topography in visual cortex. Neuroimage 81, 144157 (2013)." href="/articles/nn.3574#ref-CR57" id="ref-link-section-d31016971e2332">57</a></sup>. Thus, to ensure that most of the pRFs were sufficiently unimodal to fit an isotropic function, we used ridge regression<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Hoerl, A.E. &amp; Kennard, R.W. Ridge regression: biased estimation for nonorthogonal problems. Technometrics 12, 5567 (1970)." href="/articles/nn.3574#ref-CR56" id="ref-link-section-d31016971e2336">56</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Lee, S., Papanikolaou, A., Logothetis, N.K., Smirnakis, S.M. &amp; Keliris, G.A. A new method for estimating population receptive field topography in visual cortex. Neuroimage 81, 144157 (2013)." href="/articles/nn.3574#ref-CR57" id="ref-link-section-d31016971e2339">57</a></sup> when computing spatial filter weights for the pRF analysis. The regression equation for computing channel weights then becomes</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/lw440/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Equ5_HTML.gif" class="u-display-block" alt=""></div></div><p>where <i>I</i> is an identity matrix (<i>k</i>  <i>k</i>). To identify an optimal ridge parameter (<i></i>), we computed the Bayes information criterion<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Schwarz, G. Estimating the dimension of a model. Ann. Stat. 6, 461464 (1978)." href="/articles/nn.3574#ref-CR58" id="ref-link-section-d31016971e2365">58</a></sup> value across a range of <i></i> values (0 to 500) for each voxel using data concatenated across all three task conditions. This allowed for an unbiased selection of <i></i> with respect to task condition. The <i></i> with the minimum mean Bayes information criterion value across all voxels within a ROI was selected, and this <i></i> was used to compute channel weights for each of the three task conditions separately. An increasing <i></i> value results in greater sparseness of the best-fit channel weights for each voxel, and a <i></i> value of 0 corresponds to ordinary least-squares regression.</p><p>After computing pRFs for each task condition, we fit each pRF with the same function that was used to fit the spatial representations (equation (4)) using a similar optimization procedure. We restricted the fit size (FWHM) to be at most 8.08, which corresponds to nearly the full diagonal distance across the stimulated visual field. This boundary was typically encountered only for hMT+ and IPS0 and served to discourage the optimization procedure from fitting large, flat surfaces. Then we computed an <i>R</i><sup>2</sup> value for each fit and used only voxels for which the minimum <i>R</i><sup>2</sup> across conditions was greater than or equal to the median of the minimum <i>R</i><sup>2</sup> across conditions from all voxels in that participant's ROI (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Fig. 8a,b</a>).</p><p>Because we only have a single parameter estimate for each condition for each voxel, we evaluated whether fit size is more likely to increase or decrease between each pair of task conditions (attend stimulus compared to attend fixation, spatial working memory compared to attend stimulus and spatial working memory compared to attend fixation) for each region for each participant by determining the percentage of voxels that lie above the unity line in a plot of one condition against another (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Fig. 8d</a>).</p><h3 class="c-article__sub-heading" id="Sec30">Simulating data with different pRF properties.</h3><p>In order to assess whether our region-level multivariate spatial representation analysis would be sensitive to changes in voxel-level univariate pRFs, we generated simulated data using two different pRF modulation models.</p><p>For the first model (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">Supplementary Fig. 10a,b</a>), we randomly generated 500 pRF functions so as to uniformly sample the visual field for each of two conditions (condition A, smaller pRFs; condition B, larger pRFs). Across the two conditions, each simulated voxel's pRF maintained its preferred position while its amplitude and baseline were each randomly and independently sampled across conditions from the same normal distribution (amplitude: <i></i> = 0.8513, <i></i> = 0.25; baseline: <i></i> = 0.1952, <i></i> = 0.25; these values were taken from the average-fit pRF parameters across all participants for hV4 in the attend fixation and attend stimulus conditions; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">Supplementary Fig. 9a</a>). pRF size (FWHM) was sampled from a normal distribution with <i></i> = 0.5 and a mean of <i></i> = 4.405 for condition A (mean of pRF size for hV4, attend fixation) and <i></i> = 4.89 for condition B (mean of pRF size for hV4, attend stimulus; an increase of 11%). In our simulation, this resulted in 79% of simulated voxels showing larger pRF sizes in condition B compared to condition A. For the second model (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">Supplementary Fig. 10c,d</a>), we used the upper median split of fit pRFs for the single participant shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Figure 8c</a>, hV4 ROI, to generate the simulated BOLD data. This allowed us to simulate region-level BOLD data for each attention condition tested in our experiment and enabled us to determine whether the changes in univariate voxel-level pRF size we observed (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">Supplementary Fig. 9</a>) are consistent with the multivariate region-level spatial representations presented in the main text (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figs. 5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a>).</p><p>After generating voxel-level pRFs using each of the two models described above, we added noise to the simulated weights (Gaussian noise added independently to each channel weight, <i></i> = 0.1) and presented model voxels with six runs of all 36 spatial positions for each condition. We simulated each voxel's BOLD response as the predicted channel response (response of corresponding spatial filter; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3a</a>) to each stimulus weighted by the corresponding channel weights. We added Gaussian noise to the resulting BOLD data for each simulated voxel independently (<i></i> = 0.1). Then all analyses of multivariate spatial representations proceeded identically to those described above. We computed spatial representations using estimated channel weights computed across all conditions within a model (i.e., condition A and condition B (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">Supplementary Fig. 10a,b</a>) or attend fixation, attend stimulus and spatial working memory (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig17">Supplementary Fig. 10c,d</a>)) and then fit the average spatial representations with a smooth surface (Online Methods) to determine the amplitude and size of each spatial representation. We then averaged these parameters across all 36 positions.</p><h3 class="c-article__sub-heading" id="Sec31">Statistical procedures.</h3><p>All behavioral analyses on accuracy data were performed using two-way repeated-measures ANOVA, with task condition and stimulus eccentricity modeled as fixed effects (three levels and six levels, respectively; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig2">Figs. 2d</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig6">6a</a>).</p><p>To assess whether fit parameters to reconstructed spatial representations reliably changed as a function of task demands, we performed a multistage permutation testing procedure. This nonparametric procedure was adopted because the spatial filters (basis functions) used to estimate the spatial selectivity of each voxel during the training phase (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Fig. 3a</a>) overlapped and were not independent (violating a key assumption of standard statistical tests).</p><p>For each parameter (the rows in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figs. 5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a>), we first found ROI-parameter combinations that showed an omnibus main effect in a repeated-measures ANOVA (1 factor, 18 levels) corrected using a FDR algorithm<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Benjamini, Y. &amp; Yekutieli, D. The control of the false discovery rate in multiple testing under dependency. Ann. Stat. 29, 11651188 (2001)." href="/articles/nn.3574#ref-CR59" id="ref-link-section-d31016971e2516">59</a></sup> across all ROIs. Then we computed <i>F</i> scores for a two-way repeated measures design with eccentricity and condition as factors (six levels and three levels, respectively) for ROIs with significant omnibus main effects.</p><p>For all tests, because we had a relatively small <i>n</i> (<i>n</i> = 8 for <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Fig. 5</a>, and <i>n</i> = 4 for <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">Fig. 7</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig14">Supplementary Fig. 7</a>) and the range of parameters was in some cases restricted to be positive (size), we computed an <i>F</i> distribution for the null hypothesis that there is no main effect of the omnibus factor (omnibus test) or that there is no main effect of condition, eccentricity or their interaction (for a follow-up two-way test) by shuffling trial labels within each participant 100,000 times. For each data permutation, we computed a new <i>F</i> score for the omnibus test, and for ROI-parameter combinations with a significant omnibus effect, we computed a main effect of condition, eccentricity and their interaction. <i>P</i> values were estimated as the probability that the <i>F</i> score computed based on the shuffled data was equal to or greater than the <i>F</i> scores computed using the actual data. These additional tests were corrected for multiple comparisons using Bonferroni's method within each parameter. We also occasionally highlight trends in the data by reporting <i>P</i> values that did not reach significance under correction for multiple comparisons at this sample size as marginal effects, and such <i>P</i> values are always reported as being uncorrected in the text. For display purposes, marginally significant tests are shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figures 5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a> at uncorrected <i>P</i> &lt; 0.025.</p><p>In addition, we performed a three-factor repeated-measures ANOVA with ROI, task condition and eccentricity modeled as fixed effects to determine whether the fit parameters changed across ROIs (<i>n</i> = 8 for <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Fig. 5</a>, and <i>n</i> = 4 for <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">Fig. 7</a>). We implemented the same permutation procedure described above to compute <i>P</i> values (10,000 iterations).</p><p>To determine whether pRF size increases at higher eccentricities, we computed a linear fit to a plot of each voxel's pRF size compared to its pRF eccentricity for each ROI for each condition for each participant (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Fig. 8c</a>). To determine whether the slope of the fit line was reliably positive for a given ROI, participant and condition, we computed confidence intervals around the best-fit slopes using bootstrapping (resampled all voxels with replacement 10,000 times), and the related <i>P</i> value was defined as the as the probability that the slope was 0. We used a Bonferroni-corrected significance threshold for 48 planned comparisons (4 ROIs  4 participants  3 conditions) of <i></i> = 0.001 (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM73">Supplementary Results</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM73">Supplementary Table 2</a>).</p><p>To evaluate the statistical significance of the pRF size increase (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">Supplementary Fig. 9</a>), we first performed a two-way repeated-measures ANOVA with ROI and condition modeled as fixed effects and participant modeled as a random effect in which we shuffled ROI and condition labels for each participant and recomputed the percentage of voxels that increased in size across each condition pair. We repeated this shuffling procedure 10,000 times and compared <i>F</i> scores computed using the real labels to the distribution generated using the shuffled labels, as described above. Then we compared whether each condition pairing resulted in a significant change in pRF size for each ROI by computing a <i>T</i> score testing against the null hypothesis that 50% of voxels show an increase in pRF size. As described above, we generated a null <i>T</i> distribution by shuffling condition labels within each participant 10,000 times. For this analysis, we used a Bonferroni-corrected significance threshold for 12 planned comparisons (4 ROIs  3 conditions) of <i></i> = 0.0042.</p></div></div></section>
                </div>
            

            <div>
                <div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1"><p class="c-article-references__text" id="ref-CR1">Koch, C. &amp; Ullman, S. Shifts in selective visual attention: towards the underlying neural circuitry. <i>Hum. Neurobiol.</i> <b>4</b>, 219227 (1985).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaL283nslCrsQ%3D%3D" aria-label="CAS reference 1">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=3836989" aria-label="PubMed reference 1">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Shifts%20in%20selective%20visual%20attention%3A%20towards%20the%20underlying%20neural%20circuitry&amp;journal=Hum.%20Neurobiol.&amp;volume=4&amp;pages=219-227&amp;publication_year=1985&amp;author=Koch%2CC&amp;author=Ullman%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2"><p class="c-article-references__text" id="ref-CR2">Itti, L., Koch, C. &amp; Niebur, E. A model of saliency-based visual attention for rapid scene analysis. <i>IEEE Trans. Pattern Anal. Mach. Intell.</i> <b>20</b>, 12541259 (1998).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/34.730558" data-track-action="article reference" href="https://doi.org/10.1109%2F34.730558" aria-label="Article reference 2" data-doi="10.1109/34.730558">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20model%20of%20saliency-based%20visual%20attention%20for%20rapid%20scene%20analysis&amp;journal=IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.&amp;doi=10.1109%2F34.730558&amp;volume=20&amp;pages=1254-1259&amp;publication_year=1998&amp;author=Itti%2CL&amp;author=Koch%2CC&amp;author=Niebur%2CE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3"><p class="c-article-references__text" id="ref-CR3">Itti, L. &amp; Koch, C. Computational modelling of visual attention. <i>Nat. Rev. Neurosci.</i> <b>2</b>, 194203 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/35058500" data-track-action="article reference" href="https://doi.org/10.1038%2F35058500" aria-label="Article reference 3" data-doi="10.1038/35058500">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3MXitVyqsLs%3D" aria-label="CAS reference 3">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11256080" aria-label="PubMed reference 3">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20modelling%20of%20visual%20attention&amp;journal=Nat.%20Rev.%20Neurosci.&amp;doi=10.1038%2F35058500&amp;volume=2&amp;pages=194-203&amp;publication_year=2001&amp;author=Itti%2CL&amp;author=Koch%2CC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4"><p class="c-article-references__text" id="ref-CR4">Serences, J.T. &amp; Yantis, S. Selective visual attention and perceptual coherence. <i>Trends Cogn. Sci.</i> <b>10</b>, 3845 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2005.11.008" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2005.11.008" aria-label="Article reference 4" data-doi="10.1016/j.tics.2005.11.008">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16318922" aria-label="PubMed reference 4">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Selective%20visual%20attention%20and%20perceptual%20coherence&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2005.11.008&amp;volume=10&amp;pages=38-45&amp;publication_year=2006&amp;author=Serences%2CJT&amp;author=Yantis%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5"><p class="c-article-references__text" id="ref-CR5">Fecteau, J.H. &amp; Munoz, D.P. Salience, relevance, and firing: a priority map for target selection. <i>Trends Cogn. Sci.</i> <b>10</b>, 382390 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2006.06.011" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2006.06.011" aria-label="Article reference 5" data-doi="10.1016/j.tics.2006.06.011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16843702" aria-label="PubMed reference 5">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Salience%2C%20relevance%2C%20and%20firing%3A%20a%20priority%20map%20for%20target%20selection&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2006.06.011&amp;volume=10&amp;pages=382-390&amp;publication_year=2006&amp;author=Fecteau%2CJH&amp;author=Munoz%2CDP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6"><p class="c-article-references__text" id="ref-CR6">Bichot, N.P. &amp; Schall, J.D. Effects of similarity and history on neural mechanisms of visual selection. <i>Nat. Neurosci.</i> <b>2</b>, 549554 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/9205" data-track-action="article reference" href="https://doi.org/10.1038%2F9205" aria-label="Article reference 6" data-doi="10.1038/9205">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK1MXjsFSis7w%3D" aria-label="CAS reference 6">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10448220" aria-label="PubMed reference 6">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20of%20similarity%20and%20history%20on%20neural%20mechanisms%20of%20visual%20selection&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2F9205&amp;volume=2&amp;pages=549-554&amp;publication_year=1999&amp;author=Bichot%2CNP&amp;author=Schall%2CJD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7"><p class="c-article-references__text" id="ref-CR7">Luck, S.J., Chelazzi, L., Hillyard, S.A. &amp; Desimone, R. Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of macaque visual cortex. <i>J. Neurophysiol.</i> <b>77</b>, 2442 (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.1997.77.1.24" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.1997.77.1.24" aria-label="Article reference 7" data-doi="10.1152/jn.1997.77.1.24">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK2s7ntFejsg%3D%3D" aria-label="CAS reference 7">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9120566" aria-label="PubMed reference 7">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20mechanisms%20of%20spatial%20selective%20attention%20in%20areas%20V1%2C%20V2%2C%20and%20V4%20of%20macaque%20visual%20cortex&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.1997.77.1.24&amp;volume=77&amp;pages=24-42&amp;publication_year=1997&amp;author=Luck%2CSJ&amp;author=Chelazzi%2CL&amp;author=Hillyard%2CSA&amp;author=Desimone%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8"><p class="c-article-references__text" id="ref-CR8">Reynolds, J.H., Chelazzi, L. &amp; Desimone, R. Competitive mechanisms subserve attention in macaque areas V2 and V4. <i>J. Neurosci.</i> <b>19</b>, 17361753 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.19-05-01736.1999" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.19-05-01736.1999" aria-label="Article reference 8" data-doi="10.1523/JNEUROSCI.19-05-01736.1999">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK1MXhtlCrtbc%3D" aria-label="CAS reference 8">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10024360" aria-label="PubMed reference 8">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6782185" aria-label="PubMed Central reference 8">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Competitive%20mechanisms%20subserve%20attention%20in%20macaque%20areas%20V2%20and%20V4&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.19-05-01736.1999&amp;volume=19&amp;pages=1736-1753&amp;publication_year=1999&amp;author=Reynolds%2CJH&amp;author=Chelazzi%2CL&amp;author=Desimone%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9"><p class="c-article-references__text" id="ref-CR9">McAdams, C.J. &amp; Maunsell, J.H.R. Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4. <i>J. Neurosci.</i> <b>19</b>, 431441 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.19-01-00431.1999" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.19-01-00431.1999" aria-label="Article reference 9" data-doi="10.1523/JNEUROSCI.19-01-00431.1999">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK1MXltVSntA%3D%3D" aria-label="CAS reference 9">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9870971" aria-label="PubMed reference 9">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6782389" aria-label="PubMed Central reference 9">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20of%20attention%20on%20orientation-tuning%20functions%20of%20single%20neurons%20in%20macaque%20cortical%20area%20V4&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.19-01-00431.1999&amp;volume=19&amp;pages=431-441&amp;publication_year=1999&amp;author=McAdams%2CCJ&amp;author=Maunsell%2CJHR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10"><p class="c-article-references__text" id="ref-CR10">Connor, C.E., Preddie, D.C., Gallant, J.L. &amp; Van Essen, D.C. Spatial attention effects in macaque area V4. <i>J. Neurosci.</i> <b>17</b>, 32013214 (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.17-09-03201.1997" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.17-09-03201.1997" aria-label="Article reference 10" data-doi="10.1523/JNEUROSCI.17-09-03201.1997">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK2sXislahsrg%3D" aria-label="CAS reference 10">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9096154" aria-label="PubMed reference 10">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6573654" aria-label="PubMed Central reference 10">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20attention%20effects%20in%20macaque%20area%20V4&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.17-09-03201.1997&amp;volume=17&amp;pages=3201-3214&amp;publication_year=1997&amp;author=Connor%2CCE&amp;author=Preddie%2CDC&amp;author=Gallant%2CJL&amp;author=Van%20Essen%2CDC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11"><p class="c-article-references__text" id="ref-CR11">Treue, S. &amp; Maunsell, J.H.R. Attentional modulation of visual motion processing in cortical areas MT and MST. <i>Nature</i> <b>382</b>, 539541 (1996).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/382539a0" data-track-action="article reference" href="https://doi.org/10.1038%2F382539a0" aria-label="Article reference 11" data-doi="10.1038/382539a0">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK28XkvFGjt7g%3D" aria-label="CAS reference 11">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8700227" aria-label="PubMed reference 11">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Attentional%20modulation%20of%20visual%20motion%20processing%20in%20cortical%20areas%20MT%20and%20MST&amp;journal=Nature&amp;doi=10.1038%2F382539a0&amp;volume=382&amp;pages=539-541&amp;publication_year=1996&amp;author=Treue%2CS&amp;author=Maunsell%2CJHR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12"><p class="c-article-references__text" id="ref-CR12">Reynolds, J.H., Pasternak, T. &amp; Desimone, R. Attention increases sensitivity of V4 neurons. <i>Neuron</i> <b>26</b>, 703714 (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0896-6273(00)81206-4" data-track-action="article reference" href="https://doi.org/10.1016%2FS0896-6273%2800%2981206-4" aria-label="Article reference 12" data-doi="10.1016/S0896-6273(00)81206-4">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3cXkvVOksro%3D" aria-label="CAS reference 12">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10896165" aria-label="PubMed reference 12">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20increases%20sensitivity%20of%20V4%20neurons&amp;journal=Neuron&amp;doi=10.1016%2FS0896-6273%2800%2981206-4&amp;volume=26&amp;pages=703-714&amp;publication_year=2000&amp;author=Reynolds%2CJH&amp;author=Pasternak%2CT&amp;author=Desimone%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13"><p class="c-article-references__text" id="ref-CR13">McAdams, C.J. &amp; Maunsell, J.H.R. Attention to both space and feature modulates neuronal responses in macaque area V4. <i>J. Neurophysiol.</i> <b>83</b>, 17511755 (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.2000.83.3.1751" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.2000.83.3.1751" aria-label="Article reference 13" data-doi="10.1152/jn.2000.83.3.1751">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD3c7nslyjuw%3D%3D" aria-label="CAS reference 13">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10712494" aria-label="PubMed reference 13">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20to%20both%20space%20and%20feature%20modulates%20neuronal%20responses%20in%20macaque%20area%20V4&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.2000.83.3.1751&amp;volume=83&amp;pages=1751-1755&amp;publication_year=2000&amp;author=McAdams%2CCJ&amp;author=Maunsell%2CJHR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14"><p class="c-article-references__text" id="ref-CR14">Treue, S. &amp; Maunsell, J.H.R. Effects of attention on the processing of motion in macaque middle temporal and medial superior temporal visual cortical areas. <i>J. Neurosci.</i> <b>19</b>, 75917602 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.19-17-07591.1999" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.19-17-07591.1999" aria-label="Article reference 14" data-doi="10.1523/JNEUROSCI.19-17-07591.1999">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK1MXlslWmsL4%3D" aria-label="CAS reference 14">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10460265" aria-label="PubMed reference 14">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6782504" aria-label="PubMed Central reference 14">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20of%20attention%20on%20the%20processing%20of%20motion%20in%20macaque%20middle%20temporal%20and%20medial%20superior%20temporal%20visual%20cortical%20areas&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.19-17-07591.1999&amp;volume=19&amp;pages=7591-7602&amp;publication_year=1999&amp;author=Treue%2CS&amp;author=Maunsell%2CJHR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15"><p class="c-article-references__text" id="ref-CR15">Seidemann, E. &amp; Newsome, W.T. Effect of spatial attention on the responses of area MT neurons. <i>J. Neurophysiol.</i> <b>81</b>, 17831794 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.1999.81.4.1783" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.1999.81.4.1783" aria-label="Article reference 15" data-doi="10.1152/jn.1999.81.4.1783">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK1M3hvVajsA%3D%3D" aria-label="CAS reference 15">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10200212" aria-label="PubMed reference 15">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Effect%20of%20spatial%20attention%20on%20the%20responses%20of%20area%20MT%20neurons&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.1999.81.4.1783&amp;volume=81&amp;pages=1783-1794&amp;publication_year=1999&amp;author=Seidemann%2CE&amp;author=Newsome%2CWT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16"><p class="c-article-references__text" id="ref-CR16">Motter, B.C. Focal attention produces spatially selective processing in visual cortical areas V1, V2, and V4 in the presence of competing stimuli. <i>J. Neurophysiol.</i> <b>70</b>, 909919 (1993).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.1993.70.3.909" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.1993.70.3.909" aria-label="Article reference 16" data-doi="10.1152/jn.1993.70.3.909">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK2c%2FksFykug%3D%3D" aria-label="CAS reference 16">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8229178" aria-label="PubMed reference 16">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Focal%20attention%20produces%20spatially%20selective%20processing%20in%20visual%20cortical%20areas%20V1%2C%20V2%2C%20and%20V4%20in%20the%20presence%20of%20competing%20stimuli&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.1993.70.3.909&amp;volume=70&amp;pages=909-919&amp;publication_year=1993&amp;author=Motter%2CBC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17"><p class="c-article-references__text" id="ref-CR17">Moran, J. &amp; Desimone, R. Selective attention gates visual processing in the extrastriate cortex. <i>Science</i> <b>229</b>, 782784 (1985).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.4023713" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.4023713" aria-label="Article reference 17" data-doi="10.1126/science.4023713">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaL2M3nt1GrsQ%3D%3D" aria-label="CAS reference 17">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=4023713" aria-label="PubMed reference 17">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Selective%20attention%20gates%20visual%20processing%20in%20the%20extrastriate%20cortex&amp;journal=Science&amp;doi=10.1126%2Fscience.4023713&amp;volume=229&amp;pages=782-784&amp;publication_year=1985&amp;author=Moran%2CJ&amp;author=Desimone%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18"><p class="c-article-references__text" id="ref-CR18">Saproo, S. &amp; Serences, J.T. Spatial attention improves the quality of population codes in human visual cortex. <i>J. Neurophysiol.</i> <b>104</b>, 885895 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.00369.2010" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.00369.2010" aria-label="Article reference 18" data-doi="10.1152/jn.00369.2010">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20484525" aria-label="PubMed reference 18">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2934940" aria-label="PubMed Central reference 18">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20attention%20improves%20the%20quality%20of%20population%20codes%20in%20human%20visual%20cortex&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.00369.2010&amp;volume=104&amp;pages=885-895&amp;publication_year=2010&amp;author=Saproo%2CS&amp;author=Serences%2CJT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19"><p class="c-article-references__text" id="ref-CR19">Womelsdorf, T., Anton-Erxleben, K., Pieper, F. &amp; Treue, S. Dynamic shifts of visual receptive fields in cortical area MT by spatial attention. <i>Nat. Neurosci.</i> <b>9</b>, 11561160 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn1748" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn1748" aria-label="Article reference 19" data-doi="10.1038/nn1748">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28Xos12msbo%3D" aria-label="CAS reference 19">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16906153" aria-label="PubMed reference 19">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20shifts%20of%20visual%20receptive%20fields%20in%20cortical%20area%20MT%20by%20spatial%20attention&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn1748&amp;volume=9&amp;pages=1156-1160&amp;publication_year=2006&amp;author=Womelsdorf%2CT&amp;author=Anton-Erxleben%2CK&amp;author=Pieper%2CF&amp;author=Treue%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20"><p class="c-article-references__text" id="ref-CR20">Womelsdorf, T., Anton-Erxleben, K. &amp; Treue, S. Receptive field shift and shrinkage in macaque middle temporal area through attentional gain modulation. <i>J. Neurosci.</i> <b>28</b>, 89348944 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.4030-07.2008" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.4030-07.2008" aria-label="Article reference 20" data-doi="10.1523/JNEUROSCI.4030-07.2008">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1cXhtFSnu7fN" aria-label="CAS reference 20">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18768687" aria-label="PubMed reference 20">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6670861" aria-label="PubMed Central reference 20">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Receptive%20field%20shift%20and%20shrinkage%20in%20macaque%20middle%20temporal%20area%20through%20attentional%20gain%20modulation&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.4030-07.2008&amp;volume=28&amp;pages=8934-8944&amp;publication_year=2008&amp;author=Womelsdorf%2CT&amp;author=Anton-Erxleben%2CK&amp;author=Treue%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21"><p class="c-article-references__text" id="ref-CR21">Anton-Erxleben, K., Stephan, V.M. &amp; Treue, S. Attention reshapes center-surround receptive field structure in macaque cortical area MT. <i>Cereb. Cortex</i> <b>19</b>, 24662478 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/bhp002" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhp002" aria-label="Article reference 21" data-doi="10.1093/cercor/bhp002">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19211660" aria-label="PubMed reference 21">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2742598" aria-label="PubMed Central reference 21">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20reshapes%20center-surround%20receptive%20field%20structure%20in%20macaque%20cortical%20area%20MT&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhp002&amp;volume=19&amp;pages=2466-2478&amp;publication_year=2009&amp;author=Anton-Erxleben%2CK&amp;author=Stephan%2CVM&amp;author=Treue%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22"><p class="c-article-references__text" id="ref-CR22">Niebergall, R., Khayat, P.S., Treue, S. &amp; Martinez-Trujillo, J.C. Expansion of MT neurons excitatory receptive fields during covert attentive tracking. <i>J. Neurosci.</i> <b>31</b>, 1549915510 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.2822-11.2011" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.2822-11.2011" aria-label="Article reference 22" data-doi="10.1523/JNEUROSCI.2822-11.2011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXhsVWiu7rP" aria-label="CAS reference 22">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22031896" aria-label="PubMed reference 22">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6703514" aria-label="PubMed Central reference 22">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Expansion%20of%20MT%20neurons%20excitatory%20receptive%20fields%20during%20covert%20attentive%20tracking&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.2822-11.2011&amp;volume=31&amp;pages=15499-15510&amp;publication_year=2011&amp;author=Niebergall%2CR&amp;author=Khayat%2CPS&amp;author=Treue%2CS&amp;author=Martinez-Trujillo%2CJC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23"><p class="c-article-references__text" id="ref-CR23">Anton-Erxleben, K. &amp; Carrasco, M. Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence. <i>Nat. Rev. Neurosci.</i> <b>14</b>, 188200 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nrn3443" data-track-action="article reference" href="https://doi.org/10.1038%2Fnrn3443" aria-label="Article reference 23" data-doi="10.1038/nrn3443">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXislWisr4%3D" aria-label="CAS reference 23">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23422910" aria-label="PubMed reference 23">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3977878" aria-label="PubMed Central reference 23">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Attentional%20enhancement%20of%20spatial%20resolution%3A%20linking%20behavioural%20and%20neurophysiological%20evidence&amp;journal=Nat.%20Rev.%20Neurosci.&amp;doi=10.1038%2Fnrn3443&amp;volume=14&amp;pages=188-200&amp;publication_year=2013&amp;author=Anton-Erxleben%2CK&amp;author=Carrasco%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24"><p class="c-article-references__text" id="ref-CR24">Liu, T., Pestilli, F. &amp; Carrasco, M. Transient attention enhances perceptual performance and FMRI response in human visual cortex. <i>Neuron</i> <b>45</b>, 469477 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2004.12.039" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2004.12.039" aria-label="Article reference 24" data-doi="10.1016/j.neuron.2004.12.039">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2MXhs1WntrY%3D" aria-label="CAS reference 24">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15694332" aria-label="PubMed reference 24">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Transient%20attention%20enhances%20perceptual%20performance%20and%20FMRI%20response%20in%20human%20visual%20cortex&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2004.12.039&amp;volume=45&amp;pages=469-477&amp;publication_year=2005&amp;author=Liu%2CT&amp;author=Pestilli%2CF&amp;author=Carrasco%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25"><p class="c-article-references__text" id="ref-CR25">Gandhi, S.P., Heeger, D.J. &amp; Boynton, G.M. Spatial attention affects brain activity in human primary visual cortex. <i>Proc. Natl. Acad. Sci. USA</i> <b>96</b>, 33143319 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.96.6.3314" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.96.6.3314" aria-label="Article reference 25" data-doi="10.1073/pnas.96.6.3314">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK1MXhvFykurY%3D" aria-label="CAS reference 25">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10077681" aria-label="PubMed reference 25">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC15939" aria-label="PubMed Central reference 25">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20attention%20affects%20brain%20activity%20in%20human%20primary%20visual%20cortex&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.96.6.3314&amp;volume=96&amp;pages=3314-3319&amp;publication_year=1999&amp;author=Gandhi%2CSP&amp;author=Heeger%2CDJ&amp;author=Boynton%2CGM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26"><p class="c-article-references__text" id="ref-CR26">Kastner, S., Pinsk, M.A., De Weerd, P., Desimone, R. &amp; Ungerleider, L.G. Increased activity in human visual cortex during directed attention in the absence of visual stimulation. <i>Neuron</i> <b>22</b>, 751761 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0896-6273(00)80734-5" data-track-action="article reference" href="https://doi.org/10.1016%2FS0896-6273%2800%2980734-5" aria-label="Article reference 26" data-doi="10.1016/S0896-6273(00)80734-5">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK1MXivVOhu7s%3D" aria-label="CAS reference 26">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10230795" aria-label="PubMed reference 26">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Increased%20activity%20in%20human%20visual%20cortex%20during%20directed%20attention%20in%20the%20absence%20of%20visual%20stimulation&amp;journal=Neuron&amp;doi=10.1016%2FS0896-6273%2800%2980734-5&amp;volume=22&amp;pages=751-761&amp;publication_year=1999&amp;author=Kastner%2CS&amp;author=Pinsk%2CMA&amp;author=De%20Weerd%2CP&amp;author=Desimone%2CR&amp;author=Ungerleider%2CLG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27"><p class="c-article-references__text" id="ref-CR27">Brefczynski, J.A. &amp; DeYoe, E.A. A physiological correlate of the spotlight of visual attention. <i>Nat. Neurosci.</i> <b>2</b>, 370374 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/7280" data-track-action="article reference" href="https://doi.org/10.1038%2F7280" aria-label="Article reference 27" data-doi="10.1038/7280">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK1MXitFGjs7k%3D" aria-label="CAS reference 27">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10204545" aria-label="PubMed reference 27">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20physiological%20correlate%20of%20the%20%E2%80%9Cspotlight%E2%80%9D%20of%20visual%20attention&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2F7280&amp;volume=2&amp;pages=370-374&amp;publication_year=1999&amp;author=Brefczynski%2CJA&amp;author=DeYoe%2CEA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28"><p class="c-article-references__text" id="ref-CR28">Silver, M.A., Ress, D. &amp; Heeger, D.J. Neural correlates of sustained spatial attention in human early visual cortex. <i>J. Neurophysiol.</i> <b>97</b>, 229237 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.00677.2006" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.00677.2006" aria-label="Article reference 28" data-doi="10.1152/jn.00677.2006">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16971677" aria-label="PubMed reference 28">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20correlates%20of%20sustained%20spatial%20attention%20in%20human%20early%20visual%20cortex&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.00677.2006&amp;volume=97&amp;pages=229-237&amp;publication_year=2007&amp;author=Silver%2CMA&amp;author=Ress%2CD&amp;author=Heeger%2CDJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29"><p class="c-article-references__text" id="ref-CR29">Tootell, R.B. et al. The retinotopy of visual spatial attention. <i>Neuron</i> <b>21</b>, 14091422 (1998).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0896-6273(00)80659-5" data-track-action="article reference" href="https://doi.org/10.1016%2FS0896-6273%2800%2980659-5" aria-label="Article reference 29" data-doi="10.1016/S0896-6273(00)80659-5">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK1MXntFSmsg%3D%3D" aria-label="CAS reference 29">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9883733" aria-label="PubMed reference 29">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20retinotopy%20of%20visual%20spatial%20attention&amp;journal=Neuron&amp;doi=10.1016%2FS0896-6273%2800%2980659-5&amp;volume=21&amp;pages=1409-1422&amp;publication_year=1998&amp;author=Tootell%2CRB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30"><p class="c-article-references__text" id="ref-CR30">Murray, S.O. The effects of spatial attention in early human visual cortex are stimulus independent. <i>J. Vis.</i> <b>8</b>, 2.12.11 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1167/8.10.2" data-track-action="article reference" href="https://doi.org/10.1167%2F8.10.2" aria-label="Article reference 30" data-doi="10.1167/8.10.2">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effects%20of%20spatial%20attention%20in%20early%20human%20visual%20cortex%20are%20stimulus%20independent&amp;journal=J.%20Vis.&amp;doi=10.1167%2F8.10.2&amp;volume=8&amp;pages=2.1-2.11&amp;publication_year=2008&amp;author=Murray%2CSO">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31"><p class="c-article-references__text" id="ref-CR31">Silver, M.A., Ress, D. &amp; Heeger, D.J. Topographic maps of visual spatial attention in human parietal cortex. <i>J. Neurophysiol.</i> <b>94</b>, 13581371 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.01316.2004" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.01316.2004" aria-label="Article reference 31" data-doi="10.1152/jn.01316.2004">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15817643" aria-label="PubMed reference 31">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Topographic%20maps%20of%20visual%20spatial%20attention%20in%20human%20parietal%20cortex&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.01316.2004&amp;volume=94&amp;pages=1358-1371&amp;publication_year=2005&amp;author=Silver%2CMA&amp;author=Ress%2CD&amp;author=Heeger%2CDJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32"><p class="c-article-references__text" id="ref-CR32">Jerde, T.A., Merriam, E.P., Riggall, A.C., Hedges, J.H. &amp; Curtis, C.E. Prioritized maps of space in human frontoparietal cortex. <i>J. Neurosci.</i> <b>32</b>, 1738217390 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.3810-12.2012" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.3810-12.2012" aria-label="Article reference 32" data-doi="10.1523/JNEUROSCI.3810-12.2012">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhslOqtLvF" aria-label="CAS reference 32">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23197729" aria-label="PubMed reference 32">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3544526" aria-label="PubMed Central reference 32">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Prioritized%20maps%20of%20space%20in%20human%20frontoparietal%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.3810-12.2012&amp;volume=32&amp;pages=17382-17390&amp;publication_year=2012&amp;author=Jerde%2CTA&amp;author=Merriam%2CEP&amp;author=Riggall%2CAC&amp;author=Hedges%2CJH&amp;author=Curtis%2CCE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33"><p class="c-article-references__text" id="ref-CR33">Jehee, J.F.M., Brady, D.K. &amp; Tong, F. Attention improves encoding of task-relevant features in the human visual cortex. <i>J. Neurosci.</i> <b>31</b>, 82108219 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.6153-09.2011" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.6153-09.2011" aria-label="Article reference 33" data-doi="10.1523/JNEUROSCI.6153-09.2011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXnsFykurw%3D" aria-label="CAS reference 33">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21632942" aria-label="PubMed reference 33">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3134176" aria-label="PubMed Central reference 33">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20improves%20encoding%20of%20task-relevant%20features%20in%20the%20human%20visual%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.6153-09.2011&amp;volume=31&amp;pages=8210-8219&amp;publication_year=2011&amp;author=Jehee%2CJFM&amp;author=Brady%2CDK&amp;author=Tong%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34"><p class="c-article-references__text" id="ref-CR34">Serences, J.T. &amp; Saproo, S. Computational advances towards linking BOLD and behavior. <i>Neuropsychologia</i> <b>50</b>, 435446 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuropsychologia.2011.07.013" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuropsychologia.2011.07.013" aria-label="Article reference 34" data-doi="10.1016/j.neuropsychologia.2011.07.013">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21840553" aria-label="PubMed reference 34">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20advances%20towards%20linking%20BOLD%20and%20behavior&amp;journal=Neuropsychologia&amp;doi=10.1016%2Fj.neuropsychologia.2011.07.013&amp;volume=50&amp;pages=435-446&amp;publication_year=2012&amp;author=Serences%2CJT&amp;author=Saproo%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35"><p class="c-article-references__text" id="ref-CR35">Awh, E. &amp; Jonides, J. Overlapping mechanisms of attention and spatial working memory. <i>Trends Cogn. Sci.</i> <b>5</b>, 119126 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S1364-6613(00)01593-X" data-track-action="article reference" href="https://doi.org/10.1016%2FS1364-6613%2800%2901593-X" aria-label="Article reference 35" data-doi="10.1016/S1364-6613(00)01593-X">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BC2sbkt1GjtQ%3D%3D" aria-label="CAS reference 35">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11239812" aria-label="PubMed reference 35">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Overlapping%20mechanisms%20of%20attention%20and%20spatial%20working%20memory&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2FS1364-6613%2800%2901593-X&amp;volume=5&amp;pages=119-126&amp;publication_year=2001&amp;author=Awh%2CE&amp;author=Jonides%2CJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36"><p class="c-article-references__text" id="ref-CR36">Brouwer, G.J. &amp; Heeger, D.J. Decoding and reconstructing color from responses in human visual cortex. <i>J. Neurosci.</i> <b>29</b>, 1399214003 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.3577-09.2009" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.3577-09.2009" aria-label="Article reference 36" data-doi="10.1523/JNEUROSCI.3577-09.2009">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXhsVSmsbnK" aria-label="CAS reference 36">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19890009" aria-label="PubMed reference 36">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2799419" aria-label="PubMed Central reference 36">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20and%20reconstructing%20color%20from%20responses%20in%20human%20visual%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.3577-09.2009&amp;volume=29&amp;pages=13992-14003&amp;publication_year=2009&amp;author=Brouwer%2CGJ&amp;author=Heeger%2CDJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37"><p class="c-article-references__text" id="ref-CR37">Naselaris, T., Kay, K., Nishimoto, S. &amp; Gallant, J. Encoding and decoding in fMRI. <i>Neuroimage</i> <b>56</b>, 400410 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2010.07.073" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2010.07.073" aria-label="Article reference 37" data-doi="10.1016/j.neuroimage.2010.07.073">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20691790" aria-label="PubMed reference 37">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Encoding%20and%20decoding%20in%20fMRI&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2010.07.073&amp;volume=56&amp;pages=400-410&amp;publication_year=2011&amp;author=Naselaris%2CT&amp;author=Kay%2CK&amp;author=Nishimoto%2CS&amp;author=Gallant%2CJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38"><p class="c-article-references__text" id="ref-CR38">Scolari, M., Byers, A. &amp; Serences, J.T. Optimal deployment of attentional gain during fine discriminations. <i>J. Neurosci.</i> <b>32</b>, 77237733 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.5558-11.2012" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.5558-11.2012" aria-label="Article reference 38" data-doi="10.1523/JNEUROSCI.5558-11.2012">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XoslSgs7w%3D" aria-label="CAS reference 38">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22649250" aria-label="PubMed reference 38">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3384562" aria-label="PubMed Central reference 38">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimal%20deployment%20of%20attentional%20gain%20during%20fine%20discriminations&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.5558-11.2012&amp;volume=32&amp;pages=7723-7733&amp;publication_year=2012&amp;author=Scolari%2CM&amp;author=Byers%2CA&amp;author=Serences%2CJT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39"><p class="c-article-references__text" id="ref-CR39">Gattass, R. et al. Cortical visual areas in monkeys: location, topography, connections, columns, plasticity and cortical dynamics. <i>Phil. Trans. R. Soc. Lond. B</i> <b>360</b>, 709731 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1098/rstb.2005.1629" data-track-action="article reference" href="https://doi.org/10.1098%2Frstb.2005.1629" aria-label="Article reference 39" data-doi="10.1098/rstb.2005.1629">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Cortical%20visual%20areas%20in%20monkeys%3A%20location%2C%20topography%2C%20connections%2C%20columns%2C%20plasticity%20and%20cortical%20dynamics&amp;journal=Phil.%20Trans.%20R.%20Soc.%20Lond.%20B&amp;doi=10.1098%2Frstb.2005.1629&amp;volume=360&amp;pages=709-731&amp;publication_year=2005&amp;author=Gattass%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40"><p class="c-article-references__text" id="ref-CR40">Ben Hamed, S., Duhamel, J.R., Bremmer, F. &amp; Graf, W. Visual receptive field modulation in the lateral intraparietal area during attentive fixation and free gaze. <i>Cereb. Cortex</i> <b>12</b>, 234245 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/12.3.234" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2F12.3.234" aria-label="Article reference 40" data-doi="10.1093/cercor/12.3.234">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD387isVOmsQ%3D%3D" aria-label="CAS reference 40">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11839598" aria-label="PubMed reference 40">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20receptive%20field%20modulation%20in%20the%20lateral%20intraparietal%20area%20during%20attentive%20fixation%20and%20free%20gaze&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2F12.3.234&amp;volume=12&amp;pages=234-245&amp;publication_year=2002&amp;author=Ben%20Hamed%2CS&amp;author=Duhamel%2CJR&amp;author=Bremmer%2CF&amp;author=Graf%2CW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41"><p class="c-article-references__text" id="ref-CR41">Mohler, C.W., Goldberg, M.E. &amp; Wurtz, R.H. Visual receptive fields of frontal eye field neurons. <i>Brain Res.</i> <b>61</b>, 385389 (1973).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/0006-8993(73)90543-X" data-track-action="article reference" href="https://doi.org/10.1016%2F0006-8993%2873%2990543-X" aria-label="Article reference 41" data-doi="10.1016/0006-8993(73)90543-X">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaE2c%2FotlyitQ%3D%3D" aria-label="CAS reference 41">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=4204128" aria-label="PubMed reference 41">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20receptive%20fields%20of%20frontal%20eye%20field%20neurons&amp;journal=Brain%20Res.&amp;doi=10.1016%2F0006-8993%2873%2990543-X&amp;volume=61&amp;pages=385-389&amp;publication_year=1973&amp;author=Mohler%2CCW&amp;author=Goldberg%2CME&amp;author=Wurtz%2CRH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42"><p class="c-article-references__text" id="ref-CR42">Dumoulin, S.O. &amp; Wandell, B.A. Population receptive field estimates in human visual cortex. <i>Neuroimage</i> <b>39</b>, 647660 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2007.09.034" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2007.09.034" aria-label="Article reference 42" data-doi="10.1016/j.neuroimage.2007.09.034">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17977024" aria-label="PubMed reference 42">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Population%20receptive%20field%20estimates%20in%20human%20visual%20cortex&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2007.09.034&amp;volume=39&amp;pages=647-660&amp;publication_year=2008&amp;author=Dumoulin%2CSO&amp;author=Wandell%2CBA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43"><p class="c-article-references__text" id="ref-CR43">Sereno, M.I., Pitzalis, S. &amp; Martinez, A. Mapping of contralateral space in retinotopic coordinates by a parietal cortical area in humans. <i>Science</i> <b>294</b>, 13501354 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1063695" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1063695" aria-label="Article reference 43" data-doi="10.1126/science.1063695">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3MXotlKmu7o%3D" aria-label="CAS reference 43">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11701930" aria-label="PubMed reference 43">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Mapping%20of%20contralateral%20space%20in%20retinotopic%20coordinates%20by%20a%20parietal%20cortical%20area%20in%20humans&amp;journal=Science&amp;doi=10.1126%2Fscience.1063695&amp;volume=294&amp;pages=1350-1354&amp;publication_year=2001&amp;author=Sereno%2CMI&amp;author=Pitzalis%2CS&amp;author=Martinez%2CA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44"><p class="c-article-references__text" id="ref-CR44">Swisher, J.D., Halko, M.A., Merabet, L.B., McMains, S.A. &amp; Somers, D.C. Visual topography of human intraparietal sulcus. <i>J. Neurosci.</i> <b>27</b>, 53265337 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.0991-07.2007" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.0991-07.2007" aria-label="Article reference 44" data-doi="10.1523/JNEUROSCI.0991-07.2007">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2sXlvFGhsr8%3D" aria-label="CAS reference 44">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17507555" aria-label="PubMed reference 44">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6672354" aria-label="PubMed Central reference 44">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20topography%20of%20human%20intraparietal%20sulcus&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.0991-07.2007&amp;volume=27&amp;pages=5326-5337&amp;publication_year=2007&amp;author=Swisher%2CJD&amp;author=Halko%2CMA&amp;author=Merabet%2CLB&amp;author=McMains%2CSA&amp;author=Somers%2CDC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45"><p class="c-article-references__text" id="ref-CR45">Saygin, A.P. &amp; Sereno, M.I. Retinotopy and attention in human occipital, temporal, parietal, and frontal cortex. <i>Cereb. Cortex</i> <b>18</b>, 21582168 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/bhm242" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhm242" aria-label="Article reference 45" data-doi="10.1093/cercor/bhm242">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18234687" aria-label="PubMed reference 45">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Retinotopy%20and%20attention%20in%20human%20occipital%2C%20temporal%2C%20parietal%2C%20and%20frontal%20cortex&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhm242&amp;volume=18&amp;pages=2158-2168&amp;publication_year=2008&amp;author=Saygin%2CAP&amp;author=Sereno%2CMI">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46"><p class="c-article-references__text" id="ref-CR46">Kastner, S. et al. Modulation of sensory suppression: implications for receptive field sizes in the human visual cortex. <i>J. Neurophysiol.</i> <b>86</b>, 13981411 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.2001.86.3.1398" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.2001.86.3.1398" aria-label="Article reference 46" data-doi="10.1152/jn.2001.86.3.1398">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BD3Mvptleguw%3D%3D" aria-label="CAS reference 46">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11535686" aria-label="PubMed reference 46">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Modulation%20of%20sensory%20suppression%3A%20implications%20for%20receptive%20field%20sizes%20in%20the%20human%20visual%20cortex&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.2001.86.3.1398&amp;volume=86&amp;pages=1398-1411&amp;publication_year=2001&amp;author=Kastner%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47"><p class="c-article-references__text" id="ref-CR47">Srimal, R. &amp; Curtis, C.E. Persistent neural activity during the maintenance of spatial position in working memory. <i>Neuroimage</i> <b>39</b>, 455468 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2007.08.040" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2007.08.040" aria-label="Article reference 47" data-doi="10.1016/j.neuroimage.2007.08.040">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17920934" aria-label="PubMed reference 47">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Persistent%20neural%20activity%20during%20the%20maintenance%20of%20spatial%20position%20in%20working%20memory&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2007.08.040&amp;volume=39&amp;pages=455-468&amp;publication_year=2008&amp;author=Srimal%2CR&amp;author=Curtis%2CCE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48"><p class="c-article-references__text" id="ref-CR48">Paus, T. Location and function of the human frontal eye-field: a selective review. <i>Neuropsychologia</i> <b>34</b>, 475483 (1996).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/0028-3932(95)00134-4" data-track-action="article reference" href="https://doi.org/10.1016%2F0028-3932%2895%2900134-4" aria-label="Article reference 48" data-doi="10.1016/0028-3932(95)00134-4">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK28zkslWltg%3D%3D" aria-label="CAS reference 48">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8736560" aria-label="PubMed reference 48">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Location%20and%20function%20of%20the%20human%20frontal%20eye-field%3A%20a%20selective%20review&amp;journal=Neuropsychologia&amp;doi=10.1016%2F0028-3932%2895%2900134-4&amp;volume=34&amp;pages=475-483&amp;publication_year=1996&amp;author=Paus%2CT">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49"><p class="c-article-references__text" id="ref-CR49">Kastner, S. et al. Topographic maps in human frontal cortex revealed in memory-guided saccade and spatial working-memory tasks. <i>J. Neurophysiol.</i> <b>97</b>, 34943507 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1152/jn.00010.2007" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.00010.2007" aria-label="Article reference 49" data-doi="10.1152/jn.00010.2007">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17360822" aria-label="PubMed reference 49">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Topographic%20maps%20in%20human%20frontal%20cortex%20revealed%20in%20memory-guided%20saccade%20and%20spatial%20working-memory%20tasks&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.00010.2007&amp;volume=97&amp;pages=3494-3507&amp;publication_year=2007&amp;author=Kastner%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50"><p class="c-article-references__text" id="ref-CR50">Fischer, J. &amp; Whitney, D. Attention narrows position tuning of population responses in V1. <i>Curr. Biol.</i> <b>19</b>, 13561361 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.cub.2009.06.059" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cub.2009.06.059" aria-label="Article reference 50" data-doi="10.1016/j.cub.2009.06.059">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXhtVens7fM" aria-label="CAS reference 50">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19631540" aria-label="PubMed reference 50">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2757109" aria-label="PubMed Central reference 50">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20narrows%20position%20tuning%20of%20population%20responses%20in%20V1&amp;journal=Curr.%20Biol.&amp;doi=10.1016%2Fj.cub.2009.06.059&amp;volume=19&amp;pages=1356-1361&amp;publication_year=2009&amp;author=Fischer%2CJ&amp;author=Whitney%2CD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51"><p class="c-article-references__text" id="ref-CR51">Engel, S.A. et al. fMRI of human visual cortex. <i>Nature</i> <b>369</b>, 525 (1994).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/369525a0" data-track-action="article reference" href="https://doi.org/10.1038%2F369525a0" aria-label="Article reference 51" data-doi="10.1038/369525a0">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK2c3mtFWquw%3D%3D" aria-label="CAS reference 51">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8031403" aria-label="PubMed reference 51">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=fMRI%20of%20human%20visual%20cortex&amp;journal=Nature&amp;doi=10.1038%2F369525a0&amp;volume=369&amp;publication_year=1994&amp;author=Engel%2CSA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52"><p class="c-article-references__text" id="ref-CR52">Sereno, M.I. et al. Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging. <i>Science</i> <b>268</b>, 889893 (1995).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.7754376" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.7754376" aria-label="Article reference 52" data-doi="10.1126/science.7754376">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK2MXls1SgsLs%3D" aria-label="CAS reference 52">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=7754376" aria-label="PubMed reference 52">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Borders%20of%20multiple%20visual%20areas%20in%20humans%20revealed%20by%20functional%20magnetic%20resonance%20imaging&amp;journal=Science&amp;doi=10.1126%2Fscience.7754376&amp;volume=268&amp;pages=889-893&amp;publication_year=1995&amp;author=Sereno%2CMI">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53"><p class="c-article-references__text" id="ref-CR53">Tootell, R.B. et al. Functional analysis of human MT and related visual cortical areas using magnetic resonance imaging. <i>J. Neurosci.</i> <b>15</b>, 32153230 (1995).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.15-04-03215.1995" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.15-04-03215.1995" aria-label="Article reference 53" data-doi="10.1523/JNEUROSCI.15-04-03215.1995">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK2MXltVOitLo%3D" aria-label="CAS reference 53">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=7722658" aria-label="PubMed reference 53">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6577785" aria-label="PubMed Central reference 53">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Functional%20analysis%20of%20human%20MT%20and%20related%20visual%20cortical%20areas%20using%20magnetic%20resonance%20imaging&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.15-04-03215.1995&amp;volume=15&amp;pages=3215-3230&amp;publication_year=1995&amp;author=Tootell%2CRB">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54"><p class="c-article-references__text" id="ref-CR54">Serences, J.T. &amp; Boynton, G.M. The representation of behavioral choice for motion in human visual cortex. <i>J. Neurosci.</i> <b>27</b>, 1289312899 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.4021-07.2007" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.4021-07.2007" aria-label="Article reference 54" data-doi="10.1523/JNEUROSCI.4021-07.2007">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2sXhsVWrsrrM" aria-label="CAS reference 54">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18032662" aria-label="PubMed reference 54">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6673290" aria-label="PubMed Central reference 54">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20representation%20of%20behavioral%20choice%20for%20motion%20in%20human%20visual%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.4021-07.2007&amp;volume=27&amp;pages=12893-12899&amp;publication_year=2007&amp;author=Serences%2CJT&amp;author=Boynton%2CGM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55"><p class="c-article-references__text" id="ref-CR55">Kay, K.N., Naselaris, T., Prenger, R.J. &amp; Gallant, J.L. Identifying natural images from human brain activity. <i>Nature</i> <b>452</b>, 352355 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature06713" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature06713" aria-label="Article reference 55" data-doi="10.1038/nature06713">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1cXjsFCnsL8%3D" aria-label="CAS reference 55">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18322462" aria-label="PubMed reference 55">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3556484" aria-label="PubMed Central reference 55">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Identifying%20natural%20images%20from%20human%20brain%20activity&amp;journal=Nature&amp;doi=10.1038%2Fnature06713&amp;volume=452&amp;pages=352-355&amp;publication_year=2008&amp;author=Kay%2CKN&amp;author=Naselaris%2CT&amp;author=Prenger%2CRJ&amp;author=Gallant%2CJL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56"><p class="c-article-references__text" id="ref-CR56">Hoerl, A.E. &amp; Kennard, R.W. Ridge regression: biased estimation for nonorthogonal problems. <i>Technometrics</i> <b>12</b>, 5567 (1970).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1080/00401706.1970.10488634" data-track-action="article reference" href="https://doi.org/10.1080%2F00401706.1970.10488634" aria-label="Article reference 56" data-doi="10.1080/00401706.1970.10488634">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Ridge%20regression%3A%20biased%20estimation%20for%20nonorthogonal%20problems&amp;journal=Technometrics&amp;doi=10.1080%2F00401706.1970.10488634&amp;volume=12&amp;pages=55-67&amp;publication_year=1970&amp;author=Hoerl%2CAE&amp;author=Kennard%2CRW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57"><p class="c-article-references__text" id="ref-CR57">Lee, S., Papanikolaou, A., Logothetis, N.K., Smirnakis, S.M. &amp; Keliris, G.A. A new method for estimating population receptive field topography in visual cortex. <i>Neuroimage</i> <b>81</b>, 144157 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2013.05.026" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2013.05.026" aria-label="Article reference 57" data-doi="10.1016/j.neuroimage.2013.05.026">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23684878" aria-label="PubMed reference 57">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20method%20for%20estimating%20population%20receptive%20field%20topography%20in%20visual%20cortex&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2013.05.026&amp;volume=81&amp;pages=144-157&amp;publication_year=2013&amp;author=Lee%2CS&amp;author=Papanikolaou%2CA&amp;author=Logothetis%2CNK&amp;author=Smirnakis%2CSM&amp;author=Keliris%2CGA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58"><p class="c-article-references__text" id="ref-CR58">Schwarz, G. Estimating the dimension of a model. <i>Ann. Stat.</i> <b>6</b>, 461464 (1978).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1214/aos/1176344136" data-track-action="article reference" href="https://doi.org/10.1214%2Faos%2F1176344136" aria-label="Article reference 58" data-doi="10.1214/aos/1176344136">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimating%20the%20dimension%20of%20a%20model&amp;journal=Ann.%20Stat.&amp;doi=10.1214%2Faos%2F1176344136&amp;volume=6&amp;pages=461-464&amp;publication_year=1978&amp;author=Schwarz%2CG">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59"><p class="c-article-references__text" id="ref-CR59">Benjamini, Y. &amp; Yekutieli, D. The control of the false discovery rate in multiple testing under dependency. <i>Ann. Stat.</i> <b>29</b>, 11651188 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1214/aos/1013699998" data-track-action="article reference" href="https://doi.org/10.1214%2Faos%2F1013699998" aria-label="Article reference 59" data-doi="10.1214/aos/1013699998">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20control%20of%20the%20false%20discovery%20rate%20in%20multiple%20testing%20under%20dependency&amp;journal=Ann.%20Stat.&amp;doi=10.1214%2Faos%2F1013699998&amp;volume=29&amp;pages=1165-1188&amp;publication_year=2001&amp;author=Benjamini%2CY&amp;author=Yekutieli%2CD">
                    Google Scholar</a>
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/nn.3574?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank E. Vul and S. Itthipuripat for assistance with statistical methods and M. Scolari and M. Smith for assistance with parietal cortex mapping protocols. This work was supported by a National Science Foundation Graduate Research Fellowship to T.C.S. and by US National Institutes of Health grant R0I MH-092345 and a James S. McDonnell Scholar Award to J.T.S.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Neuroscience Graduate Program, University of California San Diego, La Jolla, California, USA</p><p class="c-article-author-affiliation__authors-list">Thomas C Sprague&amp;John T Serences</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Psychology, University of California San Diego, La Jolla, California, USA</p><p class="c-article-author-affiliation__authors-list">John T Serences</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Thomas_C-Sprague-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Thomas C Sprague</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Thomas%20C%20Sprague" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Thomas%20C%20Sprague" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Thomas%20C%20Sprague%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-John_T-Serences-Aff1-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">John T Serences</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=John%20T%20Serences" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=John%20T%20Serences" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22John%20T%20Serences%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>T.C.S. and J.T.S. designed the experiments and analysis method and wrote the manuscript. T.C.S. conducted the experiments and implemented the analyses.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:tsprague@ucsd.edu">Thomas C Sprague</a> or <a id="corresp-c2" href="mailto:jserences@ucsd.edu">John T Serences</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading">Competing interests</h3>
                <p>The authors declare no competing financial interests.</p>
              
            </div></div></section><section data-title="Integrated supplementary information"><div class="c-article-section" id="Sec32-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec32">Integrated supplementary information</h2><div class="c-article-section__content" id="Sec32-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig8"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 1 participants maintained fix" href="/articles/nn.3574/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig8_ESM.jpg">Supplementary Figure 1 Participants maintained fixation in the scanner during all three task conditions, related to Figure 2.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Average horizontal and vertical gaze position across each 3 s trial in each task condition. Neither horizontal nor vertical gaze varied as a function of either stimulus position or task demands. 2-way ANOVA for each gaze direction, with task condition and stimulus position (grouped into 6 bins corresponding to <i>x</i> or <i>y</i> coordinate for horizontal and vertical gaze plots, respectively) as factors: minimum <i>p</i> for main effects/interactions = 0.2725, which was for main effect of vertical stimulus position on vertical gaze. Note that data from null trials were not entered into the ANOVA, but subjects maintained steady fixation on these trials as well. Eyetracking data gathered in the scanner for 4 of the 8 participants. Error bars 1 S.E.M. across subjects.</p><p>
                          <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM69">Source data</a>
                        </p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig9"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 2 one-dimensional cross-secti" href="/articles/nn.3574/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig9_ESM.jpg">Supplementary Figure 2 One-dimensional cross-section of 2D basis function, related to Figure 3.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Cross-section through the center of a single basis function (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig3">Figure 3a</a>). FWHM is the full-width at half-maximum. The size constant, <i>s</i>, was set to 5<i>r</i><sub><i>stim</i></sub> (see Online Methods: Encoding model, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig10">Supplementary Fig. 3</a>), where <i>r</i><sub><i>stim</i></sub> is 1.163. This corresponds to the distance from the center at which the filter amplitude reaches 0.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig10"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 3 the relationship between ba" href="/articles/nn.3574/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig10_ESM.jpg">Supplementary Figure 3 The relationship between basis function size and spacing changes the smoothness of reconstructions, related to Figures 3 and 4.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) For a constant spatial filter separation distance of 2.09 (which matches that used in the main analysis), we varied the size parameter (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig9">Supplementary Fig. 2</a>) of 2 neighboring spatial filters, then plotted their sum as a function of position in space and filter size (which was continuously varied). Summed response is indicated by the image colorscale. (<b>b</b>) A slice from (<b>a</b>) at the FWHM of the filters used in the main analysis (dashed line in panel <b>a</b>). This value resulted in smooth reconstructions to which we could accurately fit surfaces to quantify the spatial representations (see Online Methods: <i>Curvefitting</i>), but also resulted in sufficient filter separation so that adjacent filters did not excessively overlap (see below). Smaller FWHM values would result in speckled reconstructed spatial representations which would be poorly fit using a single surface (this would be seen as a dipped black solid line in panel <b>b</b>; see panel <b>a</b> at small filter size values), and larger FWHM values would result in poorly discriminable predicted channel responses because neighboring filters would account for much of the same variance in the signal due to a high degree of overlap (see <b>a</b>, high FWHM values). At high enough FWHM values, the model cannot be estimated because overly high correlations between adjacent filters result in a rank deficient design matrix (Equation 1 in Online Methods).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig11"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 4 poor reconstructions during" href="/articles/nn.3574/figures/11" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig11_ESM.jpg">Supplementary Figure 4 Poor reconstructions during attend fixation condition for participant AG3, related to Figure 4.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Plotted as in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Figure 4</a>. All images on same color scale. Poor reconstructed spatial representations were measured during attend fixation runs across all ROIs, but more typical looking reconstructed spatial representations were observed for both of the other task conditions. Behavioral performance for this participant indicated they were awake and vigilantly performing the fixation task. This was the only participant with this issue, and their data were not included in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Figures 4</a> or <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">5</a> (see Online Methods: <i>Excluded participant</i>). Note that the same estimated channel weight matrix was used here as was used to reconstruct spatial representations during the attend stimulus and spatial working memory tasks. Furthermore, note that these data support our conclusion of higher amplitude spatial priority maps with attention and they were excluded solely because of the noisy fits in the fixation condition. All of our reported effects would be more pronounced if this participant was included (see data included in the html version of this report).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig12"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 5 encoding model does not ove" href="/articles/nn.3574/figures/12" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig12_ESM.jpg">Supplementary Figure 5 Encoding model does not overfit data and generalizes to novel stimuli, related to Figure 4.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) Reconstructions from all 36 stimulus locations under the attend stimulus condition across all 8 observers using voxels from only the ventral and dorsal aspects of V2 and V3. Color scale is identical to that used in Figure 4. Note that spatial reconstructions in the dorsal &amp; ventral aspects of V2 and V3 are more robust in the lower and upper visual field, respectively. This pattern matches the known selectivity of dorsal and ventral areas V2 and V3. (<b>b</b>) Encoding model can be generalized to reconstruct novel stimuli that were not part of the <i>training set</i>. An encoding model trained using all attend fixation, attend stimulus &amp; spatial working memory runs was able to accurately reconstruct a novel, untrained stimulus set acquired during a different scanning session on 7 of 8 participants presented in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig4">Figures 4</a>,<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">5</a> (novel test data was not available for this 8<sup>th</sup> participant, AA3). This novel stimulus display consisted of four half-circle stimuli presented at one of two eccentricities (see top row), and the model was able to reconstruct these four stimuli with a high degree of precision (see Online Methods: <i>Stimulus reconstructions  novel stimuli</i> for more details).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig13"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 6 spcs exhibits larger respon" href="/articles/nn.3574/figures/13" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig13_ESM.jpg">Supplementary Figure 6 sPCS exhibits larger responses, averaged across all voxels within the sPCS, in the attend stimulus and spatial working memory conditions, related to Figures 4 and 5.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Both left and right sPCS exhibit strong hemodynamic responses to stimuli, with increased averaged (i.e. univariate) responses during attend stimulus and spatial working memory task conditions compared to the attend fixation condition. Additionally, this mean signal increase under conditions of attention to the stimulus or spatial working memory likely accounts for much of the significant increase in the baseline offset in the reconstructed stimulus representations reported in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figure 5</a>. Error bars  SEM across subjects.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig14"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 7 ips roi primarily correspon" href="/articles/nn.3574/figures/14" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig14_ESM.jpg">Supplementary Figure 7 IPS ROI primarily corresponds to IPS 0/1, related to Figures 6 and 7.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) Polar angle preferences for each voxel plotted on the inflated surface of 4 participants' cortical sheets. Maps are liberally thresholded to show any voxel with normalized power at the stimulus frequency &gt; 0.005. Smooth polar angle transitions were used to delineate four retinotopic regions of IPS (termed IPS 0-3) in each of these 8 hemispheres. Dashed lines: lower vertical meridian (LVM); solid lines: upper vertical meridian (UVM). (<b>b</b>) For each participant and each hemisphere we compared the number of overlapping voxels between our original localizer-defined IPS ROI (see Online Methods: <i>Mapping IPS subregions</i>) and each of these 4 retinotopically mapped IPS subregions . The original IPS ROI primarily overlaps with areas IPS 0 and 1, and is therefore labeled as such in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">Figure 7</a>. Blue: left hemisphere, Red: right hemisphere. (<b>c</b>) Fit parameters to reconstructed spatial representations estimated from activation patterns across each IPS subregion for these 4 participants (analysis identical to that implemented for <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">Fig. 7</a>). Critically, fit parameters in all regions are similar to those observed for the original IPS subregion (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">Fig. 7</a>). Spatial representations of presented stimuli do not narrow in size when stimuli are attended or a target is remembered, but amplitude increases for representations in IPS0 (<i>p</i> = 0.012), and baseline increases in IPS3 (<i>p</i> = 0.002; statistics as in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figs. 5</a> &amp; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a>; error bars within-participant S.E.M.)</p><p>
                          <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM70">Source data</a>
                        </p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig15"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 8 population receptive field " href="/articles/nn.3574/figures/15" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig15_ESM.jpg">Supplementary Figure 8 Population receptive field analyses: example participant AA3B, related to Figure 7.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) Reconstructed pRFs and best-fit isotropic function for voxels at each interquartile boundary. White dashed circles are plotted at half-maximum of fit function. Quartiles were split by minimum <i>R</i><sup><i>2</i></sup> across all task conditions (see Online Methods: <i>Population receptive field estimation</i>). Above each column of pRFs is the minimum <i>R</i><sup><i>2</i></sup> value across the 3 conditions shown below. The right 3 columns (top 50%) are voxels that were included in subsequent analyses. White horizontal scale bars correspond to 1 visual angle. (<b>b</b>) Distribution of <i>R</i><sup><i>2</i></sup> (colored lines) and minimum <i>R</i><sup><i>2</i></sup> across conditions (black lines) for each voxel, plotted as a cumulative distribution. (<b>c</b>) Size vs. eccentricity for each condition for each ROI. Each data point corresponds to a single voxel. Black circles/lines are the mean size at each eccentricity bin which contains  5 voxels (these are the points which are included in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">Supplementary Fig. 9a</a>). All slopes for this example participant are significantly &gt; 0 after Bonferroni correction across all 48 tests (4 participants  4 ROIs  3 conditions, corrected  = 0.001), except hMT+, spatial working memory condition (<i>p</i> = 0.006, see Online Methods: <i>Statistical Procedures</i>). (<b>d</b>) Distribution of pRF size for each voxel across condition pairs. The percentage of voxels which lie above the unity line (that is, the percentage of voxels for which the size increases) within a ROI and condition pair is used to evaluate whether task demands significantly change pRF size (see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig16">Supplementary Fig. 9b</a>, Supplementary Results, Online Methods).</p><p>
                          <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM71">Source data</a>
                        </p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig16"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 9 population receptive fields" href="/articles/nn.3574/figures/16" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig16_ESM.jpg">Supplementary Figure 9 Population receptive fields increase size with attention, related to Figure 7.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a</b>) Summary of pRF size as a function of eccentricity across <i>n</i> = 4 participants. Each data point is plotted if  3 participants each had  5 voxels within that eccentricity bin. Error bars S.E.M. across included participants. (<b>b</b>) Summary of pRF size changes across each condition pair. For each ROI for each participant, we computed the percentage of voxels in which the pRF size was greater for the first condition than the second (e.g., cyan bars indicate the percentage of voxels in which pRF size was greater for the attend stimulus condition than for the attend fixation condition; this corresponds to the percentage of voxels which lie above unity when plotted as in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Fig. 8d</a>). Black asterisks indicate significant size changes across a condition pair for an ROI, Bonferroni-corrected (two-tailed t-test, see Online Methods: <i>Statistical procedures</i>). Gray asterisks indicate a significant size change using a one-tailed t-test. Error bars indicate S.E.M. across participants (<i>n</i> = 4).</p><p>
                          <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.3574#MOESM72">Source data</a>
                        </p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig17"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 10 simulations demonstrate th" href="/articles/nn.3574/figures/17" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_Article_BFnn3574_Fig17_ESM.jpg">Supplementary Figure 10 Simulations demonstrate that uniform changes in voxel-level pRFs are reflected in changes in region-level spatial representations, related to Figure 7.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>(<b>a-b</b>) For 500 simulated voxels, we generated data for 2 conditions in which we only manipulated the simulated pRF size (condition B uses pRFs that were on average 11% larger than pRFs in condition A, which corresponds to the measured increase in pRF size between attend stimulus and attend fixation conditions in hV4 across all 4 participants). Under these conditions, the size of the multivariate spatial representations scaled with pRF size (<b>a</b>, smaller <i>spatial representation</i> sizes in condition A than in condition B). However, note that in this scenario, there is no change in fit amplitude (<b>b</b>). This demonstrates that (1) multivariate spatial representations are sensitive to changes in pRF size, given that the changes occur uniformly across a region, and (2) that our analysis technique can detect size changes mediated by uniform changes in pRF size in the absence of amplitude changes, were they occurring. This rules out an important possibility that representation size changes might be occurring in our dataset, but they could be too small to measure (see Results: <i>Size of spatial representations across eccentricity and ROI</i>). (<b>c-d</b>) In panels <b>a-b</b> we demonstrate that multivariate region-level spatial representations can increase in size, reflecting uniform changes in the underlying univariate voxel-level pRFs. Here, we used the fit pRF parameters for 1 example participant (AA3B, shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Fig. 8</a>) and 1 ROI (hV4), which undergo non-uniform size changes across conditions, to simulate data for all 3 task conditions in the main experiment. Even with pRF size increases observed across conditions (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig15">Supplementary Fig. 8d</a>), multivariate spatial representations are shown to maintain a constant size (<b>c</b>), but increase in amplitude (<b>d</b>, mirroring our data in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig5">Figs. 5</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.3574#Fig7">7</a>). This pattern of results was also found in the other three participants (not shown). This demonstrates a decoupling of pRF size/amplitude and the size/amplitude of multivariate region-level spatial representations, and underscores the importance of exploiting all of the information available in a region to estimate the fidelity of spatial encoding.</p></div></div></div></div></div></section><section data-title="Supplementary information"><div class="c-article-section" id="Sec33-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec33">Supplementary information</h2><div class="c-article-section__content" id="Sec33-content"><div data-test="supplementary-info"><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM73"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary text and figures" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_BFnn3574_MOESM73_ESM.pdf" data-supp-info-image="">Supplementary Text and Figures</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Supplementary Figures 110, Supplementary Tables 1,2 and Supplementary Results (PDF 6486 kb)</p></div></div></div></div></div></section><section data-title="Source data"><div class="c-article-section" id="Sec34-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec34">Source data</h2><div class="c-article-section__content" id="Sec34-content"><div data-test="supplementary-info"><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM65"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="source data to fig. 1" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_BFnn3574_MOESM65_ESM.xlsx" data-supp-info-image="">Source data to Fig. 1</a></h3></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM66"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="source data to fig. 2" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_BFnn3574_MOESM66_ESM.xlsx" data-supp-info-image="">Source data to Fig. 2</a></h3></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM67"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="source data to fig. 3" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_BFnn3574_MOESM67_ESM.xlsx" data-supp-info-image="">Source data to Fig. 3</a></h3></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM68"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="source data to fig. 4" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_BFnn3574_MOESM68_ESM.xlsx" data-supp-info-image="">Source data to Fig. 4</a></h3></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM69"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="source data to supplementary fig. 5" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_BFnn3574_MOESM69_ESM.xlsx" data-supp-info-image="">Source data to Supplementary Fig. 5</a></h3></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM70"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="source data to supplementary fig. 6" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_BFnn3574_MOESM70_ESM.xlsx" data-supp-info-image="">Source data to Supplementary Fig. 6</a></h3></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM71"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="source data to supplementary fig. 7" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_BFnn3574_MOESM71_ESM.xlsx" data-supp-info-image="">Source data to Supplementary Fig. 7</a></h3></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM72"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="source data to supplementary fig. 8" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.3574/MediaObjects/41593_2013_BFnn3574_MOESM72_ESM.xlsx" data-supp-info-image="">Source data to Supplementary Fig. 8</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Attention%20modulates%20spatial%20priority%20maps%20in%20the%20human%20occipital%2C%20parietal%20and%20frontal%20cortices&amp;author=Thomas%20C%20Sprague%20et%20al&amp;contentID=10.1038%2Fnn.3574&amp;copyright=Springer%20Nature%20America%2C%20Inc.&amp;publication=1097-6256&amp;publicationDate=2013-11-10&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Sprague, T., Serences, J. Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices.
                    <i>Nat Neurosci</i> <b>16</b>, 18791887 (2013). https://doi.org/10.1038/nn.3574</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/nn.3574?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-07-19">19 July 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-10-09">09 October 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-11-10">10 November 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-12">December 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/nn.3574</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>

            
        <section>
            <div class="c-article-section js-article-section" id="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Pinging the brain to reveal the hidden attentional priority map using encephalography" href="https://doi.org/10.1038/s41467-023-40405-8">
                                        Pinging the brain to reveal the hidden attentional priority map using encephalography
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Dock H. Duncan</li><li>Dirk van Moorselaar</li><li>Jan Theeuwes</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Communications</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Fronto-parietal networks shape human conscious report through attention gain and reorienting" href="https://doi.org/10.1038/s42003-023-05108-2">
                                        Fronto-parietal networks shape human conscious report through attention gain and reorienting
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Jianghao Liu</li><li>Dimitri J. Bayle</li><li>Paolo Bartolomeo</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Communications Biology</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Expectation violations enhance neuronal encoding of sensory information in mouse primary visual cortex" href="https://doi.org/10.1038/s41467-023-36608-8">
                                        Expectation violations enhance neuronal encoding of sensory information in mouse primary visual cortex
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Matthew F. Tang</li><li>Ehsan Kheradpezhouh</li><li>Ehsan Arabzadeh</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Communications</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Accurate localization and coactivation profiles of the frontal eye field and inferior frontal junction: an ALE and MACM fMRI meta-analysis" href="https://doi.org/10.1007/s00429-023-02641-y">
                                        Accurate localization and coactivation profiles of the frontal eye field and inferior frontal junction: an ALE and MACM fMRI meta-analysis
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Marco Bedini</li><li>Emanuele Olivetti</li><li>Daniel Baldauf</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Brain Structure and Function</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:A brain-based general measure of attention" href="https://doi.org/10.1038/s41562-022-01301-1">
                                        A brain-based general measure of attention
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Kwangsun Yoo</li><li>Monica D. Rosenberg</li><li>Marvin M. Chun</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Human Behaviour</i> (2022)</p>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </section>
    

            
        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="js-context-bar-sticky-point-desktop">
        

        
            <noscript>
                
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3574.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </noscript>
            <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
                <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-right-column
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.3574.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </div>
        
    </div>

    
        
    

    
    

    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>
                            
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=nn.3574;doi=10.1038/nn.3574;techmeta=36,59;subjmeta=116,1310,2395,2613,2614,2649,378,631;kwrd=Attention,Extrastriate+cortex,Neural+encoding">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=1998330275&amp;t=pos%3Dright%26type%3Darticle%26artid%3Dnn.3574%26doi%3D10.1038/nn.3574%26techmeta%3D36,59%26subjmeta%3D116,1310,2395,2613,2614,2649,378,631%26kwrd%3DAttention,Extrastriate+cortex,Neural+encoding">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=1998330275&amp;t=pos%3Dright%26type%3Darticle%26artid%3Dnn.3574%26doi%3D10.1038/nn.3574%26techmeta%3D36,59%26subjmeta%3D116,1310,2395,2613,2614,2649,378,631%26kwrd%3DAttention,Extrastriate+cortex,Neural+encoding"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>


    
        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-and-analysis"
                                   data-track="click"
                                   data-track-action="reviews &amp; analysis"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Reviews &amp; Analysis
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/video"
                                   data-track="click"
                                   data-track-action="videos"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Videos
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/current-issue"
                                   data-track="click"
                                   data-track-action="current issue"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Current issue
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>
                        
                    
                </ul>
                <ul class="c-header__list c-header__list--js-stack">
                    
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/natureneuro"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on Twitter
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;6"
                               rel="nofollow"
                               data-track="click"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/neuro.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </nav>
    
    
        
            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/aims"
                                   data-track="click"
                                   data-track-action="aims &amp; scope"
                                   data-track-label="link">
                                    Aims &amp; Scope
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-information"
                                   data-track="click"
                                   data-track-action="journal information"
                                   data-track-label="link">
                                    Journal Information
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-impact"
                                   data-track="click"
                                   data-track-action="journal metrics"
                                   data-track-label="link">
                                    Journal Metrics
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editors"
                                   data-track="click"
                                   data-track-action="about the editors"
                                   data-track-label="link">
                                    About the Editors
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/our-publishing-models"
                                   data-track="click"
                                   data-track-action="our publishing models"
                                   data-track-label="link">
                                    Our publishing models
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-values-statement"
                                   data-track="click"
                                   data-track-action="editorial values statement"
                                   data-track-label="link">
                                    Editorial Values Statement
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-policies"
                                   data-track="click"
                                   data-track-action="editorial policies"
                                   data-track-label="link">
                                    Editorial Policies
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/content"
                                   data-track="click"
                                   data-track-action="content types"
                                   data-track-label="link">
                                    Content Types
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/web-feeds"
                                   data-track="click"
                                   data-track-action="web feeds"
                                   data-track-label="link">
                                    Web Feeds
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/posters"
                                   data-track="click"
                                   data-track-action="posters"
                                   data-track-label="link">
                                    Posters
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="research cross-journal editorial team"
                                   data-track-label="link">
                                    Research Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="reviews cross-journal editorial team"
                                   data-track-label="link">
                                    Reviews Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        

        
            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/submission-guidelines"
                                   data-track="click"
                                   data-track-action="submission guidelines"
                                   data-track-label="link">
                                    Submission Guidelines
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/for-reviewers"
                                   data-track="click"
                                   data-track-action="for reviewers"
                                   data-track-label="link">
                                    For Reviewers
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="click"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://mts-nn.nature.com/cgi-bin/main.plex?form_type&#x3D;home&amp;from_idp&#x3D;1"
                                   data-track="click"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external>Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        
    


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">
                            
                                
                                    <option value="" selected>All journals</option>
                                    <option value="neuro">This journal</option>
                                
                            
                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        

        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
            

            <p class="c-meta u-ma-0 u-flex-shrink">
                <span class="c-meta__item">
                    Nature Neuroscience (<i>Nat Neurosci</i>)
                </span>
                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="onlineIssn">1546-1726</span> (online)
    </span>
    


                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="printIssn">1097-6256</span> (print)
    </span>
    

            </p>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/protocolexchange/"
                                                  data-track="click" data-track-action="protocol exchange"
                                                  data-track-label="link">Protocol Exchange</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natitaly"
                                                  data-track="click" data-track-action="nature Italy"
                                                  data-track-label="link">Nature Italy</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ko-kr"
                                                  data-track="click" data-track-action="nature korea"
                                                  data-track-label="link">Nature Korea</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>
            
        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-ea63208b81.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2024 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path id="shape" fill-rule="evenodd" clip-rule="evenodd" d="M1 3.78571C1 2.75867 1.85698 2 2.8209 2H6.1791C7.14302 2 8 2.75867 8 3.78571V4H11.1668C11.885 4 12.5585 4.42017 12.8494 5.07033C12.9893 4.98169 13.1425 4.91101 13.3056 4.86206L16.5222 3.89704C17.4454 3.62005 18.4843 4.10046 18.7794 5.08419L22.9256 18.9042C23.2207 19.8878 22.618 20.8608 21.6947 21.1378L18.4781 22.1029C17.5548 22.3799 16.516 21.8993 16.2209 20.9157L13.0001 10.1804V20.2143C13.0001 21.255 12.1231 22 11.1668 22H7.83346C7.54206 22 7.25803 21.9308 7.00392 21.8052C6.75263 21.9305 6.47077 22 6.1791 22H2.8209C1.85693 22 1 21.2412 1 20.2143V3.78571ZM3 4V15H6V4H3ZM3 20V17H6V20H3ZM18.0749 20.1358L17.2129 17.2623L20.0863 16.4002L20.9484 19.2737L18.0749 20.1358ZM19.5116 14.4846L16.6381 15.3466L14.0519 6.72624L16.9254 5.86416L19.5116 14.4846ZM8.00012 20L8.00012 6H11.0001L11.0001 20H8.00012Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</footer>




    

    

<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">

    
    <div class="c-site-messages__banner-large">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing" src="/static/images/logos/nature-briefing-logo-n150-white-d81c9da3ec.svg" width="250" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing</em> newsletter  what matters in science, free to your inbox daily.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/briefing" method="post" data-location="banner" data-track="submit||nature_briefing_sign_up" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="MainBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick">
                        <input type="hidden" value="false" name="marketing" id="marketing">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick">
                        <input type="hidden" value="MainBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">
                            
                            <input type="hidden" value="true" name="N:nature_briefing_daily" id="defaultNewsletter">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>

    
    <div class="c-site-messages__banner-small">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner">Sign up for Nature Briefing
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/nn.3574&amp;format=js&amp;last_modified=2013-12-01" async></script>
<img src="/j66rutvv/article/nn.3574" width="1" height="1" alt="" class="u-visually-hidden">
</body>
</html>