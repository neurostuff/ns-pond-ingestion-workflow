<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>Shared memories reveal shared structure in neural activity across individuals | Nature Neuroscience</title>
    
        
<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/neuro.rss"/>


    
        

        <script id="save-data-connection-testing">
            function hasConnection() {
                return navigator.connection || navigator.mozConnection || navigator.webkitConnection || navigator.msConnection;
            }

            function createLink(src) {
                var preloadLink = document.createElement("link");
                preloadLink.rel = "preload";
                preloadLink.href = src;
                preloadLink.as = "font";
                preloadLink.type = "font/woff2";
                preloadLink.crossOrigin = "";
                document.head.insertBefore(preloadLink, document.head.firstChild);
            }

            var connectionDetail = {
                saveDataEnabled: false,
                slowConnection: false
            };

            var connection = hasConnection();
            if (connection) {
                connectionDetail.saveDataEnabled = connection.saveData;
                if (/\slow-2g|2g/.test(connection.effectiveType)) {
                    connectionDetail.slowConnection = true;
                }
            }

            if (!(connectionDetail.saveDataEnabled || connectionDetail.slowConnection)) {
                createLink("/static/fonts/HardingText-Regular-Web-cecd90984f.woff2");
            } else {
                document.documentElement.classList.add('save-data');
            }
        </script>
    

<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"cortex;long-term-memory;perception;psychology;social-neuroscience","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Nature Neuroscience","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article"}},"article":{"doi":"10.1038/nn.4450"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["Janice Chen","Yuan Chang Leong","Christopher J Honey","Chung H Yong","Kenneth A Norman","Uri Hasson"],"publishedAt":1480896000,"publishedAtString":"2016-12-05","title":"Shared memories reveal shared structure in neural activity across individuals","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"neuro","title":"nature neuroscience","volume":"20","issue":"1"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"nature-onwards-journey","active":false},{"name":"getftr-entitled","active":false},{"name":"paywall_recommendations","active":true}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"US","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://collect.nature.com',
        imprint: 'nature'
    });
</script>

<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
 



     
    
    
        
    
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card--major .c-card__title,.c-card__title,.u-h2,.u-h3,h2,h3{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-weight:700;letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.u-h3,h3{font-size:1.25rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}html{text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;line-height:1.15;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}h1{-webkit-font-smoothing:antialiased;font-family:Harding,Palatino,serif;font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h2,.u-h3,h2{font-family:Harding,Palatino,serif;letter-spacing:-.0117156rem}.c-card--major .c-card__title,.u-h2,h2{-webkit-font-smoothing:antialiased;font-size:1.5rem;font-weight:700;line-height:1.6rem}.u-h3{font-size:1.25rem}.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h4,h5,h6{-webkit-font-smoothing:antialiased;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,h3{font-family:Harding,Palatino,serif;font-size:1.25rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,h3{-webkit-font-smoothing:antialiased;font-weight:700;letter-spacing:-.0117156rem;line-height:1.4rem}.c-reading-companion__figure-title,.u-h4,h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;letter-spacing:-.0117156rem}button:focus{outline:3px solid #fece3e;will-change:transform}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Harding,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-license__badge,c-card__section{margin-top:8px}.c-code-block{border:1px solid #eee;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-status-message--success{border-bottom:2px solid #00b8b0;justify-content:center;margin-bottom:16px;padding-bottom:8px}.c-recommendations-list__item .c-card{flex-basis:100%}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 0 0 16px;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #cedbe0;height:auto;min-height:0;position:static}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-size:1rem;font-weight:700;line-height:1.4;margin:0 0 8px;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:inherit}.c-card__title-recommendation .c-card__link:hover{text-decoration:underline}.c-card__title-recommendation .MathJax_Display{display:inline!important}.c-card__link:not(.c-card__link--no-block-link):before{z-index:1}.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a,.c-article-recommendations-card__link{color:inherit}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-article-recommendations-card__meta-type,.c-meta .c-meta__item:first-child{font-weight:700}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:539px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-author-list{color:#6f6f6f;font-family:inherit;font-size:1rem;line-height:inherit;list-style:none;margin:0;padding:0}.c-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .c-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.c-author-list>li:not(:first-child):not(:last-child):before{content:", "}.c-author-list>li:not(:only-child):last-child:before{content:" & "}.c-author-list--compact{font-size:.875rem;line-height:1.4}.c-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .c-author-list__hide{display:none;visibility:hidden}.js .c-author-list__hide:first-child+*{margin-block-start:0}.c-meta{color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -8px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px 13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #069;border-radius:2px;color:#069;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-list-reset{list-style:none;margin:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.c-nature-box svg+.c-article__button-text,.u-ml-8{margin-left:8px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.u-lazy-ad-wrapper{background-color:#ccc;display:none;min-height:137px}@media only screen and (min-width:768px){.u-lazy-ad-wrapper{display:block}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.c-pdf-download__link{padding:13px 24px} } </style>




    
        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    
    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-nature-branded-950e2d5825.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-122346e276.css" media="print">
    



<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">


<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h.indexOf('preview-www.nature.com') > -1) return;

            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('nature.com') > -1) {
                if (h.indexOf('test-www.nature.com') > -1) {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                } else {
                    e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-54.js';
                    e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
                }
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-26e142e9c6.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    
                        {src: '/static/js/global-article-es6-bundle-782fd09f66.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-d66d49033d.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-aca08c055a.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-4fba787158.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-1fe07484e5.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;
                        
                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-8fc1a30809.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-e0c7186f28.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-2399be388c.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">





    
    <script type="application/ld+json">{"mainEntity":{"headline":"Shared memories reveal shared structure in neural activity across individuals","description":"The authors demonstrate that activity patterns in the default network during unguided spoken recollection of real-world events were similar between individuals recalling the same specific events. Patterns were altered between perception and recall in a systematic manner across brains. These results reveal a common spatial organization for memory representations. Our lives revolve around sharing experiences and memories with others. When different people recount the same events, how similar are their underlying neural representations? Participants viewed a 50-min movie, then verbally described the events during functional MRI, producing unguided detailed descriptions lasting up to 40 min. As each person spoke, event-specific spatial patterns were reinstated in default-network, medial-temporal, and high-level visual areas. Individual event patterns were both highly discriminable from one another and similar among people, suggesting consistent spatial organization. In many high-order areas, patterns were more similar between people recalling the same event than between recall and perception, indicating systematic reshaping of percept into memory. These results reveal the existence of a common spatial organization for memories in high-level cortical areas, where encoded information is largely abstracted beyond sensory constraints, and that neural patterns during perception are altered systematically across people into shared memory representations for real-life events.","datePublished":"2016-12-05T00:00:00Z","dateModified":"2016-12-05T00:00:00Z","pageStart":"115","pageEnd":"125","sameAs":"https://doi.org/10.1038/nn.4450","keywords":["Cortex","Long-term memory","Perception","Psychology","Social neuroscience","Biomedicine","general","Neurosciences","Behavioral Sciences","Biological Techniques","Neurobiology","Animal Genetics and Genomics"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig1_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig2_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig3_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig4_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig5_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig6_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig7_HTML.jpg","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig8_HTML.jpg"],"isPartOf":{"name":"Nature Neuroscience","issn":["1546-1726","1097-6256"],"volumeNumber":"20","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Janice Chen","affiliation":[{"name":"Princeton Neuroscience Institute, Princeton University","address":{"name":"Princeton Neuroscience Institute, Princeton University, Princeton, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Princeton University","address":{"name":"Department of Psychology, Princeton University, Princeton, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Johns Hopkins University","address":{"name":"Department of Psychological and Brain Sciences, Johns Hopkins University, Baltimore, USA","@type":"PostalAddress"},"@type":"Organization"}],"email":"janice@princeton.edu","@type":"Person"},{"name":"Yuan Chang Leong","url":"http://orcid.org/0000-0003-2499-2393","affiliation":[{"name":"Stanford University","address":{"name":"Department of Psychology, Stanford University, Stanford, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Christopher J Honey","affiliation":[{"name":"Johns Hopkins University","address":{"name":"Department of Psychological and Brain Sciences, Johns Hopkins University, Baltimore, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"University of Toronto","address":{"name":"Department of Psychology, University of Toronto, Toronto, Canada","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Chung H Yong","affiliation":[{"name":"University of Toronto","address":{"name":"Department of Psychology, University of Toronto, Toronto, Canada","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Kenneth A Norman","affiliation":[{"name":"Princeton Neuroscience Institute, Princeton University","address":{"name":"Princeton Neuroscience Institute, Princeton University, Princeton, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Princeton University","address":{"name":"Department of Psychology, Princeton University, Princeton, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Uri Hasson","affiliation":[{"name":"Princeton Neuroscience Institute, Princeton University","address":{"name":"Princeton Neuroscience Institute, Princeton University, Princeton, USA","@type":"PostalAddress"},"@type":"Organization"},{"name":"Princeton University","address":{"name":"Department of Psychology, Princeton University, Princeton, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>




    
    
    


    
    <link rel="canonical" href="https://www.nature.com/articles/nn.4450">
    
    
    <meta name="journal_id" content="41593"/>
    <meta name="dc.title" content="Shared memories reveal shared structure in neural activity across individuals"/>
    <meta name="dc.source" content="Nature Neuroscience 2016 20:1"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2016-12-05"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2016 Springer Nature America, Inc."/>
    <meta name="dc.rights" content="2016 Springer Nature America, Inc."/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="The authors demonstrate that activity patterns in the default network during unguided spoken recollection of real-world events were similar between individuals recalling the same specific events. Patterns were altered between perception and recall in a systematic manner across brains. These results reveal a common spatial organization for memory representations. Our lives revolve around sharing experiences and memories with others. When different people recount the same events, how similar are their underlying neural representations? Participants viewed a 50-min movie, then verbally described the events during functional MRI, producing unguided detailed descriptions lasting up to 40 min. As each person spoke, event-specific spatial patterns were reinstated in default-network, medial-temporal, and high-level visual areas. Individual event patterns were both highly discriminable from one another and similar among people, suggesting consistent spatial organization. In many high-order areas, patterns were more similar between people recalling the same event than between recall and perception, indicating systematic reshaping of percept into memory. These results reveal the existence of a common spatial organization for memories in high-level cortical areas, where encoded information is largely abstracted beyond sensory constraints, and that neural patterns during perception are altered systematically across people into shared memory representations for real-life events."/>
    <meta name="prism.issn" content="1546-1726"/>
    <meta name="prism.publicationName" content="Nature Neuroscience"/>
    <meta name="prism.publicationDate" content="2016-12-05"/>
    <meta name="prism.volume" content="20"/>
    <meta name="prism.number" content="1"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="115"/>
    <meta name="prism.endingPage" content="125"/>
    <meta name="prism.copyright" content="2016 Springer Nature America, Inc."/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/nn.4450"/>
    <meta name="prism.doi" content="doi:10.1038/nn.4450"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/nn.4450.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/nn.4450"/>
    <meta name="citation_journal_title" content="Nature Neuroscience"/>
    <meta name="citation_journal_abbrev" content="Nat Neurosci"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="1546-1726"/>
    <meta name="citation_title" content="Shared memories reveal shared structure in neural activity across individuals"/>
    <meta name="citation_volume" content="20"/>
    <meta name="citation_issue" content="1"/>
    <meta name="citation_publication_date" content="2017/01"/>
    <meta name="citation_online_date" content="2016/12/05"/>
    <meta name="citation_firstpage" content="115"/>
    <meta name="citation_lastpage" content="125"/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/nn.4450"/>
    <meta name="DOI" content="10.1038/nn.4450"/>
    <meta name="size" content="265776"/>
    <meta name="citation_doi" content="10.1038/nn.4450"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/nn.4450&amp;api_key="/>
    <meta name="description" content="The authors demonstrate that activity patterns in the default network during unguided spoken recollection of real-world events were similar between individuals recalling the same specific events. Patterns were altered between perception and recall in a systematic manner across brains. These results reveal a common spatial organization for memory representations. Our lives revolve around sharing experiences and memories with others. When different people recount the same events, how similar are their underlying neural representations? Participants viewed a 50-min movie, then verbally described the events during functional MRI, producing unguided detailed descriptions lasting up to 40 min. As each person spoke, event-specific spatial patterns were reinstated in default-network, medial-temporal, and high-level visual areas. Individual event patterns were both highly discriminable from one another and similar among people, suggesting consistent spatial organization. In many high-order areas, patterns were more similar between people recalling the same event than between recall and perception, indicating systematic reshaping of percept into memory. These results reveal the existence of a common spatial organization for memories in high-level cortical areas, where encoded information is largely abstracted beyond sensory constraints, and that neural patterns during perception are altered systematically across people into shared memory representations for real-life events."/>
    <meta name="dc.creator" content="Chen, Janice"/>
    <meta name="dc.creator" content="Leong, Yuan Chang"/>
    <meta name="dc.creator" content="Honey, Christopher J"/>
    <meta name="dc.creator" content="Yong, Chung H"/>
    <meta name="dc.creator" content="Norman, Kenneth A"/>
    <meta name="dc.creator" content="Hasson, Uri"/>
    <meta name="dc.subject" content="Cortex"/>
    <meta name="dc.subject" content="Long-term memory"/>
    <meta name="dc.subject" content="Perception"/>
    <meta name="dc.subject" content="Psychology"/>
    <meta name="dc.subject" content="Social neuroscience"/>
    <meta name="citation_reference" content="Isola, P., Xiao, J., Torralba, A. &amp; Oliva, A. What makes an image memorable?. in 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 145&#8211;152 (2011) 
                  doi:10.1109/CVPR.2011.5995721
                  
                ."/>
    <meta name="citation_reference" content="Halbwachs, M. The Collective Memory (Harper &amp; Row Colophon, 1980)."/>
    <meta name="citation_reference" content="Sperber, D. Explaining Culture: A Naturalistic Approach (Blackwell, 1996)."/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol. Gen.; citation_title=Cognition through a social network: the propagation of induced forgetting and practice effects; citation_author=A Coman, W Hirst; citation_volume=141; citation_publication_date=2012; citation_pages=321-336; citation_doi=10.1037/a0025247; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Collective memory: a new arena of cognitive study; citation_author=HL Roediger, M Abel; citation_volume=19; citation_publication_date=2015; citation_pages=359-361; citation_doi=10.1016/j.tics.2015.04.003; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=A default mode of brain function; citation_author=ME Raichle; citation_volume=98; citation_publication_date=2001; citation_pages=676-682; citation_doi=10.1073/pnas.98.2.676; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Intersubject synchronization of cortical activity during natural vision; citation_author=U Hasson, Y Nir, I Levy, G Fuhrmann, R Malach; citation_volume=303; citation_publication_date=2004; citation_pages=1634-1640; citation_doi=10.1126/science.1089506; citation_id=CR7"/>
    <meta name="citation_reference" content="citation_journal_title=Open Neuroimag. J.; citation_title=Inter-subject synchronization of prefrontal cortex hemodynamic activity during natural viewing; citation_author=IP J&#228;&#228;skel&#228;inen; citation_volume=2; citation_publication_date=2008; citation_pages=14-19; citation_doi=10.2174/1874440000802010014; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Beyond superior temporal cortex: intersubject correlations in narrative speech comprehension; citation_author=SM Wilson, I Molnar-Szakacs, M Iacoboni; citation_volume=18; citation_publication_date=2008; citation_pages=230-242; citation_doi=10.1093/cercor/bhm049; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Topographic mapping of a hierarchy of temporal receptive windows using a narrated story; citation_author=Y Lerner, CJ Honey, LJ Silbert, U Hasson; citation_volume=31; citation_publication_date=2011; citation_pages=2906-2915; citation_doi=10.1523/JNEUROSCI.3684-10.2011; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Not lost in translation: neural responses shared across languages; citation_author=CJ Honey, CR Thompson, Y Lerner, U Hasson; citation_volume=32; citation_publication_date=2012; citation_pages=15277-15283; citation_doi=10.1523/JNEUROSCI.1800-12.2012; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Synchronous brain activity across individuals underlies shared psychological perspectives; citation_author=JM Lahnakoski; citation_volume=100; citation_publication_date=2014; citation_pages=316-324; citation_doi=10.1016/j.neuroimage.2014.06.022; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Commun.; citation_title=Dynamic reconfiguration of the default mode network during narrative comprehension; citation_author=E Simony; citation_volume=7; citation_publication_date=2016; citation_pages=12141; citation_doi=10.1038/ncomms12141; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Selective and invariant neural responses to spoken and written narratives; citation_author=M Regev, CJ Honey, E Simony, U Hasson; citation_volume=33; citation_publication_date=2013; citation_pages=15978-15988; citation_doi=10.1523/JNEUROSCI.1580-13.2013; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Psychol.; citation_title=A cross-modal investigation of the neural substrates for ongoing cognition; citation_author=M Wang, BJ He; citation_volume=5; citation_publication_date=2014; citation_pages=945; citation_id=CR15"/>
    <meta name="citation_reference" content="Borges, J.L. Funes the Memorious. La Naci&#243;n (Mitre, 1942)."/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Memory&#39;s echo: vivid remembering reactivates sensory-specific cortex; citation_author=ME Wheeler, SE Petersen, RL Buckner; citation_volume=97; citation_publication_date=2000; citation_pages=11125-11129; citation_doi=10.1073/pnas.97.20.11125; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Bull.; citation_title=The ghosts of brain states past: remembering reactivates the brain regions engaged during encoding; citation_author=JF Danker, JR Anderson; citation_volume=136; citation_publication_date=2010; citation_pages=87-102; citation_doi=10.1037/a0017937; citation_id=CR18"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Category-specific cortical activity precedes retrieval during memory search; citation_author=SM Polyn, VS Natu, JD Cohen, KA Norman; citation_volume=310; citation_publication_date=2005; citation_pages=1963-1966; citation_doi=10.1126/science.1117645; citation_id=CR19"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Recollection, familiarity, and cortical reinstatement: a multivoxel pattern analysis; citation_author=JD Johnson, SGR McDuff, MD Rugg, KA Norman; citation_volume=63; citation_publication_date=2009; citation_pages=697-708; citation_doi=10.1016/j.neuron.2009.08.011; citation_id=CR20"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Fidelity of neural reactivation reveals competition between memories; citation_author=BA Kuhl, J Rissman, MM Chun, AD Wagner; citation_volume=108; citation_publication_date=2011; citation_pages=5903-5908; citation_doi=10.1073/pnas.1016939108; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=J. Cogn. Neurosci.; citation_title=The neural basis of vivid memory is patterned on perception; citation_author=BR Buchsbaum, S Lemire-Rodger, C Fang, H Abdi; citation_volume=24; citation_publication_date=2012; citation_pages=1867-1883; citation_doi=10.1162/jocn_a_00253; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=J. Cogn. Neurosci.; citation_title=Reinstatement of individual past events revealed by the similarity of distributed activation patterns during encoding and retrieval; citation_author=EA Wing, M Ritchey, R Cabeza; citation_volume=27; citation_publication_date=2015; citation_pages=679-691; citation_doi=10.1162/jocn_a_00740; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Consolidation of complex events via reinstatement in posterior cingulate cortex; citation_author=CM Bird, JL Keidel, LP Ing, AJ Horner, N Burgess; citation_volume=35; citation_publication_date=2015; citation_pages=14426-14434; citation_doi=10.1523/JNEUROSCI.1774-15.2015; citation_id=CR24"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Syst. Neurosci.; citation_title=Representational similarity analysis - connecting the branches of systems neuroscience; citation_author=N Kriegeskorte, M Mur, P Bandettini; citation_volume=2; citation_publication_date=2008; citation_pages=4; citation_doi=10.3389/neuro.01.016.2008; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=Ann. NY Acad. Sci.; citation_title=The brain&#39;s default network: anatomy, function, and relevance to disease; citation_author=RL Buckner, JR Andrews-Hanna, DL Schacter; citation_volume=1124; citation_publication_date=2008; citation_pages=1-38; citation_doi=10.1196/annals.1440.011; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Opin. Neurobiol.; citation_title=Brain networks underlying episodic memory retrieval; citation_author=MD Rugg, KL Vilberg; citation_volume=23; citation_publication_date=2013; citation_pages=255-260; citation_doi=10.1016/j.conb.2012.11.005; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Slow cortical dynamics and the accumulation of information over long timescales; citation_author=CJ Honey; citation_volume=76; citation_publication_date=2012; citation_pages=423-434; citation_doi=10.1016/j.neuron.2012.08.011; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Reliability of cortical activity during natural stimulation; citation_author=U Hasson, R Malach, DJ Heeger; citation_volume=14; citation_publication_date=2010; citation_pages=40-48; citation_doi=10.1016/j.tics.2009.10.011; citation_id=CR29"/>
    <meta name="citation_reference" content="citation_journal_title=Mach. Learn.; citation_title=Learning to decode cognitive states from brain images; citation_author=TM Mitchell; citation_volume=57; citation_publication_date=2004; citation_pages=145-175; citation_doi=10.1023/B:MACH.0000035475.85309.1b; citation_id=CR30"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Sci.; citation_title=Decoding the large-scale structure of brain function by classifying mental States across individuals; citation_author=RA Poldrack, YO Halchenko, SJ Hanson; citation_volume=20; citation_publication_date=2009; citation_pages=1364-1372; citation_doi=10.1111/j.1467-9280.2009.02460.x; citation_id=CR31"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Commonality of neural representations of words and pictures; citation_author=SV Shinkareva, VL Malave, RA Mason, TM Mitchell, MA Just; citation_volume=54; citation_publication_date=2011; citation_pages=2418-2425; citation_doi=10.1016/j.neuroimage.2010.10.042; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Multivariate pattern analysis reveals common neural patterns across individuals during touch observation; citation_author=JT Kaplan, K Meyer; citation_volume=60; citation_publication_date=2012; citation_pages=204-212; citation_doi=10.1016/j.neuroimage.2011.12.059; citation_id=CR33"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Low-level image properties of visual objects predict patterns of neural response across category-selective regions of the ventral visual pathway; citation_author=GE Rice, DM Watson, T Hartley, TJ Andrews; citation_volume=34; citation_publication_date=2014; citation_pages=8837-8844; citation_doi=10.1523/JNEUROSCI.5265-13.2014; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Unique semantic space in the brain of each beholder predicts perceived similarity; citation_author=I Charest, RA Kievit, TW Schmitz, D Deca, N Kriegeskorte; citation_volume=111; citation_publication_date=2014; citation_pages=14565-14570; citation_doi=10.1073/pnas.1402594111; citation_id=CR35"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Visual field maps in human cortex; citation_author=BA Wandell, SO Dumoulin, AA Brewer; citation_volume=56; citation_publication_date=2007; citation_pages=366-383; citation_doi=10.1016/j.neuron.2007.10.012; citation_id=CR36"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Mirror-symmetric tonotopic maps in human primary auditory cortex; citation_author=E Formisano; citation_volume=40; citation_publication_date=2003; citation_pages=859-869; citation_doi=10.1016/S0896-6273(03)00669-X; citation_id=CR37"/>
    <meta name="citation_reference" content="citation_journal_title=Curr. Biol.; citation_title=The retinotopic organization of striate cortex is well predicted by surface topology; citation_author=NC Benson; citation_volume=22; citation_publication_date=2012; citation_pages=2081-2085; citation_doi=10.1016/j.cub.2012.09.014; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=Annu. Rev. Neurosci.; citation_title=Place cells, grid cells, and the brain&#39;s spatial representation system; citation_author=EI Moser, E Kropff, M-B Moser; citation_volume=31; citation_publication_date=2008; citation_pages=69-89; citation_doi=10.1146/annurev.neuro.31.061307.090723; citation_id=CR39"/>
    <meta name="citation_reference" content="citation_journal_title=Exp. Brain Res.; citation_title=Hippocampal place units in the freely moving rat: why they fire where they fire; citation_author=J O&#39;Keefe, DH Conway; citation_volume=31; citation_publication_date=1978; citation_pages=573-590; citation_doi=10.1007/BF00239813; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Matching categorical object representations in inferior temporal cortex of man and monkey; citation_author=N Kriegeskorte; citation_volume=60; citation_publication_date=2008; citation_pages=1126-1141; citation_doi=10.1016/j.neuron.2008.10.043; citation_id=CR41"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Deconstructing episodic memory with construction; citation_author=D Hassabis, EA Maguire; citation_volume=11; citation_publication_date=2007; citation_pages=299-306; citation_doi=10.1016/j.tics.2007.05.001; citation_id=CR42"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Rev. Neurosci.; citation_title=Two cortical systems for memory-guided behaviour; citation_author=C Ranganath, M Ritchey; citation_volume=13; citation_publication_date=2012; citation_pages=713-726; citation_doi=10.1038/nrn3338; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_journal_title=J. Cogn. Neurosci.; citation_title=Contextual alignment of cognitive and neural dynamics; citation_author=DL Ames, CJ Honey, MA Chow, A Todorov, U Hasson; citation_volume=27; citation_publication_date=2015; citation_pages=655-664; citation_doi=10.1162/jocn_a_00728; citation_id=CR44"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Bull.; citation_title=Is memory schematic?; citation_author=JW Alba, L Hasher; citation_volume=93; citation_publication_date=1983; citation_pages=203-231; citation_doi=10.1037/0033-2909.93.2.203; citation_id=CR45"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Segmentation in the perception and memory of events; citation_author=CA Kurby, JM Zacks; citation_volume=12; citation_publication_date=2008; citation_pages=72-79; citation_doi=10.1016/j.tics.2007.11.004; citation_id=CR46"/>
    <meta name="citation_reference" content="Baldassano, C. et al. Discovering event structure in continuous narrative perception and memory. Preprint at bioRxiv 
                  http://dx.doi.org/10.1101/081018
                  
                 (2016)."/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Memory, navigation and theta rhythm in the hippocampal-entorhinal system; citation_author=G Buzs&#225;ki, EI Moser; citation_volume=16; citation_publication_date=2013; citation_pages=130-138; citation_doi=10.1038/nn.3304; citation_id=CR48"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Brain-to-brain coupling: a mechanism for creating and sharing a social world; citation_author=U Hasson, AA Ghazanfar, B Galantucci, S Garrod, C Keysers; citation_volume=16; citation_publication_date=2012; citation_pages=114-121; citation_doi=10.1016/j.tics.2011.12.007; citation_id=CR49"/>
    <meta name="citation_reference" content="Zadbood, A., Chen, J., Leong, Y.C., Norman, K.A. &amp; Hasson, U. How we transmit memories to other brains: constructing shared neural representations via communication. Preprint at bioRxiv 
                  http://dx.doi.org/10.1101/081208
                  
                 (2016)."/>
    <meta name="citation_reference" content="McGuigan, P. A Study in Pink. Sherlock (BBC, 2010)."/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Speaker&#8211;listener neural coupling underlies successful communication; citation_author=GJ Stephens, LJ Silbert, U Hasson; citation_volume=107; citation_publication_date=2010; citation_pages=14425-14430; citation_doi=10.1073/pnas.1008662107; citation_id=CR52"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Coupled neural systems underlie the production and comprehension of naturalistic narrative speech; citation_author=LJ Silbert, CJ Honey, E Simony, D Poeppel, U Hasson; citation_volume=111; citation_publication_date=2014; citation_pages=E4687-E4696; citation_doi=10.1073/pnas.1323812111; citation_id=CR53"/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest; citation_author=RS Desikan; citation_volume=31; citation_publication_date=2006; citation_pages=968-980; citation_doi=10.1016/j.neuroimage.2006.01.021; citation_id=CR54"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Decoding subject-driven cognitive states with whole-brain connectivity patterns; citation_author=WR Shirer, S Ryali, E Rykhlevskaia, V Menon, MD Greicius; citation_volume=22; citation_publication_date=2012; citation_pages=158-165; citation_doi=10.1093/cercor/bhr099; citation_id=CR55"/>
    <meta name="citation_reference" content="citation_journal_title=Proc. Natl. Acad. Sci. USA; citation_title=Information-based functional brain mapping; citation_author=N Kriegeskorte, R Goebel, P Bandettini; citation_volume=103; citation_publication_date=2006; citation_pages=3863-3868; citation_doi=10.1073/pnas.0600244103; citation_id=CR56"/>
    <meta name="citation_reference" content="Chen, P.-H. et al. in Advances in Neural Information Processing Systems 28 (eds. Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M. &amp; Garnett, R.) 460&#8211;468 (Curran Associates, 2015)."/>
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Encoding and decoding in fMRI; citation_author=T Naselaris, KN Kay, S Nishimoto, JL Gallant; citation_volume=56; citation_publication_date=2011; citation_pages=400-410; citation_doi=10.1016/j.neuroimage.2010.07.073; citation_id=CR58"/>
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Predicting human brain activity associated with the meanings of nouns; citation_author=TM Mitchell; citation_volume=320; citation_publication_date=2008; citation_pages=1191-1195; citation_doi=10.1126/science.1152876; citation_id=CR59"/>
    <meta name="citation_reference" content="Wild, F. lsa: latent semantic analysis. R package version 0.73.1 (2015)."/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Coarse-scale biases for spirals and orientation in human visual cortex; citation_author=J Freeman, DJ Heeger, EP Merriam; citation_volume=33; citation_publication_date=2013; citation_pages=19695-19703; citation_doi=10.1523/JNEUROSCI.0889-13.2013; citation_id=CR61"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Decoding the visual and subjective contents of the human brain; citation_author=Y Kamitani, F Tong; citation_volume=8; citation_publication_date=2005; citation_pages=679-685; citation_doi=10.1038/nn1444; citation_id=CR62"/>
    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=A common, high-dimensional model of the representational space in human ventral temporal cortex; citation_author=JV Haxby; citation_volume=72; citation_publication_date=2011; citation_pages=404-416; citation_doi=10.1016/j.neuron.2011.08.026; citation_id=CR63"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Probabilistic maps of visual topography in human cortex; citation_author=L Wang, REB Mruczek, MJ Arcaro, S Kastner; citation_volume=25; citation_publication_date=2015; citation_pages=3911-3931; citation_doi=10.1093/cercor/bhu277; citation_id=CR64"/>
    <meta name="citation_reference" content="citation_journal_title=Nat. Methods; citation_title=Large-scale automated synthesis of human functional neuroimaging data; citation_author=T Yarkoni, RA Poldrack, TE Nichols, DC Van Essen, TD Wager; citation_volume=8; citation_publication_date=2011; citation_pages=665-670; citation_doi=10.1038/nmeth.1635; citation_id=CR65"/>
    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Imagery and perception share cortical representations of content and location; citation_author=RM Cichy, J Heinzle, J-D Haynes; citation_volume=22; citation_publication_date=2012; citation_pages=372-380; citation_doi=10.1093/cercor/bhr106; citation_id=CR66"/>
    <meta name="citation_reference" content="citation_journal_title=J. Cogn. Neurosci.; citation_title=Distributed patterns of reactivation predict vividness of recollection; citation_author=M St-Laurent, H Abdi, BR Buchsbaum; citation_volume=27; citation_publication_date=2015; citation_pages=2000-2018; citation_doi=10.1162/jocn_a_00839; citation_id=CR67"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Bull.; citation_title=When is early visual cortex activated during visual mental imagery?; citation_author=SM Kosslyn, WL Thompson; citation_volume=129; citation_publication_date=2003; citation_pages=723-746; citation_doi=10.1037/0033-2909.129.5.723; citation_id=CR68"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Decoding reveals the contents of visual working memory in early visual areas; citation_author=SA Harrison, F Tong; citation_volume=458; citation_publication_date=2009; citation_pages=632-635; citation_doi=10.1038/nature07832; citation_id=CR69"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Sci.; citation_title=Stimulus-specific delay activity in human primary visual cortex; citation_author=JT Serences, EF Ester, EK Vogel, E Awh; citation_volume=20; citation_publication_date=2009; citation_pages=207-214; citation_doi=10.1111/j.1467-9280.2009.02276.x; citation_id=CR70"/>
    <meta name="citation_author" content="Chen, Janice"/>
    <meta name="citation_author_institution" content="Princeton Neuroscience Institute, Princeton University, Princeton, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, Princeton University, Princeton, USA"/>
    <meta name="citation_author_institution" content="Department of Psychological and Brain Sciences, Johns Hopkins University, Baltimore, USA"/>
    <meta name="citation_author" content="Leong, Yuan Chang"/>
    <meta name="citation_author_institution" content="Department of Psychology, Stanford University, Stanford, USA"/>
    <meta name="citation_author" content="Honey, Christopher J"/>
    <meta name="citation_author_institution" content="Department of Psychological and Brain Sciences, Johns Hopkins University, Baltimore, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of Toronto, Toronto, Canada"/>
    <meta name="citation_author" content="Yong, Chung H"/>
    <meta name="citation_author_institution" content="Department of Psychology, University of Toronto, Toronto, Canada"/>
    <meta name="citation_author" content="Norman, Kenneth A"/>
    <meta name="citation_author_institution" content="Princeton Neuroscience Institute, Princeton University, Princeton, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, Princeton University, Princeton, USA"/>
    <meta name="citation_author" content="Hasson, Uri"/>
    <meta name="citation_author_institution" content="Princeton Neuroscience Institute, Princeton University, Princeton, USA"/>
    <meta name="citation_author_institution" content="Department of Psychology, Princeton University, Princeton, USA"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@natureneuro"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Shared memories reveal shared structure in neural activity across individuals"/>
    <meta name="twitter:description" content="Nature Neuroscience - The authors demonstrate that activity patterns in the default network during unguided spoken recollection of real-world events were similar between individuals recalling the..."/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig1_HTML.jpg"/>
    

    
    
    <meta property="og:url" content="https://www.nature.com/articles/nn.4450"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="Shared memories reveal shared structure in neural activity across individuals - Nature Neuroscience"/>
    <meta property="og:description" content="The authors demonstrate that activity patterns in the default network during unguided spoken recollection of real-world events were similar between individuals recalling the same specific events. Patterns were altered between perception and recall in a systematic manner across brains. These results reveal a common spatial organization for memory representations."/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig1_HTML.jpg"/>
    

    <script>
        window.eligibleForRa21 = 'true'; 
    </script>
</head>
<body class="article-page">

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>



<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
        
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>

    

    <div class="u-lazy-ad-wrapper u-mbs-0">
            <div class="deferred-placeholder" data-replace="true"
                 data-placeholder="/placeholder/v1/institutionalBanner?bpids=[bpids] #institutional-banner-container"></div>
            <aside class="c-ad c-ad--728x90">
                <div class="c-ad__inner" data-container-type="banner-advert">
                    <p class="c-ad__label">Advertisement</p>
                    
        
            
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=nn.4450;doi=10.1038/nn.4450;techmeta=36,59;subjmeta=1595,1723,2167,2618,2645,2649,378,477,631;kwrd=Cortex,Long-term+memory,Perception,Psychology,Social+neuroscience">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=993815721&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Dnn.4450%26doi%3D10.1038/nn.4450%26techmeta%3D36,59%26subjmeta%3D1595,1723,2167,2618,2645,2649,378,477,631%26kwrd%3DCortex,Long-term+memory,Perception,Psychology,Social+neuroscience">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=728x90&amp;c=993815721&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Dnn.4450%26doi%3D10.1038/nn.4450%26techmeta%3D36,59%26subjmeta%3D1595,1723,2167,2618,2645,2649,378,477,631%26kwrd%3DCortex,Long-term+memory,Perception,Psychology,Social+neuroscience"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

        
    
                </div>
            </aside>
        </div>
    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#00928c">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">
                    
                    
                    <div class="c-header__logo-container">
                        
                        <a href="/neuro"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-0ccc487532906d646419e51f647ce79a.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/neuro/header-880e5942f43b9213989c58a04ab5c8e6.svg" height="32" alt="Nature Neuroscience">
                            </picture>
                        </a>
                    
                    </div>
                    
                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">
                            
                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/nn.4450'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
                            
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        
            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">
                            
                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span><span class="c-header__show-text">Explore</span> content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                                
                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>
                                
                            
                            
                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">
                            
                                <li class="c-header__item">
                                    <a class="c-header__link"
                                       href="https://idp.nature.com/auth/personal/springernature?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Fmy-account%2Falerts%2Fsubscribe-journal%3Flist-id%3D6"
                                       rel="nofollow"
                                       data-track="click"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/neuro.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        
    </header>


    
    
        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature neuroscience"><span itemprop="name">nature neuroscience</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/neuro/articles?type&#x3D;article" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="name">articles</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>
    



    

</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
        
            <div class="c-context-bar u-hide"
                 data-test="context-bar"
                 data-context-bar
                 aria-hidden="true">
                <div class="c-context-bar__container u-container">
                    <div class="c-context-bar__title">
                        Shared memories reveal shared structure in neural activity across individuals
                    </div>
                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.4450.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                </div>
            </div>
        
        <article lang="en">
            
            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Article</li>
    
    

                        <li class="c-article-identifiers__item">Published: <time datetime="2016-12-05">05 December 2016</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Shared memories reveal shared structure in neural activity across individuals</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Janice-Chen-Aff1-Aff2-Aff3" data-author-popup="auth-Janice-Chen-Aff1-Aff2-Aff3" data-author-search="Chen, Janice" data-corresp-id="c1">Janice Chen<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a>,<a href="#Aff3">3</a></sup><sup class="u-js-hide"><a href="#na1">na1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yuan_Chang-Leong-Aff4" data-author-popup="auth-Yuan_Chang-Leong-Aff4" data-author-search="Leong, Yuan Chang">Yuan Chang Leong</a><span class="u-js-hide">
            <a class="js-orcid" href="http://orcid.org/0000-0003-2499-2393"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-2499-2393</a></span><sup class="u-js-hide"><a href="#Aff4">4</a></sup><sup class="u-js-hide"><a href="#na1">na1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Christopher_J-Honey-Aff3-Aff5" data-author-popup="auth-Christopher_J-Honey-Aff3-Aff5" data-author-search="Honey, Christopher J">Christopher J Honey</a><sup class="u-js-hide"><a href="#Aff3">3</a>,<a href="#Aff5">5</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Chung_H-Yong-Aff5" data-author-popup="auth-Chung_H-Yong-Aff5" data-author-search="Yong, Chung H">Chung H Yong</a><sup class="u-js-hide"><a href="#Aff5">5</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kenneth_A-Norman-Aff1-Aff2" data-author-popup="auth-Kenneth_A-Norman-Aff1-Aff2" data-author-search="Norman, Kenneth A">Kenneth A Norman</a><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 6 authors for this article" title="Show all 6 authors for this article"></li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Uri-Hasson-Aff1-Aff2" data-author-popup="auth-Uri-Hasson-Aff1-Aff2" data-author-search="Hasson, Uri">Uri Hasson</a><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a></sup></li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>

                    

                    <p class="c-article-info-details" data-container-section="info">
                        
    <a data-test="journal-link" href="/neuro" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Nature Neuroscience</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span>20</b>,<span class="u-visually-hidden">pages </span>115125 (<span data-test="article-publication-year">2017</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>
                    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">25k <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">298 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">335 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/articles/nn.4450/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
                    
                </header>

                
    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/cortex" data-track="click" data-track-action="view subject" data-track-label="link">Cortex</a></li><li class="c-article-subject-list__subject"><a href="/subjects/long-term-memory" data-track="click" data-track-action="view subject" data-track-label="link">Long-term memory</a></li><li class="c-article-subject-list__subject"><a href="/subjects/perception" data-track="click" data-track-action="view subject" data-track-label="link">Perception</a></li><li class="c-article-subject-list__subject"><a href="/subjects/psychology" data-track="click" data-track-action="view subject" data-track-label="link">Psychology</a></li><li class="c-article-subject-list__subject"><a href="/subjects/social-neuroscience" data-track="click" data-track-action="view subject" data-track-label="link">Social neuroscience</a></li>
        </ul>
    </div>

                
    
    

    
    

                
            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs2" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs2">Abstract</h2><div class="c-article-section__content" id="Abs2-content"><p>Our lives revolve around sharing experiences and memories with others. When different people recount the same events, how similar are their underlying neural representations? Participants viewed a 50-min movie, then verbally described the events during functional MRI, producing unguided detailed descriptions lasting up to 40 min. As each person spoke, event-specific spatial patterns were reinstated in default-network, medial-temporal, and high-level visual areas. Individual event patterns were both highly discriminable from one another and similar among people, suggesting consistent spatial organization. In many high-order areas, patterns were more similar between people recalling the same event than between recall and perception, indicating systematic reshaping of percept into memory. These results reveal the existence of a common spatial organization for memories in high-level cortical areas, where encoded information is largely abstracted beyond sensory constraints, and that neural patterns during perception are altered systematically across people into shared memory representations for real-life events.</p></div></div></section>

            <noscript>
                
                    
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.4450.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                
            </noscript>

            
                <div class="js-context-bar-sticky-point-mobile">
                    
                        <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-entitled-mobile
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.4450.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

                    
                </div>
            

            
                
                    
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-023-36805-5/MediaObjects/41467_2023_36805_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41467-023-36805-5?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1038/s41467-023-36805-5">Flexible reuse of cortico-hippocampal representations during encoding and recall of naturalistic events
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">08 March 2023</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Zachariah M. Reagh &amp; Charan Ranganath</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-022-34075-1/MediaObjects/41467_2022_34075_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41467-022-34075-1?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1038/s41467-022-34075-1">Multidimensional memory topography in the medial parietal cortex identified from neuroimaging of thousands of daily memory videos
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">31 October 2022</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Wilma A. Bainbridge &amp; Chris I. Baker</p>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-022-33517-0/MediaObjects/41467_2022_33517_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41467-022-33517-0?fromPaywallRec=false"
                                           data-track="click"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1038/s41467-022-33517-0">Schemas provide a scaffold for neocortical integration of new memories over time
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">02 October 2022</span>
                                    </div>
                                </div>
                                <p class="c-article-recommendations-card__authors u-sans-serif">Sam Audrain &amp; Mary Pat McAndrews</p>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'speedy-BootstrappedUCB',
                        timestamp: 1711570738,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                
                
                <div class="main-content">
                    <section data-title="Main"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Main</h2><div class="c-article-section__content" id="Sec1-content"><p>We tend to think of memories as personal belongings, a specific set of episodes unique to each person's mind. Every person perceives the world in his or her own way and describes the past through the lens of individual history, selecting different details or themes as most important. However, memories do not seem to be entirely idiosyncratic; for example, after seeing the same list of pictures, there is considerable inter-subject similarity in which items are remembered<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Isola, P., Xiao, J., Torralba, A. &amp; Oliva, A. What makes an image memorable?. in 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 145152 (2011) &#xA;                  doi:10.1109/CVPR.2011.5995721&#xA;                  &#xA;                ." href="/articles/nn.4450#ref-CR1" id="ref-link-section-d31126455e512">1</a></sup>. The capacity to share memories is essential for our ability to interact with others and form social groups. The macro- and microprocesses by which shared experiences contribute to a community's collective memory have been extensively studied across varied disciplines<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Halbwachs, M. The Collective Memory (Harper &amp; Row Colophon, 1980)." href="/articles/nn.4450#ref-CR2" id="ref-link-section-d31126455e516">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Sperber, D. Explaining Culture: A Naturalistic Approach (Blackwell, 1996)." href="/articles/nn.4450#ref-CR3" id="ref-link-section-d31126455e519">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Coman, A. &amp; Hirst, W. Cognition through a social network: the propagation of induced forgetting and practice effects. J. Exp. Psychol. Gen. 141, 321336 (2012)." href="/articles/nn.4450#ref-CR4" id="ref-link-section-d31126455e522">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Roediger, H.L. III &amp; Abel, M. Collective memory: a new arena of cognitive study. Trends Cogn. Sci. 19, 359361 (2015)." href="/articles/nn.4450#ref-CR5" id="ref-link-section-d31126455e525">5</a></sup>, yet relatively little is known about how shared experiences shape memory representations in the brains of people who are engaged in spontaneous natural recollection. If two people freely describe the same event, how similar (across brains) are the neural codes elicited by that event?</p><p>Human brains have much in common with one another. Similarities exist not only at the anatomical level, but also in terms of functional organization. Given the same stimulusan expanding ring, for exampleregions of the brain that process sensory (visual) stimuli will respond in a highly predictable and similar manner across different individuals. This predictability is not limited to sensory systems: shared activity across people has also been observed in higher-order brain regions (for example, the default mode network<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Raichle, M.E. et al. A default mode of brain function. Proc. Natl. Acad. Sci. USA 98, 676682 (2001)." href="/articles/nn.4450#ref-CR6" id="ref-link-section-d31126455e532">6</a></sup>, or DMN) during the processing of semantically complex real-life stimuli such as movies and stories<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Hasson, U., Nir, Y., Levy, I., Fuhrmann, G. &amp; Malach, R. Intersubject synchronization of cortical activity during natural vision. Science 303, 16341640 (2004)." href="/articles/nn.4450#ref-CR7" id="ref-link-section-d31126455e536">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Jskelinen, I.P. et al. Inter-subject synchronization of prefrontal cortex hemodynamic activity during natural viewing. Open Neuroimag. J. 2, 1419 (2008)." href="/articles/nn.4450#ref-CR8" id="ref-link-section-d31126455e539">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wilson, S.M., Molnar-Szakacs, I. &amp; Iacoboni, M. Beyond superior temporal cortex: intersubject correlations in narrative speech comprehension. Cereb. Cortex 18, 230242 (2008)." href="/articles/nn.4450#ref-CR9" id="ref-link-section-d31126455e542">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Lerner, Y., Honey, C.J., Silbert, L.J. &amp; Hasson, U. Topographic mapping of a hierarchy of temporal receptive windows using a narrated story. J. Neurosci. 31, 29062915 (2011)." href="/articles/nn.4450#ref-CR10" id="ref-link-section-d31126455e545">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Honey, C.J., Thompson, C.R., Lerner, Y. &amp; Hasson, U. Not lost in translation: neural responses shared across languages. J. Neurosci. 32, 1527715283 (2012)." href="/articles/nn.4450#ref-CR11" id="ref-link-section-d31126455e548">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Lahnakoski, J.M. et al. Synchronous brain activity across individuals underlies shared psychological perspectives. Neuroimage 100, 316324 (2014)." href="/articles/nn.4450#ref-CR12" id="ref-link-section-d31126455e551">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Simony, E. et al. Dynamic reconfiguration of the default mode network during narrative comprehension. Nat. Commun. 7, 12141 (2016)." href="/articles/nn.4450#ref-CR13" id="ref-link-section-d31126455e555">13</a></sup>. Notably, shared responses in these high-order areas seem to be associated with narrative content and not with the physical form used to convey it<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Honey, C.J., Thompson, C.R., Lerner, Y. &amp; Hasson, U. Not lost in translation: neural responses shared across languages. J. Neurosci. 32, 1527715283 (2012)." href="/articles/nn.4450#ref-CR11" id="ref-link-section-d31126455e559">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Regev, M., Honey, C.J., Simony, E. &amp; Hasson, U. Selective and invariant neural responses to spoken and written narratives. J. Neurosci. 33, 1597815988 (2013)." href="/articles/nn.4450#ref-CR14" id="ref-link-section-d31126455e562">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Wang, M. &amp; He, B.J. A cross-modal investigation of the neural substrates for ongoing cognition. Front. Psychol. 5, 945 (2014)." href="/articles/nn.4450#ref-CR15" id="ref-link-section-d31126455e565">15</a></sup>. It is unknown, at any level of the cortical hierarchy, to what extent the similarity of human brains during shared perception is recapitulated during shared recollection. This prospect is made especially challenging when recall is spontaneous and spoken, and the selection of details is left up to the rememberer (rather than the experimenter), as is often the case in real life.</p><p>In memory, details may be lost or changed, motives may be reframed, and new elements may be inserted. Although a memory is an imperfect replica of the original experience, the imperfection might serve a purpose. As demonstrated by Jorge Luis Borges in his story Funes the Memorious<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Borges, J.L. Funes the Memorious. La Nacin (Mitre, 1942)." href="/articles/nn.4450#ref-CR16" id="ref-link-section-d31126455e572">16</a></sup>, a memory system that perfectly recorded all aspects of experience, without the ability to compress, abstract and generalize the to-be-remembered information, would be useless for cognition and behavior. In other words, perceptual representations undergo some manner of beneficial modification in the brain before recollection. Therefore, memory researchers can ask two complementary questions: to what extent a memory resembles the original event, and what alterations take place between perceptual experience and later recollection. The first question has been extensively explored in neuroscience: many studies have shown that neural activity during perception of an event is reactivated to some degree during recollection of that event<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Wheeler, M.E., Petersen, S.E. &amp; Buckner, R.L. Memory's echo: vivid remembering reactivates sensory-specific cortex. Proc. Natl. Acad. Sci. USA 97, 1112511129 (2000)." href="/articles/nn.4450#ref-CR17" id="ref-link-section-d31126455e576">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Danker, J.F. &amp; Anderson, J.R. The ghosts of brain states past: remembering reactivates the brain regions engaged during encoding. Psychol. Bull. 136, 87102 (2010)." href="/articles/nn.4450#ref-CR18" id="ref-link-section-d31126455e579">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Polyn, S.M., Natu, V.S., Cohen, J.D. &amp; Norman, K.A. Category-specific cortical activity precedes retrieval during memory search. Science 310, 19631966 (2005)." href="/articles/nn.4450#ref-CR19" id="ref-link-section-d31126455e582">19</a></sup>. However, the laws governing alterations between percept and recollection are not well understood.</p><p>In this paper, we introduce an inter-subject pattern correlation framework that reveals shared memory representations and shared memory alteration processes across the brain. Participants watched a movie and then were asked to verbally recount the full series of events, aloud, in their own words, without any external cues. Despite the unconstrained nature of this behavior, we found that spatial patterns of brain activity observed during movie viewing were reactivated during spoken recall (movierecall similarity). The reactivated patterns were observed in an expanse of high-order brain areas that are implicated in memory and conceptual representation, broadly overlapping with the DMN. We also observed that these spatial activity patterns were similar across people during spoken recall (recallrecall similarity) and highly specific to individual events in the narrative (that is, discriminable), suggesting the existence of a common spatial organization or code for memory representations. Strikingly, in many high-order areas, partially overlapping with the DMN, we found that recallrecall similarity was stronger than movierecall similarity, indicating that neural representations were transformed between perception and recall in a systematic manner across individuals.</p><p>Overall, the results suggest the existence of a common spatial organization for memory representations in the brains of different individuals, concentrated in high-level cortical areas (including the DMN) and robust enough to be observed as people speak freely about the past. Furthermore, neural representations in these brains regions were modified between perceptual experience and memory in a systematic manner across different individuals, suggesting a shared process of memory alteration.</p></div></div></section><section data-title="Results"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Results</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Spontaneous spoken recall</h3><p>Seventeen participants were presented with a 50-min segment of an audio-visual movie (BBC's <i>Sherlock</i>) while undergoing functional MRI (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig1">Fig. 1a</a>). Participants were screened to ensure that they had never previously seen any episode of the series. They were informed before viewing that they would later be asked to describe the movie. Following the movie, participants were instructed to describe aloud what they recalled of the movie in as much detail as they could, with no visual input or experimenter guidance (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig1">Fig. 1b</a>). Recall was performed during brain imaging. Participants were allowed to speak for as long as they wished, on whatever aspects of the movie they chose, while their speech was recorded with an MRI-compatible microphone.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Experiment design and behavior."><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 1: Experiment design and behavior.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.4450/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig1_HTML.jpg?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig1_HTML.jpg" alt="figure 1" loading="lazy" width="685" height="722"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>(<b>a</b>) In run 1, participants viewed a 50-min movie, BBC's <i>Sherlock</i> (episode 1). Images in the figure are blurred for copyright reasons; in the experiment, movies were shown at high resolution. (<b>b</b>) In the immediately following run 2, participants verbally recounted aloud what they recalled from the movie. Instructions to retell what you remember in as much detail as you can were provided before the start of the run. No form of memory cues, time cues, or any auditory/visual input were provided during the recall session. Speech was recorded via microphone. (<b>c</b>) Diagram of scene durations and order for movie viewing and spoken recall in a representative participant. Each rectangle shows, for a given scene, the temporal position (location on <i>y</i> axis) and duration (height) during movie viewing, and the temporal position (location on <i>x</i> axis) and duration (width) during recall. (<b>d</b>) Summary of durations and order for scene viewing and recall in all participants. Each line segment shows, for a given scene, the temporal position and duration during movie viewing and during recall; that is, a line segment in <b>d</b> corresponds to the diagonal of a rectangle in <b>c</b>. Each color indicates a different participant (<i>N</i> = 17). See also <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Tables 1</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">2</a>.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.4450/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>Without any guidance from the experimenters, participants were able to recall the events of the movie with remarkable accuracy and detail, with the average spoken recall session lasting 21.7 min (min: 10.8, max: 43.9, s.d. 8.9) and consisting of 2,657 words (min: 1,136, max: 5,962, s.d. 1,323.6; <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 1</a>). Participants' recollections primarily concerned the plot: characters' actions, speech and motives, and the locations in which these took place. Additionally, many participants described visual features (for example, colors and viewpoints) and emotional elements (for example, characters' feelings). The movie was divided into 50 'scenes', 11180 s (s.d. 41.6 s) long, that followed major shifts in the narrative (for example, location, topic and/or time, as defined by an independent rater; see Experimental procedures in Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a>). The same scenes were identified in the auditory recordings of the recall sessions based on each participant's speech (see Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a>). On average, 34.4 (s.d. 6.0) scenes were successfully recalled. A sample participant's complete recall behavior is depicted in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig1">Figure 1c</a>; see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig1">Figure 1d</a> for a summary of all participants' recall behavior. Scenes were recalled largely in the correct temporal order, with an average of 5.9 (s.d. 4.2) scenes recalled out of order. The temporal compression during recall (that is, the duration of recall relative to the movie; slopes in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig1">Fig. 1d</a>) varied widely, as did the specific words used by different participants (see <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 2</a> for examples).</p><h3 class="c-article__sub-heading" id="Sec4">Neural reinstatement within participants</h3><p>Before examining neural patterns shared across people, we first wished to establish to what extent, and where in the brain, the task elicited similar activity between movie viewing (encoding) and spoken recall within each participantthat is, movierecall neural pattern reinstatement. Studies of pattern reinstatement are typically performed within a given participant, using relatively simple stimuli such as single words, static pictures, or short video clips, often with many training repetitions to ensure successful and vivid recollection of studied items<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Polyn, S.M., Natu, V.S., Cohen, J.D. &amp; Norman, K.A. Category-specific cortical activity precedes retrieval during memory search. Science 310, 19631966 (2005)." href="/articles/nn.4450#ref-CR19" id="ref-link-section-d31126455e705">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Johnson, J.D., McDuff, S.G.R., Rugg, M.D. &amp; Norman, K.A. Recollection, familiarity, and cortical reinstatement: a multivoxel pattern analysis. Neuron 63, 697708 (2009)." href="/articles/nn.4450#ref-CR20" id="ref-link-section-d31126455e708">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Kuhl, B.A., Rissman, J., Chun, M.M. &amp; Wagner, A.D. Fidelity of neural reactivation reveals competition between memories. Proc. Natl. Acad. Sci. USA 108, 59035908 (2011)." href="/articles/nn.4450#ref-CR21" id="ref-link-section-d31126455e711">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Buchsbaum, B.R., Lemire-Rodger, S., Fang, C. &amp; Abdi, H. The neural basis of vivid memory is patterned on perception. J. Cogn. Neurosci. 24, 18671883 (2012)." href="/articles/nn.4450#ref-CR22" id="ref-link-section-d31126455e714">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Wing, E.A., Ritchey, M. &amp; Cabeza, R. Reinstatement of individual past events revealed by the similarity of distributed activation patterns during encoding and retrieval. J. Cogn. Neurosci. 27, 679691 (2015)." href="/articles/nn.4450#ref-CR23" id="ref-link-section-d31126455e717">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Bird, C.M., Keidel, J.L., Ing, L.P., Horner, A.J. &amp; Burgess, N. Consolidation of complex events via reinstatement in posterior cingulate cortex. J. Neurosci. 35, 1442614434 (2015)." href="/articles/nn.4450#ref-CR24" id="ref-link-section-d31126455e720">24</a></sup>. Thus, it was not known whether pattern reinstatement could be measured after a single exposure to such an extended complex stimulus and unconstrained spoken recall behavior.</p><p>For each participant, brain data were transformed to a common space and then data from movie viewing and spoken recall were each divided into the same 50 scenes as defined for the behavioral analysis. This allowed us to match time periods during the movie to time periods during recall. All time points within each scene were averaged, resulting in one pattern of brain activity for each scene. The pattern for each movie scene ('movie pattern') was compared to the pattern during spoken recall of that scene ('recollection pattern'), on a within-participant basis, using Pearson correlation (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2a</a>). The analysis was performed in a searchlight (centered on every voxel in the brain) across the brain volume (5  5  5 cubes, 3-mm voxels). Statistical significance was evaluated using a permutation analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.4450#ref-CR25" id="ref-link-section-d31126455e730">25</a></sup> that compares neural pattern similarity between matching scenes against that of nonmatching scenes, corrected for multiple comparisons by controlling the false discovery rate (FDR) (<i>q</i> = 0.05, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2a</a>). All statistical tests reported throughout the paper are two-tailed unless otherwise noted. This analysis reveals regions containing scene-specific reinstatement patterns, as statistical significance is only reached if matching scenes (same scene in movie and recall) can be differentiated from nonmatching scenes.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Pattern similarity between movie and recall."><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 2: Pattern similarity between movie and recall.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.4450/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig2_HTML.jpg?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig2_HTML.jpg" alt="figure 2" loading="lazy" width="685" height="602"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>(<b>a</b>) Schematic for within-participant movierecall (reinstatement) analysis. BOLD data from the movie and from the recall sessions were divided into scenes and then averaged across time within each scene, resulting in one vector of voxel values for each movie scene and one for each recalled scene. Correlations were computed between matching pairs of movie/recalled scenes within a participant. Statistical significance was determined by shuffling scene labels to generate a null distribution of the participant average. (<b>b</b>) Searchlight map showing where significant reinstatement was observed; FDR-corrected <i>q</i> = 0.05, <i>P</i> = 0.012. Searchlight was a 5  5  5 voxel cube. S1, subject 1; S2, subject 2. (<b>c</b>) Reinstatement values for all 17 participants in independently defined PMC. Red circles show average correlation of matching scenes and error bars show s.e.m. across scenes; black squares show average of the null distribution for that participant. At far right, the red circle shows the true participant average and error bars show s.e.m. across participants; black histogram shows the null distribution of the participant average; white square shows mean of the null distribution. ROI, region of interest. (<b>d</b>) Schematic for between-participants movierecall analysis. Same as <b>a</b>, except that correlations were computed between every matching pair of movie/recall scenes between rather than within participants. (<b>e</b>) Searchlight map showing regions where significant between-participants movierecall similarity was observed; FDR-corrected <i>q</i> = 0.05, <i>P</i> = 0.007. (<b>f</b>) Reinstatement values in PMC for each participant in the between-participants analysis; same notation as <b>c</b>.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.4450/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>The searchlight analysis revealed a large set of brain regions in which the scene-specific spatial patterns observed during movie viewing were reinstated during the spoken recall session (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2b</a>), including posterior medial cortex (PMC), medial prefrontal cortex (mPFC), parahippocampal cortex (PHC), and posterior parietal cortex (PPC). This set of regions corresponds well with the DMN<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Raichle, M.E. et al. A default mode of brain function. Proc. Natl. Acad. Sci. USA 98, 676682 (2001)." href="/articles/nn.4450#ref-CR6" id="ref-link-section-d31126455e804">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Buckner, R.L., Andrews-Hanna, J.R. &amp; Schacter, D.L. The brain's default network: anatomy, function, and relevance to disease. Ann. NY Acad. Sci. 1124, 138 (2008)." href="/articles/nn.4450#ref-CR26" id="ref-link-section-d31126455e807">26</a></sup> and encompasses areas that are known to respond during cued recollection in more traditional tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Rugg, M.D. &amp; Vilberg, K.L. Brain networks underlying episodic memory retrieval. Curr. Opin. Neurobiol. 23, 255260 (2013)." href="/articles/nn.4450#ref-CR27" id="ref-link-section-d31126455e811">27</a></sup> (see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig9">Supplementary Fig. 1a</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 3</a> for overlap). Individual participant correlation values for PMC are shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figure 2c</a>. PMC was selected for illustration purposes because the region is implicated as having a long (on the order of minutes) memory-dependent integration window in studies that use real-life stimuli such as movies and stories<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Lerner, Y., Honey, C.J., Silbert, L.J. &amp; Hasson, U. Topographic mapping of a hierarchy of temporal receptive windows using a narrated story. J. Neurosci. 31, 29062915 (2011)." href="/articles/nn.4450#ref-CR10" id="ref-link-section-d31126455e825">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Honey, C.J. et al. Slow cortical dynamics and the accumulation of information over long timescales. Neuron 76, 423434 (2012)." href="/articles/nn.4450#ref-CR28" id="ref-link-section-d31126455e828">28</a></sup>. A separate analysis showed that movierecall similarity cannot be explained by a time-varying signal evolving independently of the stimulus (see Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a>). These results show that during verbal recall of a 50-min movie, neural patterns associated with individual scenes were reactivated in the absence of any external cues. For analysis of reinstatement at a finer temporal scale, see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig10">Supplementary Figure 2</a>.</p><h3 class="c-article__sub-heading" id="Sec5">Pattern similarity between participants</h3><p>The preceding results established that freely spoken recall of an audiovisual narrative could elicit reinstatement in an array of high-level cortical regions, including those that are typically observed during episodic memory retrieval<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Rugg, M.D. &amp; Vilberg, K.L. Brain networks underlying episodic memory retrieval. Curr. Opin. Neurobiol. 23, 255260 (2013)." href="/articles/nn.4450#ref-CR27" id="ref-link-section-d31126455e846">27</a></sup>. Having mapped movierecall correlations within individual participants, we next examined correlations between participants during both movie and recall.</p><p>Previous studies have shown that viewing the same movie or listening to the same story can induce strong between-participants similarity in the time courses of brain activity in many different regions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Hasson, U., Nir, Y., Levy, I., Fuhrmann, G. &amp; Malach, R. Intersubject synchronization of cortical activity during natural vision. Science 303, 16341640 (2004)." href="/articles/nn.4450#ref-CR7" id="ref-link-section-d31126455e853">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Hasson, U., Malach, R. &amp; Heeger, D.J. Reliability of cortical activity during natural stimulation. Trends Cogn. Sci. 14, 4048 (2010)." href="/articles/nn.4450#ref-CR29" id="ref-link-section-d31126455e856">29</a></sup>. Now examining spatial (rather than temporal) similarities between participants, we found that scene-specific spatial patterns of activity were highly similar across participants during movie viewing in areas spanning the cortical hierarchy, from low-level sensory areas to higher level association areas (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig11">Supplementary Fig. 3</a>). These results also echo prior studies using cross-participant pattern analysis during shared perceptual stimulation in simpler tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Mitchell, T.M. et al. Learning to decode cognitive states from brain images. Mach. Learn. 57, 145175 (2004)." href="/articles/nn.4450#ref-CR30" id="ref-link-section-d31126455e863">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Poldrack, R.A., Halchenko, Y.O. &amp; Hanson, S.J. Decoding the large-scale structure of brain function by classifying mental States across individuals. Psychol. Sci. 20, 13641372 (2009)." href="/articles/nn.4450#ref-CR31" id="ref-link-section-d31126455e866">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Shinkareva, S.V., Malave, V.L., Mason, R.A., Mitchell, T.M. &amp; Just, M.A. Commonality of neural representations of words and pictures. Neuroimage 54, 24182425 (2011)." href="/articles/nn.4450#ref-CR32" id="ref-link-section-d31126455e869">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Kaplan, J.T. &amp; Meyer, K. Multivariate pattern analysis reveals common neural patterns across individuals during touch observation. Neuroimage 60, 204212 (2012)." href="/articles/nn.4450#ref-CR33" id="ref-link-section-d31126455e872">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Rice, G.E., Watson, D.M., Hartley, T. &amp; Andrews, T.J. Low-level image properties of visual objects predict patterns of neural response across category-selective regions of the ventral visual pathway. J. Neurosci. 34, 88378844 (2014)." href="/articles/nn.4450#ref-CR34" id="ref-link-section-d31126455e875">34</a></sup>.</p><p>Next we compared scene-specific movie patterns and scene-specific recollection patterns between participants. The analysis was identical to the reinstatement analysis described above (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2b</a>), but performed between brains rather than within each brain. For each participant, the recollection pattern for each scene was compared to the pattern from the corresponding movie scene, averaged across the remaining participants (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2d</a>). The searchlight revealed extensive movierecall correlations between participants (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2e,f</a>). These results indicate that in many areas that exhibited movierecall reinstatement effects within an individual, neural patterns elicited during spoken recollection of a given movie scene were similar to neural patterns in other individuals watching the same scene (see <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 3</a> for overlap with DMN).</p><h3 class="c-article__sub-heading" id="Sec6">Shared spatial patterns between participants during recall</h3><p>The preceding results showed that scene-specific neural patterns were shared across brains (i) during movie viewing, when all participants viewed the same stimulus (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig11">Supplementary Fig. 3</a>) and (ii) when one participant's recollection pattern was compared to other participants' movie-induced brain patterns (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2d,f</a>). These results suggested that between-participants similarities might also be present during recollection, even though during the recall session no stimulus was presented and each participant described each movie scene in their own words and for different durations. Thus, we next examined between-brain pattern similarity during the spoken recall session.</p><p>As before, brain data within each recall scene were averaged across time in each participant, resulting in one pattern of brain activity for each scene. The recollection pattern from each scene for a given participant was compared (using Pearson correlation) directly to the recollection pattern for the same scene averaged across the remaining participants (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3a</a>), in every searchlight across the brain. The analysis revealed an array of brain regions that had similar scene-specific patterns of activity between participants during spoken recall of shared experiences (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3b</a>), including high-order cortical regions throughout the DMN and category-selective high-level visual areas, but not low-level sensory areas (see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig12">Supplementary Fig. 4</a> for overlap with visual areas; see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig9">Supplementary Fig. 1</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 3</a> for overlap with DMN). Individual participant correlation values for PMC are shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Figure 3c</a>. Pattern similarity between brains could not be explained by acoustic similarities between participants' speech output, and there was no relationship to scene length (see Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Between-participants pattern similarity during spoken recall."><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 3: Between-participants pattern similarity during spoken recall.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.4450/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig3_HTML.jpg?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig3_HTML.jpg" alt="figure 3" loading="lazy" width="685" height="1167"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>(<b>a</b>) Schematic for between-participants recallrecall analysis. BOLD data from the recall sessions were divided into matching scenes and then averaged across time within each voxel, resulting in one vector of voxel values for each recalled scene. Correlations were computed between every matching pair of recalled scenes. Statistical significance was determined by shuffling scene labels to generate a null distribution of the participant average. (<b>b</b>) Searchlight map showing regions where significant recallrecall similarity was observed; FDR correction at <i>q</i> = 0.05, <i>P</i> = 0.012. Searchlight was a 5  5  5 voxel cube. (<b>c</b>) Recallrecall correlation values for all 17 participants in independently defined PMC. Red circles show average correlation of matching scenes and error bars show s.e.m. across scenes; black squares show average of the null distribution for that participant. At far right, the red circle shows the true participant average and error bars show s.e.m. across participants; black histogram shows the null distribution of the participant average; white square shows mean of the null distribution.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.4450/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>These between-brain similarities during recall were observed despite there being no stimulus present while participants spoke, and individuals' behaviorthe compression factor of recollection and the words chosen by each person to describe each eventvarying considerably (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig1">Fig. 1d</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 2</a>). The direct spatial correspondence of event-specific patterns between individuals suggests the existence of a spatial organization to the neural representations underlying recollection that is common across brains.</p><h3 class="c-article__sub-heading" id="Sec7">Classification accuracy</h3><p>How discriminable were the neural patterns for individual scenes during the movie? To address this question we performed a multivoxel classification analysis. Participants were randomly assigned to one of two groups (<i>N</i> = 8 and <i>N</i> = 9), and an average pattern for each scene was calculated within each group for PMC. Classification accuracy across groups was calculated as the proportion of scenes correctly identified out of 50. Classification rank was calculated for each scene. The entire procedure was repeated using 200 random combinations of groups (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Fig. 4a</a>) and averaged. Overall classification accuracy was 36.7%, <i>P</i> &lt; 0.001 (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Fig. 4a</a>); classification rank for individual scenes was significantly above chance for 49 of 50 scenes (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Fig. 4b</a>; FDR corrected <i>q</i> = 0.001; mean rank across scenes 4.8 out of 50).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Classification accuracy."><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 4: Classification accuracy.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.4450/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig4_HTML.jpg?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig4_HTML.jpg" alt="figure 4" loading="lazy" width="685" height="434"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>(<b>a</b>) Classification of movie scenes between brains. Participants were randomly assigned to one of two groups (<i>N</i> = 8 and <i>N</i> = 9), an average was calculated within each group, and data were extracted for the PMC region of interest. Pairwise correlations were calculated between the two group means for all 50 movie scenes. Accuracy was calculated as the proportion of scenes correctly identified out of 50. The entire procedure was repeated using 200 random combinations of two groups sized <i>N</i> = 8 and <i>N</i> = 9 (green markers), and an overall average was calculated (36.7%, black bar; chance level (2.0%), red). (<b>b</b>) Classification rank for individual movie scenes (that is, the rank of the matching scene correlation across groups among all 50 scene correlations). Green markers show the results from each combination of two groups sized <i>N</i> = 8 and <i>N</i> = 9; black bars show the average over all group combinations, 4.8 on average. *<i>q</i> = 0.001, FDR-corrected. Striped bars indicate introductory video clips at the beginning of each functional scan (see Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a>). (<b>c</b>) Classification of recalled scenes between brains. Same analysis as in <b>a</b> except that sufficient data were extant for 41 scenes. Overall classification accuracy was 15.8% (black bar, chance level 2.4%). (<b>d</b>) Classification rank for individual recalled scenes, 9.5 on average (*<i>q</i> = 0.001, FDR-corrected).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.4450/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>How discriminable were the neural patterns for individual scenes during spoken recall? Using the data from recall, we conducted classification analyses identical to the above, with the exception that data were not available for all 50 scenes for every participant (34.4 recalled scenes on average). Thus, group average patterns for each scene were calculated by averaging over extant data; for 41 scenes there were data available for at least one participant in each group, considering the 200 random combinations of participants into groups of <i>N</i> = 8 and <i>N</i> = 9. Overall classification accuracy was 15.8%, <i>P</i> &lt; 0.001 (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Fig. 4c</a>; chance level 2.4%). Classification rank for individual scenes was significantly above chance for 40 of 41 possible scenes (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Fig. 4d</a>; FDR corrected <i>q</i> = 0.001; mean rank across scenes 9.5 out of 41). See <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig13">Supplementary Figure 5</a> for scene-by-scene pattern similarity values.</p><p>To explore factors that may have contributed to discriminability of neural patterns between scenes, we examined (i) how many dimensions of information are encoded in the shared neural patterns that support scene classification and (ii) what kinds of information may be encoded in the shared neural patterns. Our analyses suggest that, at a minimum, 15 dimensions of information are encoded in the patterns shared across individuals during the movie, and approximately 12 generalized between movie and recall in PMC (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig5">Fig. 5</a>). We found that the presence of characters' speech, the presence of written text, the number of locations visited and of persons onscreen, arousal, and valence each contributed to PMC activity patterns during movie viewing (see Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a> section <i></i>Encoding model and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig14">Supplementary Fig. 6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Dimensionality of shared patterns."><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 5: Dimensionality of shared patterns.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.4450/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig5_HTML.jpg?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig5_HTML.jpg" alt="figure 5" loading="lazy" width="685" height="319"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>To quantify the number of distinct dimensions of the spatial patterns that are shared across brains and can contribute to the classification of neural responses, we used the shared response model (SRM). This algorithm operates over a series of data vectors (in this case, multiple participants' brain data) and finds a common representational space of lower dimensionality. Using SRM in the PMC, we asked: when the data are reduced to <i>k</i> dimensions, how does this affect scene-level classification across brains? How many dimensions generalize from movie to recall? (<b>a</b>) Results when using the movie data in the PMC (moviemovie). Classification accuracy improves as the number of dimensions <i>k</i> increases, starting to plateau around 15, but still rising at 50 dimensions (chance level 0.04.) (<b>b</b>) Results when training SRM on the movie data and then classifying recall scenes across participants in the PMC region (recallrecall). Classification accuracy improves as the number of dimensions increases, with maximum accuracy being reached at 12 dimensions. Note that there could be additional shared dimensions, unique to the recall data, that would not be accessible via these analyses.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.4450/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec8">Visualization of BOLD activity in individual scenes</h3><p>To visualize the neural signal shared across subjects, we randomly split the movie-viewing data into two equally sized independent groups (<i>N</i> = 8 each) and averaged blood oxygen leveldependent contrast (BOLD) values across participants within each group, as well as across time points within-scene; the same was done for the recall data, creating one brain image per group per scene (for movie data, see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig6">Figure 6ac</a>; for recall, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig6">Figure 6a,d,e</a>). This averaging procedure reveals the component of the BOLD signal that is shared across brains: that is, if a similar activity pattern can be observed between the two independent groups for an individual scene, it indicates a common neural response across groups. Visual inspection of these images suggests replication across groups for individual scenes and differentiation between scenes, as quantified in the classification analysis above (see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Fig. 4</a>; also <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig13">Supplementary Fig. 5</a> for scene-by-scene correlation values). Our data indicate that cross-participant pattern alignment was strong enough to survive spatial transformation of brain data to a standard anatomical space. While the current results reveal a relatively coarse spatial structure that is shared across people, they do not preclude the existence of finer spatial structure in the neural signal that may be captured when comparisons are made within-participant or using more sensitive methods such as hyperalignment. Further work is needed to understand the factors that influence the balance of idiosyncratic and shared signals between brains. See Spatial resolution of neural signals in Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Scene-level pattern similarity between individuals."><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 6: Scene-level pattern similarity between individuals.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.4450/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig6_HTML.jpg?as=webp"><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig6_HTML.jpg" alt="figure 6" loading="lazy" width="685" height="388"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>See <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig13">Supplementary Figure 5</a> for correlation values for all scenes. To visualize the underlying signal, we randomly split the movie-viewing data into two independent groups of equal size (<i>N</i> = 8 each) and averaged BOLD values across participants within each group. An average was made in the same manner for the recall data using the same two groups of eight. These group mean images were then averaged across time points and within each scene, exactly as in the prior analyses, creating one brain image per group per scene. (<b>a</b>) Sagittal view of these average brains during one representative scene (35) of the movie is shown for each group. (<b>b</b><b>e</b>) Visualization of the signal underlying pattern similarity between individuals, for 14 scenes that were recalled by all 16 of the participants in these groups. Average activity in a posterior medial area (white box in <b>a</b>) on the same slice for the fourteen different scenes for movie group 1 (G1; <b>b</b>), movie group 2 (G2; <b>c</b>), recall group 1 (<b>d</b>) and recall group 2 (<b>e</b>). Searchlight size shown as a red outline. See Spatial resolution of neural signals in Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a>.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.4450/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec9">Alteration of neural patterns from perception to recollection</h3><p>A key question of the experiment was how neural representations change between perception (movie viewing) and memory (recollection). If a given scene's recollection pattern is simply a noisy version of the movie scene pattern, recallrecall correlation between brains cannot be higher than the movierecall correlation. This is illustrated schematically by adding uncorrelated noise patterns to the simulated movie patterns for each brain (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Fig. 7a</a>, left); in this scenario, recollection patterns for a given scene necessarily become more dissimilar to the recollection patterns of other people than they are to the movie pattern. In contrast, if the movie patterns are altered in a systematic manner, it becomes possible for the recallrecall correlations to be higher than movierecall correlations. This is illustrated schematically by adding correlated alteration patterns to the movie patterns within each brain, resulting in recollection patterns becoming more similar to the recollection patterns of other people than to the original movie scene pattern (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Fig. 7a</a>, right). Note that the moviemovie correlation values are irrelevant for this analysis. See also <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig15">Supplementary Figure 7</a> and Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a> section Simulation of movie-to-recall pattern alteration.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7" data-title="Alteration of neural patterns from perception to recollection."><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 7: Alteration of neural patterns from perception to recollection.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.4450/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig7_HTML.jpg?as=webp"><img aria-describedby="Fig7" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig7_HTML.jpg" alt="figure 7" loading="lazy" width="685" height="538"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>(<b>a</b>) Schematic of neural activity patterns during a movie scene being modified into activity patterns at recall. For each brain, the neural patterns while viewing a given movie scene are expressed as a common underlying pattern. Each of these movie patterns is then altered in some manner to produce the recall pattern. Left: if patterns are changed in a unique way in each person's brain, then each person's movie pattern is altered by adding an 'alteration pattern' that is uncorrelated with the alteration patterns of other people. In this scenario, recall patterns necessarily become more dissimilar to the recall patterns of other people than to the movie pattern. Right: alternatively, if a systematic change is occurring across people, each movie pattern is altered by adding an alteration pattern that is correlated with the alteration patterns of other people. Thus, recall patterns for a given scene may become more similar to the recall patterns of other people than to the movie pattern. (<b>b</b>) Searchlight map showing regions where recallrecall similarity was significantly greater than between-participants movierecall similaritythat is, where the map from <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Figure 3b</a> was stronger than the map from <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figure 2e</a>. The analysis revealed regions in which neural representations changed in a systematic way across individuals between perception and recollection. S1, subject 1; S2, subject 2. (<b>c</b>) We tested whether each participant's individual scene recollection patterns could be classified better using the movie data from other participants or the recall data from other participants. A <i>t</i>-test of classification rank was performed between these two sets of values at each searchlight shown in <b>b</b>. Classification rank was higher when using the recall data as opposed to the movie data in 99% of such searchlights. Histogram of <i>t</i>-values is plotted.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.4450/figures/7" data-track-dest="link:Figure7 Full size image" aria-label="Full size image figure 7" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>To search for systematic alterations of neural representations between movie and recall, we looked for brain regions in which, for individual scenes, recollection activity patterns were more similar to recollection patterns in other individuals than they were to movie patterns. To ensure a balanced contrast, we compared the between-participants recallrecall values to the between-participants movierecall values (rather than to within-participant movierecall values). Statistical significance of the difference was calculated using a resampling test (see Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a>). The analysis revealed that recallrecall pattern similarity was stronger than movierecall pattern similarity in a number of high-order areas, including PHC and other high-level visual areas, right superior temporal pole, PMC, right mPFC, and PPC (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Fig. 7b</a>). See <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 3</a> for overlap with DMN. If recall patterns were merely a noisy version of the movie events, we would expect the opposite result: lower recallrecall similarity than movierecall similarity. No regions in the brain showed this opposite pattern. Thus, the analysis revealed alterations of neural representations between movie viewing and recall that were systematic (shared) across subjects.</p><h3 class="c-article__sub-heading" id="Sec10">How did global differences between movie and recall impact alteration?</h3><p>There were a number of global factors that differed between movie and recall: for example, more visual motion was present during movie than recall; recall involved speech and motor output while movie viewing did not. A possible concern was that the greater similarity for recallrecall than for movierecall might arise simply from such global differences. To test this concern, we examined the discriminability of individual scenes within the regions that exhibited robust pattern alterationthat is, the searchlights shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Figure 7b</a>. We asked whether each participant's individual scene recollection patterns could be classified better using movie data from other participants or recall data from other participants. Classification rank was found to be higher when using the recall data as opposed to the movie data in 99% of the searchlights (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Fig. 7c</a> for the distribution of <i>t</i>-values resulting from a test between recallrecall and movierecall). Thus, global differences between movie and recall could not explain our observation of systematic pattern alterations. (See <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig16">Supplementary Fig. 8</a> for the scene-by-scene difference in pattern similarity between recallrecall and movierecall.)</p><p>Notably, in PMC, the degree of alteration predicted the memorability of individual scenes (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig17">Supplementary Fig. 9</a>). See <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig18">Supplementary Figure 10</a> for analysis of subsequent memory effects in hippocampus and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig19">Supplementary Figure 11</a> for analysis of hippocampal sensitivity to the gap between part 1 and part 2 of the movie.</p><h3 class="c-article__sub-heading" id="Sec11">Reinstatement within versus between participants</h3><p>While our between-participants analyses explored the shared component of memory representations, neural patterns may also contain information reflecting more fine-grained individual differences in memory representations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Charest, I., Kievit, R.A., Schmitz, T.W., Deca, D. &amp; Kriegeskorte, N. Unique semantic space in the brain of each beholder predicts perceived similarity. Proc. Natl. Acad. Sci. USA 111, 1456514570 (2014)." href="/articles/nn.4450#ref-CR35" id="ref-link-section-d31126455e1342">35</a></sup>. If so, one would expect movierecall similarity to be stronger within a participant than between participants. A simple comparison of within-participant movierecall pattern similarity (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2b,c</a>) to between-participants movierecall pattern similarity (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2e,f</a>) does not suffice, because anatomical registration is better within a participant than between participants. Thus, we performed a second-order similarity analysis: correlation of representational dissimilarity matrices (RDMs) within and between participants<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.4450#ref-CR25" id="ref-link-section-d31126455e1352">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Charest, I., Kievit, R.A., Schmitz, T.W., Deca, D. &amp; Kriegeskorte, N. Unique semantic space in the brain of each beholder predicts perceived similarity. Proc. Natl. Acad. Sci. USA 111, 1456514570 (2014)." href="/articles/nn.4450#ref-CR35" id="ref-link-section-d31126455e1355">35</a></sup>. Each RDM was composed of the pairwise pattern correlations for individual scenes in the movie ('movie-RDM') and during recall ('recall-RDM') calculated within a brain. These RDMs could then be compared within and between participants.</p><p>We calculated correlations between movie-RDM and recall-RDM, on a within-participant basis, in a searchlight analysis across the brain volume (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig8">Fig. 8a</a>). The same analysis was performed between all pairs of participants (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig8">Fig. 8b</a>). Of critical interest was the difference between the within-participant comparison and the between-participants comparison. Statistical significance of the difference was evaluated using a permutation analysis that randomly swapped condition labels for within-participant and between-participants RDM correlation values, and was FDR-corrected across all voxels in the brain (<i>q</i> = 0.05). This analysis revealed a single cluster located in the right temporoparietal junction (two searchlight cubes centered on MNI coordinates [48, 48, 9] and [48, 48, 6]; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig8">Fig. 8c</a>) for which within-participant movie-RDM to recall-RDM correlation was significantly greater than between-participants movie-RDM to recall-RDM correlation, that is, in which individual-unique aspects of neural patterns contributed to reinstatement strength above and beyond the shared representation. (See also Comparison between semantic similarity and neural pattern similarity in Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a>.)</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8" data-title="Reinstatement in individual participants versus between participants."><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 8: Reinstatement in individual participants versus between participants.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/nn.4450/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig8_HTML.jpg?as=webp"><img aria-describedby="Fig8" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig8_HTML.jpg" alt="figure 8" loading="lazy" width="685" height="443"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>(<b>a</b>) Searchlight analysis showing similarity of RDMs on a within-participant basis across the brain. Each RDM was composed of the pairwise correlations of patterns for individual scenes in the movie ('movie-RDM') and separately during recall ('recall-RDM'). Each participant's movie-RDM was then compared to his or her own recall-RDM (that is, compared within-participant) using Pearson correlation. The average searchlight map across 17 participants is displayed. S1, subject 1; S2, subject 2. (<b>b</b>) Searchlight analysis showing movie-RDM versus recall-RDM correlations between participants. The average searchlight map across 272 pairwise combinations of participants is displayed. (<b>c</b>) The difference was computed between the within-participant and between-participants maps. Statistical significance of the difference was evaluated using a permutation analysis and FDR-corrected at a threshold of <i>q</i> = 0.05. A cluster of two voxels located in the temporo-parietal junction survived correction (map shown at <i>q</i> = 0.10 for visualization purposes; five-voxel cluster).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/nn.4450/figures/8" data-track-dest="link:Figure8 Full size image" aria-label="Full size image figure 8" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Discussion</h2><div class="c-article-section__content" id="Sec12-content"><p>In this study, we found that neural patterns recorded during movie viewing were reactivated, in a scene specific manner, during free and unguided verbal recollection. Furthermore, the spatial organization of the recall neural patterns was preserved across people. This shared brain activity was observed during free spoken recall as participants reported the contents of their memories (a movie they had watched) in their own words, in the absence of any sensory cues or experimental intervention. Reactivated and shared patterns were found in a large set of high-order multimodal cortical areas, including DMN areas, high-level visual areas in ventral temporal cortex, and intraparietal sulcus. In a subset of regions, brain activity patterns were modified between perception and recall in a consistent manner across individuals. The magnitude of this modification predicted the memorability of individual movie scenes, suggesting that alteration of brain patterns between percept and recollection may have been beneficial for behavior (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig17">Supplementary Fig. 9</a>). Overall, these findings show that the neural activity underlying memories for real-world events has a common spatial organization across different brains and that neural activity is altered from initial perception to recollection in a systematic manner, even as people speak freely in their own words about past events.</p><p>Our findings suggest that memory representations for real-world events, like sensory representations, are spatially organized in a functional architecture that is shared across brains. The well-studied spatial organization of sensory responses takes the form of topographic maps in the brain, for example, retinotopic or tonotopic maps of visual or auditory features, which are preserved across individuals<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Wandell, B.A., Dumoulin, S.O. &amp; Brewer, A.A. Visual field maps in human cortex. Neuron 56, 366383 (2007)." href="/articles/nn.4450#ref-CR36" id="ref-link-section-d31126455e1428">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Formisano, E. et al. Mirror-symmetric tonotopic maps in human primary auditory cortex. Neuron 40, 859869 (2003)." href="/articles/nn.4450#ref-CR37" id="ref-link-section-d31126455e1431">37</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Benson, N.C. et al. The retinotopic organization of striate cortex is well predicted by surface topology. Curr. Biol. 22, 20812085 (2012)." href="/articles/nn.4450#ref-CR38" id="ref-link-section-d31126455e1434">38</a></sup>. In contrast, little is known about the consistency of memory-related cortical patterns across individuals. Memory-relevant areas such as hippocampus and entorhinal cortex represent an animal's spatial location and trajectory<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Moser, E.I., Kropff, E. &amp; Moser, M.-B. Place cells, grid cells, and the brain's spatial representation system. Annu. Rev. Neurosci. 31, 6989 (2008)." href="/articles/nn.4450#ref-CR39" id="ref-link-section-d31126455e1438">39</a></sup>, but these representations are remapped for new spatial layouts and do not seem to be consistent across brains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="O'Keefe, J. &amp; Conway, D.H. Hippocampal place units in the freely moving rat: why they fire where they fire. Exp. Brain Res. 31, 573590 (1978)." href="/articles/nn.4450#ref-CR40" id="ref-link-section-d31126455e1442">40</a></sup>. Previous studies have compared activity between brains using the powerful representational similarity analysis approach<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.4450#ref-CR25" id="ref-link-section-d31126455e1446">25</a></sup>, in which the overall structure of inter-relationships between stimulus-evoked neural responses is compared between participants<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Charest, I., Kievit, R.A., Schmitz, T.W., Deca, D. &amp; Kriegeskorte, N. Unique semantic space in the brain of each beholder predicts perceived similarity. Proc. Natl. Acad. Sci. USA 111, 1456514570 (2014)." href="/articles/nn.4450#ref-CR35" id="ref-link-section-d31126455e1450">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Kriegeskorte, N. et al. Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron 60, 11261141 (2008)." href="/articles/nn.4450#ref-CR41" id="ref-link-section-d31126455e1453">41</a></sup>. While this type of representational similarity analysis is a second-order comparison (a correlation of correlations), our approach uses direct comparison of spatial activity patterns between brains; this establishes that the spatial structures of neural patterns underlying recollected events (as opposed to the stimulus inter-relationship structure) are common across people. The shared responses are local in that similarity can be detected in a small patch of brain (a 15  15  15 mm searchlight; see Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a> section Spatial resolution of neural signals and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig6">Fig. 6</a>), but also widespread, encompassing DMN areas, high-level visual areas in ventral temporal cortex (but not low-level visual areas; <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig12">Supplementary Fig. 4</a> and Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a> section Visual imagery), and intraparietal sulcus. Future work will explore the mapping between specific mnemonic content and the structure of neural responses in these areas (initial analyses, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig14">Supplementary Fig. 6</a>).</p><p>The brain areas in which we observed shared representations during recall overlap extensively with the DMN<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Raichle, M.E. et al. A default mode of brain function. Proc. Natl. Acad. Sci. USA 98, 676682 (2001)." href="/articles/nn.4450#ref-CR6" id="ref-link-section-d31126455e1476">6</a></sup>. The DMN has been implicated in a broad range of complex cognitive functions, including scene and situation model construction, episodic memory and internally focused thought<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Buckner, R.L., Andrews-Hanna, J.R. &amp; Schacter, D.L. The brain's default network: anatomy, function, and relevance to disease. Ann. NY Acad. Sci. 1124, 138 (2008)." href="/articles/nn.4450#ref-CR26" id="ref-link-section-d31126455e1480">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Rugg, M.D. &amp; Vilberg, K.L. Brain networks underlying episodic memory retrieval. Curr. Opin. Neurobiol. 23, 255260 (2013)." href="/articles/nn.4450#ref-CR27" id="ref-link-section-d31126455e1483">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Hassabis, D. &amp; Maguire, E.A. Deconstructing episodic memory with construction. Trends Cogn. Sci. 11, 299306 (2007)." href="/articles/nn.4450#ref-CR42" id="ref-link-section-d31126455e1486">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Ranganath, C. &amp; Ritchey, M. Two cortical systems for memory-guided behaviour. Nat. Rev. Neurosci. 13, 713726 (2012)." href="/articles/nn.4450#ref-CR43" id="ref-link-section-d31126455e1489">43</a></sup>. Multiple studies have shown that, during processing of real-life stimuli such as movies and stories, DMN activity time courses are synchronized across individuals and locked to high-level semantic information in the stimulus, but not to low-level sensory features or mid-level linguistic structure. For example, these regions evince the same narrative-specific dynamics whether a given story is presented in spoken or written form<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Regev, M., Honey, C.J., Simony, E. &amp; Hasson, U. Selective and invariant neural responses to spoken and written narratives. J. Neurosci. 33, 1597815988 (2013)." href="/articles/nn.4450#ref-CR14" id="ref-link-section-d31126455e1493">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Wang, M. &amp; He, B.J. A cross-modal investigation of the neural substrates for ongoing cognition. Front. Psychol. 5, 945 (2014)." href="/articles/nn.4450#ref-CR15" id="ref-link-section-d31126455e1496">15</a></sup> and whether it is presented in English or Russian<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Honey, C.J., Thompson, C.R., Lerner, Y. &amp; Hasson, U. Not lost in translation: neural responses shared across languages. J. Neurosci. 32, 1527715283 (2012)." href="/articles/nn.4450#ref-CR11" id="ref-link-section-d31126455e1500">11</a></sup>. Dynamics are modulated according to the perspective of the perceiver<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Lahnakoski, J.M. et al. Synchronous brain activity across individuals underlies shared psychological perspectives. Neuroimage 100, 316324 (2014)." href="/articles/nn.4450#ref-CR12" id="ref-link-section-d31126455e1504">12</a></sup>, but when comprehension of the narrative is disrupted (while keeping low-level sensory features unchanged), neural activity becomes incoherent across participants<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Lerner, Y., Honey, C.J., Silbert, L.J. &amp; Hasson, U. Topographic mapping of a hierarchy of temporal receptive windows using a narrated story. J. Neurosci. 31, 29062915 (2011)." href="/articles/nn.4450#ref-CR10" id="ref-link-section-d31126455e1509">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Ames, D.L., Honey, C.J., Chow, M.A., Todorov, A. &amp; Hasson, U. Contextual alignment of cognitive and neural dynamics. J. Cogn. Neurosci. 27, 655664 (2015)." href="/articles/nn.4450#ref-CR44" id="ref-link-section-d31126455e1512">44</a></sup>. Together, these results suggest that DMN activity tracks high-level information structure (for example, narrative or situational elements<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Ranganath, C. &amp; Ritchey, M. Two cortical systems for memory-guided behaviour. Nat. Rev. Neurosci. 13, 713726 (2012)." href="/articles/nn.4450#ref-CR43" id="ref-link-section-d31126455e1516">43</a></sup>) in the input. The current study extends previous findings by demonstrating that synchronized neural responses among individuals during encoding later give rise to shared neural responses during recollection, reinstated at will from memory without the need for any guiding stimulus, even when each person describes the past in his or her own words. Note that, while there was considerable overlap between the DMN and the recallrecall map in posterior medial areas, there was less overlap in frontal and lateral parietal areas (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig9">Supplementary Fig. 1</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 3</a>). Our results show that some DMN regions evince shared event-specific activity patterns during recollection; more work is needed to probe functional differentiation within these areas.</p><p>A memory is not a perfect replica of the original experience; perceptual representations undergo modification in the brain before recollection that may increase the usefulness of the memory, for example, by emphasizing certain aspects of the percept and discarding others. What laws govern how neural representations change between perception and memory, and how might these modifications be beneficial for future behavior? We examined whether the alteration of neural patterns from percept to memory was idiosyncratic or systematic across people, reasoning that if percept-based activity changed into memory in a structured way, then patterns at recall should become more similar to each other (across individuals) than they are to the original movie patterns. Such systematic transformations were observed in several brain regions, including PHC and other high-level visual areas, superior temporal pole, PMC, mPFC and PPC (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig6">Fig. 6b</a>). Not only the similarity but also the discriminability of events was increased during recall, indicating that the effect was not due to a common factor (for example, speech) across recalled events (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig6">Fig. 6c</a>). Notably, scenes that exhibited more neural alteration in PMC were also more likely to be recalled (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig17">Supplementary Fig. 9a</a>; this should be interpreted cautiously, as the number of data points, 45, is relatively small). A possible interpretation of these findings is that participants shared familiar notions of how certain events are structured (for example, what elements are typically present in a car chase scene) and these existing schemas guided recall. Such forms of shared knowledge might improve memory by allowing participants to think of schema-consistent scene elements, essentially providing self-generated memory cues<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Alba, J.W. &amp; Hasher, L. Is memory schematic? Psychol. Bull. 93, 203231 (1983)." href="/articles/nn.4450#ref-CR45" id="ref-link-section-d31126455e1538">45</a></sup>.</p><p>In the current study, we simplified the continuous movie and recall data by dividing them into scenes, identified by major shifts in the narrative (for example, location, topic, time). This 'event boundary' segmentation was necessary for matching time periods in the movie to periods during recall. Our approach follows from a known property of perception: people tend to segment continuous experience into discrete events in similar ways<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Kurby, C.A. &amp; Zacks, J.M. Segmentation in the perception and memory of events. Trends Cogn. Sci. 12, 7279 (2008)." href="/articles/nn.4450#ref-CR46" id="ref-link-section-d31126455e1546">46</a></sup>. While there are many reasonable ways to split the movie, matching movie scenes to recall audio becomes difficult when the number of boundaries increases, as some descriptions are more synoptic (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 2</a>). To overcome the temporal misalignment between movie and recall, and between recalls, each event was averaged across time. These averaged event patterns nonetheless retained complex information: movie scene patterns contained at least 15 dimensions that contributed to classification accuracy (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig5">Fig. 5a</a>), and approximately 12 dimensions generalized from movie to recall (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig5">Fig. 5b</a>). Furthermore, in an exploration of what semantic content might underlie these dimensions (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig14">Supplementary Fig. 6</a>), we found that the presence of speech, presence of written text, number of locations visited and persons onscreen, arousal, and valence each contributed to PMC movie activity patterns. Thus, while some information was necessarily lost when we averaged within a scene, substantial multidimensional structure was preserved. (For consideration of the role of visual imagery, see Online <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/nn.4450#Sec13">Methods</a> section Visual imagery and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig12">Supplementary Fig. 4</a>.) Recollection is obviously more complicated than a simple compression of original perception; a future direction is to formalize the hypothesis that narrative recall is 'chunked' into scenes and use this heuristic to enable data-driven discovery of optimal scene boundaries during both movie and recall, without relying on human manual definition<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Baldassano, C. et al. Discovering event structure in continuous narrative perception and memory. Preprint at bioRxiv &#xA;                  http://dx.doi.org/10.1101/081018&#xA;                  &#xA;                 (2016)." href="/articles/nn.4450#ref-CR47" id="ref-link-section-d31126455e1569">47</a></sup>.</p><p>Together, these results show that a common spatial organization for memory representations exists in high-level cortical areas (for example, the DMN), where information is largely abstracted beyond sensory constraints; and that perceptual experience is altered before recall in a systematic manner across people, a process that may benefit memory. These observations were made as individuals engaged in natural and unguided spoken recollection, testifying to the robustness and ecological validity of the phenomena. The ability to use language to reactivate, at will, the sequence of neural responses associated with the movie events, can be thought of as a form of conscious replay. Future studies may investigate whether and how such volitional cortical replay is related to the compressed and rapid forward and reverse sequential replay observed in the hippocampus during sleep and spatial navigation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Buzski, G. &amp; Moser, E.I. Memory, navigation and theta rhythm in the hippocampal-entorhinal system. Nat. Neurosci. 16, 130138 (2013)." href="/articles/nn.4450#ref-CR48" id="ref-link-section-d31126455e1576">48</a></sup>. Future work may also explore whether these shared representations facilitate the spoken communication of memories to others<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Hasson, U., Ghazanfar, A.A., Galantucci, B., Garrod, S. &amp; Keysers, C. Brain-to-brain coupling: a mechanism for creating and sharing a social world. Trends Cogn. Sci. 16, 114121 (2012)." href="/articles/nn.4450#ref-CR49" id="ref-link-section-d31126455e1580">49</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Zadbood, A., Chen, J., Leong, Y.C., Norman, K.A. &amp; Hasson, U. How we transmit memories to other brains: constructing shared neural representations via communication. Preprint at bioRxiv &#xA;                  http://dx.doi.org/10.1101/081208&#xA;                  &#xA;                 (2016)." href="/articles/nn.4450#ref-CR50" id="ref-link-section-d31126455e1583">50</a></sup> and how they might contribute to a community's collective memory<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Halbwachs, M. The Collective Memory (Harper &amp; Row Colophon, 1980)." href="/articles/nn.4450#ref-CR2" id="ref-link-section-d31126455e1587">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Sperber, D. Explaining Culture: A Naturalistic Approach (Blackwell, 1996)." href="/articles/nn.4450#ref-CR3" id="ref-link-section-d31126455e1590">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Coman, A. &amp; Hirst, W. Cognition through a social network: the propagation of induced forgetting and practice effects. J. Exp. Psychol. Gen. 141, 321336 (2012)." href="/articles/nn.4450#ref-CR4" id="ref-link-section-d31126455e1593">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Roediger, H.L. III &amp; Abel, M. Collective memory: a new arena of cognitive study. Trends Cogn. Sci. 19, 359361 (2015)." href="/articles/nn.4450#ref-CR5" id="ref-link-section-d31126455e1596">5</a></sup>.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Methods</h2><div class="c-article-section__content" id="Sec13-content"><h3 class="c-article__sub-heading" id="Sec14">Participants.</h3><p>Twenty-two participants were recruited from the Princeton community (12 male, 10 female, ages 1826, mean age 20.8). All participants were right-handed native English speakers, reported normal or corrected-to-normal vision, and had not watched any episodes of <i>Sherlock</i><sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="McGuigan, P. A Study in Pink. Sherlock (BBC, 2010)." href="/articles/nn.4450#ref-CR51" id="ref-link-section-d31126455e1614">51</a></sup> before the experiment. All participants provided informed written consent before the start of the study in accordance with experimental procedures approved by the Princeton University Institutional Review Board. The study was approximately 2 h long and participants received $20 per hour as compensation for their time. Data from 5 of the 22 participants were discarded due to excessive head motion (greater than 1 voxel; 2 participants), because recall was shorter than 10 min (2 participants), or for falling asleep during the movie (1 participant). For one participant (#5 in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figs. 2c,f,</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">3c</a>) the movie scan ended 75 s early (that is, this participant was missing data for part of scene 49 and all of scene 50). No statistical methods were used to predetermine sample sizes, but our sample sizes are similar to those reported in previous publications<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wilson, S.M., Molnar-Szakacs, I. &amp; Iacoboni, M. Beyond superior temporal cortex: intersubject correlations in narrative speech comprehension. Cereb. Cortex 18, 230242 (2008)." href="/articles/nn.4450#ref-CR9" id="ref-link-section-d31126455e1624">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Honey, C.J., Thompson, C.R., Lerner, Y. &amp; Hasson, U. Not lost in translation: neural responses shared across languages. J. Neurosci. 32, 1527715283 (2012)." href="/articles/nn.4450#ref-CR11" id="ref-link-section-d31126455e1627">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Regev, M., Honey, C.J., Simony, E. &amp; Hasson, U. Selective and invariant neural responses to spoken and written narratives. J. Neurosci. 33, 1597815988 (2013)." href="/articles/nn.4450#ref-CR14" id="ref-link-section-d31126455e1630">14</a></sup>.</p><h3 class="c-article__sub-heading" id="Sec15">Stimuli.</h3><p>The audio-visual movie stimulus was a 48-min segment of the BBC television series <i>Sherlock</i><sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="McGuigan, P. A Study in Pink. Sherlock (BBC, 2010)." href="/articles/nn.4450#ref-CR51" id="ref-link-section-d31126455e1644">51</a></sup>, taken from the beginning of the first episode of the series (a full episode is 90 min). The stimulus was further divided into two segments (23 and 25 min long); this was done to reduce the length of each individual run, as longer runs might be more prone to technical problems (for example, scanner overheating).</p><p>At the beginning of each of the two movie segments, we prepended a 30-s audiovisual cartoon (<i>Let's All Go to the Lobby</i>) that was unrelated to the <i>Sherlock</i> movie. In studies using inter-subject temporal correlation, it is common to include a short auditory or audiovisual introductory clip before the main experimental stimulus because the onset of stimulus may elicit a global arousal response. Such a response could add noise to a temporal correlation across subjects, and thus experimenters often truncate the neural signal elicited during the introductory clip. In the current experiment, we used spatial instead of temporal correlations. Because spatial correlation in one scene does not necessarily affect the correlation in another scene, we decided not to remove the introductory clips. The introductory clips at the beginning of each movie run are highly discriminable from the 48 scenes of the movie (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Fig. 4b</a>, bars C1 and C2). Furthermore, one participant described the introductory clip during their spoken recall, and this event was included for the within-brain movierecall analysis for that participant. In the absence of any obvious reason to exclude these data, we decided to retain the cartoon segments.</p><h3 class="c-article__sub-heading" id="Sec16">Experimental procedures.</h3><p>Participants were told that they would be watching the British television crime drama series <i>Sherlock</i><sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="McGuigan, P. A Study in Pink. Sherlock (BBC, 2010)." href="/articles/nn.4450#ref-CR51" id="ref-link-section-d31126455e1670">51</a></sup> in the fMRI scanner. They were given minimal instructions: to attend to the audiovisual movie, for example, watch it as you would normally watch a television show that you are interested in, and told that afterward they would be asked to verbally describe what they had watched. Participants then viewed the 50-min movie in the scanner. The scanning (and stimulus) was divided into two consecutive runs of approximately equal duration.</p><p>Data collection and analysis were not performed blind to the conditions of the experiments. All participants watched the same movie before verbally recalling the plot of the movie. To preserve the naturalistic element of the experimental design, we presented the movie in its original order. The unit of analysis was the scenes that participants freely recalled, which was not within experimental control.</p><p>The movie was projected using an LCD projector onto a rear-projection screen located in the magnet bore and viewed with an angled mirror. The Psychophysics Toolbox (<a href="http://psychtoolbox.org/">http://psychtoolbox.org/</a>) for MATLAB was used to display the movie and to synchronize stimulus onset with MRI data acquisition. Audio was delivered via in-ear headphones. Eye tracking was conducted using the iView X MRI-LR system (Sensomotoric Instruments). No behavioral responses were required from the participants during scanning, but the experimenter monitored participants' alertness via the eye tracking camera. Any participants who appeared to fall asleep, as assessed by video monitoring, were excluded from further analyses.</p><p>At the start of the spoken recall session, which took place immediately after the end of the movie, participants were instructed to describe what they recalled of the movie in as much detail as they could, to try to recount events in the original order they were viewed in, and to speak for at least 10 min if possible but that longer was better. They were told that completeness and detail were more important than temporal order, and that if at any point they realized they had missed something, to return to it. Participants were then allowed to speak for as long as they wished, and verbally indicated when they were finished (for example, I'm done). During this session they were presented with a static black screen with a central white dot (but were not asked to, and did not, fixate); there was no interaction between the participant and the experimenter until the scan ended. Functional brain images and audio were recorded during the session. Participants' speech was recorded using a customized MR-compatible recording system (FOMRI II; Optoacoustics Ltd.).</p><h3 class="c-article__sub-heading" id="Sec17">Behavioral analysis.</h3><p><i>Scene timestamps.</i> Timestamps were identified that separated the audiovisual movie into 48 scenes, following major shifts in the narrative (for example, location, topic, and/or time). These timestamps were selected by an independent coder with no knowledge of the experimental design or results. The scenes ranged from 11 to 180 s (s.d. 41.6) long. Each scene was given a descriptive label (for example, press conference). Together with the two identical cartoon segments, this resulted in 50 total scenes.</p><p><i>Transcripts.</i> Transcripts were written of the audio recording of each participant's spoken recall. Timestamps were then identified that separated each audio recording into the same 50 scenes that had been previously selected for the audiovisual stimulus. A scene was counted as recalled if the participant described any part of the scene. Scenes were counted as out of order if they were initially skipped and then described later. See <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Tables 1</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">2</a>.</p><h3 class="c-article__sub-heading" id="Sec18">fMRI acquisition.</h3><p>MRI data were collected on a 3-T full-body scanner (Siemens Skyra) with a 20-channel head coil. Functional images were acquired using a T2*-weighted echo-planar imaging (EPI) pulse sequence (TR 1,500 ms, TE 28 ms, flip angle 64, whole-brain coverage 27 slices of 4 mm thickness, in-plane resolution 3  3 mm<sup>2</sup>, FOV 192  192 mm<sup>2</sup>), with ascending interleaved acquisition. Anatomical images were acquired using a T1-weighted MPRAGE pulse sequence (0.89 mm<sup>3</sup> resolution).</p><h3 class="c-article__sub-heading" id="Sec19">fMRI preprocessing.</h3><p>Preprocessing was performed in FSL (<a href="http://fsl.fmrib.ox.ac.uk/fsl">http://fsl.fmrib.ox.ac.uk/fsl</a>), including slice time correction, motion correction, linear detrending, high-pass filtering (140 s cutoff), and coregistration and affine transformation of the functional volumes to a template brain (Montreal Neurological Institute (MNI) standard). Functional images were resampled to 3 mm isotropic voxels for all analyses. All calculations were performed in volume space. Projections onto a cortical surface for visualization were performed, as a final step, with NeuroElf (<a href="http://neuroelf.net/">http://neuroelf.net/</a>).</p><p>Motion was minimized by instructing participants to remain very still while speaking and by stabilizing participants' heads with foam padding. Artifacts generated by speech may introduce some noise, but they cannot induce positive results, as our analyses depend on spatial correlations between sessions (movierecall or recallrecall). Similar procedures regarding speech production during fMRI are described in previous publications from our group<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Stephens, G.J., Silbert, L.J. &amp; Hasson, U. Speakerlistener neural coupling underlies successful communication. Proc. Natl. Acad. Sci. USA 107, 1442514430 (2010)." href="/articles/nn.4450#ref-CR52" id="ref-link-section-d31126455e1751">52</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Silbert, L.J., Honey, C.J., Simony, E., Poeppel, D. &amp; Hasson, U. Coupled neural systems underlie the production and comprehension of naturalistic narrative speech. Proc. Natl. Acad. Sci. USA 111, E4687E4696 (2014)." href="/articles/nn.4450#ref-CR53" id="ref-link-section-d31126455e1754">53</a></sup>.</p><h3 class="c-article__sub-heading" id="Sec20">Region of interest (ROI) definition.</h3><p>An anatomical hippocampus ROI was defined on the basis of the probabilistic Harvard-Oxford Subcortical Structural Atlas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Desikan, R.S. et al. An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest. Neuroimage 31, 968980 (2006)." href="/articles/nn.4450#ref-CR54" id="ref-link-section-d31126455e1766">54</a></sup>, and an ROI for posterior medial cortex (PMC) was taken from an atlas defined from resting-state connectivity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Shirer, W.R., Ryali, S., Rykhlevskaia, E., Menon, V. &amp; Greicius, M.D. Decoding subject-driven cognitive states with whole-brain connectivity patterns. Cereb. Cortex 22, 158165 (2012)." href="/articles/nn.4450#ref-CR55" id="ref-link-section-d31126455e1770">55</a></sup>: specifically, the posterior medial cluster in the dorsal default mode network set (<a href="http://findlab.stanford.edu/functional_ROIs.html">http://findlab.stanford.edu/functional_ROIs.html</a>), see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig11">Supplementary Figure 3d</a>. A DMN ROI was created by calculating the correlation between the PMC ROI and every other voxel in the brain (that is, functional connectivity) during the movie for each subject, averaging the resulting maps across all subjects, and thresholding at <i>R</i> = 0.4 (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig9">Supplementary Fig. 1</a>). While the DMN is typically defined using resting state data, it has been previously demonstrated that this network can be mapped either during rest or during continuous narrative with largely the same results<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Simony, E. et al. Dynamic reconfiguration of the default mode network during narrative comprehension. Nat. Commun. 7, 12141 (2016)." href="/articles/nn.4450#ref-CR13" id="ref-link-section-d31126455e1791">13</a></sup>.</p><h3 class="c-article__sub-heading" id="Sec21">Pattern similarity analyses.</h3><p>The brain data were transformed to standard MNI space. For each participant, data from movie viewing and spoken recall were each divided into the same 50 scenes as defined for the behavioral analysis. BOLD data were averaged across time points within-scene, resulting in one pattern of brain activity for each scene: one 'movie pattern' elicited during each movie scene and one 'recollection pattern' elicited during spoken recall of each scene. Recollection patterns were only available for scenes that were successfully recalled; each participant possessed a different subset of recalled scenes. Each scene-level pattern could then be compared to any other scene-level pattern in any region (for example, an ROI or a searchlight cube). For cases in which a given scene was described more than once during recall, data were used from the first description only. All such comparisons were made using Pearson correlation.</p><p>For searchlight analyses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Kriegeskorte, N., Goebel, R. &amp; Bandettini, P. Information-based functional brain mapping. Proc. Natl. Acad. Sci. USA 103, 38633868 (2006)." href="/articles/nn.4450#ref-CR56" id="ref-link-section-d31126455e1806">56</a></sup>, pattern similarity was calculated in 5  5  5 voxel cubes (15  15  15 mm cubes) centered on every voxel in the brain. Statistical significance was determined by shuffling scene labels to generate a null distribution of the average across participants; that is, baseline correlations were calculated from all (matching and nonmatching) scene pairs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.4450#ref-CR25" id="ref-link-section-d31126455e1810">25</a></sup>. This procedure was performed for each searchlight cube, with one cube centered on each voxel in the brain; cubes with 50% or more of their volume outside the brain were discarded. The results were corrected for multiple comparisons across the entire brain using FDR (threshold <i>q</i> = 0.05). Importantly, the nature of this analysis ensures that the discovered patterns are content-specific at the scene level, as the correlation between neural patterns during matching scenes must on average exceed an equal-sized random draw of correlations between all (matching and nonmatching) scenes to be considered statistically significant.</p><p>Four types of pattern similarity analyses were conducted. The first three were as follows: (1) Movierecall within participant: the movie pattern was compared to the recollection pattern for each scene within each participant (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2a</a>). (2) Movierecall between participants: for each participant, the recollection pattern of each scene was compared to the movie pattern for that scene averaged across the remaining participants (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2d</a>). (3) Recallrecall between-participant: for each participant, the recollection pattern of each scene was compared to the recollection patterns for that scene averaged across the remaining participants (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3a</a>). The preceding analyses each resulted in a single brain map per participant. The average map was submitted to the shuffling-based statistical analysis described above and <i>P</i>-values were plotted on the brain for every voxel and thresholded using FDR correction over all brain voxels (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figs. 2b,e</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">3b</a>). The same pattern comparison was performed in the PMC ROI and the results plotted for each individual participant (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figs. 2c,f</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">3c</a>). For the fourth type of analysis, moviemovie between-participant, for each participant the movie pattern of each scene was compared to the movie pattern for that scene averaged across the remaining participants. The average <i>R</i> value across participants was plotted on the brain for every voxel (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig11">Supplementary Fig. 3b</a>). The values for the PMC ROI are shown for individual participants in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig11">Supplementary Figure 3c</a>.</p><h3 class="c-article__sub-heading" id="Sec22">Classification of individual scenes.</h3><p>We computed the discriminability of neural patterns for individual scenes during movie and recall (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Fig. 4</a>) in the PMC ROI (same ROI as in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figs. 2c,f</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">3c</a>). Participants were randomly assigned to one of two groups (<i>N</i> = 8 and <i>N</i> = 9), an average was calculated within each group, and data were extracted for the PMC ROI. Pairwise correlations were calculated between the two group means for all 50 movie scenes. For any given scene (for example, scene 1, group 1), the classification was labeled correct if the correlation with the matching scene in the other group (for example, scene 1, group 2) was higher than the correlation with any other scene (for example, scenes 250, group 2). Accuracy was then calculated as the proportion of scenes correctly identified out of 50 (chance level = 0.02). Classification rank was calculated for each scene as the rank of the matching scene correlation in the other group among all 50 scene correlations. The entire procedure was repeated using 200 random combinations of two groups sized <i>N</i> = 8 and <i>N</i> = 9. Statistical significance was assessed using a permutation analysis in which, for each combination of two groups, scene labels were randomized before computing the classification accuracy and rank. Accuracy was then averaged across the 200 combinations, for each scene the mean rank across the 200 combinations was calculated, and this procedure was performed 1,000 times to generate null distributions for overall accuracy and for rank of each scene (<i>P</i>-values smaller than 0.001 were calculated by interpolation from the null distribution). Classification rank <i>P</i>-values were corrected for multiple comparisons over all scenes using FDR at threshold <i>q</i> = 0.001.</p><p>All above analyses were identical for movie and recall except that data were not extant for all 50 scenes for every participant, owing to participants recalling 34.4 scenes on average. Thus, group average patterns for each scene were calculated by averaging over the extant data; for 41 scenes there were data available for at least one participant in each group, considering all 200 random combinations of participants into groups of <i>N</i> = 8 and <i>N</i> = 9.</p><h3 class="c-article__sub-heading" id="Sec23">Dimensionality of the shared neural patterns.</h3><p>The SRM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Chen, P.-H. et al. in Advances in Neural Information Processing Systems 28 (eds. Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M. &amp; Garnett, R.) 460468 (Curran Associates, 2015)." href="/articles/nn.4450#ref-CR57" id="ref-link-section-d31126455e1910">57</a></sup> algorithm operates over a series of data vectors (in this case, multiple participants' brain data) and finds a common representational space of lower dimensionality. Using SRM, we asked: when the data are reduced to <i>k</i> dimensions, how does this affect scene-level classification across brains? We tested this in the PMC region. The movie data were split randomly into two sets of scenes (25 and 25). SRM was trained on one set of 25 scenes (data not averaged within-scene). For the remaining 25 scenes, the data were split randomly into two groups (8 and 9 participants), and scene-level classification accuracy was calculated for each scene, using one group's low-dimensional pattern in SRM representational space to identify the matching scene in the other group. This entire process was repeated for 10 random splits of scenes, for 40 random splits of participants, and for different numbers of dimensions of <i>k,</i> ranging from 3 dimensions to 75 dimensions. The average of these 400 results is plotted in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig5">Figure 5a</a> (chance level 0.04). To examine how many of the movie dimensions were generalizable to recall, we performed the same analysis, training SRM on the movie data (25 scenes at a time, data not averaged within-scene) and performing scene classification across participants during spoken recall (recallrecall) using the identified movie dimensions (see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig5">Fig. 5b</a>). Unfortunately, because participants are not temporally aligned during recall, we could not use SRM to identify additional shared dimensions that might be unique to the recall data.</p><h3 class="c-article__sub-heading" id="Sec24">Visualization of the BOLD signal during movie and recall.</h3><p>To visualize the signals underlying our pattern similarity analyses, we randomly split the movie-viewing data into two independent groups of equal size (<i>N</i> = 8 each) and averaged BOLD values across participants within each group (movie group 1 and movie group 2). An average was made in the same manner for the recall data from the same groups of eight participants each (recall group 1 and recall group 2). These group mean images were then averaged across time points and within scene, exactly as in the prior analyses, creating one brain image per group per scene. A midline sagittal view of these average brains during one representative scene (scene 36) of the movie is shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig6">Figure 6a</a>. For the posterior medial area outlined by a white box in each panel of <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig6">Figure 6a</a>, we show the average activity for 14 different scenes (scenes that were recalled by all 16 of the randomly selected subjects) for each group in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig6">Figure 6be</a>.</p><h3 class="c-article__sub-heading" id="Sec25">Alteration analysis: comparison of movierecall and recallrecall maps.</h3><p>In this analysis (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Fig. 7</a>) we quantitatively compared the similarity strength of recallrecall to the similarity strength of movierecall. We compared recallrecall (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3b</a>) correlation values to between-participants movie-versus-recall (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2e</a>) correlation values using a voxel-by-voxel paired <i>t</i>-test. It was necessary to use between-participants movierecall comparisons because within-participant pattern similarity was expected to be higher than between-participants similarity merely as a result of lower anatomical variability. To assess whether the differences were statistically significant, we performed a resampling analysis wherein the individual participant correlation values for recallrecall and movierecall were randomly swapped between conditions to produce two surrogate groups of 17 members each; that is, each surrogate group contained one value from each of the 17 original participants, but the values were randomly selected to be from the recallrecall comparison or from the between-participants movierecall comparison. These two surrogate groups were compared using a <i>t</i>-test, and the procedure was repeated 100,000 times to produce a null distribution of <i>t</i> values. The veridical <i>t</i>-value was compared to the null distribution to produce a <i>P</i>-value for every voxel. The test was performed for every voxel that showed either significant recallrecall similarity (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3b</a>) or significant between-participants movierecall similarity (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3b</a>), corrected for multiple comparisons across the entire brain using an FDR threshold of <i>q</i> = 0.05 (see Pattern similarity analyses); voxels <i>P</i> &lt; 0.05 (one-tailed) were plotted on the brain (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Fig. 4a</a>). This map shows regions where between-participants recallrecall similarity was significantly greater than between-participants movierecall similaritythat is, where the map in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Figure 3b</a> was stronger than the map in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figure 2e</a>. See also <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig16">Supplementary Figure 8a</a> for scene-by-scene differences and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig16">Supplementary Figure 8b</a> for map of correlation difference values.</p><p>Discriminability of individual scenes was further assessed within the regions shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Figure 7b</a>. For every searchlight cube underlying the voxels shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Figure 7b</a>, we asked whether each participant's individual scene recollection patterns could be classified better using the movie data from other participants or the recall data from other participants. Unlike the classification analysis described in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig4">Figure 4</a>, calculations were performed at the individual participant level (for example, using each participant's recollection patterns compared to the average patterns across the remaining participants, for either movie or recall). Mean classification rank across scenes was calculated to produce one value per participant for the movie data from other participants and one for the recall data from other participants. A <i>t</i>-test between these two sets of values was performed at each searchlight cube (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Fig. 7c</a>).</p><h3 class="c-article__sub-heading" id="Sec26">Simulation of movie-to-recall pattern alteration.</h3><p>The logic of the alteration analysis is that only if the movie patterns change into recall patterns in a systematic manner across subjects is it possible for the recallrecall pattern correlations to be higher than movierecall pattern correlations. We demonstrate the logic using a simple computer simulation. In this simulation, five 125-voxel random patterns are created (five simulated subjects) and random noise is added to each one, such that the average inter-subject correlation is <i>R</i> = 1.0 or <i>R</i> = 0.3 (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig15">Supplementary Fig. 7</a>). These are the so-called movie patterns.</p><p>Next, we simulate the change from movie pattern to recall pattern by (i) adding random noise (at different levels of intensity, <i>y</i> axis) to every voxel in every subject to create the so-called recall patterns, which are noisy versions of the movie pattern; and (ii) adding a common pattern to each movie pattern to mimic the systematic alteration from movie pattern to recall pattern, plus random noise (at different levels of intensity, <i>x</i> axis). We plot the average correlation among the five simulated subjects' recall patterns (recallrecall), as well as the average correlation between movie and recall patterns (movierecall) in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig15">Supplementary Figure 7</a>.</p><p><a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig15">Supplementary Figure 7a</a> shows the results when no common pattern is addedthat is, the recall pattern is merely the movie pattern plus noise (no systematic alteration takes place): even as noise varies at the movie pattern stage and at the movie-to-recall change stage, similarity among recall patterns (recallrecall, solid lines) never exceeds the similarity of recall to movie (movierecall, dotted lines). In short, if the change from movie to recall was simply adding noise, then the recall patterns could not possibly become more similar to each other then they are to the original movie pattern.</p><p><a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig15">Supplementary Figure 7b</a> shows what happens when a common pattern is added to each subject's movie pattern, in addition to the same levels of random noise, to generate the recall pattern. Now, it becomes possible (even likely, under these simulated conditions) for the similarity among recall patterns (recallrecall, solid lines) to exceed the similarity of recall to movie (movierecall, dotted lines). In short, when the change from movie to recall involves a systematic change across subjects, recall patterns may become more similar to each other then they are to the original movie pattern.</p><p>Note that the similarity of the movie pattern to each other (moviemovie correlation) does not influence the results. In this simulation, the moviemovie correlations are presented at two levels, such that the average inter-subject correlation (on the <i>y</i>-axis) is <i>R</i> = 1.0 (red lines) or <i>R</i> = 0.3 (blue) before movierecall noise is added (at 0 on the <i>x</i> axis). In practice, moviemovie pattern correlations were higher overall than recallrecall and movierecall correlations (compare <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig11">Supplementary Fig. 3c</a> to <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figs. 2f</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">3c</a>).</p><h3 class="c-article__sub-heading" id="Sec27">Reinstatement in individual participants versus between participants.</h3><p>We performed a second-order similarity analysis: correlation of representational dissimilarity matrices (RDMs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008)." href="/articles/nn.4450#ref-CR25" id="ref-link-section-d31126455e2099">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Charest, I., Kievit, R.A., Schmitz, T.W., Deca, D. &amp; Kriegeskorte, N. Unique semantic space in the brain of each beholder predicts perceived similarity. Proc. Natl. Acad. Sci. USA 111, 1456514570 (2014)." href="/articles/nn.4450#ref-CR35" id="ref-link-section-d31126455e2102">35</a></sup> within and between participants. For each participant, an RDM was created from the pairwise correlations of patterns for individual scenes in the movie (movie-RDM) and a separate RDM created from the pairwise correlations of patterns for individual scenes during recall (recall-RDM). The movie-RDMs map the relationships between all movie scenes and the recall-RDMs map the relationships between all recalled scenes. Because the RDMs were always calculated within-brain, we were able to assess the similarity between representational structures within and between participants by comparing the movie-RDMs to the recall-RDMs within-participant and between-participants (see <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig8">Fig. 8a,b</a>, insets) in a manner less susceptible to potential anatomical misalignment between participants.</p><p>As each participant recalled a different subset of the 50 scenes, comparisons between movie-RDMs and recall-RDMs were always restricted to the extant scenes in the recall data for that participant. Thus, two different comparisons were made for each pair of participantsfor example, scene 1 movie-RDM versus scene 2 recall-RDM, and scene 2 movie-RDM versus scene 1 recall-RDM. In total this procedure yielded 17 within-participant comparisons and 272 between-participants comparisons. We calculated movie-RDM versus recall-RDM correlations, within-participant, in a searchlight analysis across the brain volume (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig8">Fig. 8a</a>). The same analysis was performed between all pairs of participants (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig8">Fig. 8b</a>).</p><p>Owing to the differing amounts of averaging in the within-participant and between-participants maps (that is, averaging over 17 rather than 272 individual maps, respectively), we did not perform significance testing on these maps separately, but instead tested the difference between the maps in a balanced manner. Statistical significance of the difference between the two was evaluated using a permutation analysis that randomly swapped condition labels for within-participant and between-participants RDM correlation values and FDR-corrected at a threshold of <i>q</i> = 0.05. Thus, averaging was performed over exactly 17 maps for each permutation.</p><h3 class="c-article__sub-heading" id="Sec28">Control analysis for elapsed time during movie and recall.</h3><p>To examine whether the brain regions revealed in the movierecall pattern similarity analysis (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2</a>) have a time-varying signal that shifts independent of the stimuli, we compared segments across subjects during recall at the same time elapsed from the start of recall. We extracted a 20-TR (30-s) window from each subject's neural data (PMC ROI) at each minute of the movie and recall, from 1 to 10 min. We then averaged across time to create a single voxel pattern for each segment, and calculated the correlation between all movie segments versus all recall segments within each subject, exactly as in the main analyses of the paper (for example, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figs. 2ac</a>). The mean correlation of matched-for-time-elapsed segments was <i>R</i> = 0.0025, and the mean of all comparisons not matched for time elapsed is <i>R</i> = 0.0023. A paired <i>t</i>-test of matched versus nonmatched segment correlation values, across subjects yields a <i>P</i>-value of 0.22. Performing the same analysis using 1-min windows (40 TRs) yields a mean diagonal of <i>R</i> = 0.0025, a mean nondiagonal of <i>R</i> = 0.0015, and a <i>P</i>-value of 0.84. This analysis suggests that PMC does not have a time-varying signal that shifts independent of the stimuli.</p><h3 class="c-article__sub-heading" id="Sec29">Was acoustic output correlated between subjects?</h3><p>We extracted the envelope of each subject's recall audio and linearly interpolated within each scene so that the lengths would be matched for all subjects (100 time points per scene). We then calculated the correlation between all audio segments across subjects (each subject versus the average of all others), exactly as in the main analyses of the paper (for example, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3</a>). The mean correlation of matching scene audio time courses was <i>R</i> = 0.0047, and the mean of all nonmatching scene audio time courses <i>R</i> = 0.0062. A paired <i>t</i>-test of matching versus nonmatching values, across subjects, yields a <i>P</i>-value of 0.98. This analysis shows that speech output during recall was not correlated across subjects.</p><h3 class="c-article__sub-heading" id="Sec30">Length of scenes versus neural pattern similarity.</h3><p>To explore whether the length of movie scenes and/or length of recall of a scene affected inter-subject neural similarity, we calculated the correlation of scene length versus inter-subject pattern similarity in PMC. During the movie, all subjects have the same length scenes, and thus we averaged across subjects to get the strongest possible signal. However, the correlation between movie scene length and inter-subject similarity of movie patterns in PMC was not significant: <i>R</i> = 0.17, <i>P</i> &gt; 0.2. During recall, subjects all have different length scenes, and thus we performed the correlation separately for each subject. The correlation between recall scene length and inter-subject similarity of recall patterns was <i>R</i> = 0.006 on average, with no <i>P</i> value less than 0.05 for any subject.</p><h3 class="c-article__sub-heading" id="Sec31">Encoding model.</h3><p>Detailed semantic labels (1,000 time segments for each of 10 labels) were used to construct an encoding model to predict neural activity patterns from semantic content<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Naselaris, T., Kay, K.N., Nishimoto, S. &amp; Gallant, J.L. Encoding and decoding in fMRI. Neuroimage 56, 400410 (2011)." href="/articles/nn.4450#ref-CR58" id="ref-link-section-d31126455e2212">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Mitchell, T.M. et al. Predicting human brain activity associated with the meanings of nouns. Science 320, 11911195 (2008)." href="/articles/nn.4450#ref-CR59" id="ref-link-section-d31126455e2215">59</a></sup>. A score was derived for each of the 50 scenes for each of the 10 labels (for example, proportion of time during a scene that Speaking was true; proportion of time that was Indoor; average Arousal; number of Locations visited within a scene; etc.). The scene-level labels/predictors are displayed in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig14">Supplementary Figure 6a</a>.</p><p><i>Encoding model: semantic labels.</i> The semantic and affective features of the stimulus were labeled by dividing the stimulus into 1,000 time segments and scoring each of 10 features in each segment. First, the movie was split into 1,000 time segments by a human rater (mean duration 3.0 s, s.d. 2.2 s) who was blind to the neural analyses. The splits were placed at shifts in the narrative (for example, location, topic, and/or time), in a procedure similar to, but much more fine-grained, than for the original 50 scenes. Each of the 1,000 segments was then labeled for the following content:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>NumberPersons: How many people are present onscreen</p>
                  </li>
                  <li>
                    <p>Location: What specific location is being shown (for example, Phone box on Brixton Road)</p>
                  </li>
                  <li>
                    <p>Indoor/Outdoor: Whether the location is indoor or outdoor</p>
                  </li>
                  <li>
                    <p>Speaking: Whether or not anyone is speaking</p>
                  </li>
                  <li>
                    <p>Arousal: Excitement/engagement/activity level</p>
                  </li>
                  <li>
                    <p>Valence: Positive or negative mood</p>
                  </li>
                  <li>
                    <p>Music: Whether or not there is music playing</p>
                  </li>
                  <li>
                    <p>WrittenWords: Whether or not there are written words onscreen</p>
                  </li>
                  <li>
                    <p>Sherlock: Whether or not the character Sherlock Holmes is onscreen</p>
                  </li>
                  <li>
                    <p>John: Whether or not the character John Watson is onscreen</p>
                  </li>
                </ul><p>For the labels Arousal and Valence, assessments were collected from four different raters (Arousal: Cronbach's  = 0.75; Valence: Cronbach's  = 0.81) and the average across raters used for model prediction. The other eight labels were deemed objective and not requiring multiple raters.</p><p><i>Encoding model: feature selection and fitting</i>. To create the encoding model, we split the data randomly into two groups of participants (8 and 9) and created an average pattern for each scene in each group. Next, for one label (for example, Arousal), we regressed each voxel's activity values from 48 scenes on the label values for those 48 scenes, constituting a prediction of the relationship between label value (for example, Arousal for a scene) and voxel activity. By calculating this fit separately for every voxel in PMC, we created a predicted pattern for each of the two held-out scenes. These predicted patterns were then compared to the true patterns for those scenes in group 1, iterating across all possible pairs of held-out scenes, enabling calculation of classification accuracy (with chance level 50%). The label with the highest accuracy (out of the 10 labels) was ranked #1. Next, the same procedure was repeated using the rank #1 label and each of the remaining 9 labels in a multiple regression (that is, we generated every possible two-predictor model including the rank #1 label; for each model, we predicted the held-out patterns and then computed classification accuracy). The label that yielded the highest accuracy in conjunction with the rank #1 label was ranked #2. This hierarchical label selection was repeated to rank all 10 labels.</p><p><i>Encoding model: testing in held-out data.</i> Using the ranking order determined from the group 1 data, we then calculated classification accuracy using the heretofore untouched group 2 data: as above, we created a predicted pattern for two held-out scenes, and these predicted patterns were compared to the true patterns for those scenes in group 2, iterating across all possible pairs of held-out scenes, enabling calculation of classification accuracy (with chance level 50%). We first did this for a model incorporating the rank #1 label, then a model incorporating the rank #1 and #2 labels, and so on. The entire procedure was performed for 100 random unique splits of participants into two groups (<i>N</i> = 8 and <i>N</i> = 9). The classification accuracy and label rankings across the 100 combinations are plotted in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig14">Supplementary Figure 6b</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig14">Supplementary Figure 6c</a>, respectively. The predictor confusion matrix is displayed in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig14">Supplementary Figure 6d.</a></p><h3 class="c-article__sub-heading" id="Sec32">Comparison between semantic similarity and neural pattern similarity.</h3><p>To examine whether shared spatial patterns during a recalled scene reflect shared content in verbal recalls, we tested whether neural pattern similarity correlates with semantic similarity in the content of participants' verbal recall. We performed latent semantic similarity analysis using the LSA package<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Wild, F. lsa: latent semantic analysis. R package version 0.73.1 (2015)." href="/articles/nn.4450#ref-CR60" id="ref-link-section-d31126455e2326">60</a></sup> in R. Transcripts from all participants were used as the corpus, with each recalled scene constituting a 'document'. The corpus was preprocessed to convert all characters to lower-case and to remove punctuation and stop words. Words were then reduced to their word stems, which then constituted the 'terms' that enter into the computation of the latent semantic space. The corpus was then converted to a term  document matrix, and singular-value decomposition was applied to compute the latent semantic space.</p><p>Using the latent semantic space, we computed the semantic similarity between recalled scenes. For each pair of participants, we computed the semantic similarity between mutually recalled scenes and correlated this vector of semantic similarities with the corresponding vector of neural pattern similarities in PMC. The correlation was then averaged across every pair of participants. To assess statistical significance, we ran a non-parametric permutation test. For each pair of participants, we shuffled the neural pattern similarity vector before correlating it with the semantic similarity vector. This was iterated 1,000 times to generate a null distribution of average <i>R</i> values. The true <i>R</i> value was compared to the null-distribution to determine the <i>P</i>-value. The correlation was weak but significant (<i>R</i> = 0.08, <i>P</i> &lt; 0.001), suggesting that the shared neural patterns might reflect shared content in recalled memories.</p><h3 class="c-article__sub-heading" id="Sec33">Spatial resolution of neural signals.</h3><p>How refined was the spatial alignment of recollection patterns across brains? The alignment had to be robust enough to overcome imperfect registration (due to anatomical variation) across different brains. Our data indicate that the pattern alignment was strong enough to survive spatial transformation of brain data to a standard anatomical space. Recently, it was argued that relatively coarse organized patterns (for example, small eccentricity-related biases toward horizontal or vertical orientations within primary visual cortex) can underlie spatial pattern correlations in the brain<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Freeman, J., Heeger, D.J. &amp; Merriam, E.P. Coarse-scale biases for spirals and orientation in human visual cortex. J. Neurosci. 33, 1969519703 (2013)." href="/articles/nn.4450#ref-CR61" id="ref-link-section-d31126455e2356">61</a></sup>. Importantly, while the current results reveal a relatively coarse spatial structure that is shared across people (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig6">Fig. 6</a>), they do not preclude the existence of finer spatial structure in the neural signal that may be captured when comparisons are made within-participant<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Charest, I., Kievit, R.A., Schmitz, T.W., Deca, D. &amp; Kriegeskorte, N. Unique semantic space in the brain of each beholder predicts perceived similarity. Proc. Natl. Acad. Sci. USA 111, 1456514570 (2014)." href="/articles/nn.4450#ref-CR35" id="ref-link-section-d31126455e2363">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Kamitani, Y. &amp; Tong, F. Decoding the visual and subjective contents of the human brain. Nat. Neurosci. 8, 679685 (2005)." href="/articles/nn.4450#ref-CR62" id="ref-link-section-d31126455e2366">62</a></sup> or using more sensitive methods such as hyperalignment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Haxby, J.V. et al. A common, high-dimensional model of the representational space in human ventral temporal cortex. Neuron 72, 404416 (2011)." href="/articles/nn.4450#ref-CR63" id="ref-link-section-d31126455e2370">63</a></sup>. Furthermore, when we calculated movie-RDM versus recall-RDM second-order correlations within and between brains, we found a region in the temporoparietal junction in which individual-unique aspects of neural patterns contributed reliably to reinstatement strength above and beyond the shared representation (though the fact that this region is small relative to the total area in which shared patterns were identified suggests that the between-brain comparison captured a substantial portion of the movierecall pattern similarity). Using a similar representational similarity analysis (RSA) approach, Charest <i>et al</i>.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Charest, I., Kievit, R.A., Schmitz, T.W., Deca, D. &amp; Kriegeskorte, N. Unique semantic space in the brain of each beholder predicts perceived similarity. Proc. Natl. Acad. Sci. USA 111, 1456514570 (2014)." href="/articles/nn.4450#ref-CR35" id="ref-link-section-d31126455e2378">35</a></sup> found individual-unique neural responses in inferior temporal cortex during visual object viewing. There were numerous differences between our study and that of Charest <i>et al</i>.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Charest, I., Kievit, R.A., Schmitz, T.W., Deca, D. &amp; Kriegeskorte, N. Unique semantic space in the brain of each beholder predicts perceived similarity. Proc. Natl. Acad. Sci. USA 111, 1456514570 (2014)." href="/articles/nn.4450#ref-CR35" id="ref-link-section-d31126455e2385">35</a></sup>, in stimulus content (Charest <i>et al</i>.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Charest, I., Kievit, R.A., Schmitz, T.W., Deca, D. &amp; Kriegeskorte, N. Unique semantic space in the brain of each beholder predicts perceived similarity. Proc. Natl. Acad. Sci. USA 111, 1456514570 (2014)." href="/articles/nn.4450#ref-CR35" id="ref-link-section-d31126455e2392">35</a></sup> used objects selected for personal significance), paradigm, and regions of interest; further work is needed to understand the factors that influence the balance of idiosyncratic and shared signals between brains.</p><h3 class="c-article__sub-heading" id="Sec34">Visual imagery.</h3><p>To what extent did spoken recollection in this study engage visual imagery? We observed extraordinarily rich recollection behavior (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig1">Fig. 1b</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Supplementary Table 2</a>), in which participants managed to recount, in largely correct order, the details of most of the movie scenes; this suggested that participants were able to mentally replay virtually the entire movie, despite the absence of any external cues. However, movierecall reinstatement effects were not found in low-level visual areas, but instead were located in high-level visual areas (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig12">Supplementary Fig. 4</a>)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Wang, L., Mruczek, R.E.B., Arcaro, M.J. &amp; Kastner, S. Probabilistic maps of visual topography in human cortex. Cereb. Cortex 25, 39113931 (2015)." href="/articles/nn.4450#ref-CR64" id="ref-link-section-d31126455e2413">64</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Yarkoni, T., Poldrack, R.A., Nichols, T.E., Van Essen, D.C. &amp; Wager, T.D. Large-scale automated synthesis of human functional neuroimaging data. Nat. Methods 8, 665670 (2011)." href="/articles/nn.4450#ref-CR65" id="ref-link-section-d31126455e2416">65</a></sup> and extensively in higher order brain regions outside of the visual system (<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2</a>). Our observation of reinstatement in high level visual areas is compatible with studies showing reinstatement in these regions during cued visual imagery<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Bird, C.M., Keidel, J.L., Ing, L.P., Horner, A.J. &amp; Burgess, N. Consolidation of complex events via reinstatement in posterior cingulate cortex. J. Neurosci. 35, 1442614434 (2015)." href="/articles/nn.4450#ref-CR24" id="ref-link-section-d31126455e2424">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Cichy, R.M., Heinzle, J. &amp; Haynes, J.-D. Imagery and perception share cortical representations of content and location. Cereb. Cortex 22, 372380 (2012)." href="/articles/nn.4450#ref-CR66" id="ref-link-section-d31126455e2427">66</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="St-Laurent, M., Abdi, H. &amp; Buchsbaum, B.R. Distributed patterns of reactivation predict vividness of recollection. J. Cogn. Neurosci. 27, 20002018 (2015)." href="/articles/nn.4450#ref-CR67" id="ref-link-section-d31126455e2430">67</a></sup>. The lack of reinstatement effects in low-level areas may be due to the natural tendency of most participants to focus on the episodic narrative (the plot) when recounting the movie, rather than on fine visual details. It has been suggested that the requirement to note high-resolution details is a key factor in eliciting activity in early visual cortex during visual imagery<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Kosslyn, S.M. &amp; Thompson, W.L. When is early visual cortex activated during visual mental imagery? Psychol. Bull. 129, 723746 (2003)." href="/articles/nn.4450#ref-CR68" id="ref-link-section-d31126455e2434">68</a></sup>. Thus, our findings do not conflict with studies showing that activity patterns in early visual cortex can be used to decode a simple image held in mind during a delay, in tasks that required vivid imagery of low-level visual features<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Harrison, S.A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. Nature 458, 632635 (2009)." href="/articles/nn.4450#ref-CR69" id="ref-link-section-d31126455e2438">69</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Serences, J.T., Ester, E.F., Vogel, E.K. &amp; Awh, E. Stimulus-specific delay activity in human primary visual cortex. Psychol. Sci. 20, 207214 (2009)." href="/articles/nn.4450#ref-CR70" id="ref-link-section-d31126455e2441">70</a></sup>.</p><h3 class="c-article__sub-heading" id="Sec35">Code availability.</h3><p>Code supporting the findings of this study are available from the corresponding author upon request.</p><h3 class="c-article__sub-heading" id="Sec36">Data availability.</h3><p>The data that support the findings of this study are available online at <a href="http://dataspace.princeton.edu/jspui/handle/88435/dsp01nz8062179">http://dataspace.princeton.edu/jspui/handle/88435/dsp01nz8062179</a>.</p><p>A <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM43">Supplementary Methods Checklist</a> is available.</p></div></div></section>
                </div>
            

            <div>
                <div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1"><p class="c-article-references__text" id="ref-CR1">Isola, P., Xiao, J., Torralba, A. &amp; Oliva, A. What makes an image memorable?. in <i>2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i> 145152 (2011) <a href="https://doi.org/10.1109/CVPR.2011.5995721" data-track="click" data-track-action="external reference" data-track-label="10.1109/CVPR.2011.5995721">doi:10.1109/CVPR.2011.5995721</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2"><p class="c-article-references__text" id="ref-CR2">Halbwachs, M. <i>The Collective Memory</i> (Harper &amp; Row Colophon, 1980).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3"><p class="c-article-references__text" id="ref-CR3">Sperber, D. <i>Explaining Culture: A Naturalistic Approach</i> (Blackwell, 1996).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4"><p class="c-article-references__text" id="ref-CR4">Coman, A. &amp; Hirst, W. Cognition through a social network: the propagation of induced forgetting and practice effects. <i>J. Exp. Psychol. Gen.</i> <b>141</b>, 321336 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/a0025247" data-track-action="article reference" href="https://doi.org/10.1037%2Fa0025247" aria-label="Article reference 4" data-doi="10.1037/a0025247">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21910558" aria-label="PubMed reference 4">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognition%20through%20a%20social%20network%3A%20the%20propagation%20of%20induced%20forgetting%20and%20practice%20effects&amp;journal=J.%20Exp.%20Psychol.%20Gen.&amp;doi=10.1037%2Fa0025247&amp;volume=141&amp;pages=321-336&amp;publication_year=2012&amp;author=Coman%2CA&amp;author=Hirst%2CW">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5"><p class="c-article-references__text" id="ref-CR5">Roediger, H.L. III &amp; Abel, M. Collective memory: a new arena of cognitive study. <i>Trends Cogn. Sci.</i> <b>19</b>, 359361 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2015.04.003" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2015.04.003" aria-label="Article reference 5" data-doi="10.1016/j.tics.2015.04.003">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25953047" aria-label="PubMed reference 5">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Collective%20memory%3A%20a%20new%20arena%20of%20cognitive%20study&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2015.04.003&amp;volume=19&amp;pages=359-361&amp;publication_year=2015&amp;author=Roediger%2CHL&amp;author=Abel%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6"><p class="c-article-references__text" id="ref-CR6">Raichle, M.E. et al. A default mode of brain function. <i>Proc. Natl. Acad. Sci. USA</i> <b>98</b>, 676682 (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.98.2.676" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.98.2.676" aria-label="Article reference 6" data-doi="10.1073/pnas.98.2.676">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3MXnslKmtw%3D%3D" aria-label="CAS reference 6">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11209064" aria-label="PubMed reference 6">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC14647" aria-label="PubMed Central reference 6">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20default%20mode%20of%20brain%20function&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.98.2.676&amp;volume=98&amp;pages=676-682&amp;publication_year=2001&amp;author=Raichle%2CME">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7"><p class="c-article-references__text" id="ref-CR7">Hasson, U., Nir, Y., Levy, I., Fuhrmann, G. &amp; Malach, R. Intersubject synchronization of cortical activity during natural vision. <i>Science</i> <b>303</b>, 16341640 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1089506" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1089506" aria-label="Article reference 7" data-doi="10.1126/science.1089506">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2cXhvFCnsbw%3D" aria-label="CAS reference 7">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15016991" aria-label="PubMed reference 7">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Intersubject%20synchronization%20of%20cortical%20activity%20during%20natural%20vision&amp;journal=Science&amp;doi=10.1126%2Fscience.1089506&amp;volume=303&amp;pages=1634-1640&amp;publication_year=2004&amp;author=Hasson%2CU&amp;author=Nir%2CY&amp;author=Levy%2CI&amp;author=Fuhrmann%2CG&amp;author=Malach%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8"><p class="c-article-references__text" id="ref-CR8">Jskelinen, I.P. et al. Inter-subject synchronization of prefrontal cortex hemodynamic activity during natural viewing. <i>Open Neuroimag. J.</i> <b>2</b>, 1419 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.2174/1874440000802010014" data-track-action="article reference" href="https://doi.org/10.2174%2F1874440000802010014" aria-label="Article reference 8" data-doi="10.2174/1874440000802010014">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19018313" aria-label="PubMed reference 8">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2577941" aria-label="PubMed Central reference 8">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Inter-subject%20synchronization%20of%20prefrontal%20cortex%20hemodynamic%20activity%20during%20natural%20viewing&amp;journal=Open%20Neuroimag.%20J.&amp;doi=10.2174%2F1874440000802010014&amp;volume=2&amp;pages=14-19&amp;publication_year=2008&amp;author=J%C3%A4%C3%A4skel%C3%A4inen%2CIP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9"><p class="c-article-references__text" id="ref-CR9">Wilson, S.M., Molnar-Szakacs, I. &amp; Iacoboni, M. Beyond superior temporal cortex: intersubject correlations in narrative speech comprehension. <i>Cereb. Cortex</i> <b>18</b>, 230242 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/bhm049" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhm049" aria-label="Article reference 9" data-doi="10.1093/cercor/bhm049">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17504783" aria-label="PubMed reference 9">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Beyond%20superior%20temporal%20cortex%3A%20intersubject%20correlations%20in%20narrative%20speech%20comprehension&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhm049&amp;volume=18&amp;pages=230-242&amp;publication_year=2008&amp;author=Wilson%2CSM&amp;author=Molnar-Szakacs%2CI&amp;author=Iacoboni%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10"><p class="c-article-references__text" id="ref-CR10">Lerner, Y., Honey, C.J., Silbert, L.J. &amp; Hasson, U. Topographic mapping of a hierarchy of temporal receptive windows using a narrated story. <i>J. Neurosci.</i> <b>31</b>, 29062915 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.3684-10.2011" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.3684-10.2011" aria-label="Article reference 10" data-doi="10.1523/JNEUROSCI.3684-10.2011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXislGitrs%3D" aria-label="CAS reference 10">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21414912" aria-label="PubMed reference 10">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3089381" aria-label="PubMed Central reference 10">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Topographic%20mapping%20of%20a%20hierarchy%20of%20temporal%20receptive%20windows%20using%20a%20narrated%20story&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.3684-10.2011&amp;volume=31&amp;pages=2906-2915&amp;publication_year=2011&amp;author=Lerner%2CY&amp;author=Honey%2CCJ&amp;author=Silbert%2CLJ&amp;author=Hasson%2CU">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11"><p class="c-article-references__text" id="ref-CR11">Honey, C.J., Thompson, C.R., Lerner, Y. &amp; Hasson, U. Not lost in translation: neural responses shared across languages. <i>J. Neurosci.</i> <b>32</b>, 1527715283 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.1800-12.2012" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.1800-12.2012" aria-label="Article reference 11" data-doi="10.1523/JNEUROSCI.1800-12.2012">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38Xhs1CjurvE" aria-label="CAS reference 11">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23115166" aria-label="PubMed reference 11">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3525075" aria-label="PubMed Central reference 11">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Not%20lost%20in%20translation%3A%20neural%20responses%20shared%20across%20languages&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.1800-12.2012&amp;volume=32&amp;pages=15277-15283&amp;publication_year=2012&amp;author=Honey%2CCJ&amp;author=Thompson%2CCR&amp;author=Lerner%2CY&amp;author=Hasson%2CU">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12"><p class="c-article-references__text" id="ref-CR12">Lahnakoski, J.M. et al. Synchronous brain activity across individuals underlies shared psychological perspectives. <i>Neuroimage</i> <b>100</b>, 316324 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2014.06.022" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2014.06.022" aria-label="Article reference 12" data-doi="10.1016/j.neuroimage.2014.06.022">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24936687" aria-label="PubMed reference 12">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Synchronous%20brain%20activity%20across%20individuals%20underlies%20shared%20psychological%20perspectives&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2014.06.022&amp;volume=100&amp;pages=316-324&amp;publication_year=2014&amp;author=Lahnakoski%2CJM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13"><p class="c-article-references__text" id="ref-CR13">Simony, E. et al. Dynamic reconfiguration of the default mode network during narrative comprehension. <i>Nat. Commun.</i> <b>7</b>, 12141 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/ncomms12141" data-track-action="article reference" href="https://doi.org/10.1038%2Fncomms12141" aria-label="Article reference 13" data-doi="10.1038/ncomms12141">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2sXksVels7c%3D" aria-label="CAS reference 13">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27424918" aria-label="PubMed reference 13">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4960303" aria-label="PubMed Central reference 13">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20reconfiguration%20of%20the%20default%20mode%20network%20during%20narrative%20comprehension&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fncomms12141&amp;volume=7&amp;publication_year=2016&amp;author=Simony%2CE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14"><p class="c-article-references__text" id="ref-CR14">Regev, M., Honey, C.J., Simony, E. &amp; Hasson, U. Selective and invariant neural responses to spoken and written narratives. <i>J. Neurosci.</i> <b>33</b>, 1597815988 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.1580-13.2013" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.1580-13.2013" aria-label="Article reference 14" data-doi="10.1523/JNEUROSCI.1580-13.2013">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhsF2lurjO" aria-label="CAS reference 14">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24089502" aria-label="PubMed reference 14">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3787506" aria-label="PubMed Central reference 14">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Selective%20and%20invariant%20neural%20responses%20to%20spoken%20and%20written%20narratives&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.1580-13.2013&amp;volume=33&amp;pages=15978-15988&amp;publication_year=2013&amp;author=Regev%2CM&amp;author=Honey%2CCJ&amp;author=Simony%2CE&amp;author=Hasson%2CU">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15"><p class="c-article-references__text" id="ref-CR15">Wang, M. &amp; He, B.J. A cross-modal investigation of the neural substrates for ongoing cognition. <i>Front. Psychol.</i> <b>5</b>, 945 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25206347" aria-label="PubMed reference 15">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4143722" aria-label="PubMed Central reference 15">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20cross-modal%20investigation%20of%20the%20neural%20substrates%20for%20ongoing%20cognition&amp;journal=Front.%20Psychol.&amp;volume=5&amp;publication_year=2014&amp;author=Wang%2CM&amp;author=He%2CBJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16"><p class="c-article-references__text" id="ref-CR16">Borges, J.L. Funes the Memorious. <i>La Nacin</i> (Mitre, 1942).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17"><p class="c-article-references__text" id="ref-CR17">Wheeler, M.E., Petersen, S.E. &amp; Buckner, R.L. Memory's echo: vivid remembering reactivates sensory-specific cortex. <i>Proc. Natl. Acad. Sci. USA</i> <b>97</b>, 1112511129 (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.97.20.11125" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.97.20.11125" aria-label="Article reference 17" data-doi="10.1073/pnas.97.20.11125">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3cXnt1aht7c%3D" aria-label="CAS reference 17">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11005879" aria-label="PubMed reference 17">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC27159" aria-label="PubMed Central reference 17">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Memory%27s%20echo%3A%20vivid%20remembering%20reactivates%20sensory-specific%20cortex&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.97.20.11125&amp;volume=97&amp;pages=11125-11129&amp;publication_year=2000&amp;author=Wheeler%2CME&amp;author=Petersen%2CSE&amp;author=Buckner%2CRL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18"><p class="c-article-references__text" id="ref-CR18">Danker, J.F. &amp; Anderson, J.R. The ghosts of brain states past: remembering reactivates the brain regions engaged during encoding. <i>Psychol. Bull.</i> <b>136</b>, 87102 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/a0017937" data-track-action="article reference" href="https://doi.org/10.1037%2Fa0017937" aria-label="Article reference 18" data-doi="10.1037/a0017937">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20063927" aria-label="PubMed reference 18">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2853176" aria-label="PubMed Central reference 18">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20ghosts%20of%20brain%20states%20past%3A%20remembering%20reactivates%20the%20brain%20regions%20engaged%20during%20encoding&amp;journal=Psychol.%20Bull.&amp;doi=10.1037%2Fa0017937&amp;volume=136&amp;pages=87-102&amp;publication_year=2010&amp;author=Danker%2CJF&amp;author=Anderson%2CJR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19"><p class="c-article-references__text" id="ref-CR19">Polyn, S.M., Natu, V.S., Cohen, J.D. &amp; Norman, K.A. Category-specific cortical activity precedes retrieval during memory search. <i>Science</i> <b>310</b>, 19631966 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1117645" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1117645" aria-label="Article reference 19" data-doi="10.1126/science.1117645">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2MXhtlagurvJ" aria-label="CAS reference 19">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16373577" aria-label="PubMed reference 19">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Category-specific%20cortical%20activity%20precedes%20retrieval%20during%20memory%20search&amp;journal=Science&amp;doi=10.1126%2Fscience.1117645&amp;volume=310&amp;pages=1963-1966&amp;publication_year=2005&amp;author=Polyn%2CSM&amp;author=Natu%2CVS&amp;author=Cohen%2CJD&amp;author=Norman%2CKA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20"><p class="c-article-references__text" id="ref-CR20">Johnson, J.D., McDuff, S.G.R., Rugg, M.D. &amp; Norman, K.A. Recollection, familiarity, and cortical reinstatement: a multivoxel pattern analysis. <i>Neuron</i> <b>63</b>, 697708 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2009.08.011" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2009.08.011" aria-label="Article reference 20" data-doi="10.1016/j.neuron.2009.08.011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXhsVChs7rO" aria-label="CAS reference 20">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19755111" aria-label="PubMed reference 20">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2771457" aria-label="PubMed Central reference 20">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Recollection%2C%20familiarity%2C%20and%20cortical%20reinstatement%3A%20a%20multivoxel%20pattern%20analysis&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2009.08.011&amp;volume=63&amp;pages=697-708&amp;publication_year=2009&amp;author=Johnson%2CJD&amp;author=McDuff%2CSGR&amp;author=Rugg%2CMD&amp;author=Norman%2CKA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21"><p class="c-article-references__text" id="ref-CR21">Kuhl, B.A., Rissman, J., Chun, M.M. &amp; Wagner, A.D. Fidelity of neural reactivation reveals competition between memories. <i>Proc. Natl. Acad. Sci. USA</i> <b>108</b>, 59035908 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1016939108" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1016939108" aria-label="Article reference 21" data-doi="10.1073/pnas.1016939108">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21436044" aria-label="PubMed reference 21">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3078372" aria-label="PubMed Central reference 21">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Fidelity%20of%20neural%20reactivation%20reveals%20competition%20between%20memories&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1016939108&amp;volume=108&amp;pages=5903-5908&amp;publication_year=2011&amp;author=Kuhl%2CBA&amp;author=Rissman%2CJ&amp;author=Chun%2CMM&amp;author=Wagner%2CAD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22"><p class="c-article-references__text" id="ref-CR22">Buchsbaum, B.R., Lemire-Rodger, S., Fang, C. &amp; Abdi, H. The neural basis of vivid memory is patterned on perception. <i>J. Cogn. Neurosci.</i> <b>24</b>, 18671883 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1162/jocn_a_00253" data-track-action="article reference" href="https://doi.org/10.1162%2Fjocn_a_00253" aria-label="Article reference 22" data-doi="10.1162/jocn_a_00253">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22640391" aria-label="PubMed reference 22">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20neural%20basis%20of%20vivid%20memory%20is%20patterned%20on%20perception&amp;journal=J.%20Cogn.%20Neurosci.&amp;doi=10.1162%2Fjocn_a_00253&amp;volume=24&amp;pages=1867-1883&amp;publication_year=2012&amp;author=Buchsbaum%2CBR&amp;author=Lemire-Rodger%2CS&amp;author=Fang%2CC&amp;author=Abdi%2CH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23"><p class="c-article-references__text" id="ref-CR23">Wing, E.A., Ritchey, M. &amp; Cabeza, R. Reinstatement of individual past events revealed by the similarity of distributed activation patterns during encoding and retrieval. <i>J. Cogn. Neurosci.</i> <b>27</b>, 679691 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1162/jocn_a_00740" data-track-action="article reference" href="https://doi.org/10.1162%2Fjocn_a_00740" aria-label="Article reference 23" data-doi="10.1162/jocn_a_00740">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25313659" aria-label="PubMed reference 23">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Reinstatement%20of%20individual%20past%20events%20revealed%20by%20the%20similarity%20of%20distributed%20activation%20patterns%20during%20encoding%20and%20retrieval&amp;journal=J.%20Cogn.%20Neurosci.&amp;doi=10.1162%2Fjocn_a_00740&amp;volume=27&amp;pages=679-691&amp;publication_year=2015&amp;author=Wing%2CEA&amp;author=Ritchey%2CM&amp;author=Cabeza%2CR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24"><p class="c-article-references__text" id="ref-CR24">Bird, C.M., Keidel, J.L., Ing, L.P., Horner, A.J. &amp; Burgess, N. Consolidation of complex events via reinstatement in posterior cingulate cortex. <i>J. Neurosci.</i> <b>35</b>, 1442614434 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.1774-15.2015" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.1774-15.2015" aria-label="Article reference 24" data-doi="10.1523/JNEUROSCI.1774-15.2015">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC28XhtFOru7bN" aria-label="CAS reference 24">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26511235" aria-label="PubMed reference 24">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4623223" aria-label="PubMed Central reference 24">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Consolidation%20of%20complex%20events%20via%20reinstatement%20in%20posterior%20cingulate%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.1774-15.2015&amp;volume=35&amp;pages=14426-14434&amp;publication_year=2015&amp;author=Bird%2CCM&amp;author=Keidel%2CJL&amp;author=Ing%2CLP&amp;author=Horner%2CAJ&amp;author=Burgess%2CN">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25"><p class="c-article-references__text" id="ref-CR25">Kriegeskorte, N., Mur, M. &amp; Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. <i>Front. Syst. Neurosci.</i> <b>2</b>, 4 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.3389/neuro.01.016.2008" data-track-action="article reference" href="https://doi.org/10.3389%2Fneuro.01.016.2008" aria-label="Article reference 25" data-doi="10.3389/neuro.01.016.2008">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19104670" aria-label="PubMed reference 25">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405" aria-label="PubMed Central reference 25">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Representational%20similarity%20analysis%20-%20connecting%20the%20branches%20of%20systems%20neuroscience&amp;journal=Front.%20Syst.%20Neurosci.&amp;doi=10.3389%2Fneuro.01.016.2008&amp;volume=2&amp;publication_year=2008&amp;author=Kriegeskorte%2CN&amp;author=Mur%2CM&amp;author=Bandettini%2CP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26"><p class="c-article-references__text" id="ref-CR26">Buckner, R.L., Andrews-Hanna, J.R. &amp; Schacter, D.L. The brain's default network: anatomy, function, and relevance to disease. <i>Ann. NY Acad. Sci.</i> <b>1124</b>, 138 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1196/annals.1440.011" data-track-action="article reference" href="https://doi.org/10.1196%2Fannals.1440.011" aria-label="Article reference 26" data-doi="10.1196/annals.1440.011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18400922" aria-label="PubMed reference 26">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20brain%27s%20default%20network%3A%20anatomy%2C%20function%2C%20and%20relevance%20to%20disease&amp;journal=Ann.%20NY%20Acad.%20Sci.&amp;doi=10.1196%2Fannals.1440.011&amp;volume=1124&amp;pages=1-38&amp;publication_year=2008&amp;author=Buckner%2CRL&amp;author=Andrews-Hanna%2CJR&amp;author=Schacter%2CDL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27"><p class="c-article-references__text" id="ref-CR27">Rugg, M.D. &amp; Vilberg, K.L. Brain networks underlying episodic memory retrieval. <i>Curr. Opin. Neurobiol.</i> <b>23</b>, 255260 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.conb.2012.11.005" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.conb.2012.11.005" aria-label="Article reference 27" data-doi="10.1016/j.conb.2012.11.005">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhslKqsL3N" aria-label="CAS reference 27">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23206590" aria-label="PubMed reference 27">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Brain%20networks%20underlying%20episodic%20memory%20retrieval&amp;journal=Curr.%20Opin.%20Neurobiol.&amp;doi=10.1016%2Fj.conb.2012.11.005&amp;volume=23&amp;pages=255-260&amp;publication_year=2013&amp;author=Rugg%2CMD&amp;author=Vilberg%2CKL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28"><p class="c-article-references__text" id="ref-CR28">Honey, C.J. et al. Slow cortical dynamics and the accumulation of information over long timescales. <i>Neuron</i> <b>76</b>, 423434 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2012.08.011" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2012.08.011" aria-label="Article reference 28" data-doi="10.1016/j.neuron.2012.08.011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhsFClt7jN" aria-label="CAS reference 28">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23083743" aria-label="PubMed reference 28">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3517908" aria-label="PubMed Central reference 28">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Slow%20cortical%20dynamics%20and%20the%20accumulation%20of%20information%20over%20long%20timescales&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2012.08.011&amp;volume=76&amp;pages=423-434&amp;publication_year=2012&amp;author=Honey%2CCJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29"><p class="c-article-references__text" id="ref-CR29">Hasson, U., Malach, R. &amp; Heeger, D.J. Reliability of cortical activity during natural stimulation. <i>Trends Cogn. Sci.</i> <b>14</b>, 4048 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2009.10.011" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2009.10.011" aria-label="Article reference 29" data-doi="10.1016/j.tics.2009.10.011">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20004608" aria-label="PubMed reference 29">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Reliability%20of%20cortical%20activity%20during%20natural%20stimulation&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2009.10.011&amp;volume=14&amp;pages=40-48&amp;publication_year=2010&amp;author=Hasson%2CU&amp;author=Malach%2CR&amp;author=Heeger%2CDJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30"><p class="c-article-references__text" id="ref-CR30">Mitchell, T.M. et al. Learning to decode cognitive states from brain images. <i>Mach. Learn.</i> <b>57</b>, 145175 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1023/B:MACH.0000035475.85309.1b" data-track-action="article reference" href="https://doi.org/10.1023%2FB%3AMACH.0000035475.85309.1b" aria-label="Article reference 30" data-doi="10.1023/B:MACH.0000035475.85309.1b">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20to%20decode%20cognitive%20states%20from%20brain%20images&amp;journal=Mach.%20Learn.&amp;doi=10.1023%2FB%3AMACH.0000035475.85309.1b&amp;volume=57&amp;pages=145-175&amp;publication_year=2004&amp;author=Mitchell%2CTM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31"><p class="c-article-references__text" id="ref-CR31">Poldrack, R.A., Halchenko, Y.O. &amp; Hanson, S.J. Decoding the large-scale structure of brain function by classifying mental States across individuals. <i>Psychol. Sci.</i> <b>20</b>, 13641372 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1111/j.1467-9280.2009.02460.x" data-track-action="article reference" href="https://doi.org/10.1111%2Fj.1467-9280.2009.02460.x" aria-label="Article reference 31" data-doi="10.1111/j.1467-9280.2009.02460.x">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19883493" aria-label="PubMed reference 31">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20the%20large-scale%20structure%20of%20brain%20function%20by%20classifying%20mental%20States%20across%20individuals&amp;journal=Psychol.%20Sci.&amp;doi=10.1111%2Fj.1467-9280.2009.02460.x&amp;volume=20&amp;pages=1364-1372&amp;publication_year=2009&amp;author=Poldrack%2CRA&amp;author=Halchenko%2CYO&amp;author=Hanson%2CSJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32"><p class="c-article-references__text" id="ref-CR32">Shinkareva, S.V., Malave, V.L., Mason, R.A., Mitchell, T.M. &amp; Just, M.A. Commonality of neural representations of words and pictures. <i>Neuroimage</i> <b>54</b>, 24182425 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2010.10.042" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2010.10.042" aria-label="Article reference 32" data-doi="10.1016/j.neuroimage.2010.10.042">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20974270" aria-label="PubMed reference 32">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Commonality%20of%20neural%20representations%20of%20words%20and%20pictures&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2010.10.042&amp;volume=54&amp;pages=2418-2425&amp;publication_year=2011&amp;author=Shinkareva%2CSV&amp;author=Malave%2CVL&amp;author=Mason%2CRA&amp;author=Mitchell%2CTM&amp;author=Just%2CMA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33"><p class="c-article-references__text" id="ref-CR33">Kaplan, J.T. &amp; Meyer, K. Multivariate pattern analysis reveals common neural patterns across individuals during touch observation. <i>Neuroimage</i> <b>60</b>, 204212 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2011.12.059" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2011.12.059" aria-label="Article reference 33" data-doi="10.1016/j.neuroimage.2011.12.059">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22227887" aria-label="PubMed reference 33">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Multivariate%20pattern%20analysis%20reveals%20common%20neural%20patterns%20across%20individuals%20during%20touch%20observation&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2011.12.059&amp;volume=60&amp;pages=204-212&amp;publication_year=2012&amp;author=Kaplan%2CJT&amp;author=Meyer%2CK">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34"><p class="c-article-references__text" id="ref-CR34">Rice, G.E., Watson, D.M., Hartley, T. &amp; Andrews, T.J. Low-level image properties of visual objects predict patterns of neural response across category-selective regions of the ventral visual pathway. <i>J. Neurosci.</i> <b>34</b>, 88378844 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.5265-13.2014" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.5265-13.2014" aria-label="Article reference 34" data-doi="10.1523/JNEUROSCI.5265-13.2014">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXhvV2ltbnM" aria-label="CAS reference 34">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24966383" aria-label="PubMed reference 34">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4069357" aria-label="PubMed Central reference 34">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Low-level%20image%20properties%20of%20visual%20objects%20predict%20patterns%20of%20neural%20response%20across%20category-selective%20regions%20of%20the%20ventral%20visual%20pathway&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.5265-13.2014&amp;volume=34&amp;pages=8837-8844&amp;publication_year=2014&amp;author=Rice%2CGE&amp;author=Watson%2CDM&amp;author=Hartley%2CT&amp;author=Andrews%2CTJ">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35"><p class="c-article-references__text" id="ref-CR35">Charest, I., Kievit, R.A., Schmitz, T.W., Deca, D. &amp; Kriegeskorte, N. Unique semantic space in the brain of each beholder predicts perceived similarity. <i>Proc. Natl. Acad. Sci. USA</i> <b>111</b>, 1456514570 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1402594111" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1402594111" aria-label="Article reference 35" data-doi="10.1073/pnas.1402594111">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXhsFyhtbnF" aria-label="CAS reference 35">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25246586" aria-label="PubMed reference 35">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4209976" aria-label="PubMed Central reference 35">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Unique%20semantic%20space%20in%20the%20brain%20of%20each%20beholder%20predicts%20perceived%20similarity&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1402594111&amp;volume=111&amp;pages=14565-14570&amp;publication_year=2014&amp;author=Charest%2CI&amp;author=Kievit%2CRA&amp;author=Schmitz%2CTW&amp;author=Deca%2CD&amp;author=Kriegeskorte%2CN">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36"><p class="c-article-references__text" id="ref-CR36">Wandell, B.A., Dumoulin, S.O. &amp; Brewer, A.A. Visual field maps in human cortex. <i>Neuron</i> <b>56</b>, 366383 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2007.10.012" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2007.10.012" aria-label="Article reference 36" data-doi="10.1016/j.neuron.2007.10.012">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2sXht12jsrvP" aria-label="CAS reference 36">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17964252" aria-label="PubMed reference 36">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20field%20maps%20in%20human%20cortex&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2007.10.012&amp;volume=56&amp;pages=366-383&amp;publication_year=2007&amp;author=Wandell%2CBA&amp;author=Dumoulin%2CSO&amp;author=Brewer%2CAA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37"><p class="c-article-references__text" id="ref-CR37">Formisano, E. et al. Mirror-symmetric tonotopic maps in human primary auditory cortex. <i>Neuron</i> <b>40</b>, 859869 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0896-6273(03)00669-X" data-track-action="article reference" href="https://doi.org/10.1016%2FS0896-6273%2803%2900669-X" aria-label="Article reference 37" data-doi="10.1016/S0896-6273(03)00669-X">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD3sXpt1ymu7w%3D" aria-label="CAS reference 37">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=14622588" aria-label="PubMed reference 37">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Mirror-symmetric%20tonotopic%20maps%20in%20human%20primary%20auditory%20cortex&amp;journal=Neuron&amp;doi=10.1016%2FS0896-6273%2803%2900669-X&amp;volume=40&amp;pages=859-869&amp;publication_year=2003&amp;author=Formisano%2CE">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38"><p class="c-article-references__text" id="ref-CR38">Benson, N.C. et al. The retinotopic organization of striate cortex is well predicted by surface topology. <i>Curr. Biol.</i> <b>22</b>, 20812085 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.cub.2012.09.014" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cub.2012.09.014" aria-label="Article reference 38" data-doi="10.1016/j.cub.2012.09.014">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhsVKrtbvO" aria-label="CAS reference 38">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23041195" aria-label="PubMed reference 38">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3494819" aria-label="PubMed Central reference 38">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20retinotopic%20organization%20of%20striate%20cortex%20is%20well%20predicted%20by%20surface%20topology&amp;journal=Curr.%20Biol.&amp;doi=10.1016%2Fj.cub.2012.09.014&amp;volume=22&amp;pages=2081-2085&amp;publication_year=2012&amp;author=Benson%2CNC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39"><p class="c-article-references__text" id="ref-CR39">Moser, E.I., Kropff, E. &amp; Moser, M.-B. Place cells, grid cells, and the brain's spatial representation system. <i>Annu. Rev. Neurosci.</i> <b>31</b>, 6989 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1146/annurev.neuro.31.061307.090723" data-track-action="article reference" href="https://doi.org/10.1146%2Fannurev.neuro.31.061307.090723" aria-label="Article reference 39" data-doi="10.1146/annurev.neuro.31.061307.090723">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1cXpt12nsr8%3D" aria-label="CAS reference 39">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18284371" aria-label="PubMed reference 39">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Place%20cells%2C%20grid%20cells%2C%20and%20the%20brain%27s%20spatial%20representation%20system&amp;journal=Annu.%20Rev.%20Neurosci.&amp;doi=10.1146%2Fannurev.neuro.31.061307.090723&amp;volume=31&amp;pages=69-89&amp;publication_year=2008&amp;author=Moser%2CEI&amp;author=Kropff%2CE&amp;author=Moser%2CM-B">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40"><p class="c-article-references__text" id="ref-CR40">O'Keefe, J. &amp; Conway, D.H. Hippocampal place units in the freely moving rat: why they fire where they fire. <i>Exp. Brain Res.</i> <b>31</b>, 573590 (1978).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="noopener" data-track-label="10.1007/BF00239813" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/BF00239813" aria-label="Article reference 40" data-doi="10.1007/BF00239813">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaE1c7ptVyqtw%3D%3D" aria-label="CAS reference 40">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=658182" aria-label="PubMed reference 40">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Hippocampal%20place%20units%20in%20the%20freely%20moving%20rat%3A%20why%20they%20fire%20where%20they%20fire&amp;journal=Exp.%20Brain%20Res.&amp;doi=10.1007%2FBF00239813&amp;volume=31&amp;pages=573-590&amp;publication_year=1978&amp;author=O%27Keefe%2CJ&amp;author=Conway%2CDH">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41"><p class="c-article-references__text" id="ref-CR41">Kriegeskorte, N. et al. Matching categorical object representations in inferior temporal cortex of man and monkey. <i>Neuron</i> <b>60</b>, 11261141 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2008.10.043" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2008.10.043" aria-label="Article reference 41" data-doi="10.1016/j.neuron.2008.10.043">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXlsVKjtA%3D%3D" aria-label="CAS reference 41">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19109916" aria-label="PubMed reference 41">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3143574" aria-label="PubMed Central reference 41">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Matching%20categorical%20object%20representations%20in%20inferior%20temporal%20cortex%20of%20man%20and%20monkey&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2008.10.043&amp;volume=60&amp;pages=1126-1141&amp;publication_year=2008&amp;author=Kriegeskorte%2CN">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42"><p class="c-article-references__text" id="ref-CR42">Hassabis, D. &amp; Maguire, E.A. Deconstructing episodic memory with construction. <i>Trends Cogn. Sci.</i> <b>11</b>, 299306 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2007.05.001" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2007.05.001" aria-label="Article reference 42" data-doi="10.1016/j.tics.2007.05.001">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17548229" aria-label="PubMed reference 42">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Deconstructing%20episodic%20memory%20with%20construction&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2007.05.001&amp;volume=11&amp;pages=299-306&amp;publication_year=2007&amp;author=Hassabis%2CD&amp;author=Maguire%2CEA">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43"><p class="c-article-references__text" id="ref-CR43">Ranganath, C. &amp; Ritchey, M. Two cortical systems for memory-guided behaviour. <i>Nat. Rev. Neurosci.</i> <b>13</b>, 713726 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nrn3338" data-track-action="article reference" href="https://doi.org/10.1038%2Fnrn3338" aria-label="Article reference 43" data-doi="10.1038/nrn3338">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC38XhtlKrs73J" aria-label="CAS reference 43">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22992647" aria-label="PubMed reference 43">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Two%20cortical%20systems%20for%20memory-guided%20behaviour&amp;journal=Nat.%20Rev.%20Neurosci.&amp;doi=10.1038%2Fnrn3338&amp;volume=13&amp;pages=713-726&amp;publication_year=2012&amp;author=Ranganath%2CC&amp;author=Ritchey%2CM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44"><p class="c-article-references__text" id="ref-CR44">Ames, D.L., Honey, C.J., Chow, M.A., Todorov, A. &amp; Hasson, U. Contextual alignment of cognitive and neural dynamics. <i>J. Cogn. Neurosci.</i> <b>27</b>, 655664 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1162/jocn_a_00728" data-track-action="article reference" href="https://doi.org/10.1162%2Fjocn_a_00728" aria-label="Article reference 44" data-doi="10.1162/jocn_a_00728">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25244122" aria-label="PubMed reference 44">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Contextual%20alignment%20of%20cognitive%20and%20neural%20dynamics&amp;journal=J.%20Cogn.%20Neurosci.&amp;doi=10.1162%2Fjocn_a_00728&amp;volume=27&amp;pages=655-664&amp;publication_year=2015&amp;author=Ames%2CDL&amp;author=Honey%2CCJ&amp;author=Chow%2CMA&amp;author=Todorov%2CA&amp;author=Hasson%2CU">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45"><p class="c-article-references__text" id="ref-CR45">Alba, J.W. &amp; Hasher, L. Is memory schematic? <i>Psychol. Bull.</i> <b>93</b>, 203231 (1983).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/0033-2909.93.2.203" data-track-action="article reference" href="https://doi.org/10.1037%2F0033-2909.93.2.203" aria-label="Article reference 45" data-doi="10.1037/0033-2909.93.2.203">Article</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Is%20memory%20schematic%3F&amp;journal=Psychol.%20Bull.&amp;doi=10.1037%2F0033-2909.93.2.203&amp;volume=93&amp;pages=203-231&amp;publication_year=1983&amp;author=Alba%2CJW&amp;author=Hasher%2CL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46"><p class="c-article-references__text" id="ref-CR46">Kurby, C.A. &amp; Zacks, J.M. Segmentation in the perception and memory of events. <i>Trends Cogn. Sci.</i> <b>12</b>, 7279 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2007.11.004" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2007.11.004" aria-label="Article reference 46" data-doi="10.1016/j.tics.2007.11.004">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18178125" aria-label="PubMed reference 46">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2263140" aria-label="PubMed Central reference 46">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Segmentation%20in%20the%20perception%20and%20memory%20of%20events&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2007.11.004&amp;volume=12&amp;pages=72-79&amp;publication_year=2008&amp;author=Kurby%2CCA&amp;author=Zacks%2CJM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47"><p class="c-article-references__text" id="ref-CR47">Baldassano, C. et al. Discovering event structure in continuous narrative perception and memory. Preprint at <i>bioRxiv</i> <a href="https://doi.org/10.1101/081018" data-track="click" data-track-action="external reference" data-track-label="10.1101/081018">http://dx.doi.org/10.1101/081018</a> (2016).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48"><p class="c-article-references__text" id="ref-CR48">Buzski, G. &amp; Moser, E.I. Memory, navigation and theta rhythm in the hippocampal-entorhinal system. <i>Nat. Neurosci.</i> <b>16</b>, 130138 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn.3304" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.3304" aria-label="Article reference 48" data-doi="10.1038/nn.3304">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhsVyisLw%3D" aria-label="CAS reference 48">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23354386" aria-label="PubMed reference 48">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4079500" aria-label="PubMed Central reference 48">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Memory%2C%20navigation%20and%20theta%20rhythm%20in%20the%20hippocampal-entorhinal%20system&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.3304&amp;volume=16&amp;pages=130-138&amp;publication_year=2013&amp;author=Buzs%C3%A1ki%2CG&amp;author=Moser%2CEI">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49"><p class="c-article-references__text" id="ref-CR49">Hasson, U., Ghazanfar, A.A., Galantucci, B., Garrod, S. &amp; Keysers, C. Brain-to-brain coupling: a mechanism for creating and sharing a social world. <i>Trends Cogn. Sci.</i> <b>16</b>, 114121 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tics.2011.12.007" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2011.12.007" aria-label="Article reference 49" data-doi="10.1016/j.tics.2011.12.007">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22221820" aria-label="PubMed reference 49">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3269540" aria-label="PubMed Central reference 49">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Brain-to-brain%20coupling%3A%20a%20mechanism%20for%20creating%20and%20sharing%20a%20social%20world&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2011.12.007&amp;volume=16&amp;pages=114-121&amp;publication_year=2012&amp;author=Hasson%2CU&amp;author=Ghazanfar%2CAA&amp;author=Galantucci%2CB&amp;author=Garrod%2CS&amp;author=Keysers%2CC">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50"><p class="c-article-references__text" id="ref-CR50">Zadbood, A., Chen, J., Leong, Y.C., Norman, K.A. &amp; Hasson, U. How we transmit memories to other brains: constructing shared neural representations via communication. Preprint at <i>bioRxiv</i> <a href="https://doi.org/10.1101/081208" data-track="click" data-track-action="external reference" data-track-label="10.1101/081208">http://dx.doi.org/10.1101/081208</a> (2016).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51"><p class="c-article-references__text" id="ref-CR51">McGuigan, P. A Study in Pink. <i>Sherlock</i> (BBC, 2010).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52"><p class="c-article-references__text" id="ref-CR52">Stephens, G.J., Silbert, L.J. &amp; Hasson, U. Speakerlistener neural coupling underlies successful communication. <i>Proc. Natl. Acad. Sci. USA</i> <b>107</b>, 1442514430 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1008662107" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1008662107" aria-label="Article reference 52" data-doi="10.1073/pnas.1008662107">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20660768" aria-label="PubMed reference 52">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2922522" aria-label="PubMed Central reference 52">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Speaker%E2%80%93listener%20neural%20coupling%20underlies%20successful%20communication&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1008662107&amp;volume=107&amp;pages=14425-14430&amp;publication_year=2010&amp;author=Stephens%2CGJ&amp;author=Silbert%2CLJ&amp;author=Hasson%2CU">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53"><p class="c-article-references__text" id="ref-CR53">Silbert, L.J., Honey, C.J., Simony, E., Poeppel, D. &amp; Hasson, U. Coupled neural systems underlie the production and comprehension of naturalistic narrative speech. <i>Proc. Natl. Acad. Sci. USA</i> <b>111</b>, E4687E4696 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.1323812111" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1323812111" aria-label="Article reference 53" data-doi="10.1073/pnas.1323812111">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXhs1ChtrnI" aria-label="CAS reference 53">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25267658" aria-label="PubMed reference 53">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4217461" aria-label="PubMed Central reference 53">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Coupled%20neural%20systems%20underlie%20the%20production%20and%20comprehension%20of%20naturalistic%20narrative%20speech&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1323812111&amp;volume=111&amp;pages=E4687-E4696&amp;publication_year=2014&amp;author=Silbert%2CLJ&amp;author=Honey%2CCJ&amp;author=Simony%2CE&amp;author=Poeppel%2CD&amp;author=Hasson%2CU">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54"><p class="c-article-references__text" id="ref-CR54">Desikan, R.S. et al. An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest. <i>Neuroimage</i> <b>31</b>, 968980 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2006.01.021" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2006.01.021" aria-label="Article reference 54" data-doi="10.1016/j.neuroimage.2006.01.021">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16530430" aria-label="PubMed reference 54">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20automated%20labeling%20system%20for%20subdividing%20the%20human%20cerebral%20cortex%20on%20MRI%20scans%20into%20gyral%20based%20regions%20of%20interest&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2006.01.021&amp;volume=31&amp;pages=968-980&amp;publication_year=2006&amp;author=Desikan%2CRS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55"><p class="c-article-references__text" id="ref-CR55">Shirer, W.R., Ryali, S., Rykhlevskaia, E., Menon, V. &amp; Greicius, M.D. Decoding subject-driven cognitive states with whole-brain connectivity patterns. <i>Cereb. Cortex</i> <b>22</b>, 158165 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/bhr099" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhr099" aria-label="Article reference 55" data-doi="10.1093/cercor/bhr099">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DC%2BC38%2FlslGhsA%3D%3D" aria-label="CAS reference 55">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21616982" aria-label="PubMed reference 55">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20subject-driven%20cognitive%20states%20with%20whole-brain%20connectivity%20patterns&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhr099&amp;volume=22&amp;pages=158-165&amp;publication_year=2012&amp;author=Shirer%2CWR&amp;author=Ryali%2CS&amp;author=Rykhlevskaia%2CE&amp;author=Menon%2CV&amp;author=Greicius%2CMD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56"><p class="c-article-references__text" id="ref-CR56">Kriegeskorte, N., Goebel, R. &amp; Bandettini, P. Information-based functional brain mapping. <i>Proc. Natl. Acad. Sci. USA</i> <b>103</b>, 38633868 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.0600244103" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.0600244103" aria-label="Article reference 56" data-doi="10.1073/pnas.0600244103">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28XivFWisbw%3D" aria-label="CAS reference 56">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16537458" aria-label="PubMed reference 56">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1383651" aria-label="PubMed Central reference 56">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Information-based%20functional%20brain%20mapping&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.0600244103&amp;volume=103&amp;pages=3863-3868&amp;publication_year=2006&amp;author=Kriegeskorte%2CN&amp;author=Goebel%2CR&amp;author=Bandettini%2CP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57"><p class="c-article-references__text" id="ref-CR57">Chen, P.-H. et al. in <i>Advances in Neural Information Processing Systems 28</i> (eds. Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M. &amp; Garnett, R.) 460468 (Curran Associates, 2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58"><p class="c-article-references__text" id="ref-CR58">Naselaris, T., Kay, K.N., Nishimoto, S. &amp; Gallant, J.L. Encoding and decoding in fMRI. <i>Neuroimage</i> <b>56</b>, 400410 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2010.07.073" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2010.07.073" aria-label="Article reference 58" data-doi="10.1016/j.neuroimage.2010.07.073">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20691790" aria-label="PubMed reference 58">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Encoding%20and%20decoding%20in%20fMRI&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2010.07.073&amp;volume=56&amp;pages=400-410&amp;publication_year=2011&amp;author=Naselaris%2CT&amp;author=Kay%2CKN&amp;author=Nishimoto%2CS&amp;author=Gallant%2CJL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59"><p class="c-article-references__text" id="ref-CR59">Mitchell, T.M. et al. Predicting human brain activity associated with the meanings of nouns. <i>Science</i> <b>320</b>, 11911195 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/science.1152876" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1152876" aria-label="Article reference 59" data-doi="10.1126/science.1152876">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1cXmt1Ois78%3D" aria-label="CAS reference 59">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18511683" aria-label="PubMed reference 59">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Predicting%20human%20brain%20activity%20associated%20with%20the%20meanings%20of%20nouns&amp;journal=Science&amp;doi=10.1126%2Fscience.1152876&amp;volume=320&amp;pages=1191-1195&amp;publication_year=2008&amp;author=Mitchell%2CTM">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60"><p class="c-article-references__text" id="ref-CR60">Wild, F. lsa: latent semantic analysis. R package version 0.73.1 (2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61"><p class="c-article-references__text" id="ref-CR61">Freeman, J., Heeger, D.J. &amp; Merriam, E.P. Coarse-scale biases for spirals and orientation in human visual cortex. <i>J. Neurosci.</i> <b>33</b>, 1969519703 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.0889-13.2013" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.0889-13.2013" aria-label="Article reference 61" data-doi="10.1523/JNEUROSCI.0889-13.2013">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3sXhvFOrtLnI" aria-label="CAS reference 61">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24336733" aria-label="PubMed reference 61">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858637" aria-label="PubMed Central reference 61">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Coarse-scale%20biases%20for%20spirals%20and%20orientation%20in%20human%20visual%20cortex&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.0889-13.2013&amp;volume=33&amp;pages=19695-19703&amp;publication_year=2013&amp;author=Freeman%2CJ&amp;author=Heeger%2CDJ&amp;author=Merriam%2CEP">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62"><p class="c-article-references__text" id="ref-CR62">Kamitani, Y. &amp; Tong, F. Decoding the visual and subjective contents of the human brain. <i>Nat. Neurosci.</i> <b>8</b>, 679685 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nn1444" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn1444" aria-label="Article reference 62" data-doi="10.1038/nn1444">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2MXjsFyktLo%3D" aria-label="CAS reference 62">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15852014" aria-label="PubMed reference 62">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1808230" aria-label="PubMed Central reference 62">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20the%20visual%20and%20subjective%20contents%20of%20the%20human%20brain&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn1444&amp;volume=8&amp;pages=679-685&amp;publication_year=2005&amp;author=Kamitani%2CY&amp;author=Tong%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63"><p class="c-article-references__text" id="ref-CR63">Haxby, J.V. et al. A common, high-dimensional model of the representational space in human ventral temporal cortex. <i>Neuron</i> <b>72</b>, 404416 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2011.08.026" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2011.08.026" aria-label="Article reference 63" data-doi="10.1016/j.neuron.2011.08.026">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXhtlKrtLzJ" aria-label="CAS reference 63">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22017997" aria-label="PubMed reference 63">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3201764" aria-label="PubMed Central reference 63">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 63" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20common%2C%20high-dimensional%20model%20of%20the%20representational%20space%20in%20human%20ventral%20temporal%20cortex&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2011.08.026&amp;volume=72&amp;pages=404-416&amp;publication_year=2011&amp;author=Haxby%2CJV">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64"><p class="c-article-references__text" id="ref-CR64">Wang, L., Mruczek, R.E.B., Arcaro, M.J. &amp; Kastner, S. Probabilistic maps of visual topography in human cortex. <i>Cereb. Cortex</i> <b>25</b>, 39113931 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/bhu277" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhu277" aria-label="Article reference 64" data-doi="10.1093/cercor/bhu277">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC28Xhs1ektbs%3D" aria-label="CAS reference 64">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25452571" aria-label="PubMed reference 64">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=Probabilistic%20maps%20of%20visual%20topography%20in%20human%20cortex&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhu277&amp;volume=25&amp;pages=3911-3931&amp;publication_year=2015&amp;author=Wang%2CL&amp;author=Mruczek%2CREB&amp;author=Arcaro%2CMJ&amp;author=Kastner%2CS">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65"><p class="c-article-references__text" id="ref-CR65">Yarkoni, T., Poldrack, R.A., Nichols, T.E., Van Essen, D.C. &amp; Wager, T.D. Large-scale automated synthesis of human functional neuroimaging data. <i>Nat. Methods</i> <b>8</b>, 665670 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nmeth.1635" data-track-action="article reference" href="https://doi.org/10.1038%2Fnmeth.1635" aria-label="Article reference 65" data-doi="10.1038/nmeth.1635">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3MXotV2gs7w%3D" aria-label="CAS reference 65">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21706013" aria-label="PubMed reference 65">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3146590" aria-label="PubMed Central reference 65">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=Large-scale%20automated%20synthesis%20of%20human%20functional%20neuroimaging%20data&amp;journal=Nat.%20Methods&amp;doi=10.1038%2Fnmeth.1635&amp;volume=8&amp;pages=665-670&amp;publication_year=2011&amp;author=Yarkoni%2CT&amp;author=Poldrack%2CRA&amp;author=Nichols%2CTE&amp;author=Van%20Essen%2CDC&amp;author=Wager%2CTD">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66"><p class="c-article-references__text" id="ref-CR66">Cichy, R.M., Heinzle, J. &amp; Haynes, J.-D. Imagery and perception share cortical representations of content and location. <i>Cereb. Cortex</i> <b>22</b>, 372380 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1093/cercor/bhr106" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhr106" aria-label="Article reference 66" data-doi="10.1093/cercor/bhr106">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21666128" aria-label="PubMed reference 66">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 66" href="http://scholar.google.com/scholar_lookup?&amp;title=Imagery%20and%20perception%20share%20cortical%20representations%20of%20content%20and%20location&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhr106&amp;volume=22&amp;pages=372-380&amp;publication_year=2012&amp;author=Cichy%2CRM&amp;author=Heinzle%2CJ&amp;author=Haynes%2CJ-D">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67"><p class="c-article-references__text" id="ref-CR67">St-Laurent, M., Abdi, H. &amp; Buchsbaum, B.R. Distributed patterns of reactivation predict vividness of recollection. <i>J. Cogn. Neurosci.</i> <b>27</b>, 20002018 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1162/jocn_a_00839" data-track-action="article reference" href="https://doi.org/10.1162%2Fjocn_a_00839" aria-label="Article reference 67" data-doi="10.1162/jocn_a_00839">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26102224" aria-label="PubMed reference 67">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 67" href="http://scholar.google.com/scholar_lookup?&amp;title=Distributed%20patterns%20of%20reactivation%20predict%20vividness%20of%20recollection&amp;journal=J.%20Cogn.%20Neurosci.&amp;doi=10.1162%2Fjocn_a_00839&amp;volume=27&amp;pages=2000-2018&amp;publication_year=2015&amp;author=St-Laurent%2CM&amp;author=Abdi%2CH&amp;author=Buchsbaum%2CBR">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68"><p class="c-article-references__text" id="ref-CR68">Kosslyn, S.M. &amp; Thompson, W.L. When is early visual cortex activated during visual mental imagery? <i>Psychol. Bull.</i> <b>129</b>, 723746 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/0033-2909.129.5.723" data-track-action="article reference" href="https://doi.org/10.1037%2F0033-2909.129.5.723" aria-label="Article reference 68" data-doi="10.1037/0033-2909.129.5.723">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12956541" aria-label="PubMed reference 68">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 68" href="http://scholar.google.com/scholar_lookup?&amp;title=When%20is%20early%20visual%20cortex%20activated%20during%20visual%20mental%20imagery%3F&amp;journal=Psychol.%20Bull.&amp;doi=10.1037%2F0033-2909.129.5.723&amp;volume=129&amp;pages=723-746&amp;publication_year=2003&amp;author=Kosslyn%2CSM&amp;author=Thompson%2CWL">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69"><p class="c-article-references__text" id="ref-CR69">Harrison, S.A. &amp; Tong, F. Decoding reveals the contents of visual working memory in early visual areas. <i>Nature</i> <b>458</b>, 632635 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature07832" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature07832" aria-label="Article reference 69" data-doi="10.1038/nature07832">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD1MXitFKmtb4%3D" aria-label="CAS reference 69">CAS</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19225460" aria-label="PubMed reference 69">PubMed</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2709809" aria-label="PubMed Central reference 69">PubMed Central</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 69" href="http://scholar.google.com/scholar_lookup?&amp;title=Decoding%20reveals%20the%20contents%20of%20visual%20working%20memory%20in%20early%20visual%20areas&amp;journal=Nature&amp;doi=10.1038%2Fnature07832&amp;volume=458&amp;pages=632-635&amp;publication_year=2009&amp;author=Harrison%2CSA&amp;author=Tong%2CF">
                    Google Scholar</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70"><p class="c-article-references__text" id="ref-CR70">Serences, J.T., Ester, E.F., Vogel, E.K. &amp; Awh, E. Stimulus-specific delay activity in human primary visual cortex. <i>Psychol. Sci.</i> <b>20</b>, 207214 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1111/j.1467-9280.2009.02276.x" data-track-action="article reference" href="https://doi.org/10.1111%2Fj.1467-9280.2009.02276.x" aria-label="Article reference 70" data-doi="10.1111/j.1467-9280.2009.02276.x">Article</a>
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19170936" aria-label="PubMed reference 70">PubMed</a>
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Stimulus-specific%20delay%20activity%20in%20human%20primary%20visual%20cortex&amp;journal=Psychol.%20Sci.&amp;doi=10.1111%2Fj.1467-9280.2009.02276.x&amp;volume=20&amp;pages=207-214&amp;publication_year=2009&amp;author=Serences%2CJT&amp;author=Ester%2CEF&amp;author=Vogel%2CEK&amp;author=Awh%2CE">
                    Google Scholar</a>
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/nn.4450?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank M. Aly, C. Baldassano, M. Arcaro and E. Simony for scientific discussions and comments on earlier versions of the manuscript; J. Edgren for help with transcription; M. Arcaro for advice regarding visual area topography; P. Johnson for improving the classification analysis; P.-H. Chen and H. Zhang for development of the SRM code; and other members of the Hasson and Norman laboratories for their comments and support. This work was supported by the US National Institutes of Health (R01-MH094480, U.H.; 2T32MH065214-11, J.C.).</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><span class="c-article-author-information__subtitle u-visually-hidden" id="author-notes">Author notes</span><ol class="c-article-author-information__list"><li class="c-article-author-information__item" id="na1"><p>Janice Chen and Yuan Chang Leong: These authors contributed equally to this work.</p></li></ol><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Princeton Neuroscience Institute, Princeton University, Princeton, New Jersey, USA</p><p class="c-article-author-affiliation__authors-list">Janice Chen,Kenneth A Norman&amp;Uri Hasson</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Psychology, Princeton University, Princeton, New Jersey, USA</p><p class="c-article-author-affiliation__authors-list">Janice Chen,Kenneth A Norman&amp;Uri Hasson</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Psychological and Brain Sciences, Johns Hopkins University, Baltimore, Maryland, USA</p><p class="c-article-author-affiliation__authors-list">Janice Chen&amp;Christopher J Honey</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Department of Psychology, Stanford University, Stanford, California, USA</p><p class="c-article-author-affiliation__authors-list">Yuan Chang Leong</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">Department of Psychology, University of Toronto, Toronto, Ontario, Canada</p><p class="c-article-author-affiliation__authors-list">Christopher J Honey&amp;Chung H Yong</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Janice-Chen-Aff1-Aff2-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Janice Chen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Janice%20Chen" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Janice%20Chen" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Janice%20Chen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Yuan_Chang-Leong-Aff4"><span class="c-article-authors-search__title u-h3 js-search-name">Yuan Chang Leong</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Yuan%20Chang%20Leong" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yuan%20Chang%20Leong" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yuan%20Chang%20Leong%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Christopher_J-Honey-Aff3-Aff5"><span class="c-article-authors-search__title u-h3 js-search-name">Christopher J Honey</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Christopher%20J%20Honey" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Christopher%20J%20Honey" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Christopher%20J%20Honey%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Chung_H-Yong-Aff5"><span class="c-article-authors-search__title u-h3 js-search-name">Chung H Yong</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Chung%20H%20Yong" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chung%20H%20Yong" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chung%20H%20Yong%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Kenneth_A-Norman-Aff1-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Kenneth A Norman</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Kenneth%20A%20Norman" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kenneth%20A%20Norman" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kenneth%20A%20Norman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Uri-Hasson-Aff1-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Uri Hasson</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Uri%20Hasson" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Uri%20Hasson" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"></span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Uri%20Hasson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>J.C., Y.C.L. and U.H. designed the experiment. J.C. and Y.C.L. collected and analyzed the data. J.C., U.H., Y.C.L., K.A.N. and C.J.H. designed analyses and wrote the manuscript. C.H.Y. produced the semantic labels.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:janice@princeton.edu">Janice Chen</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading">Competing interests</h3>
                <p>The authors declare no competing financial interests.</p>
              
            </div></div></section><section data-title="Integrated supplementary information"><div class="c-article-section" id="Sec37-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec37">Integrated supplementary information</h2><div class="c-article-section__content" id="Sec37-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig9"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 1 overlap between default mod" href="/articles/nn.4450/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig9_ESM.jpg">Supplementary Figure 1 Overlap between default mode network (DMN) and movie/recall maps.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>We defined the DMN for each individual using the posterior medial cortex ROI as a seed for functional connectivity during the first scan of the movie (23 minutes), thresholded at R &gt; 0.4; a group-level DMN map was then created by averaging across participants. While the DMN is typically defined using resting state data, it has been previously demonstrated that this network can be mapped either during rest or during continuous narrative with largely the same results. See <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/nn.4450#MOESM42">Table S3</a> for overlap calculations for all searchlight maps (from <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Figs. 2B, 2E</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">3B</a>, and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">7B</a>). Note that this DMN definition procedure is independent from the calculations of the searchlight maps, because functional connectivity is calculated across time (and during movie only), while the searchlight analyses were spatial pattern comparisons (between movie and recall). <b>A)</b> Overlap between the group DMN and the within-participant movie-recall searchlight map from <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2B</a>. 39.7% of this map falls within the DMN. <b>B)</b> Overlap between the group DMN and the between-participant recall-recall map from <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3B</a>. 50.7% of this map falls within the DMN.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig10"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 2 within-participant pattern " href="/articles/nn.4450/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig10_ESM.jpg">Supplementary Figure 2 Within-participant pattern reinstatement at a finer temporal scale.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>While averaging at the scene level was effective for observing neural reinstatement, the behavior of mnemonic recollection that we observed unfolded over time at a finer scale than the scene level. For example, participant 8 used 131 words over 67 seconds to describe scene 13. Here, we further examined reinstatement effects at individual timepoints. <b>A)</b> For each scene for a given participant, we compared the pattern of activity at each timepoint in the movie scene with the pattern from the <i>first</i> timepoint of recall of that scene in the posterior medial cortex ROI. These correlation values were averaged across all scenes and all participants. Correlations with the earliest timepoints of encoding scenes tended to be higher than correlations with later timepoints, suggesting sub-scene level specificity of reinstatement. Error bars represent standard error across subjects. <b>B)</b> For each scene for a given participant, we compared the pattern of activity at each timepoint in the movie scene with the pattern from the <i>last</i> timepoint of recall of that scene in the posterior medial cortex ROI. These correlation values were averaged across all scenes and all participants. Error bars represent standard error across subjects.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig11"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 3 pattern similarity between " href="/articles/nn.4450/figures/11" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig11_ESM.jpg">Supplementary Figure 3 Pattern similarity between participants during the movie.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>A)</b> Schematic for between-participant movie-movie analysis. BOLD data from the movie were divided into scenes, then averaged across time within-scene, resulting in one vector of voxel values for each movie scene and each recalled scene. Correlations were computed between matching pairs of movie scenes <i>between</i> participants. <b>B)</b> Searchlight map showing correlation values for across-participant pattern similarity during the movie. Searchlight was a 5x5x5 voxel cube. <b>C)</b> Correlation values for all 17 participants in independently-defined PMC (posterior medial cortex). Red circles show average correlation of matching scenes and error bars show standard error across scenes; black squares show average of the null distribution for that participant. At far right, the red circle shows the true participant average and error bars show standard error across participants; black histogram shows the null distribution of the participant average; white square shows mean of the null distribution. <b>D)</b> Posterior medial cortex region of interest, cluster in the dorsal default mode network set (<a href="http://findlab.stanford.edu/functional_ROIs.html">http://findlab.stanford.edu/functional_ROIs.html</a>).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig12"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 4 overlap of recallrecall ma" href="/articles/nn.4450/figures/12" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig12_ESM.jpg">Supplementary Figure 4 Overlap of recallrecall map with visual areas.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>To what extent did spoken recollection in this study engage visual imagery? Movie-recall reinstatement effects were not found in low level visual areas, but instead were located in high level visual areas, and extensively in higher order brain regions outside of the visual system. Our observation of reinstatement in high level visual areas is compatible with studies showing reinstatement in these regions during cued visual imagery. The lack of reinstatement effects in low-level areas may be due to the natural tendency of most participants to focus on the episodic narrative (the plot) when recounting the movie, rather than on fine visual details. See also <i>Methods: Visual imagery</i>. <b>A)</b> In gray, brain areas where recollection patterns were significantly similar across participants (map from <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3B</a>). In other colors, commonly studied visual areas. Retinotopic visual areas were taken from a published probabilistic atlas (Wang et al., 2014, <i>Cereb. Cortex</i>). Face-selective areas were generated using Neurosynth (Yarkoni et al., 2011, <i>Nat. Methods</i>). <b>B)</b> For each of the visual area ROIs shown in [A], similarity of scene-level recollection patterns was calculated between participants in the same manner as <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3</a>. Statistical significance was determined by shuffling scene labels to generate a null distribution of the participant average. For each region, red circle shows the true participant average, error bars show standard error across participants; black histogram shows null distribution of the participant average; white square shows mean of the null distribution. In low-order visual regions, recall-recall pattern similarity was not different from chance; however, significant recall-recall pattern similarity was observed in higher-order visual regions (VO/PHC and face-selective areas).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig13"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 5 between-participants patter" href="/articles/nn.4450/figures/13" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig13_ESM.jpg">Supplementary Figure 5 Between-participants pattern similarity in PMC, scene by scene.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>A)</b> Between-participants movie-movie correlation values for 50 individual scenes in the posterior medial cortex (PMC) ROI (same ROI as <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2C, 2F</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">3C</a>). For each scene, each participants movie pattern from that scene was compared to the pattern from the corresponding movie scene averaged across the remaining participants. The bars show the average across participants for each scene. Error bars represent the standard error across participants. Striped bars indicate introductory video clips at the beginning of each functional scan (see Methods). <b>B)</b> Between-participants movie-recall correlation values for individual scenes in the PMC ROI (46 scenes were recalled by two or more participants). <b>C)</b> Between-participants recall-recall correlation values for individual scenes in the PMC ROI.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig14"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 6 encoding model." href="/articles/nn.4450/figures/14" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig14_ESM.jpg">Supplementary Figure 6 Encoding model.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>To explore what semantic information is represented in the shared neural patterns that supports our ability to discriminate patterns of activity between scenes, we constructed an encoding model to predict neural activity patterns from semantic content. See Supp. Note 4 for additional details. <b>A)</b> Detailed semantic labels were generated by an independent coder: 1000 time segments spanning the entire movie stimulus, and 10 labels for each segment. A score was derived for each of the 50 scenes for each label, creating 50-element predictor vectors. It should be noted that the list of 10 labels is by no means comprehensive, and is intended merely to serve as a starting point for future analyses. <b>B)</b> Predicted patterns in PMC were generated by regressing voxel activity on label values, and scene-level classification accuracy was assessed using a hold-2-out procedure validated across 100 combinations of independent groups (N=8 and N=9). Classification accuracy increased as predictors (labels) were added to the model, peaking at 69.5% with five predictors (chance level 50%). <b>C)</b> Predictors were ranked according to how much they improved accuracy for each of the 100 combinations; the most successful predictor was the proportion time during a scene that speech was present (<i>Speaking</i>, ranked first for 80% of combinations), followed by the number of different locations visited during a scene (<i>NumberLocations</i>, ranked 2<sup>nd</sup> for 48%), arousal (Arousal, ranked 3<sup>rd</sup> for 31%), proportion time that written words were present (<i>WrittenWords</i>, ranked 4<sup>th</sup> for 51%), and valence (<i>Valence</i>, ranked 5<sup>th</sup> for 31%). The number of persons in a scene (<i>NumberPersons</i>) was ranked first for 10% of combinations. <b>D)</b> Confusion matrix for the 10 predictors. Note that when two predictors are correlated, one may dominate in the predictor rankings.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig15"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 7 simulation of movie-to-reca" href="/articles/nn.4450/figures/15" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig15_ESM.jpg">Supplementary Figure 7 Simulation of movie-to-recall pattern alteration.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>In this simulation, five 125-voxel random patterns are created (five simulated subjects) and random noise is added to each one, such that the average inter-subject correlation is R=1.0 (red lines) or R=0.3 (blue lines). These are the movie patterns. Next, we simulate the change from movie pattern to recall pattern by 1) adding random noise (at different levels of intensity, y-axis) to every voxel in every subject to create the recall patterns, which are noisy versions of the movie pattern; and 2) adding a common pattern to each movie pattern to mimic the systematic alteration from movie pattern to recall pattern, plus random noise (at different levels of intensity, x-axis). We plot the average correlation among the five simulated subjects recall patterns (Rec-Rec), as well as the average correlation between movie and recall patterns (Mov-Rec). <b>A)</b> Results when no common pattern is added, i.e., the recall pattern is merely the movie pattern plus noise (no systematic alteration takes place): Even as noise varies at the movie pattern stage and at the movie-to-recall change stage, similarity among recall patterns (<i>Rec-Rec</i>, solid lines) never exceeds the similarity of recall to movie (<i>Mov-Rec</i>, dotted lines). <b>B)</b> Results when a common pattern is added to each subjects movie pattern, in addition to the same levels of random noise, to generate the recall pattern. Now, it becomes possible (even likely, under these simulated conditions) for the similarity among recall patterns (<i>Rec-Rec</i>, solid lines) to exceed the similarity of recall to movie (<i>Mov-Rec</i>, dotted lines). In short, when the change from movie to recall involves a systematic alteration across subjects, recall patterns may become more similar to each other then they are to the original movie pattern. Note that the similarity of the movie pattern to each other (movie-movie correlation) does not impact the results. See <i>Methods: Simulation of movie-to-recall pattern alteration</i>.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig16"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 8 scene-by-scene difference o" href="/articles/nn.4450/figures/16" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig16_ESM.jpg">Supplementary Figure 8 Scene-by-scene difference of recallrecall minus movierecall in regions shown in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Figure 7b</a>.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p><b>A)</b> In <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig7">Fig. 7B</a> we plotted brain regions in which participants recollection activity patterns were more similar to the recollection patterns in other individuals than they were to movie patterns (neural alteration effect). Here we show the results broken down scene-by-scene in the same regions. Error bars represent the standard error across participants. <b>B)</b> Recall-recall minus movie-recall difference values thresholded at 0.01.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig17"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 9 subsequent memory analyses." href="/articles/nn.4450/figures/17" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig17_ESM.jpg">Supplementary Figure 9 Subsequent memory analyses.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>To examine how the systematic alteration of neural activity from movie to recall might be related to memorability, we divided scenes into remembered and forgotten for each participant. For each scene, the number of participants who had successfully recalled that scene was counted. We then extracted data from the PMC ROI and calculated the pairwise between-participants correlation during recall (same analysis as in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig3">Fig. 3A-C</a>, except pairwise), the pairwise between-participants correlation between movie and recall (same analysis as in <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig2">Fig. 2D-F</a>, except pairwise), and used the difference as the degree of neural alteration (recall-recall similarity minus movie-recall similarity), at the scene level. Pairwise comparisons were used because the mean value of pairwise correlations is not affected by the number of participants (and the number of participants was different across data points in this analysis). <b>A)</b> We calculated Spearmans rank correlation for the number of participants who successfully recalled each scene vs. the average degree of neural alteration for each scene. The magnitude of neural alteration was significantly related to how many participants remembered that scene (R = 0.33, p = 0.03). In other words, the more that a given movie scene pattern was altered in systematic manner across subjects between perception and recall, the more likely that scene was to be remembered. <b>B)</b> A control analysis in PMC showing that between-participants movie-movie pattern similarity was not predictive of the likelihood of recall (R = -0.01, p &gt; 0.9). <b>C)</b> A control analysis showing that the degree of neural alteration (i.e., recall-recall minus movie-recall) in early visual areas V1-V4 was not predictive of the likelihood of recall (R = 0.12, p = 0.43, same ROI as <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/nn.4450#Fig12">Fig. S4</a>).</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig18"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 10 hippocampal inter-subject " href="/articles/nn.4450/figures/18" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig18_ESM.jpg">Supplementary Figure 10 Hippocampal inter-subject correlation (ISC).</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>We examined hippocampal contributions to recall success. During movie viewing, we calculated the correlation between a given participants hippocampal timecourse (using an anatomically-defined whole hippocampus ROI) and the average hippocampal timecourse of all other participants, for individual scenes (i.e., the inter-subject correlation (ISC) for each scene). For each participant, scenes were binned by whether they were later remembered or forgotten. ISC was significantly greater for remembered scenes than forgotten scenes (left panel; 2-tailed paired t-test across participants, t = 2.17, p = 0.045), complementing previous results linking ISC in parahippocampal cortex to later recognition memory (Hasson et al., 2008, <i>Neuron</i>). The same analysis is shown for the hippocampus ROI split into anterior, middle, and posterior sections (second, third, and fourth panels from the left). A repeated-measures ANOVA with region (anterior, middle, posterior) and memory (remembered, forgotten) as factors revealed significant main effects of region F(2,32) = 12.02, p &lt; 0.0005 and of memory F(1,16) = 4.98, p = 0.04, but not a significant region x memory interaction F(2,32) = 1.69, p = 0.2.</p></div></div><div class="c-article-supplementary__item js-c-reading-companion-figures-item" data-test="supp-item" id="Fig19"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary figure 11 no evidence of hippocampal" href="/articles/nn.4450/figures/19" data-supp-info-image="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_Article_BFnn4450_Fig19_ESM.jpg">Supplementary Figure 11 No evidence of hippocampal sensitivity to the gap between part 1 and part 2 of the movie.</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Evidence from time cells in the rodent hippocampus might predict that the hippocampus would be sensitive to the gap between the first segment and the second. In order to explore this question, we examined recall patterns in the hippocampus for the 3 scenes just before and 3 scenes just after the gap, specifically asking where the correlations of these patterns with their corresponding movie scenes fell in the distribution of all such movie-recall scene correlations. The left panel shows the distribution of movie-vs-recall pattern correlations for all 50 scenes (averaged across subjects), and the right panel shows the distribution of movie-vs-recall pattern correlations for the 3 scenes just before and 3 scenes just after the gap. There does not appear to be anything unusual about the scenes near the gap, in terms of their pattern similarity to the corresponding movie scenes (the near-gap values fall near the middle of the distribution). Thus, in this analysis, we did not find any evidence to support the hypothesis that the hippocampus is sensitive to the gap between part 1 and part 2 of the movie during recall.</p></div></div></div></div></div></section><section data-title="Supplementary information"><div class="c-article-section" id="Sec38-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec38">Supplementary information</h2><div class="c-article-section__content" id="Sec38-content"><div data-test="supplementary-info"><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM42"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary text and figures" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_BFnn4450_MOESM42_ESM.pdf" data-supp-info-image="">Supplementary Text and Figures</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>Supplementary Figures 111 and Supplementary Tables 13 (PDF 1888 kb)</p></div></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM43"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary methods checklist" href="https://static-content.springer.com/esm/art%3A10.1038%2Fnn.4450/MediaObjects/41593_2017_BFnn4450_MOESM43_ESM.pdf" data-supp-info-image="">Supplementary Methods Checklist</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p> (PDF 503 kb)</p></div></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Shared%20memories%20reveal%20shared%20structure%20in%20neural%20activity%20across%20individuals&amp;author=Janice%20Chen%20et%20al&amp;contentID=10.1038%2Fnn.4450&amp;copyright=Springer%20Nature%20America%2C%20Inc.&amp;publication=1097-6256&amp;publicationDate=2016-12-05&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/nn.4450" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/nn.4450" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Chen, J., Leong, Y., Honey, C. <i>et al.</i> Shared memories reveal shared structure in neural activity across individuals.
                    <i>Nat Neurosci</i> <b>20</b>, 115125 (2017). https://doi.org/10.1038/nn.4450</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/nn.4450?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-07-21">21 July 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-10-28">28 October 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-12-05">05 December 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-01">January 2017</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/nn.4450</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>

            
        <section>
            <div class="c-article-section js-article-section" id="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Revealing trajectories of the mind via non-linear manifolds of brain activity" href="https://doi.org/10.1038/s43588-023-00423-4">
                                        Revealing trajectories of the mind via non-linear manifolds of brain activity
                                    </a>
                                </h3>
                            
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Computational Science</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:The default network dominates neural responses to evolving movie stories" href="https://doi.org/10.1038/s41467-023-39862-y">
                                        The default network dominates neural responses to evolving movie stories
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Enning Yang</li><li>Filip Milisav</li><li>Danilo Bzdok</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Communications</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:The human brain reactivates context-specific past information at event boundaries of naturalistic experiences" href="https://doi.org/10.1038/s41593-023-01331-6">
                                        The human brain reactivates context-specific past information at event boundaries of naturalistic experiences
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Avital Hahamy</li><li>Haim Dubossarsky</li><li>Timothy E. J. Behrens</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Neuroscience</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Synchronized affect in shared experiences strengthens social connection" href="https://doi.org/10.1038/s42003-023-05461-2">
                                        Synchronized affect in shared experiences strengthens social connection
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Jin Hyun Cheong</li><li>Zainab Molani</li><li>Luke J. Chang</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Communications Biology</i> (2023)</p>
                            </li>
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Cross-stage neural pattern similarity in the hippocampus predicts false memory derived from post-event inaccurate information" href="https://doi.org/10.1038/s41467-023-38046-y">
                                        Cross-stage neural pattern similarity in the hippocampus predicts false memory derived from post-event inaccurate information
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="c-author-list c-author-list--compact c-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Xuhao Shao</li><li>Ao Li</li><li>Bi Zhu</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Nature Communications</i> (2023)</p>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </section>
    

            
        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="js-context-bar-sticky-point-desktop">
        

        
            <noscript>
                
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        
        <p class="c-nature-box__text js-text">You have full access to this article via your institution.</p>
        
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.4450.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </noscript>
            <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
                <div class="c-nature-box c-nature-box--side u-display-none u-hide-print" aria-hidden="true" data-component="entitlement-box"
    id=entitlement-box-right-column
    
    >

    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
        
            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/nn.4450.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

        
    
</div>

            </div>
        
    </div>

    
        
    

    
    
        <div class="c-article-associated-content__container">
            <section>
                <h2 class="c-article-associated-content__title u-mb-24">Associated content</h2>
                
                    
                    
                        <div class="u-full-height u-mb-24">
                            
    <article class="u-full-height c-card c-card--flush">
        <div class="c-card__layout u-full-height">
            <div class="c-card__body">
                <h3 class="c-card__title">
                    <a href="https://www.nature.com/articles/nn.4466"
                       class="c-card__link u-link-inherit"
                       data-track="click"
                       data-track-action="view article"
                       data-track-category="associated content"
                       
                       data-track-label="news_and_views">Cracking the mnemonic code</a>
                </h3>
                
<ul data-test="author-list" class="c-author-list c-author-list--compact">
    <li>Eva Zita Patai</li><li>Hugo J Spiers</li>
</ul>

                
    <div class="c-card__section c-meta">
        
            <span class="c-meta__item">Nature Neuroscience</span>
        
        <span class="c-meta__item" data-test="article.type"><span class="c-meta__type">News &amp; Views</span></span>
        
        
            <time class="c-meta__item" datetime="2016-12-27">27 Dec 2016</time>
        
    </div>

            </div>
        </div>
    </article>


                        </div>
                    
                
            </section>
        </div>
        <script>
            window.dataLayer = window.dataLayer || [];
            window.dataLayer[0] = window.dataLayer[0] || {};
            window.dataLayer[0].content = window.dataLayer[0].content || {};
            window.dataLayer[0].content.associatedContentTypes = "news_and_views";
        </script>
    

    

    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>
                            
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/neurosci.nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=nn.4450;doi=10.1038/nn.4450;techmeta=36,59;subjmeta=1595,1723,2167,2618,2645,2649,378,477,631;kwrd=Cortex,Long-term+memory,Perception,Psychology,Social+neuroscience">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=-1713137110&amp;t=pos%3Dright%26type%3Darticle%26artid%3Dnn.4450%26doi%3D10.1038/nn.4450%26techmeta%3D36,59%26subjmeta%3D1595,1723,2167,2618,2645,2649,378,477,631%26kwrd%3DCortex,Long-term+memory,Perception,Psychology,Social+neuroscience">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/neurosci.nature.com/article&amp;sz=300x250&amp;c=-1713137110&amp;t=pos%3Dright%26type%3Darticle%26artid%3Dnn.4450%26doi%3D10.1038/nn.4450%26techmeta%3D36,59%26subjmeta%3D1595,1723,2167,2618,2645,2649,378,477,631%26kwrd%3DCortex,Long-term+memory,Perception,Psychology,Social+neuroscience"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>


    
        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-and-analysis"
                                   data-track="click"
                                   data-track-action="reviews &amp; analysis"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Reviews &amp; Analysis
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/video"
                                   data-track="click"
                                   data-track-action="videos"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Videos
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/current-issue"
                                   data-track="click"
                                   data-track-action="current issue"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Current issue
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>
                        
                    
                </ul>
                <ul class="c-header__list c-header__list--js-stack">
                    
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/natureneuro"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on Twitter
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;6"
                               rel="nofollow"
                               data-track="click"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/neuro.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </nav>
    
    
        
            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/aims"
                                   data-track="click"
                                   data-track-action="aims &amp; scope"
                                   data-track-label="link">
                                    Aims &amp; Scope
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-information"
                                   data-track="click"
                                   data-track-action="journal information"
                                   data-track-label="link">
                                    Journal Information
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/journal-impact"
                                   data-track="click"
                                   data-track-action="journal metrics"
                                   data-track-label="link">
                                    Journal Metrics
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editors"
                                   data-track="click"
                                   data-track-action="about the editors"
                                   data-track-label="link">
                                    About the Editors
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/our-publishing-models"
                                   data-track="click"
                                   data-track-action="our publishing models"
                                   data-track-label="link">
                                    Our publishing models
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-values-statement"
                                   data-track="click"
                                   data-track-action="editorial values statement"
                                   data-track-label="link">
                                    Editorial Values Statement
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/editorial-policies"
                                   data-track="click"
                                   data-track-action="editorial policies"
                                   data-track-label="link">
                                    Editorial Policies
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/content"
                                   data-track="click"
                                   data-track-action="content types"
                                   data-track-label="link">
                                    Content Types
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/web-feeds"
                                   data-track="click"
                                   data-track-action="web feeds"
                                   data-track-label="link">
                                    Web Feeds
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/posters"
                                   data-track="click"
                                   data-track-action="posters"
                                   data-track-label="link">
                                    Posters
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/research-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="research cross-journal editorial team"
                                   data-track-label="link">
                                    Research Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/reviews-cross-journal-editorial-team"
                                   data-track="click"
                                   data-track-action="reviews cross-journal editorial team"
                                   data-track-label="link">
                                    Reviews Cross-Journal Editorial Team
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        

        
            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/submission-guidelines"
                                   data-track="click"
                                   data-track-action="submission guidelines"
                                   data-track-label="link">
                                    Submission Guidelines
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/neuro/for-reviewers"
                                   data-track="click"
                                   data-track-action="for reviewers"
                                   data-track-label="link">
                                    For Reviewers
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="click"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://mts-nn.nature.com/cgi-bin/main.plex?form_type&#x3D;home&amp;from_idp&#x3D;1"
                                   data-track="click"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external>Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        
    


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">
                            
                                
                                    <option value="" selected>All journals</option>
                                    <option value="neuro">This journal</option>
                                
                            
                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        

        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
            

            <p class="c-meta u-ma-0 u-flex-shrink">
                <span class="c-meta__item">
                    Nature Neuroscience (<i>Nat Neurosci</i>)
                </span>
                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="onlineIssn">1546-1726</span> (online)
    </span>
    


                
    
    <span class="c-meta__item">
        <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="printIssn">1097-6256</span> (print)
    </span>
    

            </p>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/protocolexchange/"
                                                  data-track="click" data-track-action="protocol exchange"
                                                  data-track-label="link">Protocol Exchange</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natitaly"
                                                  data-track="click" data-track-action="nature Italy"
                                                  data-track-label="link">Nature Italy</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ko-kr"
                                                  data-track="click" data-track-action="nature korea"
                                                  data-track-label="link">Nature Korea</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>
            
        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-ea63208b81.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2024 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path id="shape" fill-rule="evenodd" clip-rule="evenodd" d="M1 3.78571C1 2.75867 1.85698 2 2.8209 2H6.1791C7.14302 2 8 2.75867 8 3.78571V4H11.1668C11.885 4 12.5585 4.42017 12.8494 5.07033C12.9893 4.98169 13.1425 4.91101 13.3056 4.86206L16.5222 3.89704C17.4454 3.62005 18.4843 4.10046 18.7794 5.08419L22.9256 18.9042C23.2207 19.8878 22.618 20.8608 21.6947 21.1378L18.4781 22.1029C17.5548 22.3799 16.516 21.8993 16.2209 20.9157L13.0001 10.1804V20.2143C13.0001 21.255 12.1231 22 11.1668 22H7.83346C7.54206 22 7.25803 21.9308 7.00392 21.8052C6.75263 21.9305 6.47077 22 6.1791 22H2.8209C1.85693 22 1 21.2412 1 20.2143V3.78571ZM3 4V15H6V4H3ZM3 20V17H6V20H3ZM18.0749 20.1358L17.2129 17.2623L20.0863 16.4002L20.9484 19.2737L18.0749 20.1358ZM19.5116 14.4846L16.6381 15.3466L14.0519 6.72624L16.9254 5.86416L19.5116 14.4846ZM8.00012 20L8.00012 6H11.0001L11.0001 20H8.00012Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</footer>




    

    

<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">

    
    <div class="c-site-messages__banner-large">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing" src="/static/images/logos/nature-briefing-logo-n150-white-d81c9da3ec.svg" width="250" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing</em> newsletter  what matters in science, free to your inbox daily.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/briefing" method="post" data-location="banner" data-track="submit||nature_briefing_sign_up" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="MainBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick">
                        <input type="hidden" value="false" name="marketing" id="marketing">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick">
                        <input type="hidden" value="MainBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">
                            
                            <input type="hidden" value="true" name="N:nature_briefing_daily" id="defaultNewsletter">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>

    
    <div class="c-site-messages__banner-small">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner">Sign up for Nature Briefing
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/nn.4450&amp;format=js&amp;last_modified=2017-01-01" async></script>
<img src="/95c5bp69/article/nn.4450" width="1" height="1" alt="" class="u-visually-hidden">
</body>
</html>